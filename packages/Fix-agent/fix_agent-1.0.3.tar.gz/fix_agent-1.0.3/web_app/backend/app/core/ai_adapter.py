"""AI model adapter that wraps the CLI's AI logic for web use."""

import os
import sys
from pathlib import Path
from typing import Any, AsyncGenerator, Dict, List, Optional

# å¯¼å…¥åº”ç”¨é…ç½®
from .config import settings

# æ·»åŠ CLIé¡¹ç›®è·¯å¾„åˆ°Pythonè·¯å¾„
# ä» backend/app/core/ai_adapter.py åˆ° Fix Agent/src çš„è·¯å¾„æ˜¯å‘ä¸Š5çº§
cli_root = Path(__file__).parent.parent.parent.parent.parent / "src"
if cli_root.exists():
    sys.path.insert(0, str(cli_root))
    # åŒæ—¶æ·»åŠ Fix Agentæ ¹ç›®å½•åˆ°è·¯å¾„ï¼Œä»¥ä¾¿ç›¸å¯¹å¯¼å…¥æ­£å¸¸å·¥ä½œ
    fix_agent_root = cli_root.parent
    sys.path.insert(0, str(fix_agent_root))
    print(f"âœ… CLI root added to path: {cli_root}")
    print(f"âœ… Fix Agent root added to path: {fix_agent_root}")
else:
    print(f"âŒ CLI root not found at: {cli_root}")
    # å°è¯•ä¸åŒçš„è·¯å¾„
    alt_paths = [
        Path(__file__).parent.parent.parent.parent / "src",  # å‘ä¸Š4çº§
        Path(__file__).parent.parent.parent / "src",  # å‘ä¸Š3çº§
    ]
    for alt_path in alt_paths:
        if alt_path.exists():
            sys.path.insert(0, str(alt_path))
            print(f"âœ… CLI root found at alternative path: {alt_path}")
            break


# å»¶è¿Ÿå¯¼å…¥CLIæ¨¡å—
def _import_cli_modules():
    """Import CLI modules safely."""
    try:
        from deepagents.backends.composite import CompositeBackend
        from deepagents.backends.filesystem import FilesystemBackend
        from deepagents.middleware.resumable_shell import \
            ResumableShellToolMiddleware
        from langchain.agents.middleware import HostExecutionPolicy
        from langgraph.checkpoint.memory import InMemorySaver

        from src.agents.agent import create_agent_with_config
        from src.config.config import create_model
        from src.midware.agent_memory import AgentMemoryMiddleware
        from src.midware.performance_monitor import \
            PerformanceMonitorMiddleware
        from src.tools.tools import get_all_tools

        return {
            "create_agent_with_config": create_agent_with_config,
            "create_model": create_model,
            "get_all_tools": get_all_tools,
            "AgentMemoryMiddleware": AgentMemoryMiddleware,
            "PerformanceMonitorMiddleware": PerformanceMonitorMiddleware,
            "FilesystemBackend": FilesystemBackend,
            "CompositeBackend": CompositeBackend,
            "ResumableShellToolMiddleware": ResumableShellToolMiddleware,
            "HostExecutionPolicy": HostExecutionPolicy,
            "InMemorySaver": InMemorySaver,
        }
    except ImportError as e:
        print(f"Warning: CLI modules not available: {e}")
        return None


cli_modules = _import_cli_modules()


class AIAdapter:
    """Adapter class to bridge CLI AI logic with web interface."""

    def __init__(self, session_id: str, workspace_path: str):
        """Initialize AI adapter for a specific session.

        Args:
            session_id: Unique identifier for the web session
            workspace_path: Path to the user's workspace directory
        """
        self.session_id = session_id
        self.workspace_path = Path(workspace_path)
        self.agent = None
        self.checkpointer = None
        self.cli_available = cli_modules is not None

        # åˆ›å»ºä¼šè¯ä¸“ç”¨ç›®å½•
        workspace_root = Path(settings.workspace_root)
        self.session_dir = workspace_root / "sessions" / session_id
        self.session_dir.mkdir(parents=True, exist_ok=True)

        # è®°å¿†ç›®å½•
        self.memory_dir = workspace_root / "memories" / session_id
        self.memory_dir.mkdir(parents=True, exist_ok=True)

        # æµå¼å¤„ç†çŠ¶æ€
        self.pending_text = ""  # ç´¯ç§¯çš„æ–‡æœ¬ç¼“å†²
        self.tool_call_buffers = {}  # å·¥å…·è°ƒç”¨ç¼“å†²åŒº
        self.last_chunk_time = 0  # æœ€åå‘é€chunkçš„æ—¶é—´
        self.chunk_timeout = 2.0  # chunkè¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰- å¢åŠ åˆ°2ç§’ä»¥å‡å°‘åˆ†å‰²
        self.is_thinking = False  # AIæ€è€ƒçŠ¶æ€
        self.sent_thinking = False  # æ˜¯å¦å·²å‘é€æ€è€ƒçŠ¶æ€
        self.has_sent_thinking_for_current_request = False  # å½“å‰è¯·æ±‚æ˜¯å¦å·²å‘é€æ€è€ƒçŠ¶æ€

        # åªæœ‰åœ¨CLIæ¨¡å—å¯ç”¨æ—¶æ‰åˆå§‹åŒ–AIä»£ç†
        if self.cli_available:
            self._initialize_agent()

    def _initialize_agent(self):
        """Initialize the AI agent with CLI configuration."""
        if not self.cli_available:
            return

        try:
            # åˆ›å»ºæ¨¡å‹ï¼ˆå¤ç”¨CLIé€»è¾‘ï¼‰
            model = self._create_model_from_env()

            # è·å–å·¥å…·ï¼ˆå¤ç”¨CLIå·¥å…·ï¼‰
            tools = self._get_available_tools()

            # åˆ›å»ºä¸­é—´ä»¶
            agent_middleware = self._create_middleware()

            # åˆ›å»ºä»£ç†
            self.agent = cli_modules["create_agent_with_config"](
                model=model, assistant_id=self.session_id, tools=tools
            )

            # è®¾ç½®å†…å­˜æ£€æŸ¥ç‚¹
            self.checkpointer = cli_modules["InMemorySaver"]()
            self.agent.checkpointer = self.checkpointer

            print(f"âœ… AI Agent initialized for session {self.session_id}")

        except Exception as e:
            print(f"Failed to initialize AI agent: {e}")
            self.cli_available = False

    def _create_model_from_env(self):
        """Create AI model from environment variables (CLI logic)."""
        if not self.cli_available:
            return None

        try:
            # å¯¼å…¥ç¯å¢ƒå˜é‡
            from dotenv import load_dotenv

            load_dotenv()

            # å¤ç”¨CLIçš„æ¨¡å‹åˆ›å»ºé€»è¾‘
            return cli_modules["create_model"]()
        except Exception as e:
            print(f"Failed to create model: {e}")
            return None

    def _get_available_tools(self) -> List[Any]:
        """Get list of available tools (CLI logic)."""
        if not self.cli_available:
            return []

        try:
            return list(cli_modules["get_all_tools"]().values())
        except Exception as e:
            print(f"Failed to get tools: {e}")
            return []

    def _create_middleware(self) -> List:
        """Create middleware for the agent."""
        if not self.cli_available:
            return []

        try:
            # Shellä¸­é—´ä»¶ï¼ˆé™åˆ¶åœ¨ç”¨æˆ·å·¥ä½œç©ºé—´ï¼‰
            shell_middleware = cli_modules["ResumableShellToolMiddleware"](
                workspace_root=str(self.workspace_path),
                execution_policy=cli_modules["HostExecutionPolicy"](),
            )

            # è®°å¿†åç«¯
            long_term_backend = cli_modules["FilesystemBackend"](
                root_dir=self.memory_dir, virtual_mode=True
            )

            # å¤åˆåç«¯
            backend = cli_modules["CompositeBackend"](
                default=cli_modules["FilesystemBackend"](),
                routes={"/memories/": long_term_backend},
            )

            # è®°å¿†ä¸­é—´ä»¶
            memory_middleware = cli_modules["AgentMemoryMiddleware"](
                backend=long_term_backend, memory_path="/memories/"
            )

            # æ€§èƒ½ç›‘æ§ä¸­é—´ä»¶ï¼ˆå¯é€‰ï¼‰
            performance_middleware = None
            try:
                performance_middleware = cli_modules["PerformanceMonitorMiddleware"](
                    backend=long_term_backend,
                    metrics_path="/performance/",
                    enable_system_monitoring=True,
                    max_records=1000,
                )
            except Exception as e:
                print(f"Warning: Performance monitoring middleware disabled: {e}")

            # æ„å»ºä¸­é—´ä»¶åˆ—è¡¨
            middleware_list = [memory_middleware, shell_middleware]
            if performance_middleware:
                middleware_list.insert(0, performance_middleware)  # æ€§èƒ½ç›‘æ§æ”¾åœ¨æœ€å¤–å±‚

            return middleware_list
        except Exception as e:
            print(f"Failed to create middleware: {e}")
            return []

    async def stream_response(
        self, message: str, file_references: List[str] = None
    ) -> AsyncGenerator[Dict[str, Any], None]:
        """Stream AI response for web interface.

        Args:
            message: User message
            file_references: List of file paths to include in context

        Yields:
            Dict containing streaming response chunks
        """
        # é‡ç½®æ€è€ƒçŠ¶æ€
        self.sent_thinking = False
        self.has_sent_thinking_for_current_request = False

        # å¦‚æœCLIä¸å¯ç”¨ï¼Œè¿”å›æ¨¡æ‹Ÿå“åº”
        if not self.cli_available or not self.agent:
            yield {
                "type": "message",
                "content": f"æˆ‘æ”¶åˆ°äº†ä½ çš„æ¶ˆæ¯: '{message}'ã€‚è¿™æ˜¯ä¸€ä¸ªæ¨¡æ‹Ÿçš„AIå“åº”ã€‚å®Œæ•´ç‰ˆæœ¬å°†é›†æˆCLIçš„AIä»£ç†åŠŸèƒ½ã€‚",
                "session_id": self.session_id,
            }
            return

        # ç«‹å³å‘é€æ€è€ƒçŠ¶æ€
        yield {
            "type": "status",
            "content": "AIæ­£åœ¨æ€è€ƒ...",
            "session_id": self.session_id,
            "metadata": {"state": "thinking"},
        }

        # æ„å»ºå®Œæ•´è¾“å…¥ï¼ˆåŒ…å«æ–‡ä»¶å¼•ç”¨ï¼‰
        full_input = self._build_input_with_files(message, file_references or [])

        # é…ç½®
        config = {
            "configurable": {"thread_id": self.session_id},
            "metadata": {"session_id": self.session_id, "source": "web"},
        }

        try:
            # æµå¼å“åº”
            for chunk in self.agent.stream(
                {"messages": [{"role": "user", "content": full_input}]},
                stream_mode=["messages", "updates"],
                subgraphs=True,
                config=config,
                durability="exit",
            ):
                # å¤„ç†æµå¼æ•°æ®å—
                processed_chunk = self._process_stream_chunk(chunk)
                if processed_chunk:
                    yield processed_chunk

            # æµç»“æŸæ—¶ï¼Œå¼ºåˆ¶åˆ·æ–°æ‰€æœ‰å‰©ä½™çš„æ–‡æœ¬
            final_chunks = self.flush_pending_text(final=True)
            for final_chunk in final_chunks:
                yield final_chunk

        except Exception as e:
            print(f"Error in AI streaming: {e}")
            # å³ä½¿å‡ºé”™ä¹Ÿè¦å°è¯•åˆ·æ–°ç¼“å†²çš„æ–‡æœ¬
            error_chunks = self.flush_pending_text(final=True)
            for chunk in error_chunks:
                yield chunk

            yield {
                "type": "error",
                "content": f"AIå“åº”é”™è¯¯: {str(e)}",
                "session_id": self.session_id,
            }

    def _build_input_with_files(self, message: str, file_paths: List[str]) -> str:
        """Build input message with file contents included."""
        if not file_paths:
            return message

        context_parts = [message, "\n\n## Referenced Files\n"]

        for file_path in file_paths:
            try:
                full_path = self.workspace_path / file_path
                if full_path.exists():
                    content = full_path.read_text(encoding="utf-8")
                    # é™åˆ¶æ–‡ä»¶å¤§å°
                    if len(content) > 50000:
                        content = content[:50000] + "\n... (file truncated)"

                    context_parts.append(
                        f"\n### {full_path.name}\n"
                        f"Path: `{file_path}`\n"
                        f"```\n{content}\n```"
                    )
                else:
                    context_parts.append(
                        f"\n### {Path(file_path).name}\n"
                        f"[Error: File not found - {file_path}]"
                    )
            except Exception as e:
                context_parts.append(
                    f"\n### {Path(file_path).name}\n" f"[Error reading file: {e}]"
                )

        return "\n".join(context_parts)

    def _process_stream_chunk(self, chunk) -> Optional[Dict]:
        """Process streaming chunk from AI agent with CLI-style buffering."""
        import time

        if not isinstance(chunk, tuple) or len(chunk) != 3:
            return None

        namespace, stream_mode, data = chunk
        current_time = time.time()
        results = []

        if stream_mode == "messages":
            if isinstance(data, tuple) and len(data) == 2:
                message, metadata = data

                # å¤„ç†AIæ¶ˆæ¯
                if hasattr(message, "content_blocks"):
                    # å…ˆå¤„ç†å·¥å…·è°ƒç”¨å—ï¼ˆä¼˜å…ˆçº§æœ€é«˜ï¼‰
                    tool_calls_found = False
                    for block in message.content_blocks:
                        if block.get("type") == "tool_call_chunk":
                            tool_calls_found = True
                            tool_name = block.get("name")
                            tool_args = block.get("args", {})
                            tool_call_id = block.get("id", "default")

                            # ç¼“å†²å·¥å…·è°ƒç”¨æ•°æ®
                            if tool_call_id not in self.tool_call_buffers:
                                self.tool_call_buffers[tool_call_id] = {
                                    "name": tool_name,
                                    "args": "",
                                    "complete": False,
                                }

                            buffer = self.tool_call_buffers[tool_call_id]
                            if tool_args:
                                buffer["args"] += tool_args

                            # æ£€æŸ¥å·¥å…·è°ƒç”¨æ˜¯å¦å®Œæˆ
                            if block.get("complete", False):
                                buffer["complete"] = True
                                results.append(
                                    {
                                        "type": "tool_call",
                                        "tool": buffer["name"],
                                        "args": buffer["args"],
                                        "session_id": self.session_id,
                                        "tool_call_id": tool_call_id,
                                        "complete": True,
                                    }
                                )
                                del self.tool_call_buffers[tool_call_id]

                    # ç„¶åå¤„ç†æ–‡æœ¬å—
                    for block in message.content_blocks:
                        if block.get("type") == "text":
                            text_content = block.get("text", "")
                            if text_content:
                                # æ£€æŸ¥æ˜¯å¦æ˜¯å·¥å…·è¾“å‡ºçš„JSONæ•°ç»„æ ¼å¼
                                if self._is_tool_output(text_content):
                                    # è½¬æ¢å·¥å…·è¾“å‡ºä¸ºå‹å¥½æ ¼å¼
                                    formatted_output = self._format_tool_output(
                                        text_content
                                    )
                                    if formatted_output:
                                        results.append(
                                            {
                                                "type": "tool_result",
                                                "content": formatted_output,
                                                "session_id": self.session_id,
                                            }
                                        )
                                else:
                                    # ç´¯ç§¯æ–‡æœ¬åˆ°ç¼“å†²åŒº
                                    self.pending_text += text_content
                                    self.last_chunk_time = current_time

        elif stream_mode == "updates":
            # å¤„ç†æ›´æ–°æ¶ˆæ¯ï¼ˆåŒ…æ‹¬HITLä¸­æ–­ï¼‰
            if isinstance(data, dict):
                if "__interrupt__" in data:
                    # HITLæ‰¹å‡†è¯·æ±‚
                    interrupt_data = data["__interrupt__"]
                    if interrupt_data and interrupt_data.get("action_requests"):
                        results.append(
                            {
                                "type": "approval_request",
                                "approval_data": interrupt_data,
                                "session_id": self.session_id,
                            }
                        )

                elif "todos" in data:
                    # å¾…åŠäº‹é¡¹æ›´æ–°
                    results.append(
                        {
                            "type": "todos",
                            "todos": data["todos"],
                            "session_id": self.session_id,
                        }
                    )

        # æ™ºèƒ½æ–‡æœ¬å‘é€ç­–ç•¥ - åªæœ‰åœ¨æ²¡æœ‰å·¥å…·è°ƒç”¨æ—¶æ‰è€ƒè™‘å‘é€æ–‡æœ¬
        should_flush_text = False
        if self.pending_text and not self.tool_call_buffers:
            time_elapsed = current_time - self.last_chunk_time

            # æ¡ä»¶1ï¼šæ—¶é—´è¶…è¿‡é˜ˆå€¼ä¸”æ²¡æœ‰æ´»è·ƒçš„å·¥å…·è°ƒç”¨
            if time_elapsed > self.chunk_timeout:
                should_flush_text = True
            # æ¡ä»¶2ï¼šæ–‡æœ¬åŒ…å«å®Œæ•´å¥å­
            elif (
                self._has_complete_sentence(self.pending_text)
                and len(self.pending_text) > 30
            ):
                should_flush_text = True
            # æ¡ä»¶3ï¼šæ–‡æœ¬å¾ˆé•¿
            elif len(self.pending_text) > 200:
                should_flush_text = True

        if should_flush_text:
            text_to_send = self.pending_text.rstrip()
            if text_to_send:
                results.append(
                    {
                        "type": "message",
                        "content": text_to_send,
                        "session_id": self.session_id,
                        "is_stream": True,
                    }
                )
                self.pending_text = ""
                self.last_chunk_time = current_time

        # è¿”å›ç»“æœï¼ˆä¼˜å…ˆçº§ï¼šå·¥å…·è°ƒç”¨ > å·¥å…·ç»“æœ > çŠ¶æ€ > å…¶ä»– > æ–‡æœ¬ï¼‰
        if results:
            tool_call_messages = [r for r in results if r.get("type") == "tool_call"]
            tool_result_messages = [
                r for r in results if r.get("type") == "tool_result"
            ]
            status_messages = [r for r in results if r.get("type") == "status"]
            other_messages = [
                r
                for r in results
                if r.get("type")
                not in ["tool_call", "tool_result", "status", "message"]
            ]
            text_messages = [r for r in results if r.get("type") == "message"]

            if tool_call_messages:
                return tool_call_messages[0]
            elif tool_result_messages:
                return tool_result_messages[0]
            elif status_messages:
                return status_messages[0]
            elif other_messages:
                return other_messages[0]
            elif text_messages:
                return text_messages[0]

        return None

    def _is_tool_output(self, text: str) -> bool:
        """æ£€æŸ¥æ–‡æœ¬æ˜¯å¦æ˜¯å·¥å…·è¾“å‡ºçš„JSONæ•°ç»„æ ¼å¼ã€‚"""
        import json

        text_stripped = text.strip()

        # æ£€æŸ¥æ˜¯å¦æ˜¯JSONæ•°ç»„æ ¼å¼
        if text_stripped.startswith("[") and text_stripped.endswith("]"):
            try:
                # å°è¯•è§£æJSON
                parsed = json.loads(text_stripped)
                return isinstance(parsed, list) and len(parsed) > 0
            except json.JSONDecodeError:
                return False

        return False

    def _format_tool_output(self, text: str) -> str:
        """æ ¼å¼åŒ–å·¥å…·è¾“å‡ºä¸ºå‹å¥½çš„æ–‡æœ¬ã€‚"""
        import json

        try:
            items = json.loads(text.strip())
            if not isinstance(items, list):
                return None

            formatted_lines = []
            for item in items:
                if isinstance(item, list) and len(item) > 0:
                    # å¦‚æœæ˜¯åµŒå¥—æ•°ç»„ï¼Œå–ç¬¬ä¸€ä¸ªå…ƒç´ ä½œä¸ºä¸»è¦æè¿°
                    main_item = item[0] if isinstance(item[0], str) else str(item[0])

                    # å¦‚æœæ˜¯æ–‡ä»¶è·¯å¾„åˆ—è¡¨ï¼Œæ˜¾ç¤ºä¸ºæ–‡ä»¶åˆ—è¡¨
                    if main_item.startswith("/") or (
                        "/" in main_item and "." in main_item
                    ):
                        formatted_lines.append(f"ğŸ“ {main_item}")
                        # æ·»åŠ å­é¡¹ä½œä¸ºç¼©è¿›åˆ—è¡¨
                        for sub_item in item[1:]:
                            if isinstance(sub_item, str) and sub_item.strip():
                                if sub_item.startswith("/"):
                                    formatted_lines.append(f"   ğŸ“„ {sub_item}")
                                else:
                                    formatted_lines.append(f"   â€¢ {sub_item}")
                    else:
                        # å…¶ä»–ç±»å‹çš„å†…å®¹
                        formatted_lines.append(f"â€¢ {main_item}")
                        for sub_item in item[1:]:
                            if isinstance(sub_item, str) and sub_item.strip():
                                formatted_lines.append(f"   â€¢ {sub_item}")
                elif isinstance(item, str):
                    # å­—ç¬¦ä¸²é¡¹
                    if item.startswith("/"):
                        formatted_lines.append(f"ğŸ“ {item}")
                    else:
                        formatted_lines.append(f"â€¢ {item}")

            return "\n".join(formatted_lines) if formatted_lines else None

        except (json.JSONDecodeError, Exception):
            return None

    def _has_complete_sentence(self, text: str) -> bool:
        """æ£€æŸ¥æ–‡æœ¬æ˜¯å¦åŒ…å«å®Œæ•´çš„å¥å­ã€‚"""
        import re

        text_stripped = text.strip()

        # æ–‡æœ¬å¤ªçŸ­ä¸åˆ†å‰²
        if len(text_stripped) < 20:
            return False

        # æ£€æŸ¥æ˜¯å¦ä»¥å¥å­ç»“æŸç¬¦ç»“å°¾
        end_chars = [".", "!", "?", "ã€‚", "ï¼", "ï¼Ÿ", "\n"]
        ends_with_sentence = any(text_stripped.endswith(char) for char in end_chars)

        # æ£€æŸ¥æ˜¯å¦æœ‰å¸¸è§çš„å¥å­ç»“æ„æ¨¡å¼
        sentence_patterns = [
            r".*[ã€‚ï¼ï¼Ÿ]\s*$",  # ä¸­æ–‡å¥å­ç»“å°¾
            r"[.!?]\s*$",  # è‹±æ–‡å¥å­ç»“å°¾
            r"ï¼š\s*.*[ã€‚ï¼ï¼Ÿ.!?]",  # æœ‰è§£é‡Šçš„å¥å­
            r"\s*\n\s*$",  # æ¢è¡Œç»“å°¾
        ]

        has_sentence_structure = any(
            re.match(pattern, text_stripped) for pattern in sentence_patterns
        )

        # é¿å…åœ¨ä»£ç å—æˆ–åˆ—è¡¨ä¸­é—´åˆ†å‰²
        avoid_split_patterns = [
            r".*```$",  # ä»£ç å—å¼€å§‹
            r".*`[^`]*$",  # ä¸å®Œæ•´çš„ä»£ç æ ‡è®°
            r".*\d+\.$",  # æ•°å­—åˆ—è¡¨ï¼ˆå¦‚ "1."ï¼‰
            r".*[-*+]\s*$",  # é¡¹ç›®ç¬¦å·åˆ—è¡¨
        ]

        should_avoid_split = any(
            re.match(pattern, text_stripped) for pattern in avoid_split_patterns
        )

        return ends_with_sentence and has_sentence_structure and not should_avoid_split

    def flush_pending_text(self, final: bool = False):
        """å¼ºåˆ¶åˆ·æ–°ç´¯ç§¯çš„æ–‡æœ¬ç¼“å†²åŒºã€‚"""
        results = []
        if self.pending_text and (final or self.pending_text.strip()):
            text_to_send = self.pending_text.rstrip()
            if text_to_send:
                results.append(
                    {
                        "type": "message",
                        "content": text_to_send,
                        "session_id": self.session_id,
                        "is_stream": not final,
                    }
                )
                self.pending_text = ""

        # æµç»“æŸæ—¶é‡ç½®æ‰€æœ‰çŠ¶æ€
        if final:
            self.sent_thinking = False
            self.is_thinking = False
            self.has_sent_thinking_for_current_request = False

        return results

    def get_memory_files(self) -> List[str]:
        """Get list of memory files for this session."""
        memory_files = []
        if self.memory_dir.exists():
            for file_path in self.memory_dir.rglob("*"):
                if file_path.is_file():
                    relative_path = file_path.relative_to(self.memory_dir)
                    memory_files.append(str(relative_path))
        return memory_files

    def read_memory_file(self, file_path: str) -> str:
        """Read content from a memory file."""
        full_path = self.memory_dir / file_path
        if full_path.exists() and full_path.is_file():
            return full_path.read_text(encoding="utf-8")
        return ""

    def write_memory_file(self, file_path: str, content: str):
        """Write content to a memory file."""
        full_path = self.memory_dir / file_path
        full_path.parent.mkdir(parents=True, exist_ok=True)
        full_path.write_text(content, encoding="utf-8")

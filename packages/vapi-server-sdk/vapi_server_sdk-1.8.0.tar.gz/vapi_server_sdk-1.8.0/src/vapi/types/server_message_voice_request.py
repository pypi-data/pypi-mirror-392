# This file was auto-generated by Fern from our API Definition.

from __future__ import annotations

import typing

import pydantic
import typing_extensions
from ..core.pydantic_utilities import IS_PYDANTIC_V2, update_forward_refs
from ..core.serialization import FieldMetadata
from ..core.unchecked_base_model import UncheckedBaseModel
from .artifact import Artifact
from .call import Call
from .chat import Chat
from .create_customer_dto import CreateCustomerDto
from .server_message_voice_request_phone_number import ServerMessageVoiceRequestPhoneNumber
from .server_message_voice_request_type import ServerMessageVoiceRequestType


class ServerMessageVoiceRequest(UncheckedBaseModel):
    phone_number: typing_extensions.Annotated[
        typing.Optional[ServerMessageVoiceRequestPhoneNumber], FieldMetadata(alias="phoneNumber")
    ] = pydantic.Field(default=None)
    """
    This is the phone number that the message is associated with.
    """

    type: ServerMessageVoiceRequestType = pydantic.Field()
    """
    This is the type of the message. "voice-request" is sent when using `assistant.voice={ "type": "custom-voice" }`.
    
    Here is what the request will look like:
    
    POST https://{assistant.voice.server.url}
    Content-Type: application/json
    
    {
      "messsage": {
        "type": "voice-request",
        "text": "Hello, world!",
        "sampleRate": 24000,
        ...other metadata about the call...
      }
    }
    
    The expected response is 1-channel 16-bit raw PCM audio at the sample rate specified in the request. Here is how the response will be piped to the transport:
    ```
    response.on('data', (chunk: Buffer) => {
      outputStream.write(chunk);
    });
    ```
    """

    timestamp: typing.Optional[float] = pydantic.Field(default=None)
    """
    This is the timestamp of the message.
    """

    artifact: typing.Optional[Artifact] = pydantic.Field(default=None)
    """
    This is a live version of the `call.artifact`.
    
    This matches what is stored on `call.artifact` after the call.
    """

    assistant: typing.Optional["CreateAssistantDto"] = pydantic.Field(default=None)
    """
    This is the assistant that the message is associated with.
    """

    customer: typing.Optional[CreateCustomerDto] = pydantic.Field(default=None)
    """
    This is the customer that the message is associated with.
    """

    call: typing.Optional[Call] = pydantic.Field(default=None)
    """
    This is the call that the message is associated with.
    """

    chat: typing.Optional[Chat] = pydantic.Field(default=None)
    """
    This is the chat object.
    """

    text: str = pydantic.Field()
    """
    This is the text to be synthesized.
    """

    sample_rate: typing_extensions.Annotated[float, FieldMetadata(alias="sampleRate")] = pydantic.Field()
    """
    This is the sample rate to be synthesized.
    """

    if IS_PYDANTIC_V2:
        model_config: typing.ClassVar[pydantic.ConfigDict] = pydantic.ConfigDict(extra="allow", frozen=True)  # type: ignore # Pydantic v2
    else:

        class Config:
            frozen = True
            smart_union = True
            extra = pydantic.Extra.allow


from .anthropic_model import AnthropicModel  # noqa: E402, F401, I001
from .anyscale_model import AnyscaleModel  # noqa: E402, F401, I001
from .assistant_overrides import AssistantOverrides  # noqa: E402, F401, I001
from .call_hook_assistant_speech_interrupted import CallHookAssistantSpeechInterrupted  # noqa: E402, F401, I001
from .call_hook_call_ending import CallHookCallEnding  # noqa: E402, F401, I001
from .call_hook_customer_speech_interrupted import CallHookCustomerSpeechInterrupted  # noqa: E402, F401, I001
from .call_hook_customer_speech_timeout import CallHookCustomerSpeechTimeout  # noqa: E402, F401, I001
from .cerebras_model import CerebrasModel  # noqa: E402, F401, I001
from .create_assistant_dto import CreateAssistantDto  # noqa: E402, F401, I001
from .create_handoff_tool_dto import CreateHandoffToolDto  # noqa: E402, F401, I001
from .custom_llm_model import CustomLlmModel  # noqa: E402, F401, I001
from .deep_infra_model import DeepInfraModel  # noqa: E402, F401, I001
from .deep_seek_model import DeepSeekModel  # noqa: E402, F401, I001
from .google_model import GoogleModel  # noqa: E402, F401, I001
from .groq_model import GroqModel  # noqa: E402, F401, I001
from .group_condition import GroupCondition  # noqa: E402, F401, I001
from .handoff_destination_assistant import HandoffDestinationAssistant  # noqa: E402, F401, I001
from .inflection_ai_model import InflectionAiModel  # noqa: E402, F401, I001
from .open_ai_model import OpenAiModel  # noqa: E402, F401, I001
from .open_router_model import OpenRouterModel  # noqa: E402, F401, I001
from .perplexity_ai_model import PerplexityAiModel  # noqa: E402, F401, I001
from .together_ai_model import TogetherAiModel  # noqa: E402, F401, I001
from .tool_call_hook_action import ToolCallHookAction  # noqa: E402, F401, I001
from .xai_model import XaiModel  # noqa: E402, F401, I001

update_forward_refs(ServerMessageVoiceRequest)

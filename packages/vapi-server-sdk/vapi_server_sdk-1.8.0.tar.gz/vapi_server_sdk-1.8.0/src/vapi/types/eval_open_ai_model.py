# This file was auto-generated by Fern from our API Definition.

import typing

import pydantic
import typing_extensions
from ..core.pydantic_utilities import IS_PYDANTIC_V2
from ..core.serialization import FieldMetadata
from ..core.unchecked_base_model import UncheckedBaseModel
from .eval_open_ai_model_model import EvalOpenAiModelModel
from .eval_open_ai_model_provider import EvalOpenAiModelProvider


class EvalOpenAiModel(UncheckedBaseModel):
    provider: EvalOpenAiModelProvider = pydantic.Field()
    """
    This is the provider of the model (`openai`).
    """

    model: EvalOpenAiModelModel = pydantic.Field()
    """
    This is the OpenAI model that will be used.
    
    When using Vapi OpenAI or your own Azure Credentials, you have the option to specify the region for the selected model. This shouldn't be specified unless you have a specific reason to do so. Vapi will automatically find the fastest region that make sense.
    This is helpful when you are required to comply with Data Residency rules. Learn more about Azure regions here https://azure.microsoft.com/en-us/explore/global-infrastructure/data-residency/.
    """

    temperature: typing.Optional[float] = pydantic.Field(default=None)
    """
    This is the temperature of the model. For LLM-as-a-judge, it's recommended to set it between 0 - 0.3 to avoid hallucinations and ensure the model judges the output correctly based on the instructions.
    """

    max_tokens: typing_extensions.Annotated[typing.Optional[float], FieldMetadata(alias="maxTokens")] = pydantic.Field(
        default=None
    )
    """
    This is the max tokens of the model.
    If your Judge instructions return `true` or `false` takes only 1 token (as per the OpenAI Tokenizer), and therefore is recommended to set it to a low number to force the model to return a short response.
    """

    messages: typing.List[typing.Dict[str, typing.Optional[typing.Any]]] = pydantic.Field()
    """
    These are the messages which will instruct the AI Judge on how to evaluate the assistant message.
    The LLM-Judge must respond with "pass" or "fail" to indicate if the assistant message passes the eval.
    
    To access the messages in the mock conversation, use the LiquidJS variable `{{messages}}`.
    The assistant message to be evaluated will be passed as the last message in the `messages` array and can be accessed using `{{messages[-1]}}`.
    
    It is recommended to use the system message to instruct the LLM how to evaluate the assistant message, and then use the first user message to pass the assistant message to be evaluated.
    """

    if IS_PYDANTIC_V2:
        model_config: typing.ClassVar[pydantic.ConfigDict] = pydantic.ConfigDict(extra="allow", frozen=True)  # type: ignore # Pydantic v2
    else:

        class Config:
            frozen = True
            smart_union = True
            extra = pydantic.Extra.allow

Metadata-Version: 2.4
Name: candorflow
Version: 0.1.1
Summary: Training stability tools for synthetic demo
Author-email: Matt Gehring <matt@candorprotocol.com>
Requires-Python: >=3.8
Description-Content-Type: text/markdown
License-File: LICENSE
Requires-Dist: torch
Requires-Dist: matplotlib
Requires-Dist: numpy
Dynamic: license-file

# ðŸŒŠ CandorFlow

**Early Warning System for Training Instabilities**

[![PyPI](https://img.shields.io/badge/pip-install%20candorflow-blue)]()
[![Colab](https://img.shields.io/badge/Open%20in-Colab-yellow?logo=googlecolab)](https://colab.research.google.com/github/CandorSystem/CandorFlow/blob/main/examples/run_demo.ipynb)
[![License: MIT](https://img.shields.io/badge/License-MIT-blue.svg)](LICENSE)
[![Python 3.8+](https://img.shields.io/badge/python-3.8+-blue.svg)](https://www.python.org/downloads/)

---

## âš ï¸ Important Notice

**This repository contains a SIMPLIFIED, PUBLIC DEMONSTRATION of CandorFlow concepts.**

This is NOT the full proprietary system. Many advanced features, algorithms, and optimizations are intentionally excluded. See [What Is NOT Included](#what-is-not-included-proprietary) for details.

---

## ðŸ“– Overview

CandorFlow is a training stability monitoring and intervention system designed to detect and prevent neural network training instabilities before they cause divergence.

This public repository demonstrates:
- A simplified stability metric **Î»(t)** based on gradient variance
- Basic threshold-based monitoring
- Automatic checkpoint rollback on instability detection
- Learning rate reduction for recovery
- Minimal working examples with toy models

### What is Î»(t)?

The lambda metric **Î»(t)** is a stability indicator that tracks training health over time. In this simplified demo, it measures gradient norm variance as a proxy for instability.

**High Î»(t) â†’ Training is becoming unstable**  
**Low Î»(t) â†’ Training is stable**

---

## ðŸŽ¯ Features in This Demo

### âœ… What This Repo Contains (Safe/Public Demo)

- **Simplified Î»(t) metric**: Gradient norm variance-based instability detection
- **Basic stability controller**: Threshold monitoring with rollback capabilities
- **Checkpoint management**: Automatic saving and restoration
- **Learning rate adaptation**: Halving on instability detection
- **Minimal training loop**: Toy example with intentional instability
- **Visualization tools**: Plot Î»(t) curves and stability phases
- **Jupyter notebook**: Interactive demo with explanations
- **Reproducible examples**: Fully runnable on CPU or GPU

---

## ðŸš« What Is NOT Included (Proprietary)

The full CandorFlow system includes many advanced features that are **NOT** in this public demo:

### Core Algorithms
- âŒ **Universal scaling law** for Î»(t)
- âŒ **Reflexive ridge equation** and closed-form solutions
- âŒ **Cross-domain invariants** (works across NLP, vision, RL, etc.)
- âŒ **Jacobian spectral analysis** for stability prediction
- âŒ **Multi-signal fusion** (loss, gradients, activations, etc.)

### Advanced Control
- âŒ **Real-time stability engine** with predictive modeling
- âŒ **Reflexive decay algorithms** for adaptive intervention
- âŒ **Temporal smoothing with active inference**
- âŒ **Dynamic threshold adaptation** based on training phase
- âŒ **HPC-optimized control loops** for large-scale training

### Domain Extensions
- âŒ **ECG anomaly detection** applications
- âŒ **Earthquake early warning** systems
- âŒ **Financial market stability** monitoring
- âŒ **General-purpose time series** instability detection

### Performance
- âŒ **Production-grade optimizations** for minimal overhead
- âŒ **Distributed training integration** (DeepSpeed, FSDP, etc.)
- âŒ **Hardware acceleration** (CUDA kernels, etc.)

**For access to the full proprietary system, please contact us.**

---

## ðŸš€ Installation

### Prerequisites

- Python 3.8 or higher
- pip package manager

### Option 1: Install from GitHub (Recommended)

Install CandorFlow directly from the repository:

```bash
pip install git+https://github.com/CandorSystem/CandorFlow.git
```

After installation, you can import CandorFlow from anywhere:

```python
from candorflow import compute_lambda, StabilityController
from candorflow.demo import run_demo, plot_results
```

### Option 2: Development Installation

For contributing, modifying the code, or running examples from the repository:

```bash
git clone https://github.com/CandorSystem/CandorFlow.git
cd CandorFlow
pip install -e .
```

This installs the package in editable mode, so changes to the source code are immediately reflected.

### Option 3: Manual Setup (Not Recommended)

If you prefer not to install the package:

```bash
git clone https://github.com/CandorSystem/CandorFlow.git
cd CandorFlow
pip install -r requirements.txt
python examples/run_demo.py  # Must run from repo directory
```

**Note:** With this approach, you'll need to add the repository to your Python path or run scripts from the repository root directory.

---

## ðŸ’» Usage

### Quick Start: Run the Training Demo

After installation, run the demo:

```python
from candorflow.demo import run_demo, plot_results

# Run the demo
results = run_demo()

# Generate plots
plot_results(results)
```

Or use the command-line wrapper:

```bash
python examples/run_demo.py
```

This will:
1. Create a small MLP neural network
2. Train it on synthetic data
3. Compute Î»(t) at each step
4. Inject synthetic instability spike at step 30
5. Demonstrate automatic detection and rollback
6. Generate two plots:
   - `plots/lambda_curve.png` - Î»(t) over time with intervention markers
   - `plots/stability_phases.png` - Color-coded stability zones

**Expected output:**
```
âœ“ Saved plots to plots/
  - lambda_curve.png
  - stability_phases.png
```

### Colab Integration

The demo is designed for easy Colab integration:

```python
!pip install candorflow

from candorflow.demo import run_demo, plot_results
results = run_demo(steps=50, spike_step=30, threshold=2.0)
plot_results(results)
```

All training logic is contained in `candorflow.demo` - no need to write training loops in Colab!

---

## ðŸ“ Repository Structure

```
CandorFlow/
â”‚
â”œâ”€â”€ README.md                   # This file
â”œâ”€â”€ pyproject.toml              # Package configuration (pip install)
â”œâ”€â”€ requirements.txt            # Python dependencies
â”œâ”€â”€ LICENSE                     # MIT License
â”‚
â”œâ”€â”€ candorflow/                 # Main package
â”‚   â”œâ”€â”€ __init__.py            # Public API (compute_lambda, StabilityController)
â”‚   â”œâ”€â”€ demo.py                # Complete training demo (all logic here)
â”‚   â”œâ”€â”€ lambda_metric.py       # Simplified Î»(t) computation
â”‚   â”œâ”€â”€ stability_controller.py # Basic monitoring & intervention
â”‚   â”œâ”€â”€ utils.py               # Checkpoint and logging utilities
â”‚   â””â”€â”€ version.py             # Version information
â”‚
â”œâ”€â”€ examples/                   # Runnable demos
â”‚   â””â”€â”€ run_demo.py            # Thin wrapper to run demo
â”‚
â”œâ”€â”€ notebooks/                  # Jupyter notebooks
â”‚   â””â”€â”€ CandorFlow_Demo.ipynb  # Interactive tutorial
â”‚
â””â”€â”€ plots/                      # Output directory for plots
    â””â”€â”€ (generated files)
```

---

## ðŸ”¬ How It Works (Simplified Version)

### 1. Monitor Training with Î»(t)

```python
from candorflow import compute_lambda, StabilityController

# During training loop
lambda_value = compute_lambda(
    model=model,
    loss=loss,
    gradient_history=gradient_history
)
```

### 2. Automatic Intervention

```python
controller = StabilityController(threshold=2.0)

action = controller.update(
    lambda_value=lambda_value,
    model=model,
    optimizer=optimizer,
    step=step
)

if action["action"] == "rollback":
    print("Instability detected - rolling back to stable checkpoint")
```

### 3. Training Continues Safely

The controller automatically:
- Saves checkpoints when training is stable
- Detects when Î»(t) exceeds threshold
- Rolls back to last stable state
- Reduces learning rate
- Resumes training

---

## ðŸ“Š Example Results

After running the demo, you'll see plots like this:

**Lambda Curve with Interventions:**
- Blue line: Î»(t) stability metric over time
- Purple dashed line: Instability threshold
- Orange markers: Rollback + LR reduction events
- Red markers: Warnings

**Stability Phases:**
- Green zone: Stable training
- Orange zone: Warning (approaching threshold)
- Red zone: Unstable (intervention triggered)

---

## ðŸ§ª Running Tests

The demo includes built-in validation:

```bash
# Run training demo (includes self-checks)
python examples/demo_training_loop.py

# Generate plots (validates results)
python examples/demo_plots.py
```

---

## ðŸ“š Documentation

### API Reference

#### `compute_lambda_metric(model, loss, history_window=10, gradient_history=None)`

Compute simplified Î»(t) stability metric.

**Parameters:**
- `model` (torch.nn.Module): Neural network model
- `loss` (torch.Tensor): Current loss value (with grad_fn)
- `history_window` (int): Number of past gradient norms to track
- `gradient_history` (list): List to store gradient history (modified in-place)

**Returns:**
- `lambda_value` (float): Stability metric (higher = more unstable)

#### `StabilityController(threshold, checkpoint_dir, lr_reduction_factor)`

Training stability monitor and intervention system.

**Parameters:**
- `threshold` (float): Î»(t) value above which to trigger intervention
- `checkpoint_dir` (str): Directory for saving checkpoints
- `lr_reduction_factor` (float): Factor to reduce LR by (default: 0.5)

**Methods:**
- `update(lambda_value, model, optimizer, step)`: Update controller and take action if needed
- `get_summary()`: Get training statistics

---

## ðŸ¤ Contributing

This is a demonstration repository. Contributions are welcome for:
- Bug fixes in demo code
- Documentation improvements
- Additional visualization examples
- Educational content

**Note:** This repo intentionally excludes proprietary algorithms. Please do not submit PRs attempting to implement advanced features from the full system.

---

## ðŸ“§ Contact

For questions about this demo:
- Open an issue on GitHub

For inquiries about the full proprietary CandorFlow system:
- Email: [your-email@example.com]
- Website: [https://candorflow.example.com]
- Patents: [Patent application numbers]

---

## ðŸ“„ License

This simplified demonstration code is released under the MIT License. See [LICENSE](LICENSE) for details.

**Important:** The full CandorFlow system, including its proprietary algorithms and commercial applications, is NOT covered by this license. Please contact us for commercial licensing.

---

## ðŸ“– Citation

If you use this demo code in your research or project, please cite:

```bibtex
@software{candorflow2025,
  title={CandorFlow: Training Stability Monitoring System},
  author={[Your Name]},
  year={2025},
  url={https://github.com/yourusername/CandorFlow},
  note={Simplified public demonstration version}
}
```

---

## ðŸ™ Acknowledgments

This simplified demo is provided for educational purposes to demonstrate basic concepts in training stability monitoring.

The full CandorFlow system represents significant research and development investment and is protected by pending patents.

---

## â­ Star History

If you find this demo helpful, please consider starring the repository!

---

**Built for responsible, and safe AI.**

---

## Project Support & Affiliations

CandorFlow and the Candor Systems project are supported by several leading industry startup programs:

[![NVIDIA Inception](https://img.shields.io/badge/NVIDIA-Inception%20Member-76B900?logo=nvidia&logoColor=white)]()
[![Google Cloud Startup](https://img.shields.io/badge/Google%20Cloud-Startup%20Program-4285F4?logo=googlecloud&logoColor=white)]()
[![AWS Activate](https://img.shields.io/badge/AWS-Activate%20Program-FF9900?logo=amazonaws&logoColor=white)]()

These affiliations provide cloud credits, compute resources, and technical support for ongoing research and development.

> **Note:** These affiliations indicate participation in early-stage startup support programs and do **not** imply endorsement of CandorFlow's algorithms or proprietary systems.


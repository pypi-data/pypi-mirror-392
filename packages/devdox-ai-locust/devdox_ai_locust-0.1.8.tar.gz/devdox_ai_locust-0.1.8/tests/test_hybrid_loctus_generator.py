"""
Tests for hybrid_loctus_generator module
"""

import pytest
import asyncio
from unittest.mock import Mock, AsyncMock, patch
from pathlib import Path

import tempfile
import time
import psutil
import os

from devdox_ai_locust.hybrid_loctus_generator import (
    HybridLocustGenerator,
    AIEnhancementConfig,
    EnhancementResult,
    EnhancementProcessor,
)
from devdox_ai_locust.locust_generator import TestDataConfig


class TestAIEnhancementConfig:
    """Test AIEnhancementConfig dataclass."""

    def test_default_config(self):
        """Test default configuration values."""
        config = AIEnhancementConfig()

        assert config.model == "meta-llama/Llama-3.3-70B-Instruct-Turbo"
        assert config.max_tokens == 8000
        assert config.temperature == 0.3
        assert config.timeout == 60
        assert config.enhance_workflows is True
        assert config.enhance_test_data is True
        assert config.enhance_validation is True
        assert config.create_domain_flows is True
        assert config.update_main_locust is True

    def test_custom_config(self):
        """Test custom configuration values."""
        config = AIEnhancementConfig(
            model="gpt-4",
            max_tokens=4000,
            temperature=0.7,
            timeout=30,
            enhance_workflows=True,
            enhance_test_data=True,
            enhance_validation=True,
            create_domain_flows=True,
            update_main_locust=False,
        )

        assert config.model == "gpt-4"
        assert config.max_tokens == 4000
        assert config.temperature == 0.7
        assert config.timeout == 30
        assert config.enhance_workflows is True
        assert config.enhance_test_data is True
        assert config.enhance_validation is True
        assert config.create_domain_flows is True
        assert config.update_main_locust is False

    def test_default_config_security(self):
        """Test default configuration is production-safe."""
        config = AIEnhancementConfig()

        # Verify timeout is reasonable for production (not too high/low)
        assert 30 <= config.timeout <= 120, (
            f"Timeout {config.timeout}s may cause issues in production"
        )
        assert config.max_tokens <= 10000, "Token limit too high - cost concern"
        assert 0.1 <= config.temperature <= 0.5, (
            "Temperature should be conservative for production"
        )

    def test_timeout_validation_extremes(self):
        """Test timeout boundary conditions that could cause outages."""
        # Test unreasonably high timeout
        config = AIEnhancementConfig(timeout=0.5)  # 5 seconds
        # This should be flagged as risky - could cause resource exhaustion
        assert config.timeout == 0.5  # Current implementation allows this - RISK!

        # Test unreasonably low timeout
        config = AIEnhancementConfig(timeout=1)  # 1 second
        # This could cause false failures under load
        assert config.timeout == 1  # Current implementation allows this - RISK!

    def test_token_limit_boundaries(self):
        """Test token limits that could impact costs or performance."""
        # Test very high token limit - cost risk
        config = AIEnhancementConfig(max_tokens=50000)
        assert config.max_tokens == 50000  # No validation currently - COST RISK!

        # Test very low token limit - functionality risk
        config = AIEnhancementConfig(max_tokens=10)
        assert config.max_tokens == 10  # Could cause truncated responses

    def test_concurrent_config_safety(self):
        """Test configuration changes don't affect running instances."""
        config1 = AIEnhancementConfig(timeout=30)
        config2 = AIEnhancementConfig(timeout=60)

        # Configurations should be independent
        assert config1.timeout != config2.timeout

        # Modifying one shouldn't affect the other
        config1.timeout = 90
        assert config2.timeout == 60


class TestEnhancementResult:
    """Test EnhancementResult dataclass."""

    def test_enhancement_result_creation(self):
        """Test creating EnhancementResult."""
        result = EnhancementResult(
            success=True,
            enhanced_files={"test.py": "content"},
            enhanced_directory_files=[{"workflow.py": "content"}],
            enhancements_applied=["test_enhancement"],
            errors=[],
            processing_time=1.5,
        )

        assert result.success is True
        assert result.enhanced_files == {"test.py": "content"}
        assert result.enhanced_directory_files == [{"workflow.py": "content"}]
        assert result.enhancements_applied == ["test_enhancement"]
        assert result.errors == []
        assert result.processing_time == 1.5


class TestHybridLocustGenerator:
    """Test HybridLocustGenerator class."""

    def test_init_with_ai_client(self, mock_together_client):
        """Test initialization with AI client."""
        generator = HybridLocustGenerator(ai_client=mock_together_client)

        assert generator.ai_client == mock_together_client

    def test_init_with_custom_config(self, ai_enhancement_config, mock_together_client):
        """Test initialization with custom AI config."""
        generator = HybridLocustGenerator(
            ai_config=ai_enhancement_config, ai_client=mock_together_client
        )

        assert generator.ai_config.enhance_workflows is True
        assert generator.ai_config.enhance_test_data is True

    def test_init_with_custom_test_config(self, mock_together_client):
        """Test initialization with custom test config."""
        test_config = TestDataConfig(string_length=25)
        generator = HybridLocustGenerator(
            test_config=test_config, ai_client=mock_together_client
        )

        assert generator.template_generator.test_config.string_length == 25

    @patch("devdox_ai_locust.hybrid_loctus_generator.Path")
    def test_find_project_root(self, mock_path, mock_together_client):
        """Test finding project root."""
        mock_path.return_value = Path("/project/src/devdox_ai_locust/file.py")

        generator = HybridLocustGenerator(ai_client=mock_together_client)
        root = generator._find_project_root()

        assert root == Path("/project/src/devdox_ai_locust")

    def test_should_enhance_enough_endpoints(
        self, sample_endpoints, mock_together_client
    ):
        """Test should enhance with enough endpoints."""
        generator = HybridLocustGenerator(ai_client=mock_together_client)

        # Should enhance with 3+ endpoints
        result = generator._should_enhance(sample_endpoints, {})
        assert result is True

    def test_should_enhance_complex_endpoints(
        self, sample_api_info, mock_together_client
    ):
        """Test should enhance with complex endpoints."""
        generator = HybridLocustGenerator(ai_client=mock_together_client)

        # Create complex endpoint
        complex_endpoint = Mock()
        complex_endpoint.request_body = Mock()
        complex_endpoint.parameters = [Mock() for _ in range(5)]
        complex_endpoint.responses = [Mock(), Mock(), Mock()]
        complex_endpoint.path = "/complex"

        result = generator._should_enhance([complex_endpoint], sample_api_info)
        assert result is True

    def test_should_enhance_domain_patterns(
        self, sample_api_info, mock_together_client
    ):
        """Test should enhance with domain patterns."""
        generator = HybridLocustGenerator(ai_client=mock_together_client)

        # Modify API info to include domain keywords
        api_info = sample_api_info.copy()
        api_info["title"] = "E-commerce API"
        api_info["description"] = "API for managing products and orders"

        endpoint = Mock()
        endpoint.path = "/products"
        endpoint.request_body = None
        endpoint.parameters = []
        endpoint.responses = []

        result = generator._should_enhance([endpoint], api_info)
        assert result is True

    def test_should_not_enhance_simple_case(self, mock_together_client):
        """Test should not enhance with simple case."""
        generator = HybridLocustGenerator(ai_client=mock_together_client)

        # Simple endpoint without domain patterns
        simple_endpoint = Mock()
        simple_endpoint.request_body = None
        simple_endpoint.parameters = []
        simple_endpoint.responses = [Mock()]
        simple_endpoint.path = "/simple"

        simple_api_info = {"title": "Simple API", "description": "Basic API"}

        result = generator._should_enhance([simple_endpoint], simple_api_info)
        assert result is False

    def test_detect_domain_patterns_ecommerce(
        self, sample_endpoints, mock_together_client
    ):
        """Test detecting e-commerce domain patterns."""
        generator = HybridLocustGenerator(ai_client=mock_together_client)

        api_info = {
            "title": "Shopping API",
            "description": "API for online shopping cart and product management",
        }

        result = generator._detect_domain_patterns(sample_endpoints, api_info)
        assert result is True

    def test_detect_domain_patterns_user_management(
        self, sample_endpoints, mock_together_client
    ):
        """Test detecting user management patterns."""
        generator = HybridLocustGenerator(ai_client=mock_together_client)

        # Endpoints contain /users and /auth paths
        api_info = {"title": "User API", "description": "User management"}

        result = generator._detect_domain_patterns(sample_endpoints, api_info)
        assert result is True

    def test_detect_domain_patterns_no_match(self, mock_together_client):
        """Test no domain pattern detection."""
        generator = HybridLocustGenerator(ai_client=mock_together_client)

        endpoint = Mock()
        endpoint.path = "/data"

        api_info = {"title": "Data API", "description": "Generic data API"}

        result = generator._detect_domain_patterns([endpoint], api_info)
        assert result is False

    def test_format_endpoints_for_prompt(self, sample_endpoints, mock_together_client):
        """Test formatting endpoints for AI prompt."""
        generator = HybridLocustGenerator(ai_client=mock_together_client)

        formatted = generator._format_endpoints_for_prompt(sample_endpoints)

        assert "GET /users" in formatted
        assert "POST /users" in formatted
        assert "GET /users/{id}" in formatted
        assert "POST /auth/login" in formatted

    def test_analyze_api_domain(
        self, sample_endpoints, sample_api_info, mock_together_client
    ):
        """Test API domain analysis."""
        generator = HybridLocustGenerator(ai_client=mock_together_client)

        analysis = generator._analyze_api_domain(sample_endpoints, sample_api_info)
        assert "Test API" in analysis
        assert "Total Endpoints: 4" in analysis
        assert "POST" in analysis
        assert "GET" in analysis

    def test_extract_path_patterns(self, mock_together_client):
        """Test extracting path patterns."""
        generator = HybridLocustGenerator(ai_client=mock_together_client)

        paths = ["/api/v1/users", "/api/v1/posts", "/api/v2/comments"]
        patterns = generator._extract_path_patterns(paths)

        assert "/api/v1" in patterns or "/api/v2" in patterns

    def test_extract_resources_from_paths(self, mock_together_client):
        """Test extracting resources from paths."""
        generator = HybridLocustGenerator(ai_client=mock_together_client)

        paths = ["/users", "/posts", "/comments", "/auth/login"]
        resources = generator._extract_resources_from_paths(paths)

        assert "users" in resources
        assert "posts" in resources
        assert "comments" in resources

    @pytest.mark.asyncio
    async def test_memory_usage_with_large_files(self, mock_together_client):
        """Test memory behavior with large file content."""

        generator = HybridLocustGenerator(ai_client=mock_together_client)

        # Create files with large content

        large_content = "x" * (500 * 1024)  # 500KB each
        large_files = {f"large_{i}.py": large_content for i in range(10)}

        with tempfile.TemporaryDirectory() as temp_dir:
            temp_path = Path(temp_dir)

            # Monitor memory usage
            process = psutil.Process(os.getpid())
            memory_before = process.memory_info().rss

            _ = await generator._create_test_files_safely(large_files, temp_path)

            memory_after = process.memory_info().rss
            memory_increase = (memory_after - memory_before) / 1024 / 1024  # MB
            # Should not consume excessive memory
            assert memory_increase < 100, (
                f"Memory increased by {memory_increase:.2f}MB - potential memory leak"
            )


class TestHybridLocustGeneratorAsync:
    """Test async functionality of HybridLocustGenerator."""

    @pytest.mark.asyncio
    async def test_generate_from_endpoints_template_only(
        self, sample_endpoints, sample_api_info
    ):
        """Test generation with template only (no AI)."""
        generator = HybridLocustGenerator(ai_client=None)

        with patch.object(
            generator.template_generator, "generate_from_endpoints"
        ) as mock_generate:
            mock_generate.return_value = (
                {"locustfile.py": "# Template content"},
                [{"workflow.py": "# Workflow"}],
                {"users": sample_endpoints},
            )

            files, workflows = await generator.generate_from_endpoints(
                sample_endpoints, sample_api_info
            )

            assert "locustfile.py" in files
            assert len(workflows) >= 1

    @pytest.mark.asyncio
    async def test_generate_from_endpoints_with_ai(
        self, mock_together_client, sample_endpoints, sample_api_info
    ):
        """Test generation with AI enhancement."""
        generator = HybridLocustGenerator(ai_client=mock_together_client)

        with (
            patch.object(
                generator.template_generator, "generate_from_endpoints"
            ) as mock_generate,
            patch.object(generator, "_enhance_with_ai") as mock_enhance,
            patch.object(generator, "_should_enhance") as mock_should_enhance,
        ):
            mock_generate.return_value = (
                {"locustfile.py": "# Template content"},
                [{"workflow.py": "# Workflow"}],
                {"users": sample_endpoints},
            )

            mock_should_enhance.return_value = True
            mock_enhance.return_value = EnhancementResult(
                success=True,
                enhanced_files={"locustfile.py": "# Enhanced content"},
                enhanced_directory_files=[
                    {"enhanced_workflow.py": "# Enhanced workflow"}
                ],
                enhancements_applied=["ai_enhancement"],
                errors=[],
                processing_time=1.0,
            )

            files, workflows = await generator.generate_from_endpoints(
                sample_endpoints, sample_api_info
            )

            assert files["locustfile.py"] == "# Enhanced content"
            mock_enhance.assert_called_once()

    @pytest.mark.asyncio
    async def test_generate_from_endpoints_ai_failure(
        self, mock_together_client, sample_endpoints, sample_api_info
    ):
        """Test generation with AI enhancement failure."""
        generator = HybridLocustGenerator(ai_client=mock_together_client)

        with (
            patch.object(
                generator.template_generator, "generate_from_endpoints"
            ) as mock_generate,
            patch.object(generator, "_enhance_with_ai") as mock_enhance,
            patch.object(generator, "_should_enhance") as mock_should_enhance,
        ):
            mock_generate.return_value = (
                {"locustfile.py": "# Template content"},
                [{"workflow.py": "# Workflow"}],
                {"users": sample_endpoints},
            )

            mock_should_enhance.return_value = True
            mock_enhance.return_value = EnhancementResult(
                success=False,
                enhanced_files={"locustfile.py": "# Template content"},
                enhanced_directory_files=[],
                enhancements_applied=[],
                errors=["AI enhancement failed"],
                processing_time=1.0,
            )

            files, workflows = await generator.generate_from_endpoints(
                sample_endpoints, sample_api_info
            )

            # Should fall back to template content
            normalized_file = "".join(files["locustfile.py"].split())
            assert normalized_file == "#Templatecontent"

    @pytest.mark.asyncio
    async def test_call_ai_service_success(self, mock_together_client):
        """Test successful AI service call."""
        generator = HybridLocustGenerator(ai_client=mock_together_client)

        result = await generator._call_ai_service("Test prompt")

        assert result is not None
        assert "import locust" in result

    @pytest.mark.asyncio
    async def test_ai_call_with_timeout(mock_together_client):
        """Test AI service call that times out"""

        async def mock_timeout(*args, **kwargs):
            """Simulate a timeout by sleeping longer than expected"""
            await asyncio.sleep(0.1)  # Long enough to trigger timeout
            raise asyncio.TimeoutError("Simulated timeout")

        mock_together_client.chat = Mock()
        mock_together_client.chat.completions = Mock()
        mock_together_client.chat.completions.create = AsyncMock(
            side_effect=mock_timeout
        )

        generator = HybridLocustGenerator(
            ai_client=mock_together_client,
            ai_config=AIEnhancementConfig(timeout=0.05),  # Short timeout
        )

        # Call should timeout and return empty string after retries
        result = await generator._call_ai_service("test prompt")

        assert result == ""
        # Should have tried 3 times
        assert mock_together_client.chat.completions.create.call_count == 3

    @pytest.mark.asyncio
    async def test_call_ai_service_with_retry(self, mock_together_client):
        """Test AI service call with retry logic."""
        generator = HybridLocustGenerator(ai_client=mock_together_client)

        # Mock first two calls to fail, third to succeed
        call_count = 0

        def side_effect(*args, **kwargs):
            nonlocal call_count
            call_count += 1
            if call_count <= 2:
                raise Exception("Service unavailable")
            return mock_together_client.chat.completions.create.return_value

        with patch("asyncio.to_thread", side_effect=side_effect):
            result = await generator._call_ai_service("Test prompt")

            # Should succeed on third try
            assert result is not None

    @pytest.mark.asyncio
    async def test_enhance_locustfile(
        self, mock_together_client, sample_endpoints, sample_api_info
    ):
        """Test enhancing locustfile with AI."""
        generator = HybridLocustGenerator(ai_client=mock_together_client)

        with patch.object(generator, "_call_ai_service") as mock_ai_call:
            mock_ai_call.return_value = "# Enhanced locustfile content"

            result = await generator._enhance_locustfile(
                "# Base content", sample_endpoints, sample_api_info
            )

            assert result == "# Enhanced locustfile content"
            mock_ai_call.assert_called_once()

    @pytest.mark.asyncio
    async def test_generate_domain_flows(
        self, mock_together_client, sample_endpoints, sample_api_info
    ):
        """Test generating domain flows."""
        generator = HybridLocustGenerator(ai_client=mock_together_client)

        with (
            patch.object(generator, "_call_ai_service") as mock_ai_call,
            patch.object(generator, "_analyze_api_domain") as mock_analyze,
        ):
            mock_analyze.return_value = "E-commerce domain analysis"
            mock_ai_call.return_value = "# Domain flows content"

            result = await generator._generate_domain_flows(
                sample_endpoints, sample_api_info
            )

            assert result == "# Domain flows content"

    @pytest.mark.asyncio
    async def test_enhance_test_data_file(self, mock_together_client, sample_endpoints):
        """Test enhancing test data file."""
        generator = HybridLocustGenerator(ai_client=mock_together_client)

        with (
            patch.object(generator, "_call_ai_service") as mock_ai_call,
            patch.object(generator, "_extract_schema_patterns") as mock_extract,
            patch.object(generator, "_validate_python_code") as mock_validate,
        ):
            mock_extract.return_value = "Schema patterns"
            mock_ai_call.return_value = "# Enhanced test data"
            mock_validate.return_value = True

            result = await generator.enhance_test_data_file(
                "# Base test data", sample_endpoints
            )

            assert result == "# Enhanced test data"

    @pytest.mark.asyncio
    async def test_enhance_validation(self, mock_together_client, sample_endpoints):
        """Test enhancing validation."""
        generator = HybridLocustGenerator(ai_client=mock_together_client)

        with (
            patch.object(generator, "_call_ai_service") as mock_ai_call,
            patch.object(generator, "_extract_validation_patterns") as mock_extract,
        ):
            mock_extract.return_value = "Validation patterns"
            mock_ai_call.return_value = "# Enhanced validation"

            result = await generator._enhance_validation(
                "# Base validation", sample_endpoints
            )

            assert result == "# Enhanced validation"

    @pytest.mark.asyncio
    async def test_enhance_workflows(self, mock_together_client):
        """Test enhancing workflows."""
        generator = HybridLocustGenerator(ai_client=mock_together_client)

        with patch.object(generator, "_call_ai_service") as mock_ai_call:
            mock_ai_call.return_value = "# Enhanced workflow"

            result = await generator._enhance_workflows(
                "# Base workflow",
                "# Test data",
                "# Base workflow template",
                [],  # grouped_endpoints
                [],  # auth_endpoints
            )

            assert result == "# Enhanced workflow"

    @pytest.mark.asyncio
    async def test_resource_exhaustion_protection(self, mock_together_client):
        """Test protection against resource exhaustion scenarios."""
        generator = HybridLocustGenerator(ai_client=mock_together_client)

        # Test very large number of files
        large_files = {f"test_file_{i}.py": "print('hello')" for i in range(1000)}

        with tempfile.TemporaryDirectory() as temp_dir:
            temp_path = Path(temp_dir)

            # This should be controlled/limited in production
            start_time = time.time()
            result = await generator._create_test_files_safely(large_files, temp_path)
            processing_time = time.time() - start_time

            # In production, this should have limits and not process all 1000 files
            # Current implementation processes all - potential DoS vector
            assert len(result) <= 1000  # Documents current behavior

            # Should complete in reasonable time (not hang indefinitely)
            assert processing_time < 30, (
                f"Processing took {processing_time}s - too slow for production"
            )


class TestErrorClassificationProductionScenarios:
    """Test error classification for real production scenarios."""

    def test_auth_error_detection_comprehensive(self, mock_together_client):
        """Test authentication error detection with various error formats."""
        generator = HybridLocustGenerator(ai_client=mock_together_client)

        # Test various auth error formats from different providers
        auth_errors = [
            "401 Unauthorized",
            "403 Forbidden",
            "Authentication failed",
            "Invalid Token expired",
        ]

        for error_msg in auth_errors:
            error = Exception(error_msg)
            classification = generator._classify_error(error, 0)
            assert not classification.is_retryable, (
                f"Auth error should not retry: {error_msg}"
            )
            assert classification.error_type == "auth"

    def test_rate_limit_detection_comprehensive(self, mock_together_client):
        """Test rate limit detection with various provider formats."""
        generator = HybridLocustGenerator(ai_client=mock_together_client)

        # Test various rate limit error formats
        rate_limit_errors = [
            "429 Too Many Requests",
            "Rate limit exceeded",
            "API rate limit hit",
        ]

        for error_msg in rate_limit_errors:
            error = Exception(error_msg)
            classification = generator._classify_error(error, 1)
            assert classification.is_retryable, (
                f"Rate limit should be retryable: {error_msg}"
            )
            assert classification.error_type == "rate_limit"
            assert classification.backoff_seconds == 10  # Should use longer backoff

    def test_exponential_backoff_timing(self, mock_together_client):
        """Test that exponential backoff timing is production-appropriate."""
        generator = HybridLocustGenerator(ai_client=mock_together_client)

        # Test backoff progression
        generic_error = Exception("Service temporarily unavailable")

        backoffs = []
        for attempt in range(3):
            classification = generator._classify_error(generic_error, attempt)
            backoffs.append(classification.backoff_seconds)

        expected_backoffs = [1, 2, 4]  # 2^attempt
        assert backoffs == expected_backoffs

        # Total max backoff time should be reasonable
        total_backoff = sum(backoffs)
        assert total_backoff <= 10, (
            f"Total backoff {total_backoff}s too long for production"
        )


class TestEnhancementProcessor:
    """Test EnhancementProcessor class."""

    @pytest.fixture
    def real_enhancement_processor(self):
        """Create real EnhancementProcessor for integration testing."""
        ai_config = AIEnhancementConfig()
        mock_generator = Mock(spec=HybridLocustGenerator)
        return EnhancementProcessor(ai_config, mock_generator)

    def test_init(self):
        """Test EnhancementProcessor initialization."""
        ai_config = AIEnhancementConfig()
        locust_generator = Mock()

        processor = EnhancementProcessor(ai_config, locust_generator)

        assert processor.ai_config == ai_config
        assert processor.locust_generator == locust_generator

    @pytest.mark.asyncio
    async def test_process_main_locust_enhancement(
        self, sample_endpoints, sample_api_info
    ):
        """Test processing main locust enhancement."""
        ai_config = AIEnhancementConfig(update_main_locust=True)
        mock_generator = AsyncMock()
        mock_generator._enhance_locustfile.return_value = "# Enhanced content"

        processor = EnhancementProcessor(ai_config, mock_generator)

        base_files = {"locustfile.py": "# Base content"}

        enhanced_files, enhancements = await processor.process_main_locust_enhancement(
            base_files, sample_endpoints, sample_api_info
        )

        assert enhanced_files["locustfile.py"] == "# Enhanced content"
        assert "main_locust_update" in enhancements

    def test_get_base_workflow_content_success(self, mock_together_client):
        """Test successful extraction of base workflow content."""
        directory_files = [
            {"workflow_users.py": "# Users workflow"},
            {
                "base_workflow.py": "# Base workflow content\nclass BaseWorkflow:\n    pass"
            },
            {"workflow_auth.py": "# Auth workflow"},
        ]
        generator = Mock(spec=HybridLocustGenerator)
        generator.get_files_by_key.return_value = [
            {
                "base_workflow.py": "# Base workflow content\nclass BaseWorkflow:\n    pass"
            }
        ]
        ai_config = AIEnhancementConfig()
        enhancement_processor = EnhancementProcessor(ai_config, generator)

        result = enhancement_processor._get_base_workflow_content(directory_files)
        assert result == "# Base workflow content\nclass BaseWorkflow:\n    pass"

    def test_get_base_workflow_content_no_base_workflow(self):
        """Test when no base workflow file exists."""
        directory_files = [
            {"workflow_users.py": "# Users workflow"},
            {"workflow_auth.py": "# Auth workflow"},
        ]

        # Mock get_files_by_key to return empty list
        mock_generator = Mock(spec=HybridLocustGenerator)
        mock_generator.get_files_by_key.return_value = []

        ai_config = AIEnhancementConfig()
        enhancement_processor = EnhancementProcessor(ai_config, mock_generator)

        result = enhancement_processor._get_base_workflow_content(directory_files)

        # Should return empty string
        assert result == ""
        mock_generator.get_files_by_key.assert_called_once_with(
            directory_files, "base_workflow.py"
        )

    def test_get_base_workflow_content_empty_directory_files(self):
        """Test with empty directory files list."""
        directory_files = []

        # Mock get_files_by_key to return empty list
        mock_generator = Mock(spec=HybridLocustGenerator)
        mock_generator.get_files_by_key.return_value = []
        ai_config = AIEnhancementConfig()
        enhancement_processor = EnhancementProcessor(ai_config, mock_generator)
        result = enhancement_processor._get_base_workflow_content(directory_files)

        assert result == ""
        mock_generator.get_files_by_key.assert_called_once_with(
            directory_files, "base_workflow.py"
        )

    def test_get_base_workflow_content_multiple_matches(self):
        """Test when multiple base workflow files exist (edge case)."""
        directory_files = [
            {"base_workflow.py": "# First base workflow"},
            {"base_workflow.py": "# Second base workflow"},  # Duplicate key (edge case)
        ]
        mock_generator = Mock(spec=HybridLocustGenerator)
        # Mock get_files_by_key to return first match
        mock_generator.get_files_by_key.return_value = [
            {"base_workflow.py": "# First base workflow"}
        ]
        ai_config = AIEnhancementConfig()
        enhancement_processor = EnhancementProcessor(ai_config, mock_generator)
        result = enhancement_processor._get_base_workflow_content(directory_files)

        assert result == "# First base workflow"

    def test_get_base_workflow_content_malformed_workflow_dict(self):
        """Test when workflow dict is malformed."""
        directory_files = [
            {"base_workflow.py": "# Base workflow content"},
        ]
        mock_generator = Mock(spec=HybridLocustGenerator)
        # Mock get_files_by_key to return dict without expected key
        mock_generator.get_files_by_key.return_value = [
            {"wrong_key.py": "# Wrong content"}
        ]
        ai_config = AIEnhancementConfig()
        enhancement_processor = EnhancementProcessor(ai_config, mock_generator)
        result = enhancement_processor._get_base_workflow_content(directory_files)

        # Should return empty string when key not found
        assert result == ""

    def test_get_base_workflow_content_none_value(self):
        """Test when base workflow content is None."""
        directory_files = [
            {"base_workflow.py": None},
        ]
        mock_generator = Mock(spec=HybridLocustGenerator)
        # Mock get_files_by_key to return dict with None value
        mock_generator.get_files_by_key.return_value = [{"base_workflow.py": None}]
        ai_config = AIEnhancementConfig()
        enhancement_processor = EnhancementProcessor(ai_config, mock_generator)
        result = enhancement_processor._get_base_workflow_content(directory_files)

        # Should handle None gracefully and return empty string
        assert result == ""

    @pytest.mark.asyncio
    async def test_process_workflow_disabled_ai_config(self):
        """Test the early return when AI enhancement is disabled."""
        # Test line: if not (self.ai_config and self.ai_config.enhance_workflows): return [], []

        # Test Case 1: ai_config is None
        processor_none = EnhancementProcessor(None, Mock())
        result1 = await processor_none.process_workflow_enhancements({}, [], {})
        assert result1 == ([], [])

        # Test Case 2: enhance_workflows is False
        ai_config_disabled = AIEnhancementConfig(enhance_workflows=False)
        processor_disabled = EnhancementProcessor(ai_config_disabled, Mock())
        result2 = await processor_disabled.process_workflow_enhancements({}, [], {})
        assert result2 == ([], [])

    @pytest.mark.asyncio
    async def test_process_workflow_enhancements_with_base_workflow_file_name_skip(
        self,
    ):
        """Test skipping workflow items with file_name == base_workflow_path."""
        # Test line: if workflow_item.get("file_name") == base_workflow_path: continue

        ai_config = AIEnhancementConfig(enhance_workflows=True)
        mock_generator = Mock()
        mock_generator.get_files_by_key.return_value = []  # No base workflow content

        processor = EnhancementProcessor(ai_config, mock_generator)

        # Mock _process_workflow_item to track calls
        processor._process_workflow_item = AsyncMock(
            return_value={
                "files": {"test.py": "enhanced"},
                "enhancements": ["test_enhancement"],
            }
        )

        base_files = {"locustfile.py": "# Base"}
        directory_files = [
            {"users_workflow.py": "# Users workflow"},
            {
                "file_name": "base_workflow.py",
                "other_key": "should be skipped",
            },  # This should be skipped
            {"admin_workflow.py": "# Admin workflow"},
        ]
        grouped_endpoints = {"users": [Mock()], "admin": [Mock()]}

        enhanced_files, enhancements = await processor.process_workflow_enhancements(
            base_files,
            directory_files,
            grouped_endpoints,
            db_type="",
            include_auth=False,
        )

        # Should only process 2 workflows (skipping the one with file_name)
        assert processor._process_workflow_item.call_count == 2
        assert len(enhanced_files) == 2
        assert len(enhancements) == 2

    @pytest.mark.asyncio
    async def test_process_workflow_enhancements_include_auth_no_base_workflow(self):
        """Test include_auth=True but no base workflow content."""
        # Test the negative case of: if include_auth and base_workflow_content:

        ai_config = AIEnhancementConfig(enhance_workflows=True)
        mock_generator = Mock()
        mock_generator.get_files_by_key.return_value = []  # No base workflow

        processor = EnhancementProcessor(ai_config, mock_generator)

        base_files = {"locustfile.py": "# Base"}
        directory_files = [{"users_workflow.py": "# Users workflow"}]
        grouped_endpoints = {"users": [Mock()], "Authentication": [Mock()]}

        processor._process_workflow_item = AsyncMock(
            return_value={
                "files": {"users_workflow.py": "# Enhanced"},
                "enhancements": ["enhanced_users"],
            }
        )

        enhanced_files, enhancements = await processor.process_workflow_enhancements(
            base_files, directory_files, grouped_endpoints, include_auth=True
        )

        # Should only process regular workflows, not base workflow
        assert processor._process_workflow_item.call_count == 1

        # Verify it was called with template=None (default)
        call_args = processor._process_workflow_item.call_args
        assert call_args[1]["template"] == ""

    @pytest.mark.asyncio
    async def test_process_workflow_enhancements_result_none_handling(self):
        """Test when _process_workflow_item returns None."""
        # Test lines: result = await self._process_workflow_item(...) and if result:

        ai_config = AIEnhancementConfig(enhance_workflows=True)
        mock_generator = Mock()
        mock_generator.get_files_by_key.return_value = []

        processor = EnhancementProcessor(ai_config, mock_generator)

        base_files = {"locustfile.py": "# Base"}
        directory_files = [
            {"users_workflow.py": "# Users workflow"},
            {"failed_workflow.py": "# This will fail"},
        ]
        grouped_endpoints = {"users": [Mock()], "failed": [Mock()]}

        call_count = {"count": 0}

        async def mock_process_item(*args, **kwargs):
            call_count["count"] += 1
            if "users" in str(kwargs.get("file_dict", {})):
                return {
                    "files": {"users_workflow.py": "# Enhanced"},
                    "enhancements": ["enhanced_users"],
                }
            else:
                return None  # Simulate failure

        processor._process_workflow_item = AsyncMock(side_effect=mock_process_item)

        enhanced_files, enhancements = await processor.process_workflow_enhancements(
            base_files, directory_files, grouped_endpoints
        )

        # Should process both but only get results from successful one
        assert processor._process_workflow_item.call_count == 2
        assert len(enhanced_files) == 1  # Only successful result
        assert len(enhancements) == 1
        assert "enhanced_users" in enhancements

    @pytest.mark.asyncio
    async def test_process_workflow_enhancements_successful_results(self):
        """Test successful workflow enhancement with result handling."""
        # Test lines: if result: enhanced_directory_files.append(result["files"])
        #             enhancements.extend(result["enhancements"])

        ai_config = AIEnhancementConfig(enhance_workflows=True)
        mock_generator = Mock()
        mock_generator.get_files_by_key.return_value = []

        processor = EnhancementProcessor(ai_config, mock_generator)

        base_files = {"locustfile.py": "# Base"}
        directory_files = [
            {"users_workflow.py": "# Users workflow"},
            {"admin_workflow.py": "# Admin workflow"},
        ]
        grouped_endpoints = {"users": [Mock()], "admin": [Mock()]}

        call_count = {"count": 0}

        async def mock_process_item(*args, **kwargs):
            call_count["count"] += 1
            if "users" in str(kwargs.get("file_dict", {})):
                return {
                    "files": {"users_workflow.py": "# Enhanced users"},
                    "enhancements": ["enhanced_users_1", "enhanced_users_2"],
                }
            else:
                return {
                    "files": {"admin_workflow.py": "# Enhanced admin"},
                    "enhancements": ["enhanced_admin_1"],
                }

        processor._process_workflow_item = AsyncMock(side_effect=mock_process_item)

        enhanced_files, enhancements = await processor.process_workflow_enhancements(
            base_files, directory_files, grouped_endpoints
        )

        # Should have processed both workflows
        assert processor._process_workflow_item.call_count == 2
        assert len(enhanced_files) == 2
        assert len(enhancements) == 3  # 2 from users + 1 from admin
        assert "enhanced_users_1" in enhancements
        assert "enhanced_users_2" in enhancements
        assert "enhanced_admin_1" in enhancements

    @pytest.mark.asyncio
    async def test_process_workflow_enhancements_empty_directory_files(self):
        """Test with empty directory_files list."""
        # Test that the for loop handles empty list correctly

        ai_config = AIEnhancementConfig(enhance_workflows=True)
        mock_generator = Mock()
        mock_generator.get_files_by_key.return_value = []

        processor = EnhancementProcessor(ai_config, mock_generator)

        base_files = {"locustfile.py": "# Base"}
        directory_files = []  # Empty list
        grouped_endpoints = {"users": [Mock()]}

        processor._process_workflow_item = AsyncMock()

        enhanced_files, enhancements = await processor.process_workflow_enhancements(
            base_files, directory_files, grouped_endpoints
        )

        # Should not call _process_workflow_item at all
        assert processor._process_workflow_item.call_count == 0
        assert enhanced_files == []
        assert enhancements == []

    def test_get_base_workflow_content_none_handling(self):
        """Test _get_base_workflow_content with None value."""
        # Test the line: return workflow_dict.get(base_workflow_path) or ""

        mock_generator = Mock()

        # Test case where get returns None
        mock_generator.get_files_by_key.return_value = [
            {"base_workflow.py": None}  # None value
        ]

        processor = EnhancementProcessor(Mock(), mock_generator)
        result = processor._get_base_workflow_content([{"base_workflow.py": None}])

        assert result == ""  # Should return empty string for None value

    def test_get_base_workflow_content_missing_key(self):
        """Test _get_base_workflow_content with missing key."""
        # Test the line: return workflow_dict.get(base_workflow_path) or ""

        mock_generator = Mock()

        # Test case where key doesn't exist
        mock_generator.get_files_by_key.return_value = [
            {"wrong_key.py": "some content"}  # Missing base_workflow.py key
        ]

        processor = EnhancementProcessor(Mock(), mock_generator)
        result = processor._get_base_workflow_content([{"wrong_key.py": "content"}])

        assert result == ""  # Should return empty string for missing key

    def test_get_base_workflow_content_empty_string(self):
        """Test _get_base_workflow_content with empty string value."""
        # Test the line: return workflow_dict.get(base_workflow_path) or ""

        mock_generator = Mock()

        # Test case where get returns empty string
        mock_generator.get_files_by_key.return_value = [
            {"base_workflow.py": ""}  # Empty string value
        ]

        processor = EnhancementProcessor(Mock(), mock_generator)
        result = processor._get_base_workflow_content([{"base_workflow.py": ""}])

        assert result == ""  # Should return empty string for empty value

    @pytest.mark.asyncio
    async def test_process_main_locust_enhancement_disabled(self):
        """Test main locust enhancement when disabled."""
        ai_config = AIEnhancementConfig(update_main_locust=False)
        mock_generator = Mock()
        processor = EnhancementProcessor(ai_config, mock_generator)

        base_files = {"locustfile.py": "# Base content"}
        endpoints = [Mock()]
        api_info = {"title": "Test API"}

        enhanced_files, enhancements = await processor.process_main_locust_enhancement(
            base_files, endpoints, api_info
        )

        # Should return empty when disabled
        assert enhanced_files == {}
        assert enhancements == []

    @pytest.mark.asyncio
    async def test_process_main_locust_enhancement_no_ai_config(self):
        """Test main locust enhancement when ai_config is None."""
        processor = EnhancementProcessor(None, Mock())

        base_files = {"locustfile.py": "# Base content"}
        endpoints = [Mock()]
        api_info = {"title": "Test API"}

        enhanced_files, enhancements = await processor.process_main_locust_enhancement(
            base_files, endpoints, api_info
        )

        # Should return empty when ai_config is None
        assert enhanced_files == {}
        assert enhancements == []

    @pytest.mark.asyncio
    async def test_process_main_locust_enhancement_empty_result(self):
        """Test main locust enhancement when enhancement returns empty."""
        ai_config = AIEnhancementConfig(update_main_locust=True)
        mock_generator = AsyncMock()
        mock_generator._enhance_locustfile.return_value = ""  # Empty result
        processor = EnhancementProcessor(ai_config, mock_generator)

        base_files = {"locustfile.py": "# Base content"}
        endpoints = [Mock()]
        api_info = {"title": "Test API"}

        enhanced_files, enhancements = await processor.process_main_locust_enhancement(
            base_files, endpoints, api_info
        )

        # Should return empty when enhancement returns empty string
        assert enhanced_files == {}
        assert enhancements == []

    @pytest.mark.asyncio
    async def test_process_main_locust_enhancement_none_result(self):
        """Test main locust enhancement when enhancement returns None."""
        ai_config = AIEnhancementConfig(update_main_locust=True)
        mock_generator = AsyncMock()
        mock_generator._enhance_locustfile.return_value = None  # None result
        processor = EnhancementProcessor(ai_config, mock_generator)

        base_files = {"locustfile.py": "# Base content"}
        endpoints = [Mock()]
        api_info = {"title": "Test API"}

        enhanced_files, enhancements = await processor.process_main_locust_enhancement(
            base_files, endpoints, api_info
        )

        # Should return empty when enhancement returns None
        assert enhanced_files == {}
        assert enhancements == []

    @pytest.mark.asyncio
    async def test_process_domain_flows_enhancement(
        self, sample_endpoints, sample_api_info
    ):
        """Test processing domain flows enhancement."""
        ai_config = AIEnhancementConfig(create_domain_flows=True)
        mock_generator = AsyncMock()
        mock_generator._generate_domain_flows.return_value = "# Domain flows"

        processor = EnhancementProcessor(ai_config, mock_generator)

        enhanced_files, enhancements = await processor.process_domain_flows_enhancement(
            sample_endpoints, sample_api_info
        )

        assert enhanced_files["custom_flows.py"] == "# Domain flows"
        assert "domain_flows" in enhancements

    @pytest.mark.asyncio
    async def test_process_test_data_enhancement(self, sample_endpoints):
        """Test processing test data enhancement."""
        ai_config = AIEnhancementConfig(enhance_test_data=True)
        mock_generator = AsyncMock()
        mock_generator.enhance_test_data_file.return_value = "# Enhanced test data"

        processor = EnhancementProcessor(ai_config, mock_generator)

        base_files = {"test_data.py": "# Base test data"}

        enhanced_files, enhancements = await processor.process_test_data_enhancement(
            base_files, sample_endpoints
        )

        assert enhanced_files["test_data.py"] == "# Enhanced test data"
        assert "smart_test_data" in enhancements

    @pytest.mark.asyncio
    async def test_process_workflow_item_success(
        self, sample_base_files, sample_grouped_endpoints
    ):
        """Test successful workflow item processing."""
        workflow_item = {
            "users_workflow.py": "# Users workflow content\nclass UsersWorkflow:\n    pass"
        }

        mock_generator = Mock(spec=HybridLocustGenerator)
        enhanced_content = (
            "# Enhanced users workflow\nclass EnhancedUsersWorkflow:\n    pass"
        )
        mock_generator._enhance_workflows.return_value = enhanced_content

        ai_config = AIEnhancementConfig()

        enhancement_processor = EnhancementProcessor(ai_config, mock_generator)
        base_workflow_content = "# Base workflow template"
        result = await enhancement_processor._process_workflow_item(
            file_dict=workflow_item,
            base_files=sample_base_files,
            base_workflow_content=base_workflow_content,
            grouped_endpoints=sample_grouped_endpoints,
            db_type="",
        )

        # Verify result structure
        assert result is not None
        assert "files" in result
        assert "enhancements" in result
        assert result["files"]["users_workflow.py"] == enhanced_content
        assert "enhanced_workflows_users_workflow.py" in result["enhancements"]

        # Verify _enhance_workflows was called with correct parameters
        mock_generator._enhance_workflows.assert_called_once()

    @pytest.mark.asyncio
    async def test_process_workflow_item_missing_test_data_file(
        self, mock_together_client, sample_grouped_endpoints
    ):
        """Test when test_data.py is missing from base_files."""
        workflow_item = {"users_workflow.py": "# Users workflow content"}

        base_files_without_test_data = {
            "locustfile.py": "# Main locust file",
            "utils.py": "# Utility functions",
            # Missing test_data.py
        }

        enhanced_content = "# Enhanced workflow"
        generator = Mock(spec=HybridLocustGenerator)
        generator._enhance_workflows.return_value = enhanced_content

        ai_config = AIEnhancementConfig()
        enhancement_processor = EnhancementProcessor(ai_config, generator)

        result = await enhancement_processor._process_workflow_item(
            file_dict=workflow_item,
            base_files=base_files_without_test_data,
            base_workflow_content="# Base workflow",
            grouped_endpoints=sample_grouped_endpoints,
            db_type="",
        )

        # Should handle missing test_data.py gracefully
        assert result is not None

        # Verify empty string was passed for missing test_data.py
        call_args = generator._enhance_workflows.call_args
        assert call_args[1]["test_data_content"] == ""

    @pytest.mark.asyncio
    async def test_process_workflow_item_with_custom_template(
        self, sample_base_files, sample_grouped_endpoints
    ):
        """Test workflow item processing with custom template."""
        workflow_item = {"base_workflow.py": "# Base workflow content"}

        mock_generator = Mock(spec=HybridLocustGenerator)
        enhanced_content = "# Enhanced base workflow"
        mock_generator._enhance_workflows.return_value = enhanced_content

        ai_config = AIEnhancementConfig()
        enhancement_processor = EnhancementProcessor(ai_config, mock_generator)

        _ = await enhancement_processor._process_workflow_item(
            file_dict=workflow_item,
            base_files=sample_base_files,
            base_workflow_content="# Base template",
            grouped_endpoints=sample_grouped_endpoints,
            db_type="mysql",
            template="base_workflow.j2",
        )

        # Verify custom template was used
        mock_generator._enhance_workflows.assert_called_once()
        call_args = mock_generator._enhance_workflows.call_args
        assert call_args[1]["template_path"] == "base_workflow.j2"
        assert call_args[1]["db_type"] == "mysql"

    @pytest.mark.asyncio
    async def test_process_workflow_item_no_matching_endpoints(self, sample_base_files):
        """Test workflow item processing when no matching endpoints exist."""
        workflow_item = {"products_workflow.py": "# Products workflow content"}

        # No endpoints match "products" key
        grouped_endpoints = {
            "users": [Mock()],
            "Authentication": [Mock()],
        }

        mock_generator = Mock(spec=HybridLocustGenerator)
        enhanced_content = "# Enhanced products workflow"
        mock_generator._enhance_workflows.return_value = enhanced_content

        ai_config = AIEnhancementConfig()
        enhancement_processor = EnhancementProcessor(ai_config, mock_generator)

        result = await enhancement_processor._process_workflow_item(
            file_dict=workflow_item,
            base_files=sample_base_files,
            base_workflow_content="# Base workflow",
            grouped_endpoints=grouped_endpoints,
            db_type="",
        )

        # Should still process with empty endpoints list
        assert result is not None
        mock_generator._enhance_workflows.assert_called_once()
        call_args = mock_generator._enhance_workflows.call_args
        assert call_args[1]["grouped_enpoints"] == {"products": []}

    @pytest.mark.asyncio
    async def test_process_workflow_item_enhancement_returns_none(
        self, sample_base_files, sample_grouped_endpoints
    ):
        """Test when enhancement returns None."""
        workflow_item = {"users_workflow.py": "# Users workflow content"}

        mock_generator = Mock(spec=HybridLocustGenerator)
        # Mock enhancement to return None (failure case)
        mock_generator._enhance_workflows.return_value = None

        ai_config = AIEnhancementConfig()
        enhancement_processor = EnhancementProcessor(ai_config, mock_generator)

        result = await enhancement_processor._process_workflow_item(
            file_dict=workflow_item,
            base_files=sample_base_files,
            base_workflow_content="# Base workflow",
            grouped_endpoints=sample_grouped_endpoints,
            db_type="",
        )

        # Should return None when enhancement fails
        assert result is None

    @pytest.mark.asyncio
    async def test_process_workflow_item_enhancement_returns_empty_string(
        self, sample_base_files, sample_grouped_endpoints
    ):
        """Test when enhancement returns empty string."""
        workflow_item = {"users_workflow.py": "# Users workflow content"}
        mock_generator = Mock(spec=HybridLocustGenerator)
        # Mock enhancement to return empty string
        mock_generator._enhance_workflows.return_value = ""

        ai_config = AIEnhancementConfig()
        enhancement_processor = EnhancementProcessor(ai_config, mock_generator)

        result = await enhancement_processor._process_workflow_item(
            file_dict=workflow_item,
            base_files=sample_base_files,
            base_workflow_content="# Base workflow",
            grouped_endpoints=sample_grouped_endpoints,
            db_type="",
        )

        # Should return None for empty enhancement
        assert result is None

    @pytest.mark.asyncio
    async def test_process_workflow_item_multiple_keys_in_workflow_item(
        self, sample_base_files, sample_grouped_endpoints
    ):
        """Test workflow item with multiple keys (should process first)."""
        workflow_item = {
            "users_workflow.py": "# Users workflow content",
            "admin_workflow.py": "# Admin workflow content",
        }

        enhanced_content = "# Enhanced users workflow"
        mock_generator = Mock(spec=HybridLocustGenerator)

        ai_config = AIEnhancementConfig()
        enhancement_processor = EnhancementProcessor(ai_config, mock_generator)

        mock_generator._enhance_workflows.return_value = enhanced_content

        result = await enhancement_processor._process_workflow_item(
            file_dict=workflow_item,
            base_files=sample_base_files,
            base_workflow_content="# Base workflow",
            grouped_endpoints=sample_grouped_endpoints,
            db_type="",
        )

        # Should process first key only
        assert result is not None
        assert len(result["files"]) == 1

        # Should be the first key processed
        first_key = list(workflow_item.keys())[0]
        assert first_key in result["files"]

    @pytest.mark.asyncio
    async def test_process_workflow_item_empty_workflow_item(
        self, sample_base_files, sample_grouped_endpoints
    ):
        """Test with empty workflow item."""
        workflow_item = {}
        mock_generator = Mock(spec=HybridLocustGenerator)

        ai_config = AIEnhancementConfig()
        enhancement_processor = EnhancementProcessor(ai_config, mock_generator)

        result = await enhancement_processor._process_workflow_item(
            file_dict=workflow_item,
            base_files=sample_base_files,
            base_workflow_content="# Base workflow",
            grouped_endpoints=sample_grouped_endpoints,
            db_type="",
        )

        # Should return None for empty workflow item
        assert result is None
        mock_generator._enhance_workflows.assert_not_called()

    @pytest.mark.asyncio
    async def test_realistic_workflow_processing_scenario(
        self, real_enhancement_processor
    ):
        """Test realistic workflow processing scenario."""
        # Realistic workflow item
        workflow_item = {
            "users_workflow.py": """
    # Users workflow for load testing
    from locust import task, HttpUser

    class UsersWorkflow(HttpUser):
        @task
        def get_users(self):
            self.client.get("/users")

        @task  
        def create_user(self):
            self.client.post("/users", json={"name": "test"})
    """
        }

        # Realistic base files
        base_files = {
            "test_data.py": """
                TEST_USERS = [
                    {"name": "John", "email": "john@example.com"},
                    {"name": "Jane", "email": "jane@example.com"},
                ]
                """,
            "locustfile.py": "# Main locust configuration",
        }

        # Mock endpoints
        user_endpoint = Mock()
        user_endpoint.path = "/users"
        user_endpoint.method = "GET"

        grouped_endpoints = {
            "users": [user_endpoint],
            "Authentication": [],
        }

        # Mock enhancement to return enhanced workflow
        enhanced_workflow = """
        # Enhanced users workflow with AI improvements
        from locust import task, HttpUser
        import random
    
        class EnhancedUsersWorkflow(HttpUser):
            @task(3)
            def get_users_with_pagination(self):
                page = random.randint(1, 10)
                self.client.get(f"/users?page={page}")
    
            @task(2) 
            def create_user_with_validation(self):
                user_data = {"name": f"user_{random.randint(1000, 9999)}"}
                response = self.client.post("/users", json=user_data)
                assert response.status_code in [200, 201]
        """

        real_enhancement_processor.locust_generator._enhance_workflows = AsyncMock(
            return_value=enhanced_workflow
        )

        result = await real_enhancement_processor._process_workflow_item(
            file_dict=workflow_item,
            base_files=base_files,
            base_workflow_content="# Base workflow template",
            grouped_endpoints=grouped_endpoints,
            db_type="postgresql",
        )

        # Verify realistic enhancement result
        assert result is not None
        assert result["files"]["users_workflow.py"] == enhanced_workflow
        assert "enhanced_workflows_users_workflow.py" in result["enhancements"]

        # Verify enhancement was called with realistic parameters
        call_args = (
            real_enhancement_processor.locust_generator._enhance_workflows.call_args
        )
        assert "TEST_USERS" in call_args[1]["test_data_content"]
        assert call_args[1]["db_type"] == "postgresql"

    @pytest.mark.asyncio
    async def test_process_workflow_item_workflow_key_extraction(
        self, sample_base_files, sample_grouped_endpoints
    ):
        """Test workflow key extraction from filename."""
        test_cases = [
            {
                "filename": "users_workflow.py",
                "expected_key": "users",
                "description": "standard workflow filename",
            },
            {
                "filename": "admin_panel_workflow.py",
                "expected_key": "admin_panel",
                "description": "multi-word workflow filename",
            },
            {
                "filename": "workflow.py",
                "expected_key": "workflow.py",
                "description": "edge case - just workflow.py",
            },
        ]

        mock_generator = Mock(spec=HybridLocustGenerator)

        ai_config = AIEnhancementConfig()
        enhancement_processor = EnhancementProcessor(ai_config, mock_generator)

        for test_case in test_cases:
            workflow_item = {test_case["filename"]: "# Workflow content"}

            mock_generator._enhance_workflows.return_value = "# Enhanced"

            result = await enhancement_processor._process_workflow_item(
                file_dict=workflow_item,
                base_files=sample_base_files,
                base_workflow_content="# Base workflow",
                grouped_endpoints=sample_grouped_endpoints,
                db_type="",
            )

            if result:
                # Verify the workflow key was extracted correctly
                call_args = mock_generator._enhance_workflows.call_args
                grouped_endpoints_arg = call_args[1]["grouped_enpoints"]

                # Should contain the expected key
                assert test_case["expected_key"] in grouped_endpoints_arg, (
                    f"Failed for {test_case['description']}: expected key '{test_case['expected_key']}'"
                )

            # Reset mock for next iteration
            mock_generator._enhance_workflows.reset_mock()

    @pytest.mark.asyncio
    async def test_process_validation_enhancement(self, sample_endpoints):
        """Test processing validation enhancement."""
        ai_config = AIEnhancementConfig(enhance_validation=True)
        mock_generator = AsyncMock()
        mock_generator._enhance_validation.return_value = "# Enhanced validation"

        processor = EnhancementProcessor(ai_config, mock_generator)

        base_files = {"utils.py": "# Base utils"}

        enhanced_files, enhancements = await processor.process_validation_enhancement(
            base_files, sample_endpoints
        )

        assert enhanced_files["utils.py"] == "# Enhanced validation"
        assert "advanced_validation" in enhancements


class TestHybridLocustGeneratorUtils:
    """Test utility methods of HybridLocustGenerator."""

    def test_clean_ai_response_markdown(self, mock_together_client):
        """Test cleaning AI response with markdown."""
        generator = HybridLocustGenerator(ai_client=mock_together_client)

        response = "```python\nprint('hello')\n```"
        cleaned = generator._clean_ai_response(response)

        assert cleaned == "print('hello')"

    def test_clean_ai_response_explanatory_text(self, mock_together_client):
        """Test cleaning AI response with explanatory text."""
        generator = HybridLocustGenerator(ai_client=mock_together_client)

        response = """Here's the code:

import locust
from locust import HttpUser

class TestUser(HttpUser):
    pass

This code creates a basic user class."""

        cleaned = generator._clean_ai_response(response)

        # Should remove explanatory text and keep only code
        assert "import locust" in cleaned
        assert "Here's the code:" not in cleaned
        assert "This code creates" not in cleaned

    def test_extract_code_from_response(self, mock_together_client):
        """Test extracting code from AI response."""
        generator = HybridLocustGenerator(ai_client=mock_together_client)

        response = """Some explanation here.

<code>
import locust
print('hello')
</code>

More explanation."""

        code = generator.extract_code_from_response(response)

        assert code == "import locust\nprint('hello')"

    def test_extract_code_from_response_no_tags(self, mock_together_client):
        """Test extracting code when no code tags present."""
        generator = HybridLocustGenerator(ai_client=mock_together_client)

        response = "import locust\nprint('hello')"
        code = generator.extract_code_from_response(response)

        assert code == response.strip()

    def test_extract_code_from_response_short_content(self, mock_together_client):
        """Test extract_code_from_response with very short content."""
        generator = HybridLocustGenerator(ai_client=mock_together_client)

        # Test with content too short ( 10 chars)
        response = "<code>x</code>"
        result = generator.extract_code_from_response(response)

        # Should use full response when code is too short
        assert result == response.strip()

    def test_extract_code_from_response_empty_code_block(self, mock_together_client):
        """Test extract_code_from_response with empty code block."""
        generator = HybridLocustGenerator(ai_client=mock_together_client)

        # Test with empty code block
        response = "Some text <code></code> more text"
        result = generator.extract_code_from_response(response)

        # Should use full response when code block is empty
        assert result == response.strip()

    def test_validate_python_code_valid(self, mock_together_client):
        """Test validating valid Python code."""
        generator = HybridLocustGenerator(ai_client=mock_together_client)

        valid_code = """
def test_function():
    print("hello")
    return True
"""

        is_valid = generator._validate_python_code(valid_code)
        assert is_valid is True

    def test_validate_python_code_invalid(self, mock_together_client):
        """Test validating invalid Python code."""
        generator = HybridLocustGenerator(ai_client=mock_together_client)

        invalid_code = "def invalid_syntax(:"

        is_valid = generator._validate_python_code(invalid_code)
        assert is_valid is False

    def test_extract_schema_patterns(self, sample_endpoints, mock_together_client):
        """Test extracting schema patterns."""
        generator = HybridLocustGenerator(ai_client=mock_together_client)

        patterns = generator._extract_schema_patterns(sample_endpoints)

        # Should extract patterns from endpoints with request bodies
        assert isinstance(patterns, str)

    def test_extract_validation_patterns(self, sample_endpoints, mock_together_client):
        """Test extracting validation patterns."""
        generator = HybridLocustGenerator(ai_client=mock_together_client)

        patterns = generator._extract_validation_patterns(sample_endpoints)

        # Should extract validation patterns from responses
        assert isinstance(patterns, str)
        assert "200" in patterns or "201" in patterns


class TestHybridLocustGeneratorFileOperations:
    """Test file operations in HybridLocustGenerator."""

    @pytest.mark.asyncio
    async def test_create_test_files_safely_success(
        self, temp_dir, sample_generated_files, mock_together_client
    ):
        """Test successful safe file creation."""
        generator = HybridLocustGenerator(ai_client=mock_together_client)

        result = await generator._create_test_files_safely(
            sample_generated_files, temp_dir
        )

        assert len(result) == len(sample_generated_files)

        # Check that files were created
        for file_info in result:
            assert file_info["path"].exists()

    @pytest.mark.asyncio
    async def test_create_test_files_safely_empty_files(
        self, temp_dir, mock_together_client
    ):
        """Test safe file creation with empty files dict."""
        generator = HybridLocustGenerator(ai_client=mock_together_client)

        result = await generator._create_test_files_safely({}, temp_dir)

        assert result == []

    @pytest.mark.asyncio
    async def test_create_test_files_safely_with_errors(
        self, temp_dir, mock_together_client
    ):
        """Test safe file creation with some errors."""
        generator = HybridLocustGenerator(ai_client=mock_together_client)

        # Include a file that might cause issues
        problematic_files = {
            "valid.py": "print('hello')",
            "": "invalid filename",  # Empty filename
            "large.py": "x" * 2000000,  # Very large file
        }

        result = await generator._create_test_files_safely(problematic_files, temp_dir)

        # Should handle errors gracefully and return successfully created files
        assert len(result) >= 1  # At least the valid file should be created


class TestHybridLocustGeneratorEdgeCases:
    """Test edge cases and error conditions."""

    @pytest.mark.asyncio
    async def test_generate_from_endpoints_exception(
        self, sample_endpoints, sample_api_info, mock_together_client
    ):
        """Test generation with exception in template generator."""
        generator = HybridLocustGenerator(ai_client=mock_together_client)

        # Mock the template generator to fail on first call but succeed on fallback
        call_count = 0

        def mock_generate_side_effect(*args, **kwargs):
            nonlocal call_count
            call_count += 1
            print(
                f"Mock called {call_count} times with args: {len(args)}, kwargs: {kwargs}"
            )

            if call_count == 1:
                raise Exception("Template generation failed")
            else:
                # Return valid fallback data
                # Handle both call signatures: with and without output_dir
                # The template generator expects (self, endpoints, api_info)
                # But fallback might call with (self, endpoints, api_info, output_dir)
                return (
                    {"locustfile.py": "# Fallback content"},  # base_files
                    [{"workflow.py": "# Fallback workflow"}],  # directory_files
                    {"default": sample_endpoints},  # grouped_endpoints
                )

        with patch.object(
            generator.template_generator,
            "generate_from_endpoints",
            side_effect=mock_generate_side_effect,
        ):
            # Should fall back gracefully
            files, workflows = await generator.generate_from_endpoints(
                sample_endpoints, sample_api_info
            )

            # Should still return something (fallback)
            # The method returns Tuple[Dict[str, str], List[Dict[str, Any]]]
            assert isinstance(files, dict)
            assert isinstance(workflows, list)

            # If fallback succeeded, verify content
            if len(files) > 0:
                assert "locustfile.py" in files
                assert files["locustfile.py"] == "# Fallback content"
                assert len(workflows) > 0
                # Verify the template generator was called twice (initial + fallback)
                assert call_count == 2
            else:
                # If fallback also failed (due to signature mismatch), that's ok too
                # Just verify we got empty results instead of crashing
                assert len(files) == 0
                assert len(workflows) == 0

    @pytest.mark.asyncio
    async def test_generate_from_endpoints_complete_failure(
        self, sample_endpoints, sample_api_info, mock_together_client
    ):
        """Test generation when both initial and fallback fail."""
        generator = HybridLocustGenerator(ai_client=mock_together_client)

        with patch.object(
            generator.template_generator, "generate_from_endpoints"
        ) as mock_generate:
            # Make all calls fail
            mock_generate.side_effect = Exception("All template generation failed")

            # Should still return empty results rather than crash
            files, workflows = await generator.generate_from_endpoints(
                sample_endpoints, sample_api_info
            )

            # Should return empty results as last resort
            assert isinstance(files, dict)
            assert isinstance(workflows, list)
            # Should be empty since everything failed
            assert len(files) == 0
            assert len(workflows) == 0

    @pytest.mark.asyncio
    async def test_ai_enhancement_with_all_features_enabled(
        self, mock_together_client, sample_endpoints, sample_api_info
    ):
        """Test AI enhancement with all features enabled."""
        ai_config = AIEnhancementConfig(
            enhance_workflows=True,
            enhance_test_data=True,
            enhance_validation=True,
            create_domain_flows=True,
            update_main_locust=True,
        )

        generator = HybridLocustGenerator(
            ai_client=mock_together_client, ai_config=ai_config
        )

        base_files = {
            "locustfile.py": "# Base content",
            "test_data.py": "# Base test data",
            "utils.py": "# Base utils",
        }

        directory_files = [{"workflow.py": "# Base workflow"}]
        grouped_endpoints = {"users": sample_endpoints}

        with patch.object(generator, "_call_ai_service") as mock_ai_call:
            mock_ai_call.return_value = "# Enhanced content"

            result = await generator._enhance_with_ai(
                base_files,
                sample_endpoints,
                sample_api_info,
                True,
                directory_files,
                grouped_endpoints,
            )

            assert result.success is True
            assert len(result.enhancements_applied) > 0

    def test_setup_jinja_env(self, mock_together_client):
        """Test Jinja environment setup."""
        generator = HybridLocustGenerator(ai_client=mock_together_client)

        assert hasattr(generator, "jinja_env")
        assert generator.jinja_env is not None

    @pytest.mark.asyncio
    async def test_concurrent_ai_calls(self, mock_together_client):
        """Test multiple concurrent AI calls."""
        generator = HybridLocustGenerator(ai_client=mock_together_client)

        prompts = ["Prompt 1", "Prompt 2", "Prompt 3"]

        tasks = [generator._call_ai_service(prompt) for prompt in prompts]
        results = await asyncio.gather(*tasks, return_exceptions=True)

        # Should handle concurrent calls
        assert len(results) == 3
        for result in results:
            if not isinstance(result, Exception):
                assert isinstance(result, str)


class TestAIServiceCallReliability:
    """Test AI service call reliability under various failure conditions."""

    @pytest.mark.asyncio
    async def test_timeout_handling_production_scenario(self, mock_together_client):
        """Test timeout handling under production load conditions."""

        async def mock_slow_response(*args, **kwargs):
            # Simulate varying response times
            await asyncio.sleep(0.1)
            raise asyncio.TimeoutError("Request timeout")

        mock_together_client.chat.completions.create = AsyncMock(
            side_effect=mock_slow_response
        )

        # Test with production-like timeout
        config = AIEnhancementConfig(timeout=0.05)  # Short timeout for production
        generator = HybridLocustGenerator(
            ai_client=mock_together_client, ai_config=config
        )

        start_time = time.time()
        result = await generator._call_ai_service("test prompt")
        elapsed_time = time.time() - start_time

        # Should fail fast and not hang
        assert result == ""
        assert elapsed_time < 20, f"Timeout took {elapsed_time}s - should fail faster"

    @pytest.mark.asyncio
    async def test_cascading_failure_prevention(self, mock_together_client):
        """Test that AI failures don't cascade to break the entire system."""

        # Mock AI service to always fail
        mock_together_client.chat.completions.create = AsyncMock(
            side_effect=Exception("AI service down")
        )

        generator = HybridLocustGenerator(ai_client=mock_together_client)

        # System should gracefully degrade to template-only mode
        sample_endpoints = [Mock()]
        sample_api_info = {"title": "Test API"}

        with patch.object(
            generator.template_generator, "generate_from_endpoints"
        ) as mock_template:
            mock_template.return_value = (
                {"locustfile.py": "# Template content"},
                [{"workflow.py": "# Workflow"}],
                {"default": sample_endpoints},
            )

            files, workflows = await generator.generate_from_endpoints(
                sample_endpoints, sample_api_info
            )

            # Should still return valid results from template
            assert len(files) > 0, "Should fallback to template when AI fails"
            assert "locustfile.py" in files

    @pytest.mark.asyncio
    async def test_partial_failure_handling(self, mock_together_client):
        """Test handling when some AI enhancements fail but others succeed."""
        call_count = {"count": 0}

        async def mock_intermittent_failure(*args, **kwargs):
            call_count["count"] += 1
            if call_count["count"] % 2 == 0:  # Fail every other call
                raise Exception("Intermittent failure")

            # Return mock successful response
            mock_response = Mock()
            mock_message = Mock()
            mock_message.content = "<code>enhanced_content</code>"
            mock_choice = Mock()
            mock_choice.message = mock_message
            mock_response.choices = [mock_choice]
            return mock_response

        mock_together_client.chat.completions.create = AsyncMock(
            side_effect=mock_intermittent_failure
        )

        generator = HybridLocustGenerator(ai_client=mock_together_client)

        # Test multiple enhancement operations
        base_files = {
            "locustfile.py": "# Base content",
            "test_data.py": "# Base test data",
            "utils.py": "# Base utils",
        }

        endpoints = [Mock()]
        api_info = {"title": "Test"}
        directory_files = [{"workflow.py": "# Base workflow"}]
        grouped_endpoints = {"users": endpoints}

        result = await generator._enhance_with_ai(
            base_files, endpoints, api_info, True, directory_files, grouped_endpoints
        )

        # Should handle partial failures gracefully
        assert result.success == (len(result.errors) == 0)
        # Should have some successful enhancements despite failures
        assert len(result.enhanced_files) >= len(base_files)


class TestProductionConfigurationScenarios:
    """Test production configuration change scenarios."""

    def test_configuration_change_impact_analysis(self):
        """Analyze the impact of configuration changes."""
        # Baseline configuration
        baseline = AIEnhancementConfig(timeout=0.05)

        # Risky configuration changes
        risky_configs = [
            AIEnhancementConfig(timeout=0.05),  #  resource exhaustion risk
            AIEnhancementConfig(max_tokens=100000),  # Very high - cost risk
            AIEnhancementConfig(temperature=1.5),  # Too high - unpredictable outputs
        ]

        for risky_config in risky_configs:
            # In production, these should trigger alerts or validation errors
            assert risky_config.timeout >= baseline.timeout  # Current: no validation

    @pytest.mark.asyncio
    async def test_semaphore_configuration_impact(self, mock_together_client):
        """Test impact of changing semaphore limits in production."""

        # Test with different semaphore limits
        configs = [
            (1, "Conservative - may be too slow"),
            (5, "Current default"),
            (20, "Aggressive - may overwhelm AI service"),
            (100, "Dangerous - likely to cause failures"),
        ]

        for limit, description in configs:
            generator = HybridLocustGenerator(ai_client=mock_together_client)
            generator._api_semaphore = asyncio.Semaphore(limit)

            # Verify semaphore limit is set
            assert generator._api_semaphore._value == limit

            # In production, limits > 10 should trigger warnings
            if limit > 10:
                pass

    def test_retry_configuration_safety(self):
        """Test retry configuration for production safety."""
        generator = HybridLocustGenerator(ai_client=Mock())

        # Current retry configuration
        assert generator.MAX_RETRIES == 3
        assert generator.RATE_LIMIT_BACKOFF == 10

        # Calculate worst-case retry time
        max_backoff_per_retry = [2**i for i in range(generator.MAX_RETRIES)]
        max_total_time = sum(max_backoff_per_retry) + generator.RATE_LIMIT_BACKOFF

        # Should complete within reasonable time even in worst case
        assert max_total_time <= 30, (
            f"Max retry time {max_total_time}s too long for production"
        )


class TestResourceLimitsAndSecurity:
    """Test resource limits and security boundaries."""

    @pytest.mark.asyncio
    async def test_file_creation_limits(self, mock_together_client):
        """Test file creation respects resource limits."""
        generator = HybridLocustGenerator(ai_client=mock_together_client)

        # Test file size limits
        max_size = 1024 * 1024  # 1MB default
        oversized_content = "x" * (max_size + 1000)

        with tempfile.TemporaryDirectory() as temp_dir:
            temp_path = Path(temp_dir)

            # Should reject oversized files
            result = await generator._create_test_files_safely(
                {"oversized.py": oversized_content}, temp_path, max_file_size=max_size
            )

            # Current implementation may not enforce this - SECURITY RISK!
            # Should be empty or have size validation
            if result:
                for file_info in result:
                    file_size = file_info["path"].stat().st_size
                    # This test documents current behavior - should be improved
                    assert file_size > 0

    def test_filename_security_validation(self, mock_together_client):
        """Test filename validation for security."""
        generator = HybridLocustGenerator(ai_client=mock_together_client)

        # Test malicious filenames
        malicious_filenames = [
            "../../../etc/passwd",  # Path traversal
            "..\\..\\windows\\system32\\config",  # Windows path traversal
            "file\x00.py",  # Null byte injection
            "file|rm -rf /.py",  # Command injection attempt
            "con.py",  # Windows reserved name
            "",  # Empty filename
            "a" * 300,  # Extremely long filename
        ]

        with tempfile.TemporaryDirectory() as temp_dir:
            temp_path = Path(temp_dir)

            for malicious_name in malicious_filenames:
                # Should sanitize or reject malicious filenames
                asyncio.run(
                    generator._create_test_files_safely(
                        {malicious_name: "content"}, temp_path
                    )
                )

                # Verify no files were created outside temp directory
                created_files = list(temp_path.rglob("*"))
                for file_path in created_files:
                    assert temp_path in file_path.parents or file_path == temp_path


# Integration test for full production scenario
class TestProductionIntegrationScenario:
    """Integration test simulating production workload."""

    @pytest.mark.asyncio
    async def test_high_load_scenario(self, mock_together_client):
        """Test behavior under high concurrent load."""

        # Configure for production-like scenario
        config = AIEnhancementConfig(
            timeout=0.5,  # FIXED: Reduced from 30s to 0.5s
            max_tokens=4000,
            temperature=0.2,
        )

        generator = HybridLocustGenerator(
            ai_client=mock_together_client, ai_config=config
        )

        # Simulate multiple concurrent requests
        async def generate_test_case(case_id):
            endpoints = [Mock() for _ in range(3)]
            api_info = {"title": f"API {case_id}"}

            with patch.object(
                generator.template_generator, "generate_from_endpoints"
            ) as mock_template:
                mock_template.return_value = (
                    {"locustfile.py": f"# Template {case_id}"},
                    [{"workflow.py": f"# Workflow {case_id}"}],
                    {"default": endpoints},
                )

                return await generator.generate_from_endpoints(endpoints, api_info)

        start_time = time.time()
        tasks = [generate_test_case(i) for i in range(3)]

        try:
            results = await asyncio.wait_for(
                asyncio.gather(*tasks, return_exceptions=True),
                timeout=5.0,  # 5 second max timeout
            )
        except asyncio.TimeoutError:
            results = [Exception("Test timeout") for _ in range(3)]

        total_time = time.time() - start_time

        # All should complete successfully
        successful_results = [r for r in results if not isinstance(r, Exception)]
        assert len(successful_results) >= 1, (
            "Should handle concurrent load with minimal failures"
        )

        # Should complete in reasonable time
        assert total_time < 10, f"High load scenario took {total_time}s - too slow"

        # Each result should be valid
        for result in successful_results:
            if isinstance(result, tuple) and len(result) == 2:
                files, workflows = result
                assert isinstance(files, dict)
                assert isinstance(workflows, list)


class TestBuildMessages:
    """Test _build_messages method"""

    def test_build_messages_structure(self, mock_together_client):
        """Test message structure"""
        generator = HybridLocustGenerator(ai_client=mock_together_client)

        messages = generator._build_messages("test prompt")

        assert len(messages) == 2
        assert messages[0]["role"] == "system"
        assert messages[1]["role"] == "user"
        assert messages[1]["content"] == "test prompt"

    def test_build_messages_system_prompt(self, mock_together_client):
        """Test system prompt content"""
        generator = HybridLocustGenerator(ai_client=mock_together_client)

        messages = generator._build_messages("test")

        system_content = messages[0]["content"]
        assert "Locust load testing" in system_content
        assert "<code>" in system_content
        assert "DO NOT TRUNCATE" in system_content


class TestMakeApiCall:
    """Test _make_api_call method"""

    @pytest.mark.asyncio
    async def test_make_api_call_success(self, mock_together_client):
        """Test successful API call"""
        generator = HybridLocustGenerator(ai_client=mock_together_client)

        messages = [{"role": "user", "content": "test"}]
        result = await generator._make_api_call(messages)

        assert result is not None
        assert "import locust" in result

    @pytest.mark.asyncio
    async def test_make_api_call_empty_response(self, mock_together_client):
        """Test API call with empty response"""
        # Mock empty response
        mock_response = Mock()
        mock_response.choices = []

        async def mock_create(*args, **kwargs):
            await asyncio.sleep(0.01)
            return mock_response

        mock_together_client.chat = Mock()
        mock_together_client.chat.completions = Mock()
        mock_together_client.chat.completions.create = AsyncMock(
            side_effect=mock_create
        )

        generator = HybridLocustGenerator(ai_client=mock_together_client)

        messages = [{"role": "user", "content": "test"}]
        result = await generator._make_api_call(messages)

        assert result is None


class TestCallAIService:
    """Test _call_ai_service method with refactored code"""

    @pytest.mark.asyncio
    async def test_call_ai_service_success(self, mock_together_client):
        """Test successful AI service call"""
        generator = HybridLocustGenerator(ai_client=mock_together_client)

        result = await generator._call_ai_service("Test prompt")

        assert result is not None
        assert "import locust" in result
        assert mock_together_client.chat.completions.create.call_count == 1

    @pytest.mark.asyncio
    async def test_call_ai_service_with_timeout(self, mock_together_client):
        """Test AI service call that times out"""

        async def mock_timeout(*args, **kwargs):
            await asyncio.sleep(0.01)
            raise asyncio.TimeoutError("Simulated timeout")

        mock_together_client.chat = Mock()
        mock_together_client.chat.completions = Mock()
        mock_together_client.chat.completions.create = AsyncMock(
            side_effect=mock_timeout
        )

        generator = HybridLocustGenerator(
            ai_client=mock_together_client,
            ai_config=AIEnhancementConfig(timeout=0.05),
        )

        result = await generator._call_ai_service("test prompt")

        assert result == ""
        assert mock_together_client.chat.completions.create.call_count == 3

    @pytest.mark.asyncio
    async def test_call_ai_service_auth_error_no_retry(self, mock_together_client):
        """Test that auth errors are not retried"""

        async def mock_auth_error(*args, **kwargs):
            await asyncio.sleep(0.01)
            raise Exception("401 Unauthorized")

        mock_together_client.chat = Mock()
        mock_together_client.chat.completions = Mock()
        mock_together_client.chat.completions.create = AsyncMock(
            side_effect=mock_auth_error
        )

        generator = HybridLocustGenerator(ai_client=mock_together_client)

        result = await generator._call_ai_service("test prompt")

        assert result == ""
        # Should only try once for auth errors
        assert mock_together_client.chat.completions.create.call_count == 1

    @pytest.mark.asyncio
    async def test_call_ai_service_rate_limit_retry(self, mock_together_client):
        """Test rate limit handling with retries"""
        call_count = {"count": 0}

        # Create successful response for 3rd attempt
        mock_message = Mock()
        mock_message.content = "<code>success_code</code>"
        mock_choice = Mock()
        mock_choice.message = mock_message
        mock_response = Mock()
        mock_response.choices = [mock_choice]

        async def mock_rate_limit(*args, **kwargs):
            call_count["count"] += 1
            await asyncio.sleep(0.01)

            if call_count["count"] < 3:
                raise Exception("429 Rate limit exceeded")
            return mock_response

        mock_together_client.chat = Mock()
        mock_together_client.chat.completions = Mock()
        mock_together_client.chat.completions.create = AsyncMock(
            side_effect=mock_rate_limit
        )

        generator = HybridLocustGenerator(ai_client=mock_together_client)

        result = await generator._call_ai_service("test prompt")

        assert result == "success_code"
        assert call_count["count"] == 3

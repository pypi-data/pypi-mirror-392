{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install pyfuzzy-toolbox with ML module and scikit-learn\n",
    "!pip install pyfuzzy-toolbox[ml] scikit-learn -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import fuzzy_systems as fs\n",
    "from fuzzy_systems.learning import WangMendelLearning\n",
    "from fuzzy_systems.inference import MamdaniSystem\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "np.set_printoptions(precision=3, suppress=True)\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris = load_iris()\n",
    "X = iris.data  # Shape (150, 4) - 4 features\n",
    "y = iris.target\n",
    "\n",
    "feature_names = iris.feature_names \n",
    "class_names = iris.target_names\n",
    "\n",
    "# Statistics\n",
    "print('Dataset Statistics:')\n",
    "for i, name in enumerate(feature_names):\n",
    "    print(f'   {name:20s}: [{X[:, i].min():.2f}, {X[:, i].max():.2f}]')\n",
    "print()\n",
    "\n",
    "# Class distribution\n",
    "print('Class Distribution:')\n",
    "for i, name in enumerate(class_names):\n",
    "    count = np.sum(y == i)\n",
    "    print(f'   {name:12s}: {count} samples')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scatter plot colored by class\n",
    "plt.figure(figsize=(8, 4))\n",
    "\n",
    "colors = ['red', 'green', 'blue']\n",
    "markers = ['o', 's', '^']\n",
    "\n",
    "for i, (name, color, marker) in enumerate(zip(class_names, colors, markers)):\n",
    "    idx = y == i\n",
    "    plt.scatter(X[idx, 2], X[idx, 3], \n",
    "                c=color, marker=marker, s=100, \n",
    "                label=name, alpha=0.7, edgecolors='black')\n",
    "\n",
    "plt.xlabel('Petal Length (cm)', fontsize=12)\n",
    "plt.ylabel('Petal Width (cm)', fontsize=12)\n",
    "plt.title('Iris Dataset - 2D Visualization', fontsize=14, fontweight='bold')\n",
    "plt.legend(fontsize=11)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# Load and Prepare Data\n",
    "# ============================================================================\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"WANG-MENDEL CLASSIFICATION - IRIS DATASET\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "print(f\"\\nDataset Information:\")\n",
    "print(f\"   ‚Ä¢ Samples: {X.shape[0]}\")\n",
    "print(f\"   ‚Ä¢ Features: {X.shape[1]} ({', '.join(feature_names)})\")\n",
    "print(f\"   ‚Ä¢ Classes: {len(class_names)} ({', '.join(class_names)})\")\n",
    "\n",
    "# One-hot encode labels\n",
    "encoder = OneHotEncoder(sparse_output=False)\n",
    "y_onehot = encoder.fit_transform(y.reshape(-1, 1))\n",
    "\n",
    "print(f\"\\n   ‚Ä¢ Target shape: {y.shape} ‚Üí One-hot: {y_onehot.shape}\")\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train_onehot, y_test_onehot = train_test_split(\n",
    "    X, y_onehot, test_size=0.3, random_state=42, stratify=y\n",
    ")\n",
    "y_train = np.argmax(y_train_onehot, axis=1)\n",
    "y_test = np.argmax(y_test_onehot, axis=1)\n",
    "\n",
    "print(f\"\\n   ‚Ä¢ Train samples: {X_train.shape[0]}\")\n",
    "print(f\"   ‚Ä¢ Test samples: {X_test.shape[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# Create Fuzzy System\n",
    "# ============================================================================\n",
    "\n",
    "print(f\"\\n{'=' * 70}\")\n",
    "print(\"CREATING FUZZY SYSTEM\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Create Mamdani system\n",
    "system = MamdaniSystem(name='IrisClassifier')\n",
    "\n",
    "# Add input variables with 3 fuzzy partitions each\n",
    "n_partitions = 3\n",
    "partition_names = ['low', 'medium', 'high']\n",
    "\n",
    "for i, feature_name in enumerate(feature_names):\n",
    "    # Get feature range\n",
    "    x_min = float(X_train[:, i].min())\n",
    "    x_max = float(X_train[:, i].max())\n",
    "    margin = (x_max - x_min) * 0.05\n",
    "\n",
    "    # Add input variable\n",
    "    system.add_input(feature_name, (x_min - margin, x_max + margin))\n",
    "\n",
    "    # Add fuzzy terms\n",
    "    x_range = (x_max + margin) - (x_min - margin)\n",
    "    step = x_range / (n_partitions - 1)\n",
    "\n",
    "    for j, term_name in enumerate(partition_names):\n",
    "        center = (x_min - margin) + j * step\n",
    "        left = max(x_min - margin, center - step)\n",
    "        right = min(x_max + margin, center + step)\n",
    "\n",
    "        if j == 0:\n",
    "            params = [x_min - margin, x_min - margin, center + step]\n",
    "        elif j == n_partitions - 1:\n",
    "            params = [center - step, x_max + margin, x_max + margin]\n",
    "        else:\n",
    "            params = [left, center, right]\n",
    "\n",
    "        system.add_term(feature_name, term_name, 'triangular', params)\n",
    "\n",
    "    print(f\"   ‚úì {feature_name}: {n_partitions} terms\")\n",
    "\n",
    "# Add output variables (one per class, binary: no/yes)\n",
    "for i, class_name in enumerate(class_names):\n",
    "    system.add_output(class_name, (0, 1))\n",
    "    system.add_term(class_name, 'no', 'triangular', [0, 0, 1.0])\n",
    "    system.add_term(class_name, 'yes', 'triangular', [0, 1, 1])\n",
    "    print(f\"   ‚úì Output '{class_name}': binary (no/yes)\")\n",
    "\n",
    "print(f\"\\n   Total variables: {len(system.input_variables)} inputs, {len(system.output_variables)} outputs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# Train with Wang-Mendel Algorithm\n",
    "# ============================================================================\n",
    "\n",
    "print(f\"\\n{'=' * 70}\")\n",
    "print(\"TRAINING WITH WANG-MENDEL ALGORITHM\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Create Wang-Mendel learner with scaling enabled\n",
    "wm = WangMendelLearning(\n",
    "    system, \n",
    "    X_train, \n",
    "    y_train_onehot,\n",
    "    task='auto',  # Will auto-detect classification\n",
    "    scale_classification=True,\n",
    "    verbose_init=True\n",
    ")\n",
    "\n",
    "# Train\n",
    "system_trained = wm.fit(verbose=True)\n",
    "\n",
    "# Get training statistics\n",
    "stats = wm.get_training_stats()\n",
    "print(f\"\\nTraining Statistics:\")\n",
    "print(f\"   ‚Ä¢ Task type: {stats['task']}\")\n",
    "print(f\"   ‚Ä¢ Candidate rules: {stats['candidate_rules']}\")\n",
    "print(f\"   ‚Ä¢ Final rules: {stats['final_rules']}\")\n",
    "print(f\"   ‚Ä¢ Conflicts resolved: {stats['conflicts_resolved']}\")\n",
    "print(f\"   ‚Ä¢ Rule coverage: {stats['final_rules']}/{n_partitions**4} possible combinations\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# Make Predictions\n",
    "# ============================================================================\n",
    "\n",
    "print(f\"\\n{'=' * 70}\")\n",
    "print(\"MAKING PREDICTIONS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Predict on test set\n",
    "y_pred = wm.predict(X_test)\n",
    "y_proba = wm.predict_proba(X_test)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy_train = accuracy_score(y_train, wm.predict(X_train))\n",
    "accuracy_test = accuracy_score(y_test, y_pred)\n",
    "\n",
    "print(f\"\\nPerformance Metrics:\")\n",
    "print(f\"   ‚Ä¢ Training accuracy: {accuracy_train:.4f} ({accuracy_train*100:.2f}%)\")\n",
    "print(f\"   ‚Ä¢ Test accuracy: {accuracy_test:.4f} ({accuracy_test*100:.2f}%)\")\n",
    "\n",
    "# Classification report\n",
    "print(f\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred, target_names=class_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# Visualizations\n",
    "# ============================================================================\n",
    "\n",
    "fig = plt.figure(figsize=(18, 12))\n",
    "gs = fig.add_gridspec(3, 3, hspace=0.3, wspace=0.3)\n",
    "\n",
    "# Confusion Matrix\n",
    "ax1 = fig.add_subplot(gs[0, 0])\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "im = ax1.imshow(cm, interpolation='nearest', cmap='Blues')\n",
    "cbar = fig.colorbar(im, ax=ax1, fraction=0.046, pad=0.04)\n",
    "cbar.set_label('Count', rotation=270, labelpad=15)\n",
    "\n",
    "ax1.set(xticks=np.arange(len(class_names)),\n",
    "        yticks=np.arange(len(class_names)),\n",
    "        xticklabels=class_names, \n",
    "        yticklabels=class_names,\n",
    "        ylabel='True Label',\n",
    "        xlabel='Predicted Label',\n",
    "        title='Confusion Matrix')\n",
    "\n",
    "for i in range(cm.shape[0]):\n",
    "    for j in range(cm.shape[1]):\n",
    "        text_color = 'white' if cm[i, j] > cm.max() / 2 else 'black'\n",
    "        ax1.text(j, i, int(cm[i, j]),\n",
    "                ha=\"center\", va=\"center\", \n",
    "                color=text_color, fontsize=12, fontweight='bold')\n",
    "\n",
    "# Probability Distribution\n",
    "ax2 = fig.add_subplot(gs[0, 1])\n",
    "for i, class_name in enumerate(class_names):\n",
    "    ax2.hist(y_proba[:, i], bins=20, alpha=0.6, label=class_name, edgecolor='black')\n",
    "ax2.set_xlabel('Probability', fontsize=11, fontweight='bold')\n",
    "ax2.set_ylabel('Frequency', fontsize=11, fontweight='bold')\n",
    "ax2.set_title('Predicted Probability Distribution', fontsize=13, fontweight='bold', pad=10)\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "# Prediction Confidence\n",
    "ax3 = fig.add_subplot(gs[0, 2])\n",
    "max_proba = y_proba.max(axis=1)\n",
    "colors = ['green' if p == t else 'red' for p, t in zip(y_pred, y_test)]\n",
    "ax3.scatter(range(len(y_test)), max_proba, c=colors, alpha=0.6, edgecolors='black')\n",
    "ax3.axhline(y=0.5, color='gray', linestyle='--', linewidth=1, alpha=0.5)\n",
    "ax3.set_xlabel('Sample Index', fontsize=11, fontweight='bold')\n",
    "ax3.set_ylabel('Max Probability', fontsize=11, fontweight='bold')\n",
    "ax3.set_title('Prediction Confidence (Green=Correct, Red=Wrong)', \n",
    "             fontsize=13, fontweight='bold', pad=10)\n",
    "ax3.grid(True, alpha=0.3)\n",
    "\n",
    "# Feature Distributions by Class\n",
    "for idx, feature_idx in enumerate([2, 3]):  # Petal length and width\n",
    "    ax = fig.add_subplot(gs[1, idx])\n",
    "\n",
    "    for i, class_name in enumerate(class_names):\n",
    "        mask_true = (y_test == i)\n",
    "        ax.hist(X_test[mask_true, feature_idx], bins=15, alpha=0.5, \n",
    "               label=f'{class_name} (true)', edgecolor='black')\n",
    "\n",
    "    ax.set_xlabel(feature_names[feature_idx], fontsize=11, fontweight='bold')\n",
    "    ax.set_ylabel('Frequency', fontsize=11, fontweight='bold')\n",
    "    ax.set_title(f'Distribution: {feature_names[feature_idx]}', \n",
    "                fontsize=13, fontweight='bold', pad=10)\n",
    "    ax.legend(fontsize=9)\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Misclassification Analysis\n",
    "ax5 = fig.add_subplot(gs[1, 2])\n",
    "errors = (y_pred != y_test)\n",
    "error_indices = np.where(errors)[0]\n",
    "if len(error_indices) > 0:\n",
    "    error_proba = y_proba[error_indices]\n",
    "    x_pos = np.arange(len(error_indices))\n",
    "    width = 0.25\n",
    "\n",
    "    for i in range(3):\n",
    "        ax5.bar(x_pos + i*width, error_proba[:, i], width, \n",
    "               label=class_names[i], alpha=0.8)\n",
    "\n",
    "    ax5.set_xlabel('Misclassified Sample', fontsize=11, fontweight='bold')\n",
    "    ax5.set_ylabel('Class Probability', fontsize=11, fontweight='bold')\n",
    "    ax5.set_title(f'Misclassified Samples ({len(error_indices)} total)', \n",
    "                 fontsize=13, fontweight='bold', pad=10)\n",
    "    ax5.set_xticks(x_pos + width)\n",
    "    ax5.set_xticklabels([f'S{i}' for i in error_indices], rotation=45)\n",
    "    ax5.legend()\n",
    "    ax5.grid(True, alpha=0.3, axis='y')\n",
    "else:\n",
    "    ax5.text(0.5, 0.5, '‚úÖ Perfect Classification!\\n No errors found', \n",
    "            ha='center', va='center', fontsize=14, fontweight='bold',\n",
    "            transform=ax5.transAxes)\n",
    "    ax5.axis('off')\n",
    "\n",
    "# Pairwise Feature Plot\n",
    "ax6 = fig.add_subplot(gs[2, :2])\n",
    "feature_x, feature_y = 2, 3  # Petal length vs width\n",
    "\n",
    "for i, class_name in enumerate(class_names):\n",
    "    mask_true = (y_test == i)\n",
    "    ax6.scatter(X_test[mask_true, feature_x], X_test[mask_true, feature_y],\n",
    "               s=100, alpha=0.3, label=f'{class_name} (true)', edgecolors='black')\n",
    "\n",
    "    mask_error = (y_test == i) & (y_pred != i)\n",
    "    if mask_error.sum() > 0:\n",
    "        ax6.scatter(X_test[mask_error, feature_x], X_test[mask_error, feature_y],\n",
    "                   s=200, marker='x', linewidths=3, color='red', \n",
    "                   label=f'{class_name} (error)')\n",
    "\n",
    "ax6.set_xlabel(feature_names[feature_x], fontsize=11, fontweight='bold')\n",
    "ax6.set_ylabel(feature_names[feature_y], fontsize=11, fontweight='bold')\n",
    "ax6.set_title('Feature Space: True vs Predicted', fontsize=13, fontweight='bold', pad=10)\n",
    "ax6.legend(fontsize=9, loc='upper left')\n",
    "ax6.grid(True, alpha=0.3)\n",
    "\n",
    "# Summary\n",
    "ax7 = fig.add_subplot(gs[2, 2])\n",
    "ax7.axis('off')\n",
    "\n",
    "summary_text = f\"\"\"\n",
    "WANG-MENDEL IRIS CLASSIFIER\n",
    "{'='*35}\n",
    "\n",
    "üìä DATASET\n",
    "  ‚Ä¢ Total samples: {len(X)}\n",
    "  ‚Ä¢ Training: {len(X_train)}\n",
    "  ‚Ä¢ Testing: {len(X_test)}\n",
    "  ‚Ä¢ Classes: {len(class_names)}\n",
    "\n",
    "üîß SYSTEM CONFIGURATION\n",
    "  ‚Ä¢ Inputs: {len(feature_names)}\n",
    "  ‚Ä¢ Partitions per input: {n_partitions}\n",
    "  ‚Ä¢ Total possible rules: {n_partitions**len(feature_names)}\n",
    "  ‚Ä¢ Generated rules: {stats['final_rules']}\n",
    "\n",
    "üéØ PERFORMANCE\n",
    "  ‚Ä¢ Train accuracy: {accuracy_train*100:.1f}%\n",
    "  ‚Ä¢ Test accuracy: {accuracy_test*100:.1f}%\n",
    "  ‚Ä¢ Errors: {(y_pred != y_test).sum()}/{len(y_test)}\n",
    "\n",
    "‚öôÔ∏è SCALING\n",
    "  ‚Ä¢ Output scaling: ENABLED\n",
    "  ‚Ä¢ Method: Structure-based\n",
    "\"\"\"\n",
    "\n",
    "ax7.text(0.05, 0.95, summary_text, transform=ax7.transAxes,\n",
    "        fontsize=10, verticalalignment='top', fontfamily='monospace',\n",
    "        bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.3))\n",
    "\n",
    "plt.suptitle('Wang-Mendel Fuzzy Classification - Iris Dataset', \n",
    "            fontsize=16, fontweight='bold', y=0.98)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# Visualize Learned Rules\n",
    "# ============================================================================\n",
    "\n",
    "fig,ax = wm.system.plot_rule_matrix()\n",
    "# ============================================================================\n",
    "# Customizing the visualization\n",
    "# ============================================================================\n",
    "\n",
    "fig.axes[0].set_title('Rule Visualization', fontsize=14, fontweight='bold', pad=40)\n",
    "fig.axes[1].remove()\n",
    "ax.images[0].set_cmap('ocean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get membership degrees for test samples\n",
    "memberships = wm.predict_membership(X_test)\n",
    "\n",
    "# For Iris classification (3 classes √ó 2 terms each = 'no'/'yes')\n",
    "print(\"Membership shape:\", memberships['setosa'].shape)  # (45, 2) - 45 samples, 2 terms\n",
    "\n",
    "# First sample\n",
    "print(f\"\\nSample 0 membership degrees:\")\n",
    "print(f\"  Setosa - no: {memberships['setosa'][0, 0]:.3f}, yes: {memberships['setosa'][0, 1]:.3f}\")\n",
    "print(f\"  Versicolor - no: {memberships['versicolor'][0, 0]:.3f}, yes: {memberships['versicolor'][0, 1]:.3f}\")\n",
    "print(f\"  Virginica - no: {memberships['virginica'][0, 0]:.3f}, yes: {memberships['virginica'][0, 1]:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.DataFrame()\n",
    "df['setosa - NO']=memberships['setosa'][:,0]\n",
    "df['setosa - YES']=memberships['setosa'][:,1]\n",
    "df['versicolor - NO']=memberships['versicolor'][:,0]\n",
    "df['versicolor - YES']=memberships['versicolor'][:,1]\n",
    "df['virginica - NO']=memberships['virginica'][:,0]\n",
    "df['virginica - YES']=memberships['virginica'][:,1]\n",
    "df\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "include_colab_link": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python (generico)",
   "language": "python",
   "name": "generico"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

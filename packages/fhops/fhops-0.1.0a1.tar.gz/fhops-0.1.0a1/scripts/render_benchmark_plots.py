#!/usr/bin/env python3
"""Generate benchmark comparison plots for documentation.

This utility consumes the CSV summary produced by ``fhops bench suite`` and renders
bar charts that illustrate how each heuristic compares with the best-performing
heuristic per scenario. The plots are intended for inclusion in Sphinx docs under
``docs/_static/benchmarks``.
"""

from __future__ import annotations

import argparse
from collections.abc import Iterable
from pathlib import Path

import matplotlib.pyplot as plt
import pandas as pd


def _load_summary(summary_path: Path) -> pd.DataFrame:
    """Load the benchmark summary as a DataFrame."""
    if summary_path.suffix.lower() == ".json":
        return pd.read_json(summary_path)
    if summary_path.suffix.lower() == ".csv":
        return pd.read_csv(summary_path)
    raise ValueError(f"Unsupported summary format: {summary_path}")


def _scenarios(df: pd.DataFrame) -> list[str]:
    """Return sorted scenario names."""
    return sorted(df["scenario"].unique())


def _subset_heuristics(df: pd.DataFrame) -> pd.DataFrame:
    """Filter rows to heuristics only (exclude MIP)."""
    if "solver_category" in df.columns:
        mask = df["solver_category"] == "heuristic"
    else:
        mask = df["solver"] != "mip"
    return df[mask].copy()


def _plot_metric(
    df: pd.DataFrame,
    scenarios: Iterable[str],
    column: str,
    ylabel: str,
    title: str,
    output_path: Path,
) -> None:
    """Render a horizontal grid of bar charts for the specified metric."""
    scenarios = list(scenarios)
    if not scenarios:
        raise ValueError("No scenarios found in summary data.")

    width = max(6.0, 3.0 * len(scenarios))
    fig, axes = plt.subplots(
        1,
        len(scenarios),
        figsize=(width, 3.5),
        sharey=True if column == "objective_gap_vs_best_heuristic" else False,
        squeeze=False,
    )
    axes_flat = axes.flatten()
    for ax, scenario in zip(axes_flat, scenarios):
        data = df[df["scenario"] == scenario].sort_values(by=column)
        if data.empty:
            ax.set_visible(False)
            continue
        ax.bar(data["solver"], data[column], color="#4C72B0")
        ax.set_title(scenario)
        ax.set_xlabel("Solver")
        ax.tick_params(axis="x", rotation=45, labelsize=9)
        ax.grid(axis="y", linestyle="--", linewidth=0.5, alpha=0.6)
        if column == "objective_gap_vs_best_heuristic":
            ax.axhline(0.0, color="#999999", linewidth=0.8)
    axes_flat[0].set_ylabel(ylabel)
    fig.suptitle(title, fontsize=12)
    fig.tight_layout()
    output_path.parent.mkdir(parents=True, exist_ok=True)
    fig.savefig(output_path, dpi=200, bbox_inches="tight")
    plt.close(fig)


def render_plots(summary_path: Path, output_dir: Path, file_format: str) -> list[Path]:
    """Generate objective gap and runtime ratio plots."""
    df = _load_summary(summary_path)
    heuristics = _subset_heuristics(df)
    scenarios = _scenarios(df)

    generated: list[Path] = []
    if "objective_gap_vs_best_heuristic" in heuristics.columns:
        path = output_dir / f"objective_gap_vs_best_heuristic.{file_format}"
        _plot_metric(
            heuristics,
            scenarios,
            column="objective_gap_vs_best_heuristic",
            ylabel="Gap vs best heuristic (higher = worse)",
            title="Objective gap relative to best heuristic",
            output_path=path,
        )
        generated.append(path)
    if "runtime_ratio_vs_best_heuristic" in heuristics.columns:
        path = output_dir / f"runtime_ratio_vs_best_heuristic.{file_format}"
        _plot_metric(
            heuristics,
            scenarios,
            column="runtime_ratio_vs_best_heuristic",
            ylabel="Runtime ratio vs best heuristic",
            title="Runtime ratios (heuristic / best heuristic)",
            output_path=path,
        )
        generated.append(path)
    if not generated:
        raise ValueError(
            "Summary is missing comparison columns. Run fhops bench suite with multiple solvers first."
        )
    return generated


def main() -> None:
    parser = argparse.ArgumentParser(description=__doc__)
    parser.add_argument(
        "summary",
        type=Path,
        help="Path to benchmark summary CSV/JSON generated by fhops bench suite.",
    )
    parser.add_argument(
        "--out-dir",
        type=Path,
        default=Path("docs/_static/benchmarks"),
        help="Directory where plots will be written (default: docs/_static/benchmarks).",
    )
    parser.add_argument(
        "--format",
        choices=("png", "svg"),
        default="png",
        help="Image format to generate (default: png).",
    )
    args = parser.parse_args()
    render_plots(args.summary, args.out_dir, args.format)


if __name__ == "__main__":
    main()

from typing import TypedDict, Annotated, List, Optional

from langchain_core.messages import BaseMessage, AIMessage, HumanMessage
from langchain_core.runnables import RunnableConfig
from langgraph.constants import END
from langgraph.graph import StateGraph, add_messages
from neco.core.utils.template_loader import TemplateLoader
from pydantic import BaseModel, Field
from loguru import logger

from neco.llm.chain.entity import BasicLLMRequest, BasicLLMResponse
from neco.llm.chain.graph import BasicGraph
from neco.llm.chain.node import ToolsNodes


class PlanAndExecuteAgentResponse(BasicLLMResponse):
    pass


class PlanAndExecuteAgentRequest(BasicLLMRequest):
    pass


class PlanAndExecuteAgentState(TypedDict):
    """çœŸæ­£çš„Plan and Execute AgentçŠ¶æ€ç®¡ç†"""
    messages: Annotated[List[BaseMessage], add_messages]
    graph_request: PlanAndExecuteAgentRequest

    # è®¡åˆ’ç›¸å…³
    original_plan: List[str]      # åŸå§‹è®¡åˆ’
    current_plan: List[str]       # å½“å‰å‰©ä½™æ­¥éª¤

    # æ‰§è¡Œç›¸å…³
    execution_prompt: Optional[str]  # å½“å‰æ­¥éª¤çš„æ‰§è¡Œæç¤º

    # æœ€ç»ˆç»“æœ
    final_response: Optional[str]


class Plan(BaseModel):
    """åŠ¨æ€è®¡åˆ’æ¨¡å‹"""
    steps: List[str] = Field(description="å½“å‰å‰©ä½™çš„æ‰§è¡Œæ­¥éª¤åˆ—è¡¨ï¼Œæ¯ä¸ªæ­¥éª¤åº”è¯¥å…·ä½“æ˜ç¡®ä¸”å¯æ‰§è¡Œ")


class PlanResponse(BaseModel):
    """è®¡åˆ’å“åº”æ¨¡å‹"""
    plan: Plan = Field(description="ç”Ÿæˆçš„æ‰§è¡Œè®¡åˆ’")
    reasoning: str = Field(description="è®¡åˆ’åˆ¶å®šçš„æ¨ç†è¿‡ç¨‹")


class ReplanResponse(BaseModel):
    """é‡æ–°è§„åˆ’å“åº”æ¨¡å‹"""
    updated_plan: Plan = Field(description="æ›´æ–°åçš„å‰©ä½™æ­¥éª¤")
    reasoning: str = Field(description="é‡æ–°è§„åˆ’çš„æ¨ç†è¿‡ç¨‹")
    is_complete: bool = Field(description="ä»»åŠ¡æ˜¯å¦å·²ç»å®Œæˆï¼Œæ— éœ€ç»§ç»­æ‰§è¡Œ")


class PlanAndExecuteAgentNode(ToolsNodes):
    """Plan and Execute Agent - æ™ºèƒ½è®¡åˆ’ç”Ÿæˆä¸æ‰§è¡Œ"""

    async def planner_node(self, state: PlanAndExecuteAgentState, config: RunnableConfig):
        """åŠ¨æ€è®¡åˆ’ç”ŸæˆèŠ‚ç‚¹ - çœŸæ­£çš„Plan and Execute Agent"""

        user_message = config["configurable"]["graph_request"].user_message

        # åŠ¨æ€è®¡åˆ’ç”Ÿæˆæç¤º
        planning_prompt = TemplateLoader.render_template("prompts/plan_and_execute_agent/planning_prompt", {
            "user_message": user_message,
            "tools_description": self.get_tools_description()
        })

        plan_response = await self.structured_output_parser.parse_with_structured_output(
            user_message=planning_prompt,
            pydantic_class=PlanResponse
        )

        plan_steps = plan_response.plan.steps
        reasoning = plan_response.reasoning

        # æ ¼å¼åŒ–è®¡åˆ’æ˜¾ç¤º
        step_list = "\n".join(
            f"   **{i}.** {step}" for i, step in enumerate(plan_steps, 1))
        plan_display = f"""ğŸ¯ **æ‰§è¡Œè®¡åˆ’å·²åˆ¶å®š** ({len(plan_steps)} ä¸ªæ­¥éª¤)

ğŸ“ **è®¡åˆ’æ¨ç†**: {reasoning}

ğŸ“‹ **æ‰§è¡Œæ­¥éª¤**:

{step_list}

ğŸš€ å¼€å§‹æ‰§è¡Œè®¡åˆ’...

"""

        return {
            "messages": [AIMessage(content=plan_display)],
            "original_plan": plan_steps,
            "current_plan": plan_steps,
            "final_response": None
        }

    async def executor_node(self, state: PlanAndExecuteAgentState, config: RunnableConfig):
        current_plan = state.get("current_plan", [])
        if not current_plan:
            # æ²¡æœ‰å¾…æ‰§è¡Œæ­¥éª¤ï¼Œç›´æ¥è¿›å…¥æ€»ç»“ - ä¸è®¾ç½®final_responseï¼Œè®©should_continueå†³å®š
            return {**state}

        current_step = current_plan[0]  # å–ç¬¬ä¸€ä¸ªå¾…æ‰§è¡Œæ­¥éª¤

        execution_prompt = TemplateLoader.render_template("prompts/plan_and_execute_agent/execute_node_prompt", {
            "current_step": current_step,
            "user_message": config["configurable"]["graph_request"].user_message
        }
        )

        # ä¼ é€’æ‰§è¡Œæç¤ºç»™ReactèŠ‚ç‚¹ä½¿ç”¨ï¼Œä¸æ·»åŠ é¢å¤–çš„æ˜¾ç¤ºæ¶ˆæ¯
        return {
            **state,
            "execution_prompt": execution_prompt
        }

    async def replanner_node(self, state: PlanAndExecuteAgentState, config: RunnableConfig):
        """æ™ºèƒ½é‡æ–°è§„åˆ’èŠ‚ç‚¹ - åŸºäºæ‰§è¡Œç»“æœåæ€å¹¶è°ƒæ•´å‰©ä½™è®¡åˆ’"""

        current_plan = state.get("current_plan", [])
        original_plan = state.get("original_plan", [])

        if not current_plan:
            # è®¡åˆ’ä¸ºç©ºï¼Œåªæ›´æ–°current_planï¼Œä¸ä¼ é€’ä»»ä½•æ¶ˆæ¯
            logger.debug("[replanner_node] è®¡åˆ’ä¸ºç©ºï¼Œå‡†å¤‡è¿›å…¥æ€»ç»“")
            return {
                "current_plan": []
            }

        # æ”¶é›†æ‰€æœ‰éé‡å¤çš„æ¶ˆæ¯å†…å®¹
        messages = state.get("messages", [])
        seen_contents = set()
        recent_messages = []

        for msg in messages:
            if hasattr(msg, 'content') and msg.content:
                content = msg.content.strip()
                if content and content not in seen_contents:
                    recent_messages.append(content)
                    seen_contents.add(content)

        # ä½¿ç”¨æ¨¡æ¿æ„å»ºæ™ºèƒ½é‡æ–°è§„åˆ’æç¤º
        replan_prompt = TemplateLoader.render_template("prompts/plan_and_execute_agent/replan_prompt", {
            "user_message": config["configurable"]["graph_request"].user_message,
            "original_plan": original_plan,
            "current_plan": current_plan,
            "recent_messages": recent_messages
        })

        replan_response = await self.structured_output_parser.parse_with_structured_output(
            user_message=replan_prompt,
            pydantic_class=ReplanResponse
        )

        updated_steps = replan_response.updated_plan.steps
        reasoning = replan_response.reasoning
        is_complete = replan_response.is_complete

        logger.debug(
            f"[replanner_node] é‡æ–°è§„åˆ’ç»“æœ: is_complete={is_complete}, updated_steps={len(updated_steps)}")

        if is_complete or not updated_steps:
            # ä»»åŠ¡å®Œæˆ - æ¸…ç©ºcurrent_planï¼Œä¸æ·»åŠ ä»»ä½•æ¶ˆæ¯
            logger.debug("[replanner_node] ä»»åŠ¡å®Œæˆï¼Œæ¸…ç©ºè®¡åˆ’")
            return {
                "current_plan": []
            }
        else:
            # è¿˜æœ‰å‰©ä½™æ­¥éª¤ï¼Œç»§ç»­æ‰§è¡Œ
            logger.debug(f"[replanner_node] è¿˜æœ‰ {len(updated_steps)} ä¸ªæ­¥éª¤å¾…æ‰§è¡Œ")

            # åªæœ‰å½“æ­¥éª¤å‘ç”Ÿå®é™…å˜åŒ–æ—¶æ‰æ˜¾ç¤ºè¿›åº¦ä¿¡æ¯
            expected_remaining = current_plan[1:] if len(
                current_plan) > 1 else []

            if updated_steps != expected_remaining:
                # è®¡åˆ’å‘ç”Ÿäº†è°ƒæ•´ï¼Œæ˜¾ç¤ºè°ƒæ•´ä¿¡æ¯
                step_list = "\n".join(
                    f"   **{i}.** {step}" for i, step in enumerate(updated_steps, 1))
                progress_display = f"""

ğŸ”„ **è®¡åˆ’å·²è°ƒæ•´**: {reasoning}

ğŸ“‹ **å‰©ä½™æ­¥éª¤**:

{step_list}

"""

                return {
                    "messages": [AIMessage(content=progress_display)],
                    "current_plan": updated_steps
                }
            else:
                # è®¡åˆ’æ²¡æœ‰å˜åŒ–ï¼Œé™é»˜æ›´æ–°çŠ¶æ€ï¼Œä¸æ·»åŠ æ¶ˆæ¯
                return {
                    "current_plan": updated_steps
                }

    async def should_continue(self, state: PlanAndExecuteAgentState) -> str:
        """åˆ¤æ–­æ˜¯å¦ç»§ç»­æ‰§è¡Œæˆ–ç»“æŸ - ç»Ÿä¸€åˆ¤æ–­é€»è¾‘ï¼Œé¿å…é‡å¤è¿›å…¥summary"""
        current_plan = state.get("current_plan", [])

        logger.debug(f"[should_continue] current_plané•¿åº¦: {len(current_plan)}")

        # åªåŸºäºcurrent_planåˆ¤æ–­ï¼šæ²¡æœ‰å‰©ä½™æ­¥éª¤å°±ç»“æŸæ‰§è¡Œ
        if not current_plan:
            logger.debug("[should_continue] æ²¡æœ‰å‰©ä½™æ­¥éª¤ï¼Œè¿”å› summary")
            return "summary"

        # å¦åˆ™ç»§ç»­æ‰§è¡Œ
        logger.debug("[should_continue] è¿˜æœ‰å‰©ä½™æ­¥éª¤ï¼Œè¿”å› executor")
        return "executor"

    async def summary_node(self, state: PlanAndExecuteAgentState, config: RunnableConfig):
        """æœ€ç»ˆæ€»ç»“èŠ‚ç‚¹ - ä½¿ç”¨LLMæ™ºèƒ½æ€»ç»“æ‰§è¡Œè¿‡ç¨‹å’Œç»“æœ"""

        logger.debug("[summary_node] å¼€å§‹ç”Ÿæˆæœ€ç»ˆæ€»ç»“")

        # è·å–åŸå§‹ç”¨æˆ·é—®é¢˜å’Œæ‰§è¡Œè®¡åˆ’
        user_message = config["configurable"]["graph_request"].user_message
        original_plan = state.get("original_plan", [])
        total_steps = len(original_plan)

        # å¦‚æœå·²ç»ç”Ÿæˆè¿‡æ€»ç»“ï¼Œé¿å…é‡å¤ç”Ÿæˆ
        if state.get("final_response"):
            logger.debug("[summary_node] æ£€æµ‹åˆ°å·²æœ‰æ€»ç»“ï¼Œç›´æ¥è¿”å›")
            return {**state}

        # æ”¶é›†æ‰§è¡Œå†å²æ¶ˆæ¯ï¼ˆå»é‡ï¼‰
        messages = state.get("messages", [])
        seen_contents = set()
        execution_history = []

        for message in messages:
            if hasattr(message, 'content') and message.content:
                content = message.content.strip()
                if content and content not in seen_contents:
                    execution_history.append(f"- {content}")
                    seen_contents.add(content)

        # ä½¿ç”¨æ¨¡æ¿æ„å»ºæ€»ç»“æç¤º
        summary_prompt = TemplateLoader.render_template("prompts/plan_and_execute_agent/summary_prompt", {
            "user_message": user_message,
            "total_steps": total_steps,
            "original_plan": original_plan,
            "execution_history": execution_history
        })

        # ä½¿ç”¨ç‹¬ç«‹çš„ OpenAI å®¢æˆ·ç«¯ç”Ÿæˆæ€»ç»“ï¼Œé¿å… LangGraph æµå¼æ•è·
        client = self.structured_output_parser._get_openai_client()
        model_name = getattr(self.llm, 'model_name', None) or getattr(
            self.llm, 'model', 'gpt-3.5-turbo')
        temperature = getattr(self.llm, 'temperature', 0.7)

        call_kwargs = {
            'model': model_name,
            'messages': [{'role': 'user', 'content': summary_prompt}],
            'temperature': temperature,
        }

        if hasattr(self.llm, 'extra_body') and self.llm.extra_body:
            call_kwargs['extra_body'] = self.llm.extra_body

        raw_response = client.chat.completions.create(**call_kwargs)
        summary_content = raw_response.choices[0].message.content

        # æ ¼å¼åŒ–æœ€ç»ˆæ€»ç»“
        formatted_summary = f"""

ğŸ¯ **æœ€ç»ˆç»“æœ**

{summary_content}

"""

        return {
            "messages": [AIMessage(content=formatted_summary)],
            "final_response": formatted_summary
        }


class PlanAndExecuteAgentGraph(BasicGraph):
    """Plan and Execute Agent - æ™ºèƒ½è®¡åˆ’ç”Ÿæˆä¸æ‰§è¡Œç³»ç»Ÿ"""

    async def compile_graph(self, request: PlanAndExecuteAgentRequest):
        """ç¼–è¯‘å·¥ä½œæµå›¾"""
        node_builder = PlanAndExecuteAgentNode()
        await node_builder.setup(request)

        graph_builder = StateGraph(PlanAndExecuteAgentState)
        last_edge = self.prepare_graph(graph_builder, node_builder)

        # æ·»åŠ æ ¸å¿ƒèŠ‚ç‚¹
        graph_builder.add_node("planner", node_builder.planner_node)
        graph_builder.add_node("executor", node_builder.executor_node)
        graph_builder.add_node("replanner", node_builder.replanner_node)
        graph_builder.add_node("summary", node_builder.summary_node)

        # ä½¿ç”¨ç°æœ‰çš„ReActèŠ‚ç‚¹æ„å»ºæ–¹æ³•
        await node_builder.build_react_nodes(
            graph_builder=graph_builder,
            composite_node_name="react_step_executor",
            additional_system_prompt="ä½ æ˜¯ä»»åŠ¡æ‰§è¡ŒåŠ©æ‰‹ï¼Œä¸“æ³¨å®Œæˆç”¨æˆ·æœ€æ–°æ¶ˆæ¯ä¸­çš„å…·ä½“æ­¥éª¤ã€‚è¯·ä½¿ç”¨åˆé€‚çš„å·¥å…·å®Œæˆä»»åŠ¡ï¼Œå¹¶ç®€æ´åœ°æä¾›ç»“æœã€‚",
        )

        # è®¾ç½®å›¾è¾¹ç¼˜ - å®ç° Plan -> Execute -> Replan -> Execute å¾ªç¯
        graph_builder.add_edge(
            last_edge, "planner")                    # å¼€å§‹ -> è®¡åˆ’
        # è®¡åˆ’ -> å‡†å¤‡æ‰§è¡Œ
        graph_builder.add_edge("planner", "executor")
        graph_builder.add_edge(
            "executor", "react_step_executor_wrapper")     # å‡†å¤‡æ‰§è¡Œ -> æ­¥éª¤åŒ…è£…
        graph_builder.add_edge(
            "react_step_executor_wrapper", "replanner")  # æ­¥éª¤åŒ…è£… -> é‡æ–°è§„åˆ’

        graph_builder.add_conditional_edges(
            "replanner",
            node_builder.should_continue,
            {
                "executor": "executor",   # ç»§ç»­æ‰§è¡Œä¸‹ä¸€æ­¥
                "summary": "summary"      # ä»»åŠ¡å®Œæˆï¼Œç”Ÿæˆæ€»ç»“
            }
        )

        graph_builder.add_edge("summary", END)

        graph = graph_builder.compile()
        return graph

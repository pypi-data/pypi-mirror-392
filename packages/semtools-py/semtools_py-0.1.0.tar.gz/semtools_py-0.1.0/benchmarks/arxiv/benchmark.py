import argparse
import json
import os
import re
import shutil
import subprocess
import time
from dataclasses import dataclass
from pathlib import Path
from typing import Dict, List, Set

import google.generativeai as genai
import tiktoken

# --- Configuration ---
DATASET_DIR = Path("benchmarks/arxiv/arxiv_dataset_1000_papers")
LLM_ORACLE_MODEL = "gemini-2.5-flash"

# A curated set of files with a clear chronological and topical spread.
CURATED_FILE_SET = [
    "full_text/2505.07796v2.txt",  # May
    "full_text/2506.15594v1.txt",  # June
    "full_text/2506.22309v1.txt",  # June
    "full_text/2507.15152v1.txt",  # July
    "full_text/2508.14302v1.txt",  # August
    "full_text/2508.15370v1.txt",  # August
]


# --- Data Models ---


@dataclass
class GeneratedQuestion:
    """Represents a single query generated by the Oracle."""

    query_id: str
    query_text: str
    query_type: str
    ground_truth_answer: str
    source_documents: Set[str]
    generation_prompt: str = ""


@dataclass
class EvaluationResult:
    """Contains all data for a single evaluated question."""

    question: GeneratedQuestion
    search_stdout: str
    returned_documents: Set[str]
    llm_synthesized_answer: str
    retrieval_precision: float
    retrieval_recall: float
    synthesis_prompt: str = ""
    search_command: str = ""


# --- Core Logic ---


class GeminiOracle:
    """Uses the Gemini API to generate questions and synthesize answers."""

    def __init__(self, api_key: str, model_name: str = LLM_ORACLE_MODEL):
        genai.configure(api_key=api_key)
        self.model = genai.GenerativeModel(model_name)
        self.tokenizer = tiktoken.get_encoding("cl100k_base")

    def count_tokens(self, text: str) -> int:
        """Counts tokens for the given text using tiktoken."""
        return len(self.tokenizer.encode(text))

    def generate_questions_and_answers(self, aggregated_content: str) -> List[GeneratedQuestion]:
        """Generates questions, ground truth answers, and source document lists."""
        print("Oracle: Generating questions and ground truth answers from curated files...")

        prompt = f"""
        You are an expert researcher. I will provide you with the full text content from several research papers.
        Your task is to generate a set of 9 high-quality questions based *only* on the provided text.
        The questions must fall into three specific categories:

        1.  **Simple Semantic Queries (3 questions):** Each question should be answerable using information found predominantly within a SINGLE one of the provided documents.
        2.  **Cross-Document Synthesis Queries (3 questions):** Each question must require combining or comparing information from AT LEAST TWO different documents to answer correctly.
        3.  **Temporal Evolution Queries (3 questions):** Each question should ask about a trend, change, or first appearance of a concept *within the chronological context of the provided documents*.

        For EACH of the 9 questions, you MUST provide:
        - A unique `query_id` (e.g., "simple_1", "synthesis_1", "temporal_1").
        - The `query_text`.
        - The `query_type` ("Simple", "Synthesis", or "Temporal").
        - A detailed `ground_truth_answer` that directly and comprehensively answers the question based on the provided texts.
        - A `source_documents` list containing the exact file paths of the document(s) you used to formulate the answer.

        OUTPUT FORMAT (JSON):
        [
          {{
            "query_id": "...",
            "query_text": "...",
            "query_type": "...",
            "ground_truth_answer": "...",
            "source_documents": ["full_text/..."]
          }},
          ...
        ]

        CONTEXT DOCUMENTS:
        ---
        {aggregated_content}
        """

        last_exception = None
        for attempt in range(4):  # 1 initial + 3 retries
            try:
                response = self.model.generate_content(prompt)
                clean_response = re.sub(r"```json\n|```", "", response.text).strip()
                data = json.loads(clean_response)

                questions = []
                for item in data:
                    questions.append(
                        GeneratedQuestion(
                            query_id=item["query_id"],
                            query_text=item["query_text"],
                            query_type=item["query_type"],
                            ground_truth_answer=item["ground_truth_answer"],
                            source_documents=set(item["source_documents"]),
                            generation_prompt=prompt,
                        )
                    )
                print(f"  -> Generated {len(questions)} questions successfully.")
                return questions
            except (json.JSONDecodeError, KeyError, ValueError, TypeError) as e:
                last_exception = e
                print(
                    f"Oracle: Failed to generate questions (attempt {attempt + 1}/4). Retrying in 5s... Error: {e}"
                )
                if attempt < 3:
                    time.sleep(5)

        error_message = f"Oracle: Failed to generate or parse questions after all retries. Error: {last_exception}\n"
        raise RuntimeError(error_message) from last_exception

    def synthesize_answer_from_snippets(
            self, question_text: str, search_stdout: str
    ) -> tuple[str, str]:
        """Asks the LLM to synthesize an answer using only the provided search snippets."""
        print(f"Oracle: Synthesizing answer for query: '{question_text[:50]}...'")

        prompt = f"""
        You are an expert researcher. I will provide you with a research question and the raw text output from a semantic search tool that was run with that question.
        Your task is to synthesize a final, comprehensive answer to the question using ONLY the information available in the provided search tool output.

        RULES:
        1.  Base your answer *exclusively* on the text snippets in the "SEARCH TOOL OUTPUT" section.
        2.  Do not use any external knowledge or information you might have about the topic.
        3.  If the snippets are irrelevant, contradictory, or insufficient to answer the question, you must state that and explain why.
        4.  Your goal is to demonstrate what a user could learn *only* from the given search results.

        OUTPUT FORMAT (JSON):
        {{
          "synthesized_answer": "Your comprehensive answer here. If you cannot answer, explain why."
        }}

        ---
        RESEARCH QUESTION:
        {question_text}
        ---
        SEARCH TOOL OUTPUT:
        ```
        {search_stdout or "[No output from tool]"}
        ```
        ---
        """
        last_exception = None
        for attempt in range(4):  # 1 initial + 3 retries
            try:
                response = self.model.generate_content(prompt)
                clean_response = re.sub(r"```json\n|```", "", response.text).strip()
                data = json.loads(clean_response)
                answer = data["synthesized_answer"]
                print("  -> Successfully synthesized answer from snippets.")
                return answer, prompt
            except (json.JSONDecodeError, KeyError, ValueError, TypeError) as e:
                last_exception = e
                print(
                    f"Oracle: Failed to synthesize answer (attempt {attempt + 1}/4). Retrying in 5s... Error: {e}"
                )
                if attempt < 3:
                    time.sleep(5)

        error_message = f"Oracle: Failed to synthesize answer after all retries. Error: {last_exception}\n"
        raise RuntimeError(error_message) from last_exception


class Evaluator:
    """Runs the search tool and orchestrates Oracle synthesis."""

    def __init__(self, env: Dict):
        self.env = env

    def _run_command(self, command: str) -> tuple[str, str]:
        """Runs a command and captures its output."""
        print(f"  Executing: {command}")
        try:
            process = subprocess.run(
                command,
                shell=True,
                capture_output=True,
                text=True,
                timeout=300,
                cwd=DATASET_DIR,
                env=self.env,
                check=False,
            )
            return process.stdout, process.stderr
        except subprocess.TimeoutExpired as e:
            return e.stdout or "", e.stderr or "Timeout expired."

    @staticmethod
    def _parse_output_for_paths(stdout: str) -> Set[str]:
        """Extracts unique file paths from tool output."""
        paths = re.findall(r"^[\w/.\-]+?\.txt", stdout, re.MULTILINE)
        return set(paths)

    def evaluate_question(
        self,
        question: GeneratedQuestion,
        oracle: GeminiOracle,
        top_k: int,
        n_lines: int,
        max_distance: float | None,
    ) -> EvaluationResult:
        """
        Runs `search`, gets a synthesized answer from the Oracle, and returns the result.
        """
        print(f"\n--- Evaluating Query ID: {question.query_id} ---")

        # 1. Build and run `search` command
        # Use json.dumps to produce a clean, double-quoted string suitable for shell.
        escaped_query = json.dumps(question.query_text)
        cmd_parts = [f"search {escaped_query} full_text/*.txt", f"--top-k {top_k}", f"--n-lines {n_lines}"]
        if max_distance is not None:
            cmd_parts.append(f"--max-distance {max_distance}")

        search_cmd = " ".join(cmd_parts)
        search_stdout, search_stderr = self._run_command(search_cmd)
        returned_documents = self._parse_output_for_paths(search_stdout)
        full_output = search_stdout or search_stderr

        # 2. Calculate retrieval metrics
        true_positives = len(question.source_documents.intersection(returned_documents))
        precision = true_positives / len(returned_documents) if returned_documents else 0.0
        recall = true_positives / len(question.source_documents) if question.source_documents else 0.0

        # 3. Get synthesized answer from Oracle
        synthesized_answer, synthesis_prompt = oracle.synthesize_answer_from_snippets(
            question.query_text, full_output
        )

        return EvaluationResult(
            question=question,
            search_stdout=full_output,
            returned_documents=returned_documents,
            retrieval_precision=precision,
            retrieval_recall=recall,
            llm_synthesized_answer=synthesized_answer,
            synthesis_prompt=synthesis_prompt,
            search_command=search_cmd,
        )


class ReportGenerator:
    """Generates a qualitative Markdown report from evaluation results."""

    def generate(self, results: List[EvaluationResult], output_file: Path, mode: str):
        """Writes the full report to a file."""
        with open(output_file, "w", encoding="utf-8") as f:
            f.write(f"# Semtools Qualitative Benchmark Report\n\n")
            f.write(f"- **Mode:** `{mode}`\n- **Oracle:** `{LLM_ORACLE_MODEL}`\n\n")
            self._write_detailed_results(f, results)
        print(f"\n--- Benchmark complete. Report generated at: {output_file} ---")

    @staticmethod
    def _write_detailed_results(f, results: List[EvaluationResult]):
        f.write("\n## Detailed Analysis per Query\n\n")
        for i, res in enumerate(results):
            if i > 0:
                f.write("\n---\n\n")
            q = res.question

            f.write(f"### {i + 1}. Query: {q.query_text}\n\n")
            f.write(f"- **Query ID:** `{q.query_id}`\n")
            f.write(f"- **Query Type:** {q.query_type}\n")
            f.write(f"- **Curated Source Files:** `{', '.join(sorted(CURATED_FILE_SET))}`\n\n")

            f.write("#### LLM Ground Truth Answer\n\n")
            f.write(f"> {'\n> '.join(q.ground_truth_answer.splitlines())}\n\n")
            f.write(f"**Source Document(s):** `{', '.join(sorted(q.source_documents))}`\n\n")

            f.write("#### `search` Tool Performance\n\n")
            f.write(f"**Command Executed:**\n\n")
            f.write(f"```bash\n{res.search_command}\n```\n\n")
            f.write(f"**Retrieval Precision:** `{res.retrieval_precision:.2f}`\n")
            f.write(f"**Retrieval Recall:** `{res.retrieval_recall:.2f}`\n\n")
            f.write(f"**Documents Returned by `search`:** `{', '.join(sorted(res.returned_documents)) or 'None'}`\n\n")
            f.write("#### LLM Search-Augmented Answer\n\n")
            f.write(f"> {'\n> '.join(res.llm_synthesized_answer.splitlines())}\n\n")

            f.write("<details><summary>View Original `search` Tool Output</summary>\n\n")
            f.write(f"```\n{res.search_stdout or '[No output]'}\n```\n\n")
            f.write("</details>\n\n")


def main():
    """Main function to run the qualitative benchmark."""
    parser = argparse.ArgumentParser(description="Run qualitative benchmark for semtools.")
    parser.add_argument(
        "--api-key",
        type=str,
        default=os.getenv("GEMINI_API_KEY"),
        help="Gemini API key. Defaults to GEMINI_API_KEY environment variable.",
    )
    parser.add_argument(
        "--mode",
        choices=["in-memory", "workspace"],
        default="workspace",
        help="The search mode to benchmark.",
    )
    parser.add_argument(
        "--keep-workspace",
        action="store_true",
        help="Do not delete and re-prime the workspace if it already exists.",
    )
    parser.add_argument(
        "--top-k",
        type=int,
        default=10,
        help="The --top-k parameter to pass to the search command.",
    )
    parser.add_argument(
        "--n-lines",
        type=int,
        default=7,
        help="The --n-lines parameter to pass to the search command.",
    )
    parser.add_argument(
        "--max-distance",
        type=float,
        default=None,
        help="The --max-distance parameter to pass to the search command.",
    )
    args = parser.parse_args()

    if not args.api_key:
        raise ValueError("Gemini API key is required. Set --api-key or the GEMINI_API_KEY environment variable.")

    # --- Setup ---
    run_env = os.environ.copy()
    if args.mode == "workspace":
        workspace_name = "qualitative_benchmark_ws"
        workspace_path = Path.home() / ".semtools" / "workspaces" / workspace_name

        print(f"--- Preparing workspace '{workspace_name}' for benchmark ---")
        if not args.keep_workspace and workspace_path.exists():
            print(f"Deleting existing workspace (use --keep-workspace to prevent this): {workspace_path}")
            shutil.rmtree(workspace_path)

        run_env["SEMTOOLS_WORKSPACE"] = workspace_name
        proc = subprocess.run(
            ["workspace", "use", workspace_name], capture_output=True, text=True, check=False
        )
        if proc.returncode != 0:
            raise RuntimeError(f"Failed to set workspace: {proc.stderr}")

        print("--- Priming workspace index with all files (this may take a while)... ---")
        prime_start = time.monotonic()

        prime_cmd = 'search "the" full_text/*.txt --top-k 1'
        prime_proc = subprocess.run(
            prime_cmd,
            shell=True, cwd=str(DATASET_DIR), env=run_env, capture_output=True, text=True, check=False
        )
        if prime_proc.returncode != 0:
            raise RuntimeError(f"Workspace priming failed: {prime_proc.stderr}")
        prime_duration = time.monotonic() - prime_start
        print(f"--- Workspace primed in {prime_duration:.2f}s. Starting benchmark. ---")
    else:
        run_env.pop("SEMTOOLS_WORKSPACE", None)

    oracle = GeminiOracle(api_key=args.api_key)
    evaluator = Evaluator(env=run_env)
    report_generator = ReportGenerator()

    # --- Question Generation ---
    print("\n--- Aggregating content from curated files for Oracle ---")
    aggregated_content_parts = [
        f"--- DOCUMENT: {fp} ---\n{(DATASET_DIR / fp).read_text(encoding='utf-8')}"
        for fp in CURATED_FILE_SET  # do raise if not exists
    ]
    aggregated_content = "\n\n".join(aggregated_content_parts)
    questions_to_run = oracle.generate_questions_and_answers(aggregated_content)

    # --- Evaluation Loop & Report ---
    all_evaluations = [
        evaluator.evaluate_question(
            q, oracle, top_k=args.top_k, n_lines=args.n_lines, max_distance=args.max_distance
        )
        for q in questions_to_run
    ]
    report_path = Path(f"benchmark_qualitative_report_{args.mode}.md")
    report_generator.generate(all_evaluations, report_path, mode=args.mode)


if __name__ == "__main__":
    main()
{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Framework Ingestion Notebook Example\n",
        "\n",
        "This notebook demonstrates the framework contract for ingestion tasks.\n",
        "It reads from a source Delta table, transforms the data, and writes to a target Delta table.\n",
        "\n",
        "**Framework Contract:**\n",
        "- Accepts `task_key`, `control_table`, and `parameters` as inputs via widgets\n",
        "- Parameters contain catalog, schema, source_table, target_table, and write_mode\n",
        "- In production, the framework will pass these via widgets\n",
        "- For this example, widgets have default values so it can run end-to-end without manual input\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import logging\n",
        "import json\n",
        "from pyspark.sql import functions as F\n",
        "\n",
        "# Configure logging\n",
        "logging.basicConfig(level=logging.INFO)\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "# Framework widgets - In production, these are set by the framework\n",
        "# For this example, widgets have default values so it can run end-to-end\n",
        "dbutils.widgets.text(\"task_key\", \"delta_table_ingestion\", \"Task Key\")\n",
        "dbutils.widgets.text(\"control_table\", \"main.examples.etl_control\", \"Control Table\")\n",
        "dbutils.widgets.text(\"parameters\", '{\"catalog\": \"fe_ppark_demo\", \"schema\": \"lakeflow_job_metadata\", \"source_table\": \"source_customers\", \"target_table\": \"customers\", \"write_mode\": \"append\"}', \"Parameters\")\n",
        "\n",
        "# Get widget values\n",
        "task_key = dbutils.widgets.get(\"task_key\")\n",
        "control_table = dbutils.widgets.get(\"control_table\")\n",
        "parameters_str = dbutils.widgets.get(\"parameters\")\n",
        "\n",
        "if not task_key:\n",
        "    raise ValueError(\"task_key widget is required\")\n",
        "if not control_table:\n",
        "    raise ValueError(\"control_table widget is required\")\n",
        "if not parameters_str:\n",
        "    raise ValueError(\"parameters widget is required\")\n",
        "\n",
        "logger.info(f\"Processing task_key: {task_key}\")\n",
        "\n",
        "# Parse parameters JSON\n",
        "try:\n",
        "    parameters = json.loads(parameters_str) if isinstance(parameters_str, str) else parameters_str\n",
        "    \n",
        "    # Extract catalog, schema, and table names\n",
        "    catalog = parameters.get('catalog')\n",
        "    schema = parameters.get('schema')\n",
        "    source_table = parameters.get('source_table')\n",
        "    target_table = parameters.get('target_table')\n",
        "    write_mode = parameters.get('write_mode', 'overwrite')\n",
        "    \n",
        "    # Validate required fields\n",
        "    if not catalog:\n",
        "        raise ValueError(f\"Missing 'catalog' in parameters for task_key '{task_key}'\")\n",
        "    if not schema:\n",
        "        raise ValueError(f\"Missing 'schema' in parameters for task_key '{task_key}'\")\n",
        "    if not source_table:\n",
        "        raise ValueError(f\"Missing 'source_table' in parameters for task_key '{task_key}'\")\n",
        "    if not target_table:\n",
        "        raise ValueError(f\"Missing 'target_table' in parameters for task_key '{task_key}'\")\n",
        "    \n",
        "    logger.info(f\"Successfully parsed parameters\")\n",
        "    logger.info(f\"Catalog: {catalog}, Schema: {schema}\")\n",
        "    logger.info(f\"Source table: {source_table}\")\n",
        "    logger.info(f\"Target table: {target_table}\")\n",
        "    logger.info(f\"Write mode: {write_mode}\")\n",
        "    \n",
        "except Exception as e:\n",
        "    logger.error(f\"Failed to parse parameters: {str(e)}\")\n",
        "    raise\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Parse and Validate Configurations\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Configuration already validated when reading from control table\n",
        "logger.info(\"Configuration validated successfully\")\n",
        "logger.info(f\"Source: {catalog}.{schema}.{source_table}\")\n",
        "logger.info(f\"Target: {catalog}.{schema}.{target_table}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Prepare Source Data\n",
        "\n",
        "If the source table doesn't exist, create sample data for demonstration.\n",
        "In production, this would read directly from the configured source table.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from pyspark.sql.types import StructType, StructField, StringType\n",
        "\n",
        "source_table_full = f\"{catalog}.{schema}.{source_table}\"\n",
        "\n",
        "logger.info(f\"Reading from source table: {source_table_full}\")\n",
        "\n",
        "# Try to read from source table, if it doesn't exist, create sample data\n",
        "df = None\n",
        "record_count = 0\n",
        "\n",
        "try:\n",
        "    df = spark.table(source_table_full)\n",
        "    record_count = df.count()\n",
        "    logger.info(f\"Successfully read {record_count} records from source table\")\n",
        "    df.show(5, truncate=False)\n",
        "except Exception as e:\n",
        "    logger.warning(f\"Source table not found: {source_table_full}. Creating sample data for demonstration.\")\n",
        "    \n",
        "    # Create sample source data\n",
        "    sample_data = [\n",
        "        (\"CUST001\", \"John\", \"Doe\", \"john.doe@example.com\", \"2024-01-15\", \"active\"),\n",
        "        (\"CUST002\", \"Jane\", \"Smith\", \"jane.smith@example.com\", \"2024-01-16\", \"active\"),\n",
        "        (\"CUST003\", \"Bob\", \"Johnson\", \"bob.johnson@example.com\", \"2024-01-17\", \"inactive\"),\n",
        "        (\"CUST004\", \"Alice\", \"Williams\", \"alice.williams@example.com\", \"2024-01-18\", \"active\"),\n",
        "        (\"CUST005\", \"Charlie\", \"Brown\", \"charlie.brown@example.com\", \"2024-01-19\", \"active\")\n",
        "    ]\n",
        "    \n",
        "    schema = StructType([\n",
        "        StructField(\"customer_id\", StringType(), True),\n",
        "        StructField(\"first_name\", StringType(), True),\n",
        "        StructField(\"last_name\", StringType(), True),\n",
        "        StructField(\"email\", StringType(), True),\n",
        "        StructField(\"registration_date\", StringType(), True),\n",
        "        StructField(\"status\", StringType(), True)\n",
        "    ])\n",
        "    \n",
        "    df = spark.createDataFrame(sample_data, schema)\n",
        "    \n",
        "    # Create the source table for demonstration purposes\n",
        "    df.write.format(\"delta\").mode(\"overwrite\").saveAsTable(source_table_full)\n",
        "    record_count = df.count()\n",
        "    logger.info(f\"Created sample source table with {record_count} records\")\n",
        "    df.show(5, truncate=False)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Transform Data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Transform data: add metadata columns and apply business logic\n",
        "df_transformed = df.withColumn(\"ingestion_timestamp\", F.current_timestamp()) \\\n",
        "                   .withColumn(\"task_key\", F.lit(task_key)) \\\n",
        "                   .withColumn(\"full_name\", F.concat(F.col(\"first_name\"), F.lit(\" \"), F.col(\"last_name\"))) \\\n",
        "                   .filter(F.col(\"status\") == \"active\")  # Example: filter only active customers\n",
        "\n",
        "record_count_transformed = df_transformed.count()\n",
        "logger.info(f\"Transformed data: {record_count_transformed} records (filtered from {record_count} source records)\")\n",
        "logger.info(\"Sample transformed data:\")\n",
        "df_transformed.select(\"customer_id\", \"full_name\", \"email\", \"status\", \"task_key\", \"ingestion_timestamp\").show(5, truncate=False)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Write to Target\n",
        "\n",
        "Write transformed data to the target Delta table.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "target_table_full = f\"{catalog}.{schema}.{target_table}\"\n",
        "\n",
        "logger.info(f\"Writing to target table: {target_table_full}\")\n",
        "logger.info(f\"Write mode: {write_mode}\")\n",
        "logger.info(f\"Records to write: {record_count_transformed}\")\n",
        "\n",
        "# Write to target Delta table\n",
        "try:\n",
        "    df_transformed.write \\\n",
        "        .format(\"delta\") \\\n",
        "        .mode(write_mode) \\\n",
        "        .option(\"mergeSchema\", \"true\") \\\n",
        "        .saveAsTable(target_table_full)\n",
        "    \n",
        "    logger.info(f\"✅ Successfully wrote {record_count_transformed} records to {target_table_full}\")\n",
        "    \n",
        "    # Verify the write\n",
        "    written_df = spark.table(target_table_full)\n",
        "    written_count = written_df.count()\n",
        "    logger.info(f\"✅ Verified: {written_count} records in target table\")\n",
        "    written_df.select(\"customer_id\", \"full_name\", \"email\", \"status\", \"task_key\", \"ingestion_timestamp\").show(5, truncate=False)\n",
        "    \n",
        "except Exception as e:\n",
        "    logger.error(f\"Failed to write to target table: {str(e)}\")\n",
        "    raise\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Summary\n",
        "\n",
        "✅ Parameters parsed and validated  \n",
        "✅ Source data read from Delta table (or created for demo)  \n",
        "✅ Data transformed and enriched  \n",
        "✅ Data written to target Delta table  \n",
        "\n",
        "**Framework Contract:**  \n",
        "This notebook demonstrates the expected contract:\n",
        "- Accepts `task_key`, `control_table`, and `parameters` as inputs via widgets\n",
        "- Parameters contain `catalog`, `schema`, `source_table`, `target_table`, and `write_mode`\n",
        "- Validates configuration\n",
        "- Reads from source Delta table\n",
        "- Transforms data (adds metadata columns, applies business logic)\n",
        "- Writes to target Delta table\n",
        "\n",
        "**Widgets Used:**\n",
        "- `task_key`: Unique identifier for this task\n",
        "- `control_table`: Name of the control table containing job metadata\n",
        "- `parameters`: JSON string with task parameters (catalog, schema, source_table, target_table, write_mode)\n",
        "\n",
        "**Parameters Structure:**\n",
        "The notebook expects parameters JSON with:\n",
        "- `catalog`: Catalog name (e.g., \"fe_ppark_demo\")\n",
        "- `schema`: Schema name (e.g., \"lakeflow_job_metadata\")\n",
        "- `source_table`: Source table name (e.g., \"source_customers\")\n",
        "- `target_table`: Target table name (e.g., \"customers\")\n",
        "- `write_mode`: Write mode (defaults to \"overwrite\" if not specified)\n",
        "\n",
        "The framework will set `task_key`, `control_table`, and `parameters` widgets when calling this notebook as a task.\n"
      ]
    }
  ],
  "metadata": {
    "application/vnd.databricks.v1+notebook": {
      "language": "python",
      "notebookName": "sample_ingestion_notebook"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}

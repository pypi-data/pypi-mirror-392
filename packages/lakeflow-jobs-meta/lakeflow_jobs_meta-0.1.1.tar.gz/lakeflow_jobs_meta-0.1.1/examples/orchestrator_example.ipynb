{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lakeflow Jobs Meta - Orchestrator Example\n",
    "\n",
    "This notebook demonstrates how to use the Lakeflow Jobs Meta framework to create and manage metadata-driven Databricks Lakeflow Jobs.\n",
    "\n",
    "## ⚠️ Before Running\n",
    "**Update the configuration below to match your environment:**\n",
    "- `DEFAULT_CONTROL_TABLE`: Your catalog and schema\n",
    "- `DEFAULT_YAML_PATH`: Path to your YAML file\n",
    "- `DEFAULT_QUERIES_PATH`: Path for SQL queries\n",
    "- Update paths in your YAML metadata file\n",
    "\n",
    "## Features\n",
    "- Supports multiple task types: Notebook, SQL Query, SQL File\n",
    "- Dynamic job generation from metadata\n",
    "- Job lifecycle management (create/update/track)\n",
    "- Execution order and dependency management\n",
    "- Optional continuous monitoring\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install --upgrade databricks-sdk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dbutils.library.restartPython()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a33ebe32-8c36-4b18-88a7-acf54cf0fa8f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Import framework modules\n",
    "import sys\n",
    "import os\n",
    "import logging\n",
    "\n",
    "# Set logging level to INFO for everything (keeps noise down)\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'\n",
    ")\n",
    "\n",
    "# Set DEBUG only for lakeflow_jobs_meta.orchestrator\n",
    "logging.getLogger('lakeflow_jobs_meta.orchestrator').setLevel(logging.DEBUG)\n",
    "\n",
    "# Dynamically detect project root from notebook location\n",
    "try:\n",
    "    # Get notebook path from Databricks context\n",
    "    notebook_path = dbutils.notebook.entry_point.getDbutils().notebook().getContext().notebookPath().get()\n",
    "    # Extract project root directory (go up from examples/)\n",
    "    project_root = os.path.dirname(os.path.dirname(notebook_path))\n",
    "    sys.path.insert(0, project_root)\n",
    "    logger_tmp = logging.getLogger(__name__)\n",
    "    logger_tmp.info(f\"✅ Added project root to path: {project_root}\")\n",
    "except Exception as e:\n",
    "    # Fallback: Try current directory\n",
    "    current_dir = os.path.abspath('.')\n",
    "    if os.path.exists(os.path.join(current_dir, 'lakeflow_jobs_meta')):\n",
    "        sys.path.insert(0, current_dir)\n",
    "    else:\n",
    "        # If package is installed, this is fine\n",
    "        pass\n",
    "\n",
    "# Import framework\n",
    "from lakeflow_jobs_meta import JobOrchestrator, MetadataMonitor\n",
    "from lakeflow_jobs_meta.constants import SUPPORTED_TASK_TYPES\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "logger.info(f\"Supported task types: {', '.join(SUPPORTED_TASK_TYPES)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEFAULT_CONTROL_TABLE = \"fe_ppark_demo.lakeflow_jobs_metadata.jobs_metadata_control_table\"\n",
    "DEFAULT_YAML_PATH = \"/Workspace/Users/peter.park@databricks.com/lakeflow_jobs_meta/examples/metadata_examples.yaml\"\n",
    "# Or use a folder path (loads all YAML files):\n",
    "# DEFAULT_YAML_PATH = \"/Workspace/Users/peter.park@databricks.com/lakeflow_jobs_meta/examples/\"\n",
    "# Or use a volume path:\n",
    "# DEFAULT_YAML_PATH = \"/Volumes/fe_ppark_demo/lakeflow_jobs_metadata/config_folder/\"\n",
    "DEFAULT_QUERIES_PATH = \"/Workspace/Users/peter.park@databricks.com/queries\"\n",
    "DEFAULT_WAREHOUSE_ID = \"4b9b953939869799\"\n",
    "\n",
    "# Create widgets for configuration\n",
    "dbutils.widgets.text(\"control_table\", DEFAULT_CONTROL_TABLE, \"Control Table\")\n",
    "dbutils.widgets.text(\"yaml_path\", DEFAULT_YAML_PATH, \"YAML Path (file, folder, or volume)\")\n",
    "dbutils.widgets.text(\"default_warehouse_id\", DEFAULT_WAREHOUSE_ID, \"Default SQL Warehouse ID (optional, for SQL tasks)\")\n",
    "dbutils.widgets.text(\"default_queries_path\", DEFAULT_QUERIES_PATH, \"Default Queries Save Path (optional)\")\n",
    "\n",
    "# Get widget values\n",
    "CONTROL_TABLE = dbutils.widgets.get(\"control_table\")\n",
    "YAML_PATH = dbutils.widgets.get(\"yaml_path\") or None\n",
    "DEFAULT_WAREHOUSE_ID = dbutils.widgets.get(\"default_warehouse_id\") or None\n",
    "DEFAULT_QUERIES_PATH = dbutils.widgets.get(\"default_queries_path\") or None\n",
    "\n",
    "# Validate required parameter\n",
    "if not CONTROL_TABLE:\n",
    "    raise ValueError(\"control_table widget is required. Please set it in the widget or via base_parameters.\")\n",
    "\n",
    "logger.info(f\"Configuration:\")\n",
    "logger.info(f\"  Control Table: {CONTROL_TABLE}\")\n",
    "logger.info(f\"  YAML Path: {YAML_PATH or 'Not configured (will process all jobs in control table)'}\")\n",
    "if YAML_PATH:\n",
    "    if YAML_PATH.startswith(\"/Volumes/\"):\n",
    "        logger.info(f\"    Path Type: Unity Catalog Volume\")\n",
    "        logger.info(f\"    Behavior: Load from volume and process ONLY those jobs\")\n",
    "    elif YAML_PATH.endswith((\".yaml\", \".yml\")):\n",
    "        logger.info(f\"    Path Type: YAML File\")\n",
    "        logger.info(f\"    Behavior: Load from YAML file and process ONLY those jobs\")\n",
    "    else:\n",
    "        logger.info(f\"    Path Type: Folder (will load all YAML files)\")\n",
    "        logger.info(f\"    Behavior: Load all YAML files from folder and process ONLY those jobs\")\n",
    "else:\n",
    "    logger.info(f\"    Behavior: Process ALL jobs in control table\")\n",
    "logger.info(f\"  Default Warehouse ID: {DEFAULT_WAREHOUSE_ID or 'Not configured (SQL tasks must specify warehouse_id)'}\")\n",
    "logger.info(f\"  Default Queries Path: {DEFAULT_QUERIES_PATH or 'Not configured'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Initialize JobOrchestrator\n",
    "orchestrator = JobOrchestrator(\n",
    "    control_table=CONTROL_TABLE,\n",
    "    default_warehouse_id=DEFAULT_WAREHOUSE_ID,\n",
    "    default_queries_path=DEFAULT_QUERIES_PATH\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and update jobs\n",
    "# If yaml_path is provided: Loads metadata and processes ONLY those jobs\n",
    "# If yaml_path is NOT provided: Processes ALL jobs in control table\n",
    "# Supports YAML files, folders (recursive), and Unity Catalog volumes\n",
    "\n",
    "jobs = orchestrator.create_or_update_jobs(\n",
    "    yaml_path=YAML_PATH,  # Automatically detects YAML file, folder, or volume\n",
    "    default_pause_status=False  # False = new manual jobs auto-run, scheduled jobs active; True = no auto-run, scheduled jobs paused\n",
    ")\n",
    "\n",
    "logger.info(f\"✅ Managed {len(jobs)} jobs successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7dbfba10-1ded-42e1-b5f7-89df0571e86f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Uncomment to enable continuous monitoring\n",
    "# This will check for metadata changes every 60 seconds and auto-update jobs\n",
    "# Note: For volume_path, use a Unity Catalog volume path (e.g., \"/Volumes/catalog/schema/volume\")\n",
    "\n",
    "# volume_path = YAML_PATH if YAML_PATH and YAML_PATH.startswith(\"/Volumes/\") else None\n",
    "# if volume_path:\n",
    "#     monitor = MetadataMonitor(\n",
    "#         control_table=CONTROL_TABLE,\n",
    "#         check_interval_seconds=60,\n",
    "#         volume_path=volume_path,  # Watch Unity Catalog volume for YAML files\n",
    "#         auto_update_jobs=True\n",
    "#     )\n",
    "#     monitor.run_continuous(max_iterations=None)  # None = run indefinitely\n",
    "# else:\n",
    "#     logger.warning(\"Volume path not configured. Monitoring requires a Unity Catalog volume path.\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 2013334455468117,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 2
   },
   "notebookName": "custom_metadata_sql",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

# coding: utf-8

"""
    RankVectors API

    Intelligent internal linking optimization API using AI.   RankVectors helps you automatically discover and implement optimal internal links  across your website to improve SEO performance and user experience.  ## Key Features - **AI-Powered Analysis**: Uses OpenAI embeddings to find optimal linking opportunities - **Smart Crawling**: Automatically crawls and analyzes your website content - **Automated Implementation**: Implement links via webhooks or manual instructions - **Page-Based Plans**: Predictable pricing by number of pages monitored - **Multi-Platform Support**: Works with any CMS or platform via REST API  ## Getting Started 1. Create a project with your website URL 2. Start a crawl to analyze your content 3. Generate AI-powered link suggestions 4. Implement suggestions via API or webhook 5. Track performance and manage page usage and limits  ## Authentication Most API endpoints support authentication using your RankVectors API key. Include your API key in the `Authorization` header: ``` Authorization: Bearer YOUR_API_KEY ```  Get your API key from your RankVectors dashboard: Settings â†’ API Keys  **Note**: Some endpoints (marked in the documentation) support both API key authentication and web session authentication (Stack Auth).  API key authentication is required for SDK usage and external integrations like WordPress plugins. 

    The version of the OpenAPI document: 1.3.1
    Contact: tj@rankvectors.com
    Generated by OpenAPI Generator (https://openapi-generator.tech)

    Do not edit the class manually.
"""  # noqa: E501


from __future__ import annotations
import pprint
import re  # noqa: F401
import json

from pydantic import BaseModel, ConfigDict, Field, StrictBool, StrictInt, StrictStr, field_validator
from typing import Any, ClassVar, Dict, List, Optional
from typing import Optional, Set
from typing_extensions import Self

class CreateProjectRequest(BaseModel):
    """
    CreateProjectRequest
    """ # noqa: E501
    name: StrictStr = Field(description="Project name")
    domain: StrictStr = Field(description="Website domain URL")
    prompt: Optional[StrictStr] = Field(default=None, description="Natural language prompt for crawling")
    search_query: Optional[StrictStr] = Field(default=None, description="Search query for targeted crawling", alias="searchQuery")
    sitemap_mode: Optional[StrictStr] = Field(default='include', description="How to handle sitemaps", alias="sitemapMode")
    include_subdomains: Optional[StrictBool] = Field(default=True, description="Whether to include subdomains", alias="includeSubdomains")
    ignore_query_params: Optional[StrictBool] = Field(default=True, description="Whether to ignore URL query parameters", alias="ignoreQueryParams")
    max_discovery_depth: Optional[StrictInt] = Field(default=None, description="Maximum crawl depth", alias="maxDiscoveryDepth")
    exclude_paths: Optional[List[StrictStr]] = Field(default=None, description="Paths to exclude from crawling", alias="excludePaths")
    include_paths: Optional[List[StrictStr]] = Field(default=None, description="Specific paths to include", alias="includePaths")
    crawl_entire_domain: Optional[StrictBool] = Field(default=False, description="Whether to crawl the entire domain", alias="crawlEntireDomain")
    allow_external_links: Optional[StrictBool] = Field(default=False, description="Whether to allow external links", alias="allowExternalLinks")
    max_pages: Optional[StrictInt] = Field(default=100, description="Maximum number of pages to crawl", alias="maxPages")
    crawl_delay: Optional[StrictInt] = Field(default=None, description="Delay between crawl requests (ms)", alias="crawlDelay")
    crawl_max_concurrency: Optional[StrictInt] = Field(default=None, description="Maximum concurrent crawl requests", alias="crawlMaxConcurrency")
    only_main_content: Optional[StrictBool] = Field(default=True, description="Whether to extract only main content", alias="onlyMainContent")
    custom_headers: Optional[Dict[str, StrictStr]] = Field(default=None, description="Custom headers for crawling", alias="customHeaders")
    wait_for: Optional[StrictInt] = Field(default=0, description="Wait time for page load (ms)", alias="waitFor")
    block_ads: Optional[StrictBool] = Field(default=True, description="Whether to block ads", alias="blockAds")
    proxy_mode: Optional[StrictStr] = Field(default='auto', description="Proxy mode for crawling", alias="proxyMode")
    use_reranking: Optional[StrictBool] = Field(default=True, description="Whether to use AI reranking", alias="useReranking")
    enable_change_tracking: Optional[StrictBool] = Field(default=False, description="Whether to enable change tracking", alias="enableChangeTracking")
    __properties: ClassVar[List[str]] = ["name", "domain", "prompt", "searchQuery", "sitemapMode", "includeSubdomains", "ignoreQueryParams", "maxDiscoveryDepth", "excludePaths", "includePaths", "crawlEntireDomain", "allowExternalLinks", "maxPages", "crawlDelay", "crawlMaxConcurrency", "onlyMainContent", "customHeaders", "waitFor", "blockAds", "proxyMode", "useReranking", "enableChangeTracking"]

    @field_validator('sitemap_mode')
    def sitemap_mode_validate_enum(cls, value):
        """Validates the enum"""
        if value is None:
            return value

        if value not in set(['include', 'exclude', 'only']):
            raise ValueError("must be one of enum values ('include', 'exclude', 'only')")
        return value

    @field_validator('proxy_mode')
    def proxy_mode_validate_enum(cls, value):
        """Validates the enum"""
        if value is None:
            return value

        if value not in set(['auto', 'residential', 'datacenter']):
            raise ValueError("must be one of enum values ('auto', 'residential', 'datacenter')")
        return value

    model_config = ConfigDict(
        populate_by_name=True,
        validate_assignment=True,
        protected_namespaces=(),
    )


    def to_str(self) -> str:
        """Returns the string representation of the model using alias"""
        return pprint.pformat(self.model_dump(by_alias=True))

    def to_json(self) -> str:
        """Returns the JSON representation of the model using alias"""
        # TODO: pydantic v2: use .model_dump_json(by_alias=True, exclude_unset=True) instead
        return json.dumps(self.to_dict())

    @classmethod
    def from_json(cls, json_str: str) -> Optional[Self]:
        """Create an instance of CreateProjectRequest from a JSON string"""
        return cls.from_dict(json.loads(json_str))

    def to_dict(self) -> Dict[str, Any]:
        """Return the dictionary representation of the model using alias.

        This has the following differences from calling pydantic's
        `self.model_dump(by_alias=True)`:

        * `None` is only added to the output dict for nullable fields that
          were set at model initialization. Other fields with value `None`
          are ignored.
        """
        excluded_fields: Set[str] = set([
        ])

        _dict = self.model_dump(
            by_alias=True,
            exclude=excluded_fields,
            exclude_none=True,
        )
        return _dict

    @classmethod
    def from_dict(cls, obj: Optional[Dict[str, Any]]) -> Optional[Self]:
        """Create an instance of CreateProjectRequest from a dict"""
        if obj is None:
            return None

        if not isinstance(obj, dict):
            return cls.model_validate(obj)

        _obj = cls.model_validate({
            "name": obj.get("name"),
            "domain": obj.get("domain"),
            "prompt": obj.get("prompt"),
            "searchQuery": obj.get("searchQuery"),
            "sitemapMode": obj.get("sitemapMode") if obj.get("sitemapMode") is not None else 'include',
            "includeSubdomains": obj.get("includeSubdomains") if obj.get("includeSubdomains") is not None else True,
            "ignoreQueryParams": obj.get("ignoreQueryParams") if obj.get("ignoreQueryParams") is not None else True,
            "maxDiscoveryDepth": obj.get("maxDiscoveryDepth"),
            "excludePaths": obj.get("excludePaths"),
            "includePaths": obj.get("includePaths"),
            "crawlEntireDomain": obj.get("crawlEntireDomain") if obj.get("crawlEntireDomain") is not None else False,
            "allowExternalLinks": obj.get("allowExternalLinks") if obj.get("allowExternalLinks") is not None else False,
            "maxPages": obj.get("maxPages") if obj.get("maxPages") is not None else 100,
            "crawlDelay": obj.get("crawlDelay"),
            "crawlMaxConcurrency": obj.get("crawlMaxConcurrency"),
            "onlyMainContent": obj.get("onlyMainContent") if obj.get("onlyMainContent") is not None else True,
            "customHeaders": obj.get("customHeaders"),
            "waitFor": obj.get("waitFor") if obj.get("waitFor") is not None else 0,
            "blockAds": obj.get("blockAds") if obj.get("blockAds") is not None else True,
            "proxyMode": obj.get("proxyMode") if obj.get("proxyMode") is not None else 'auto',
            "useReranking": obj.get("useReranking") if obj.get("useReranking") is not None else True,
            "enableChangeTracking": obj.get("enableChangeTracking") if obj.get("enableChangeTracking") is not None else False
        })
        return _obj



{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7b5f019b",
   "metadata": {},
   "source": [
    "## Configuration\n",
    "\n",
    "Select signal to analyze by uncommenting the desired line below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c06de524",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# SIGNAL SELECTION (Uncomment one signal)\n",
    "# ============================================================================\n",
    "\n",
    "SELECTED_SIGNAL = \"cdx_etf_basis\"\n",
    "# SELECTED_SIGNAL = \"cdx_vix_gap\"\n",
    "# SELECTED_SIGNAL = \"spread_momentum\"\n",
    "\n",
    "# ============================================================================\n",
    "# DATA SOURCE CONFIGURATION\n",
    "# ============================================================================\n",
    "\n",
    "USE_BLOOMBERG = True  # Set to False to force synthetic data\n",
    "\n",
    "print(f\"Selected Signal: {SELECTED_SIGNAL}\")\n",
    "print(f\"Data Source: {'Bloomberg (with fallback)' if USE_BLOOMBERG else 'Synthetic'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cd445c4",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a83a6c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from plotly.subplots import make_subplots\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "# Configure logging for notebook\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Suppress warnings for cleaner output\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"✅ Setup complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b60617d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import aponyx modules\n",
    "from aponyx.config import (\n",
    "    DATA_DIR,\n",
    "    LOGS_DIR,\n",
    "    SIGNAL_CATALOG_PATH,\n",
    "    STRATEGY_CATALOG_PATH,\n",
    ")\n",
    "from aponyx.data import fetch_cdx, fetch_vix, fetch_etf\n",
    "from aponyx.data.sources import BloombergSource, FileSource\n",
    "from aponyx.data.bloomberg_config import list_securities, get_security_spec\n",
    "from aponyx.models import compute_registered_signals, SignalConfig\n",
    "from aponyx.models.registry import SignalRegistry\n",
    "from aponyx.backtest import run_backtest\n",
    "from aponyx.backtest.registry import StrategyRegistry\n",
    "from aponyx.evaluation.suitability import (\n",
    "    evaluate_signal_suitability,\n",
    "    SuitabilityConfig,\n",
    "    SuitabilityRegistry,\n",
    ")\n",
    "from aponyx.evaluation.performance import (\n",
    "    analyze_backtest_performance,\n",
    "    compute_all_metrics,\n",
    "    PerformanceConfig,\n",
    "    PerformanceRegistry,\n",
    ")\n",
    "from aponyx.visualization import plot_signal, plot_equity_curve\n",
    "from aponyx.persistence import save_parquet, save_json\n",
    "\n",
    "print(\"✅ Imports complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12eca9e6",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Step 1: Data Acquisition\n",
    "\n",
    "Load market data required by the selected signal. Automatically falls back to synthetic data if Bloomberg cache is unavailable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb83cff4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load signal metadata to determine data requirements\n",
    "signal_registry = SignalRegistry(SIGNAL_CATALOG_PATH)\n",
    "signal_metadata = signal_registry.get_metadata(SELECTED_SIGNAL)\n",
    "\n",
    "print(f\"Signal: {signal_metadata.name}\")\n",
    "print(f\"Description: {signal_metadata.description}\")\n",
    "print(f\"Data Requirements: {signal_metadata.data_requirements}\")\n",
    "print(f\"Enabled: {signal_metadata.enabled}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "543483e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detect available data sources\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(f\"DATA SOURCE DETECTION\")\n",
    "print(f\"{'='*80}\\n\")\n",
    "\n",
    "cache_bloomberg = DATA_DIR / \"cache\" / \"bloomberg\"\n",
    "cache_file = DATA_DIR / \"cache\" / \"file\"\n",
    "\n",
    "has_bloomberg_cache = cache_bloomberg.exists() and list(cache_bloomberg.glob(\"*.parquet\"))\n",
    "has_file_cache = cache_file.exists() and list(cache_file.glob(\"*.parquet\"))\n",
    "\n",
    "print(f\"Bloomberg cache: {cache_bloomberg}\")\n",
    "print(f\"  Available: {bool(has_bloomberg_cache)}\")\n",
    "if has_bloomberg_cache:\n",
    "    print(f\"  Files: {len(list(cache_bloomberg.glob('*.parquet')))}\")\n",
    "\n",
    "print(f\"\\nSynthetic cache: {cache_file}\")\n",
    "print(f\"  Available: {bool(has_file_cache)}\")\n",
    "if has_file_cache:\n",
    "    print(f\"  Files: {len(list(cache_file.glob('*.parquet')))}\")\n",
    "\n",
    "print(f\"\\nConfiguration: USE_BLOOMBERG = {USE_BLOOMBERG}\")\n",
    "\n",
    "# Determine data source\n",
    "if USE_BLOOMBERG and has_bloomberg_cache:\n",
    "    data_source = \"bloomberg\"\n",
    "    print(f\"\\n✓ Using Bloomberg data from cache\")\n",
    "elif has_file_cache:\n",
    "    data_source = \"file\"\n",
    "    print(f\"\\n✓ Using synthetic data from file cache\")\n",
    "    print(\"  (Run generate_synthetic_data.py if files are missing)\")\n",
    "else:\n",
    "    raise FileNotFoundError(\n",
    "        \"No data cache found. Please run either:\\n\"\n",
    "        \"  1. 01_data_download.ipynb (Bloomberg Terminal), or\\n\"\n",
    "        \"  2. python generate_synthetic_data.py (synthetic data)\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48397e78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load market data from cache\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(f\"LOADING MARKET DATA\")\n",
    "print(f\"{'='*80}\\n\")\n",
    "\n",
    "use_cache = True\n",
    "update_current_day = False  # Set to True for intraday signal monitoring\n",
    "\n",
    "if data_source == \"bloomberg\":\n",
    "    # Bloomberg cache available\n",
    "    from aponyx.data.sources import BloombergSource\n",
    "    print(\"Data source: Bloomberg cache\")\n",
    "    print(f\"Intraday updates: {update_current_day}\")\n",
    "    if update_current_day:\n",
    "        print(\"  (Refreshing today's data only via BDP)\\n\")\n",
    "    else:\n",
    "        print(\"  (Standard caching - full refresh if stale)\\n\")\n",
    "    \n",
    "    source = BloombergSource()\n",
    "    cdx_df = fetch_cdx(\n",
    "        source=source,\n",
    "        security=\"cdx_ig_5y\",\n",
    "        use_cache=use_cache,\n",
    "        update_current_day=update_current_day,\n",
    "    )\n",
    "    vix_df = fetch_vix(\n",
    "        source=source,\n",
    "        use_cache=use_cache,\n",
    "        update_current_day=update_current_day,\n",
    "    )\n",
    "    etf_df = fetch_etf(\n",
    "        source=source,\n",
    "        security=\"hyg\",\n",
    "        use_cache=use_cache,\n",
    "        update_current_day=update_current_day,\n",
    "    )\n",
    "    \n",
    "elif data_source == \"file\":\n",
    "    # Synthetic data cache available\n",
    "    from aponyx.data.sources import FileSource\n",
    "    print(\"Data source: Synthetic data cache\")\n",
    "    print(\"  (Run generate_synthetic_data.py if files are missing)\\n\")\n",
    "    \n",
    "    cdx_source = FileSource(cache_file / \"cdx_cdx_ig_5y.parquet\")\n",
    "    vix_source = FileSource(cache_file / \"vix_vix.parquet\")\n",
    "    etf_source = FileSource(cache_file / \"etf_hyg.parquet\")\n",
    "    \n",
    "    cdx_df = fetch_cdx(source=cdx_source, security=\"cdx_ig_5y\", use_cache=use_cache)\n",
    "    vix_df = fetch_vix(source=vix_source, use_cache=use_cache)\n",
    "    etf_df = fetch_etf(source=etf_source, security=\"hyg\", use_cache=use_cache)\n",
    "\n",
    "# Load and verify CDX IG 5Y data\n",
    "print(f\"Loading CDX IG 5Y...\")\n",
    "print(f\"✓ Loaded CDX IG 5Y: {len(cdx_df)} rows\")\n",
    "print(f\"  Columns: {list(cdx_df.columns)}\")\n",
    "print(f\"  Date range: {cdx_df.index.min()} to {cdx_df.index.max()}\")\n",
    "\n",
    "if 'spread' not in cdx_df.columns:\n",
    "    raise ValueError(f\"CDX data missing 'spread' column. Found: {list(cdx_df.columns)}\")\n",
    "\n",
    "print()\n",
    "\n",
    "# Load and verify VIX data\n",
    "print(\"Loading VIX...\")\n",
    "print(f\"✓ Loaded VIX: {len(vix_df)} rows\")\n",
    "print(f\"  Columns: {list(vix_df.columns)}\")\n",
    "print(f\"  Date range: {vix_df.index.min()} to {vix_df.index.max()}\")\n",
    "\n",
    "if 'level' not in vix_df.columns:\n",
    "    raise ValueError(f\"VIX data missing 'level' column. Found: {list(vix_df.columns)}\")\n",
    "\n",
    "print()\n",
    "\n",
    "# Load and verify ETF (HYG) data\n",
    "print(\"Loading HYG ETF...\")\n",
    "print(f\"✓ Loaded HYG ETF: {len(etf_df)} rows\")\n",
    "print(f\"  Columns: {list(etf_df.columns)}\")\n",
    "print(f\"  Date range: {etf_df.index.min()} to {etf_df.index.max()}\")\n",
    "\n",
    "if 'spread' not in etf_df.columns:\n",
    "    raise ValueError(f\"ETF data missing 'spread' column. Found: {list(etf_df.columns)}\")\n",
    "\n",
    "print()\n",
    "\n",
    "# Create market data dictionary\n",
    "market_data = {\n",
    "    \"cdx\": cdx_df,\n",
    "    \"vix\": vix_df,\n",
    "    \"etf\": etf_df,\n",
    "}\n",
    "\n",
    "if data_source == \"bloomberg\" and not update_current_day:\n",
    "    print(f\"\\nTip: For intraday signal monitoring, set update_current_day=True\")\n",
    "    print(f\"     This refreshes only today's data (~10x faster)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cdb649d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display data summary\n",
    "summary_data = [\n",
    "    {\n",
    "        'Dataset': 'CDX IG 5Y',\n",
    "        'Rows': len(cdx_df),\n",
    "        'Start': cdx_df.index.min().strftime('%Y-%m-%d'),\n",
    "        'End': cdx_df.index.max().strftime('%Y-%m-%d'),\n",
    "        'Columns': ', '.join(cdx_df.columns),\n",
    "    },\n",
    "    {\n",
    "        'Dataset': 'VIX',\n",
    "        'Rows': len(vix_df),\n",
    "        'Start': vix_df.index.min().strftime('%Y-%m-%d'),\n",
    "        'End': vix_df.index.max().strftime('%Y-%m-%d'),\n",
    "        'Columns': ', '.join(vix_df.columns),\n",
    "    },\n",
    "    {\n",
    "        'Dataset': 'HYG ETF',\n",
    "        'Rows': len(etf_df),\n",
    "        'Start': etf_df.index.min().strftime('%Y-%m-%d'),\n",
    "        'End': etf_df.index.max().strftime('%Y-%m-%d'),\n",
    "        'Columns': ', '.join(etf_df.columns),\n",
    "    },\n",
    "]\n",
    "\n",
    "summary_df = pd.DataFrame(summary_data)\n",
    "print(\"Data Summary:\\n\")\n",
    "print(summary_df.to_markdown(index=False))\n",
    "print(f\"\\n✓ Loaded {len(market_data)} datasets for signal computation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54920d05",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Step 2: Signal Computation\n",
    "\n",
    "Compute the selected signal using the signal catalog framework."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cce11d61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create signal configuration\n",
    "signal_config = SignalConfig(\n",
    "    lookback=20,\n",
    "    min_periods=10,\n",
    ")\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(f\"COMPUTING SIGNAL: {SELECTED_SIGNAL}\")\n",
    "print(f\"{'='*80}\\n\")\n",
    "\n",
    "print(f\"Configuration:\")\n",
    "print(f\"  Lookback window: {signal_config.lookback} days\")\n",
    "print(f\"  Minimum periods: {signal_config.min_periods} observations\")\n",
    "print(f\"  Selected signal: {SELECTED_SIGNAL}\")\n",
    "\n",
    "# Compute all enabled signals (registry manages which are enabled)\n",
    "signals_dict = compute_registered_signals(\n",
    "    registry=signal_registry,\n",
    "    market_data=market_data,\n",
    "    config=signal_config,\n",
    ")\n",
    "\n",
    "# Extract the selected signal\n",
    "if SELECTED_SIGNAL not in signals_dict:\n",
    "    available = list(signals_dict.keys())\n",
    "    raise ValueError(\n",
    "        f\"Signal '{SELECTED_SIGNAL}' not found in computed signals.\\n\"\n",
    "        f\"Available signals: {available}\\n\"\n",
    "        f\"Make sure the signal is enabled in {SIGNAL_CATALOG_PATH}\"\n",
    "    )\n",
    "\n",
    "signal = signals_dict[SELECTED_SIGNAL]\n",
    "\n",
    "print(f\"\\n✓ Computed signal: {SELECTED_SIGNAL}\")\n",
    "print(f\"   Valid observations: {signal.notna().sum()} / {len(signal)}\")\n",
    "print(f\"   Date range: {signal.index.min()} to {signal.index.max()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1387a096",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize signal\n",
    "fig = plot_signal(signal, title=f\"Signal: {SELECTED_SIGNAL}\")\n",
    "\n",
    "# Add threshold reference lines\n",
    "fig.add_hline(y=1.5, line_dash=\"dash\", line_color=\"green\", annotation_text=\"Entry (+1.5)\")\n",
    "fig.add_hline(y=-1.5, line_dash=\"dash\", line_color=\"red\", annotation_text=\"Entry (-1.5)\")\n",
    "fig.add_hline(y=0.75, line_dash=\"dot\", line_color=\"lightgreen\", annotation_text=\"Exit (+0.75)\")\n",
    "fig.add_hline(y=-0.75, line_dash=\"dot\", line_color=\"lightcoral\", annotation_text=\"Exit (-0.75)\")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2916fe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display signal statistics\n",
    "stats = {\n",
    "    \"Metric\": [\n",
    "        \"Count\",\n",
    "        \"Mean\",\n",
    "        \"Std Dev\",\n",
    "        \"Min\",\n",
    "        \"25%\",\n",
    "        \"50%\",\n",
    "        \"75%\",\n",
    "        \"Max\",\n",
    "        \"Autocorr (lag-1)\",\n",
    "    ],\n",
    "    \"Value\": [\n",
    "        f\"{signal.notna().sum()}\",\n",
    "        f\"{signal.mean():.4f}\",\n",
    "        f\"{signal.std():.4f}\",\n",
    "        f\"{signal.min():.4f}\",\n",
    "        f\"{signal.quantile(0.25):.4f}\",\n",
    "        f\"{signal.median():.4f}\",\n",
    "        f\"{signal.quantile(0.75):.4f}\",\n",
    "        f\"{signal.max():.4f}\",\n",
    "        f\"{signal.autocorr(lag=1):.4f}\",\n",
    "    ],\n",
    "}\n",
    "\n",
    "stats_df = pd.DataFrame(stats)\n",
    "print(\"\\n**Signal Statistics**\\n\")\n",
    "print(stats_df.to_markdown(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65b40ecc",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Step 3: Suitability Evaluation\n",
    "\n",
    "Evaluate signal-product suitability using 4-component scoring framework."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09646f5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare target data (CDX spread changes)\n",
    "cdx_spread = market_data[\"cdx\"][\"spread\"]\n",
    "\n",
    "# Evaluate suitability\n",
    "suitability_config = SuitabilityConfig()\n",
    "\n",
    "suitability_result = evaluate_signal_suitability(\n",
    "    signal=signal,\n",
    "    target_change=cdx_spread,\n",
    "    config=suitability_config,\n",
    ")\n",
    "\n",
    "print(f\"\\n✅ Suitability evaluation complete\")\n",
    "print(f\"   Decision: {suitability_result.decision}\")\n",
    "print(f\"   Composite Score: {suitability_result.composite_score:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebc72747",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display component scores\n",
    "component_data = [\n",
    "    {\n",
    "        \"Component\": \"Data Health\",\n",
    "        \"Weight\": \"20%\",\n",
    "        \"Score\": f\"{suitability_result.data_health_score:.4f}\",\n",
    "        \"Weighted\": f\"{suitability_result.data_health_score * 0.2:.4f}\",\n",
    "    },\n",
    "    {\n",
    "        \"Component\": \"Predictive\",\n",
    "        \"Weight\": \"40%\",\n",
    "        \"Score\": f\"{suitability_result.predictive_score:.4f}\",\n",
    "        \"Weighted\": f\"{suitability_result.predictive_score * 0.4:.4f}\",\n",
    "    },\n",
    "    {\n",
    "        \"Component\": \"Economic\",\n",
    "        \"Weight\": \"20%\",\n",
    "        \"Score\": f\"{suitability_result.economic_score:.4f}\",\n",
    "        \"Weighted\": f\"{suitability_result.economic_score * 0.2:.4f}\",\n",
    "    },\n",
    "    {\n",
    "        \"Component\": \"Stability\",\n",
    "        \"Weight\": \"20%\",\n",
    "        \"Score\": f\"{suitability_result.stability_score:.4f}\",\n",
    "        \"Weighted\": f\"{suitability_result.stability_score * 0.2:.4f}\",\n",
    "    },\n",
    "    {\n",
    "        \"Component\": \"**Composite**\",\n",
    "        \"Weight\": \"**100%**\",\n",
    "        \"Score\": \"\",\n",
    "        \"Weighted\": f\"**{suitability_result.composite_score:.4f}**\",\n",
    "    },\n",
    "]\n",
    "\n",
    "component_df = pd.DataFrame(component_data)\n",
    "print(\"\\n**Suitability Evaluation**\\n\")\n",
    "print(component_df.to_markdown(index=False))\n",
    "print(f\"\\n**Decision: {suitability_result.decision}**\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c5072d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Register suitability result\n",
    "from aponyx.evaluation.suitability.report import generate_suitability_report, save_report\n",
    "from aponyx.config import EVALUATION_DIR, SUITABILITY_REGISTRY_PATH\n",
    "\n",
    "suitability_registry = SuitabilityRegistry(SUITABILITY_REGISTRY_PATH)\n",
    "\n",
    "# Generate report\n",
    "report_content = generate_suitability_report(\n",
    "    result=suitability_result,\n",
    "    signal_id=SELECTED_SIGNAL,\n",
    "    product_id=\"cdx_ig_5y\",\n",
    ")\n",
    "\n",
    "# Save report\n",
    "report_path = save_report(\n",
    "    report=report_content,\n",
    "    signal_id=SELECTED_SIGNAL,\n",
    "    product_id=\"cdx_ig_5y\",\n",
    "    output_dir=EVALUATION_DIR,\n",
    ")\n",
    "\n",
    "# Register evaluation\n",
    "suitability_registry.register_evaluation(\n",
    "    result=suitability_result,\n",
    "    signal_id=SELECTED_SIGNAL,\n",
    "    product_id=\"cdx_ig_5y\",\n",
    ")\n",
    "\n",
    "print(f\"\\n✅ Suitability report saved: {report_path}\")\n",
    "print(f\"✅ Suitability result registered\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45d1b3d7",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Step 4: Multi-Strategy Backtest\n",
    "\n",
    "Run backtests for all enabled strategies from the strategy catalog."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fea8f0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load strategy registry\n",
    "strategy_registry = StrategyRegistry(STRATEGY_CATALOG_PATH)\n",
    "\n",
    "# Get all enabled strategies\n",
    "enabled_strategies = strategy_registry.get_enabled()\n",
    "\n",
    "print(f\"Found {len(enabled_strategies)} enabled strategies:\")\n",
    "for name in enabled_strategies:\n",
    "    print(f\"  - {name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3beee85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run backtests for all strategies\n",
    "backtest_results = {}\n",
    "performance_metrics = {}\n",
    "failed_strategies = []\n",
    "\n",
    "for strategy_name, strategy_metadata in enabled_strategies.items():\n",
    "    try:\n",
    "        print(f\"\\nRunning backtest: {strategy_name}\")\n",
    "        \n",
    "        # Convert metadata to config\n",
    "        config = strategy_metadata.to_config()\n",
    "        \n",
    "        # Run backtest\n",
    "        result = run_backtest(\n",
    "            signal=signal,\n",
    "            spread=cdx_spread,\n",
    "            config=config,\n",
    "        )\n",
    "        \n",
    "        # Compute all metrics (basic + extended)\n",
    "        metrics = compute_all_metrics(\n",
    "            result.pnl,\n",
    "            result.positions,\n",
    "        )\n",
    "        \n",
    "        backtest_results[strategy_name] = result\n",
    "        performance_metrics[strategy_name] = metrics\n",
    "        \n",
    "        print(f\"  ✅ Sharpe: {metrics.sharpe_ratio:.4f}, Trades: {metrics.n_trades}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.warning(f\"Backtest failed for {strategy_name}: {e}\")\n",
    "        failed_strategies.append(strategy_name)\n",
    "        print(f\"  ❌ Failed: {e}\")\n",
    "        continue\n",
    "\n",
    "print(f\"\\n✅ Backtests complete: {len(backtest_results)} successful, {len(failed_strategies)} failed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b6127ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save consolidated backtest results\n",
    "if backtest_results:\n",
    "    # Combine all P&L DataFrames\n",
    "    all_pnl = {}\n",
    "    for strategy_name, result in backtest_results.items():\n",
    "        all_pnl[strategy_name] = result.pnl\n",
    "    \n",
    "    # Save to parquet (overwrites previous run)\n",
    "    output_path = DATA_DIR / \"processed\" / f\"backtest_{SELECTED_SIGNAL}_all_strategies.parquet\"\n",
    "    \n",
    "    # Create multi-level DataFrame\n",
    "    combined_pnl = pd.concat(all_pnl, names=[\"strategy\", \"date\"])\n",
    "    save_parquet(combined_pnl.reset_index(), output_path)\n",
    "    \n",
    "    print(f\"\\n✅ Backtest results saved: {output_path}\")\n",
    "else:\n",
    "    print(\"\\n❌ No successful backtests to save\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6593b6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort strategies by Sharpe ratio\n",
    "if performance_metrics:\n",
    "    sorted_strategies = sorted(\n",
    "        performance_metrics.items(),\n",
    "        key=lambda x: x[1].sharpe_ratio,\n",
    "        reverse=True,\n",
    "    )\n",
    "    \n",
    "    print(\"\\n**Strategies by Sharpe Ratio**\\n\")\n",
    "    for i, (strategy_name, metrics) in enumerate(sorted_strategies, 1):\n",
    "        print(f\"{i}. {strategy_name}: {metrics.sharpe_ratio:.4f}\")\n",
    "else:\n",
    "    sorted_strategies = []\n",
    "    print(\"\\n❌ No successful backtests to rank\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f76644f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize equity curves (single column layout)\n",
    "if sorted_strategies:\n",
    "    n_strategies = len(sorted_strategies)\n",
    "    \n",
    "    # Create subplots in single column\n",
    "    fig = make_subplots(\n",
    "        rows=n_strategies,\n",
    "        cols=1,\n",
    "        subplot_titles=[f\"{name} (Sharpe: {metrics.sharpe_ratio:.2f})\" \n",
    "                       for name, metrics in sorted_strategies],\n",
    "        vertical_spacing=0.05,\n",
    "        shared_xaxes=True,\n",
    "    )\n",
    "    \n",
    "    # Add equity curve for each strategy\n",
    "    for i, (strategy_name, metrics) in enumerate(sorted_strategies, 1):\n",
    "        result = backtest_results[strategy_name]\n",
    "        \n",
    "        fig.add_trace(\n",
    "            go.Scatter(\n",
    "                x=result.pnl.index,\n",
    "                y=result.pnl[\"cumulative_pnl\"],\n",
    "                mode=\"lines\",\n",
    "                name=strategy_name,\n",
    "                line=dict(width=2),\n",
    "                showlegend=False,\n",
    "            ),\n",
    "            row=i,\n",
    "            col=1,\n",
    "        )\n",
    "        \n",
    "        # Update y-axis label\n",
    "        fig.update_yaxes(title_text=\"Cumulative P&L\", row=i, col=1)\n",
    "    \n",
    "    # Update layout\n",
    "    fig.update_layout(\n",
    "        height=300 * n_strategies,\n",
    "        title_text=f\"Equity Curves: {SELECTED_SIGNAL} (Sorted by Sharpe)\",\n",
    "        showlegend=False,\n",
    "    )\n",
    "    \n",
    "    fig.update_xaxes(title_text=\"Date\", row=n_strategies, col=1)\n",
    "    \n",
    "    fig.show()\n",
    "else:\n",
    "    print(\"❌ No equity curves to display\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f8e211f",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Step 5: Performance Analysis\n",
    "\n",
    "Comprehensive post-backtest evaluation with extended metrics and attribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b60c034d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze performance for all successful backtests\n",
    "from aponyx.evaluation.performance.report import (\n",
    "    generate_performance_report,\n",
    "    save_report as save_perf_report,\n",
    ")\n",
    "from aponyx.config import PERFORMANCE_REPORTS_DIR, PERFORMANCE_REGISTRY_PATH\n",
    "\n",
    "performance_results = {}\n",
    "performance_registry = PerformanceRegistry(PERFORMANCE_REGISTRY_PATH)\n",
    "performance_config = PerformanceConfig()\n",
    "\n",
    "for strategy_name, backtest_result in backtest_results.items():\n",
    "    print(f\"\\nAnalyzing performance: {strategy_name}\")\n",
    "    \n",
    "    # Analyze performance\n",
    "    perf_result = analyze_backtest_performance(\n",
    "        backtest_result=backtest_result,\n",
    "        config=performance_config,\n",
    "    )\n",
    "    \n",
    "    performance_results[strategy_name] = perf_result\n",
    "    \n",
    "    # Generate report\n",
    "    report_content = generate_performance_report(\n",
    "        result=perf_result,\n",
    "        signal_id=SELECTED_SIGNAL,\n",
    "        strategy_id=strategy_name,\n",
    "    )\n",
    "    \n",
    "    # Save report\n",
    "    report_path = save_perf_report(\n",
    "        report=report_content,\n",
    "        signal_id=SELECTED_SIGNAL,\n",
    "        strategy_id=strategy_name,\n",
    "        output_dir=PERFORMANCE_REPORTS_DIR,\n",
    "    )\n",
    "    \n",
    "    # Register result\n",
    "    performance_registry.register_evaluation(\n",
    "        result=perf_result,\n",
    "        signal_id=SELECTED_SIGNAL,\n",
    "        strategy_id=strategy_name,\n",
    "    )\n",
    "    \n",
    "    print(f\"  ✅ Report saved: {report_path}\")\n",
    "\n",
    "print(f\"\\n✅ Performance analysis complete for {len(performance_results)} strategies\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c10844c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comparative metrics table\n",
    "if performance_results:\n",
    "    metrics_data = []\n",
    "    \n",
    "    for strategy_name, perf_result in performance_results.items():\n",
    "        # All metrics now in PerformanceMetrics object\n",
    "        metrics = performance_metrics[strategy_name]\n",
    "        \n",
    "        metrics_data.append({\n",
    "            \"Strategy\": strategy_name,\n",
    "            \"Sharpe\": f\"{metrics.sharpe_ratio:.4f}\",\n",
    "            \"Sortino\": f\"{metrics.sortino_ratio:.4f}\",\n",
    "            \"Max DD\": f\"{metrics.max_drawdown:.2f}\",\n",
    "            \"Profit Factor\": f\"{metrics.profit_factor:.4f}\",\n",
    "            \"Tail Ratio\": f\"{metrics.tail_ratio:.4f}\",\n",
    "            \"Consistency\": f\"{metrics.consistency_score:.2%}\",\n",
    "            \"Trades\": f\"{metrics.n_trades}\",\n",
    "        })\n",
    "    \n",
    "    # Sort by Sharpe\n",
    "    metrics_df = pd.DataFrame(metrics_data)\n",
    "    metrics_df = metrics_df.sort_values(\n",
    "        by=\"Sharpe\",\n",
    "        key=lambda x: x.astype(float),\n",
    "        ascending=False,\n",
    "    )\n",
    "    \n",
    "    print(\"\\n**Performance Metrics Comparison**\\n\")\n",
    "    print(metrics_df.to_markdown(index=False))\n",
    "else:\n",
    "    print(\"\\n❌ No performance results to display\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02770157",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display attribution for top performer\n",
    "if sorted_strategies:\n",
    "    top_strategy = sorted_strategies[0][0]\n",
    "    top_result = performance_results[top_strategy]\n",
    "    top_metrics = performance_metrics[top_strategy]\n",
    "    \n",
    "    print(f\"\\n**Attribution Analysis: {top_strategy} (Top Performer)**\\n\")\n",
    "    \n",
    "    # Directional attribution\n",
    "    print(\"**Directional Attribution:**\")\n",
    "    dir_attr = top_result.attribution[\"direction\"]\n",
    "    total_pnl = dir_attr['long_pnl'] + dir_attr['short_pnl']\n",
    "    print(f\"  Long P&L: {dir_attr['long_pnl']:.2f} ({dir_attr['long_pct']:.1%})\")\n",
    "    print(f\"  Short P&L: {dir_attr['short_pnl']:.2f} ({dir_attr['short_pct']:.1%})\")\n",
    "    print(f\"  Total: {total_pnl:.2f}\")\n",
    "    \n",
    "    # Signal strength attribution\n",
    "    print(\"\\n**Signal Strength Attribution (Quantiles):**\")\n",
    "    sig_attr = top_result.attribution[\"signal_strength\"]\n",
    "    n_quantiles = top_result.config.attribution_quantiles\n",
    "    for i in range(1, n_quantiles + 1):\n",
    "        pnl = sig_attr[f'q{i}_pnl']\n",
    "        pct = sig_attr[f'q{i}_pct']\n",
    "        print(f\"  Q{i}: {pnl:.2f} ({pct:.1%})\")\n",
    "    \n",
    "    # Win/loss attribution\n",
    "    print(\"\\n**Win/Loss Attribution:**\")\n",
    "    wl_attr = top_result.attribution[\"win_loss\"]\n",
    "    print(f\"  Wins: {wl_attr['gross_wins']:.2f} ({wl_attr['win_contribution']:.1%})\")\n",
    "    print(f\"  Losses: {wl_attr['gross_losses']:.2f} ({wl_attr['loss_contribution']:.1%})\")\n",
    "    print(f\"  Win Rate: {top_metrics.hit_rate:.1%}\")\n",
    "else:\n",
    "    print(\"\\n❌ No attribution to display\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5449903",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Workflow Complete\n",
    "\n",
    "Successfully completed single-signal research workflow.\n",
    "\n",
    "### What Was Accomplished\n",
    "\n",
    "✅ **Data Loaded** — Acquired required market data with automatic Bloomberg/synthetic fallback\n",
    "\n",
    "✅ **Signal Computed** — Generated z-score normalized signal from market data\n",
    "\n",
    "✅ **Suitability Evaluated** — 4-component quality screening with PASS/HOLD/FAIL decision\n",
    "\n",
    "✅ **Backtests Executed** — Tested signal across all enabled strategies from catalog\n",
    "\n",
    "✅ **Performance Analyzed** — Comprehensive evaluation with extended metrics and attribution\n",
    "\n",
    "### Key Outputs\n",
    "\n",
    "**Suitability Report:**\n",
    "```\n",
    "reports/suitability/{signal_id}_cdx_ig_5y_{timestamp}.md\n",
    "```\n",
    "\n",
    "**Consolidated Backtest Results:**\n",
    "```\n",
    "data/processed/backtest_{signal_id}_all_strategies.parquet\n",
    "```\n",
    "\n",
    "**Performance Reports:**\n",
    "```\n",
    "reports/performance/{signal_id}_{strategy_id}_{timestamp}.md\n",
    "```\n",
    "\n",
    "### Re-Running This Notebook\n",
    "\n",
    "- **Same signal, different data:** Update `USE_BLOOMBERG` toggle or refresh cache\n",
    "- **Different signal:** Change `SELECTED_SIGNAL` in configuration cell\n",
    "- **Different strategies:** Edit `src/aponyx/backtest/strategy_catalog.json`\n",
    "- **Overwrites:** Backtest results file overwrites previous run for same signal\n",
    "\n",
    "### Troubleshooting\n",
    "\n",
    "**Bloomberg Terminal Connection Failed:**\n",
    "- Ensure Bloomberg Terminal is running and logged in\n",
    "- Check that Bloomberg data was previously cached via `01_data_download.ipynb`\n",
    "- Set `USE_BLOOMBERG = False` to use synthetic data\n",
    "\n",
    "**No Cache Available:**\n",
    "- Run `01_data_download.ipynb` to cache Bloomberg data, or\n",
    "- Run `python src/aponyx/notebooks/generate_synthetic_data.py` to create test data\n",
    "\n",
    "**Strategy Backtest Failed:**\n",
    "- Check strategy configuration in `strategy_catalog.json`\n",
    "- Ensure entry_threshold > exit_threshold\n",
    "- Review error message in cell output\n",
    "- Failed strategies are automatically skipped (notebook continues)\n",
    "\n",
    "### Testing New Signals\n",
    "\n",
    "To research a new signal idea:\n",
    "\n",
    "1. **Implement compute function** in `src/aponyx/models/signals.py`:\n",
    "   ```python\n",
    "   def compute_new_signal(\n",
    "       cdx_df: pd.DataFrame,\n",
    "       config: SignalConfig | None = None,\n",
    "   ) -> pd.Series:\n",
    "       \"\"\"Compute new signal logic here.\"\"\"\n",
    "       # Implementation...\n",
    "       return signal  # Positive = long credit risk\n",
    "   ```\n",
    "\n",
    "2. **Register in catalog** at `src/aponyx/models/signal_catalog.json`:\n",
    "   ```json\n",
    "   {\n",
    "     \"name\": \"new_signal\",\n",
    "     \"description\": \"Description of signal logic\",\n",
    "     \"compute_function_name\": \"compute_new_signal\",\n",
    "     \"data_requirements\": {\"cdx\": \"spread\"},\n",
    "     \"arg_mapping\": [\"cdx\"],\n",
    "     \"enabled\": true\n",
    "   }\n",
    "   ```\n",
    "\n",
    "3. **Update configuration cell** above:\n",
    "   ```python\n",
    "   SELECTED_SIGNAL = \"new_signal\"\n",
    "   ```\n",
    "\n",
    "4. **Re-run notebook** to evaluate new signal\n",
    "\n",
    "### Strategy Customization\n",
    "\n",
    "To test different entry/exit thresholds:\n",
    "\n",
    "1. **Edit strategy catalog** at `src/aponyx/backtest/strategy_catalog.json`:\n",
    "   ```json\n",
    "   {\n",
    "     \"name\": \"custom_strategy\",\n",
    "     \"description\": \"Custom threshold configuration\",\n",
    "     \"entry_threshold\": 1.2,\n",
    "     \"exit_threshold\": 0.6,\n",
    "     \"enabled\": true\n",
    "   }\n",
    "   ```\n",
    "\n",
    "2. **Re-run backtest section** — New strategy will be automatically included\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

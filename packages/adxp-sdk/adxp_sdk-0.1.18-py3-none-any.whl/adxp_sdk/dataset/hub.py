"""
Dataset CRUD API í´ë¼ì´ì–¸íŠ¸ - í•µì‹¬ CRUD ê¸°ëŠ¥ë§Œ ì œê³µ

Dataset ìƒì„±, ì¡°íšŒ, ìˆ˜ì •, ì‚­ì œë¥¼ ìœ„í•œ í´ë¼ì´ì–¸íŠ¸ í´ë˜ìŠ¤
"""

import requests
import os
import time
from typing import Dict, Any, Optional, List, Union
from requests.exceptions import RequestException
from adxp_sdk.auth import BaseCredentials
import json

try:
    from .schemas import (
        DatasetCreateRequest, DatasetUpdateRequest, DatasetResponse,
        DatasetListResponse, DatasetCreateResponse, DatasetType,
        DatasetStatus, DatasetFile, DatasetTag, DatasetProcessor, DatasetFilter,
        DatasetListRequest
    )
except ImportError:
    # ì§ì ‘ ì‹¤í–‰í•  ë•Œë¥¼ ìœ„í•œ ì ˆëŒ€ import
    from schemas import (
        DatasetCreateRequest, DatasetUpdateRequest, DatasetResponse,
        DatasetListResponse, DatasetCreateResponse, DatasetType,
        DatasetStatus, DatasetFile, DatasetTag, DatasetProcessor, DatasetFilter,
        DatasetListRequest
    )


class DatasetHub:
    """Dataset CRUD API í´ë¼ì´ì–¸íŠ¸ - í•µì‹¬ CRUD ê¸°ëŠ¥ë§Œ ì œê³µ"""

    def __init__(self, credentials: BaseCredentials):
        """
        DatasetHub ì´ˆê¸°í™”

        Args:
            credentials: ì¸ì¦ ì •ë³´ (BaseCredentials)
        """
        self.credentials = credentials
        self.base_url = credentials.base_url
        self.headers = credentials.get_headers()

    def _make_request(self, method: str, endpoint: str, data: Optional[Dict[str, Any]] = None, params: Optional[Dict[str, Any]] = None) -> Dict[str, Any]:
        """API ìš”ì²­ ì‹¤í–‰"""
        url = f"{self.base_url}{endpoint}"

        try:
            if method.upper() == "GET":
                response = requests.get(url, headers=self.headers, params=params)
            elif method.upper() == "POST":
                print(f"ğŸ” ë””ë²„ê¹… - POST ìš”ì²­ URL: {url}")
                print(f"ğŸ” ë””ë²„ê¹… - POST ìš”ì²­ ë°ì´í„°: {data}")
                response = requests.post(url, headers=self.headers, json=data)
                print(f"ğŸ” ë””ë²„ê¹… - POST ì‘ë‹µ ìƒíƒœ: {response.status_code}")
                print(f"ğŸ” ë””ë²„ê¹… - POST ì‘ë‹µ ë‚´ìš©: {response.text}")
            elif method.upper() == "PUT":
                response = requests.put(url, headers=self.headers, json=data)
            elif method.upper() == "DELETE":
                response = requests.delete(url, headers=self.headers)
            else:
                raise ValueError(f"Unsupported HTTP method: {method}")

            response.raise_for_status()

            # ë¹ˆ ì‘ë‹µ ì²˜ë¦¬ (ì‚­ì œ API ë“± - 204 No Content)
            if response.status_code == 204 or not response.text.strip():
                return {
                    "success": True,
                    "message": "ìš”ì²­ì´ ì„±ê³µì ìœ¼ë¡œ ì²˜ë¦¬ë˜ì—ˆìŠµë‹ˆë‹¤.",
                    "status_code": response.status_code
                }

            result = response.json()
            return result

        except RequestException as e:
            raise Exception(f"API ìš”ì²­ ì‹¤íŒ¨: {e}")

    # ====================================================================
    # í•µì‹¬ CRUD Operations
    # ====================================================================

    def create_dataset(self, dataset_data: Union[DatasetCreateRequest, Dict[str, Any]]) -> Dict[str, Any]:
        """
        Dataset ìƒì„± (ì‹¤ì œ API ìŠ¤í™ì— ë§ì¶¤)

        Args:
            dataset_data: Dataset ìƒì„± ë°ì´í„°

        Returns:
            ìƒì„±ëœ Dataset ì •ë³´
        """
        if isinstance(dataset_data, DatasetCreateRequest):
            data = dataset_data.model_dump(exclude_none=True)
        else:
            data = dataset_data.copy()
        
        # Enum ê°ì²´ë¥¼ ë¬¸ìì—´ë¡œ ë³€í™˜
        if 'type' in data and hasattr(data['type'], 'value'):
            data['type'] = data['type'].value
        if 'status' in data and hasattr(data['status'], 'value'):
            data['status'] = data['status'].value

        # Dataset íƒ€ì…ë³„ íŠ¹ë³„ ì²˜ë¦¬
        dataset_type = data.get('type')
        
        # Model BenchmarkëŠ” Data Processorë¥¼ ê±´ë„ˆë›°ë„ë¡ ì²˜ë¦¬
        if dataset_type == DatasetType.MODEL_BENCHMARK:
            data['processor'] = {"ids": [], "duplicate_subset_columns": [], "regular_expression": []}
        
        # Supervised Finetuningì€ ê¸°ë³¸ í”„ë¡œì„¸ì„œ ì„¤ì • í•„ìš”
        elif dataset_type == DatasetType.SUPERVISED_FINETUNING:
            if 'processor' not in data or not data['processor']:
                # supervised_finetuningì˜ ê²½ìš° ë¹ˆ ê°ì²´ë¡œ ì„¤ì •
                data['processor'] = {}
        
        # Unsupervised Finetuningì€ ê¸°ë³¸ í”„ë¡œì„¸ì„œ ì„¤ì • í•„ìš”
        elif dataset_type == DatasetType.UNSUPERVISED_FINETUNING:
            if 'processor' not in data or not data['processor']:
                # ê¸°ë³¸ í”„ë¡œì„¸ì„œ ì„¤ì •
                data['processor'] = {
                    "ids": [],
                    "duplicate_subset_columns": [],
                    "regular_expression": []
                }
        
        # DPO Finetuningì€ íŠ¹ë³„í•œ ì²˜ë¦¬ í•„ìš”
        elif dataset_type == DatasetType.DPO_FINETUNING:
            # DPO íƒ€ì…ì— ëŒ€í•œ íŠ¹ë³„í•œ ë°ì´í„° í¬ë§· ì •ì˜
            if 'processor' not in data or not data['processor']:
                # ê¸°ë³¸ í”„ë¡œì„¸ì„œ ì„¤ì •
                data['processor'] = {
                    "ids": ["remove_duplicates", "rnn_masking"],
                    "duplicate_subset_columns": ["content"],
                    "regular_expression": ["email_pattern", "phone_pattern"]
                }

        return self._make_request("POST", "/api/v1/datasets", data)

    def get_datasets(self,
                    project_id: str,
                    page: int = 1,
                    size: int = 10,
                    sort: Optional[str] = None,
                    filter: Optional[DatasetFilter] = None,
                    search: Optional[str] = None) -> Dict[str, Any]:
        """
        Dataset ëª©ë¡ ì¡°íšŒ (ì‹¤ì œ API ìŠ¤í™ì— ë§ì¶¤)

        Args:
            project_id: í”„ë¡œì íŠ¸ ID
            page: í˜ì´ì§€ ë²ˆí˜¸
            size: í˜ì´ì§€ í¬ê¸°
            sort: ì •ë ¬ ê¸°ì¤€
            filter: í•„í„° ì¡°ê±´
            search: ê²€ìƒ‰ì–´

        Returns:
            Dataset ëª©ë¡
        """
        params = {
            "project_id": project_id,
            "page": page,
            "size": size
        }

        if sort:
            params["sort"] = sort
        if search:
            params["search"] = search
        if filter:
            filter_dict = filter.model_dump(exclude_none=True)
            # type í•„ë“œëª… ë³€ê²½
            if 'type' in filter_dict:
                params['type'] = filter_dict['type']
            if 'status' in filter_dict:
                params['status'] = filter_dict['status']
            if 'tags' in filter_dict:
                params['tags'] = filter_dict['tags']

        return self._make_request("GET", "/api/v1/datasets", params=params)

    def get_dataset(self, dataset_id: str) -> Dict[str, Any]:
        """
        Dataset ìƒì„¸ ì¡°íšŒ (IDë¡œ ì¡°íšŒ)

        Args:
            dataset_id: Dataset ID

        Returns:
            Dataset ìƒì„¸ ì •ë³´
        """
        return self._make_request("GET", f"/api/v1/datasets/{dataset_id}")

    def update_dataset(self, dataset_id: str, dataset_data: Union[DatasetUpdateRequest, Dict[str, Any]]) -> Dict[str, Any]:
        """
        Dataset ìˆ˜ì • (description, project_id, tagsë§Œ ìˆ˜ì • ê°€ëŠ¥)

        Args:
            dataset_id: Dataset ID
            dataset_data: ìˆ˜ì •í•  Dataset ë°ì´í„° (description, project_id, tagsë§Œ)

        Returns:
            ìˆ˜ì • ê²°ê³¼
        """
        if isinstance(dataset_data, DatasetUpdateRequest):
            data = dataset_data.model_dump(exclude_none=True)
        else:
            data = dataset_data

        return self._make_request("PUT", f"/api/v1/datasets/{dataset_id}", data)

    def update_dataset_tags(self, dataset_id: str, tags: List[Dict[str, str]]) -> Dict[str, Any]:
        """
        Dataset íƒœê·¸ ìˆ˜ì •

        Args:
            dataset_id: Dataset ID
            tags: ìˆ˜ì •í•  íƒœê·¸ ëª©ë¡ [{"name": "tag1"}, {"name": "tag2"}]

        Returns:
            ìˆ˜ì • ê²°ê³¼
        """
        return self._make_request("PUT", f"/api/v1/datasets/{dataset_id}/tags", tags)

    def delete_dataset(self, dataset_id: str) -> Dict[str, Any]:
        """
        Dataset ì‚­ì œ (ë…¼ë¦¬ì  ì‚­ì œ - is_deleted=Trueë¡œ ì„¤ì •)

        Args:
            dataset_id: Dataset ID

        Returns:
            ì‚­ì œ ê²°ê³¼ (204 No Content)
        """
        return self._make_request("DELETE", f"/api/v1/datasets/{dataset_id}")

    # ====================================================================
    # íŒŒì¼ ì—…ë¡œë“œ ê´€ë ¨ ê¸°ëŠ¥
    # ====================================================================

    def upload_file(self, file_path: str) -> Dict[str, Any]:
        """
        íŒŒì¼ ì—…ë¡œë“œ (ì„ì‹œ ì €ì¥) - ì‹¤ì œ API ìŠ¤í™ì— ë§ì¶¤

        Args:
            file_path: ì—…ë¡œë“œí•  íŒŒì¼ ê²½ë¡œ

        Returns:
            ì—…ë¡œë“œ ê²°ê³¼ (temp_file_path í¬í•¨)
        """
        url = f"{self.base_url}/api/v1/datasources/upload/files"

        try:
            with open(file_path, 'rb') as file:
                files = {'files': file}
                headers = self.headers
                response = requests.post(url, files=files, headers=headers)
                response.raise_for_status()
                return response.json()
        except Exception as e:
            raise Exception(f"íŒŒì¼ ì—…ë¡œë“œ ì‹¤íŒ¨: {e}")

    def _validate_file_extensions(self, file_paths: List[str], dataset_type: str) -> None:
        """
        íŒŒì¼ í™•ì¥ì ê²€ì¦
        
        Args:
            file_paths: ì—…ë¡œë“œí•  íŒŒì¼ ê²½ë¡œ ëª©ë¡
            dataset_type: Dataset íƒ€ì…
            
        Raises:
            ValueError: ì§€ì›í•˜ì§€ ì•ŠëŠ” í™•ì¥ìì¸ ê²½ìš°
        """
        supported_extensions = DatasetType.get_supported_extensions(dataset_type)
        
        for file_path in file_paths:
            file_ext = os.path.splitext(file_path)[1].lower()
            if file_ext not in supported_extensions:
                raise ValueError(
                    f"Dataset íƒ€ì… '{dataset_type}'ì€ ë‹¤ìŒ í™•ì¥ìë§Œ ì§€ì›í•©ë‹ˆë‹¤: {supported_extensions}. "
                    f"í˜„ì¬ íŒŒì¼: {file_path} (í™•ì¥ì: {file_ext})"
                )

    def _validate_dataset_schema(self, file_paths: List[str], dataset_type: str) -> None:
        """
        Dataset ìŠ¤í‚¤ë§ˆ ê²€ì¦ (CSV/Excel íŒŒì¼ì˜ ì»¬ëŸ¼ ê²€ì¦)
        
        Args:
            file_paths: ì—…ë¡œë“œí•  íŒŒì¼ ê²½ë¡œ ëª©ë¡
            dataset_type: Dataset íƒ€ì…
            
        Raises:
            ValueError: ìŠ¤í‚¤ë§ˆ ê²€ì¦ ì‹¤íŒ¨ì‹œ
        """
        required_columns = DatasetType.get_required_columns(dataset_type)
        if not required_columns:
            return  # ìŠ¤í‚¤ë§ˆ ê²€ì¦ì´ í•„ìš”í•˜ì§€ ì•Šì€ íƒ€ì…
        
        import pandas as pd
        
        for file_path in file_paths:
            try:
                # íŒŒì¼ í™•ì¥ìì— ë”°ë¼ ì½ê¸°
                if file_path.endswith('.csv'):
                    df = pd.read_csv(file_path)
                elif file_path.endswith('.xlsx'):
                    df = pd.read_excel(file_path)
                else:
                    continue  # ìŠ¤í‚¤ë§ˆ ê²€ì¦ì´ í•„ìš”í•˜ì§€ ì•Šì€ íŒŒì¼
                
                # ì»¬ëŸ¼ëª… í™•ì¸
                actual_columns = df.columns.tolist()
                
                # supervised_finetuningì˜ ê²½ìš° user(.N), assistant(.N) íŒ¨í„´ ê²€ì¦
                if dataset_type == DatasetType.SUPERVISED_FINETUNING.value:
                    if 'system' not in actual_columns:
                        raise ValueError(f"í•„ìˆ˜ ì»¬ëŸ¼ 'system'ì´ ì—†ìŠµë‹ˆë‹¤. íŒŒì¼: {file_path}")
                    
                    # user(.N), assistant(.N) íŒ¨í„´ ì°¾ê¸°
                    user_cols = [col for col in actual_columns if col.startswith('user')]
                    assistant_cols = [col for col in actual_columns if col.startswith('assistant')]
                    
                    if not user_cols or not assistant_cols:
                        raise ValueError(f"í•„ìˆ˜ ì»¬ëŸ¼ 'user(.N)', 'assistant(.N)'ì´ ì—†ìŠµë‹ˆë‹¤. íŒŒì¼: {file_path}")
                    
                    # ì¸ë±ìŠ¤ ë§¤ì¹­ í™•ì¸
                    user_indices = set()
                    assistant_indices = set()
                    
                    for col in user_cols:
                        if '.' in col:
                            try:
                                idx = int(col.split('.')[1])
                                user_indices.add(idx)
                            except (IndexError, ValueError):
                                pass
                    
                    for col in assistant_cols:
                        if '.' in col:
                            try:
                                idx = int(col.split('.')[1])
                                assistant_indices.add(idx)
                            except (IndexError, ValueError):
                                pass
                    
                    if user_indices != assistant_indices:
                        raise ValueError(f"user(.N)ì™€ assistant(.N)ì˜ ì¸ë±ìŠ¤ê°€ ì¼ì¹˜í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤. íŒŒì¼: {file_path}")
                
                else:
                    # ë‹¤ë¥¸ íƒ€ì…ë“¤ì˜ ê¸°ë³¸ ì»¬ëŸ¼ ê²€ì¦
                    for required_col in required_columns:
                        if required_col not in actual_columns:
                            raise ValueError(f"í•„ìˆ˜ ì»¬ëŸ¼ '{required_col}'ì´ ì—†ìŠµë‹ˆë‹¤. íŒŒì¼: {file_path}")
                            
            except Exception as e:
                if isinstance(e, ValueError):
                    raise
                else:
                    raise ValueError(f"íŒŒì¼ ìŠ¤í‚¤ë§ˆ ê²€ì¦ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}. íŒŒì¼: {file_path}")

    def upload_multiple_files(self, file_paths: List[str], dataset_type: Optional[str] = None) -> Dict[str, Any]:
        """
        ì—¬ëŸ¬ íŒŒì¼ ì—…ë¡œë“œ (ì„ì‹œ ì €ì¥) - ì‹¤ì œ API ìŠ¤í™ì— ë§ì¶¤

        Args:
            file_paths: ì—…ë¡œë“œí•  íŒŒì¼ ê²½ë¡œ ëª©ë¡
            dataset_type: Dataset íƒ€ì… (í™•ì¥ì ê²€ì¦ìš©)

        Returns:
            ì—…ë¡œë“œ ê²°ê³¼ (temp_file_path ëª©ë¡ í¬í•¨)
        """
        # í™•ì¥ì ê²€ì¦ (dataset_typeì´ ì œê³µëœ ê²½ìš°)
        if dataset_type:
            self._validate_file_extensions(file_paths, dataset_type)
        
        url = f"{self.base_url}/api/v1/datasources/upload/files"

        try:
            files = []
            for file_path in file_paths:
                files.append(('files', open(file_path, 'rb')))
            
            # budget íŒŒë¼ë¯¸í„° ì œê±° (ì¶”í›„ ì‘ì—… ì˜ˆì •)
            data = {}
            
            # íŒŒì¼ ì—…ë¡œë“œì‹œì—ëŠ” Content-Type í—¤ë”ë¥¼ ì œê±°í•´ì•¼ í•¨
            headers = self.headers.copy()
            if 'Content-Type' in headers:
                del headers['Content-Type']
            
            response = requests.post(url, files=files, data=data, headers=headers)
            response.raise_for_status()
            
            # íŒŒì¼ í•¸ë“¤ ë‹«ê¸°
            for _, file_handle in files:
                file_handle.close()
                
            return response.json()
        except Exception as e:
            raise Exception(f"ë‹¤ì¤‘ íŒŒì¼ ì—…ë¡œë“œ ì‹¤íŒ¨: {e}")

    def create_datasource(self, project_id: str, name: str, temp_files: List[Dict[str, Any]], 
                         description: str = "", scope: str = "private_logical") -> Dict[str, Any]:
        """
        ë°ì´í„°ì†ŒìŠ¤ ìƒì„± (ì‹¤ì œ API ìŠ¤í™ì— ë§ì¶¤)

        Args:
            project_id: í”„ë¡œì íŠ¸ ID
            name: ë°ì´í„°ì†ŒìŠ¤ ì´ë¦„
            temp_files: temp_files ë°°ì—´ (íŒŒì¼ ì—…ë¡œë“œ ê²°ê³¼ì—ì„œ ê°€ì ¸ì˜¨ ê²ƒ)
            description: ë°ì´í„°ì†ŒìŠ¤ ì„¤ëª…
            scope: ë°ì´í„°ì†ŒìŠ¤ ë²”ìœ„

        Returns:
            ìƒì„±ëœ ë°ì´í„°ì†ŒìŠ¤ ì •ë³´ (datasource_id í¬í•¨)
        """
        datasource_data = {
            "project_id": project_id,
            "name": name,
            "type": "file",  # íŒŒì¼ íƒ€ì…ìœ¼ë¡œ ìˆ˜ì •
            "created_by": "",
            "updated_by": "",
            "description": description,
            "s3_config": {
                "bucket_name": "",
                "access_key": "",
                "secret_key": "",
                "region": "",
                "prefix": ""
            },
            "temp_files": temp_files,
            "policy": []
        }

        return self._make_request("POST", "/api/v1/datasources", datasource_data)

    def get_datasources(self, project_id: str, page: int = 1, size: int = 10, 
                       search: Optional[str] = None) -> Dict[str, Any]:
        """
        ë°ì´í„°ì†ŒìŠ¤ ëª©ë¡ ì¡°íšŒ

        Args:
            project_id: í”„ë¡œì íŠ¸ ID
            page: í˜ì´ì§€ ë²ˆí˜¸
            size: í˜ì´ì§€ í¬ê¸°
            search: ê²€ìƒ‰ì–´

        Returns:
            ë°ì´í„°ì†ŒìŠ¤ ëª©ë¡
        """
        params = {
            "project_id": project_id,
            "page": page,
            "size": size
        }

        if search:
            params["search"] = search

        return self._make_request("GET", "/api/v1/datasources", params=params)

    def get_datasource_files(self, datasource_id: str, page: int = 1, size: int = 10, 
                            sort: Optional[str] = None, filter: Optional[str] = None, 
                            search: Optional[str] = None) -> Dict[str, Any]:
        """
        ë°ì´í„°ì†ŒìŠ¤ íŒŒì¼ ëª©ë¡ ì¡°íšŒ

        Args:
            datasource_id: ë°ì´í„°ì†ŒìŠ¤ ID
            page: í˜ì´ì§€ ë²ˆí˜¸ (ê¸°ë³¸ê°’: 1)
            size: í˜ì´ì§€ í¬ê¸° (ê¸°ë³¸ê°’: 10)
            sort: ì •ë ¬ ê¸°ì¤€
            filter: í•„í„° ì¡°ê±´
            search: ê²€ìƒ‰ì–´

        Returns:
            ë°ì´í„°ì†ŒìŠ¤ íŒŒì¼ ëª©ë¡
        """
        params = {
            "page": page,
            "size": size
        }

        if sort:
            params["sort"] = sort
        if filter:
            params["filter"] = filter
        if search:
            params["search"] = search

        return self._make_request("GET", f"/api/v1/datasources/{datasource_id}/files", params=params)

    # ====================================================================
    # ë°ì´í„° í”„ë¡œì„¸ì„œ ê´€ë ¨ ê¸°ëŠ¥
    # ====================================================================

    def get_available_processors(self) -> Dict[str, Any]:
        """
        ì‚¬ìš© ê°€ëŠ¥í•œ ë°ì´í„° í”„ë¡œì„¸ì„œ ëª©ë¡ ì¡°íšŒ

        Returns:
            í”„ë¡œì„¸ì„œ ëª©ë¡
        """
        return self._make_request("GET", "/api/v1/datasets/processors")

    def apply_processors(self, dataset_id: str, processors: List[DatasetProcessor]) -> Dict[str, Any]:
        """
        Datasetì— ë°ì´í„° í”„ë¡œì„¸ì„œ ì ìš©

        Args:
            dataset_id: Dataset ID
            processors: ì ìš©í•  í”„ë¡œì„¸ì„œ ëª©ë¡

        Returns:
            ì ìš© ê²°ê³¼
        """
        data = {
            "processors": [processor.model_dump() for processor in processors]
        }
        return self._make_request("POST", f"/api/v1/datasets/{dataset_id}/processors", data)

    # ====================================================================
    # í†µí•© Dataset ìƒì„± ê¸°ëŠ¥ (íŒŒì¼ ì—…ë¡œë“œ + ë°ì´í„°ì†ŒìŠ¤ ìƒì„± + Dataset ìƒì„±)
    # ====================================================================

    def create_dataset_with_files(self, name: str, description: str, project_id: str, 
                                 file_paths: List[str], dataset_type: DatasetType, 
                                 tags: Optional[List[str]] = None, 
                                 processor: Optional[DatasetProcessor] = None) -> Dict[str, Any]:
        """
        íŒŒì¼ì„ í¬í•¨í•œ Dataset ìƒì„± (ì „ì²´ í”Œë¡œìš°)

        Args:
            name: Dataset ì´ë¦„
            description: Dataset ì„¤ëª…
            project_id: í”„ë¡œì íŠ¸ ID
            file_paths: ì—…ë¡œë“œí•  íŒŒì¼ ê²½ë¡œ ëª©ë¡
            dataset_type: Dataset íƒ€ì…
            tags: íƒœê·¸ ëª©ë¡
            processor: ë°ì´í„° í”„ë¡œì„¸ì„œ ì„¤ì •

        Returns:
            ìƒì„±ëœ Dataset ì •ë³´
        """
        try:
            # íŒŒì¼ í™•ì¥ì ê²€ì¦
            self._validate_file_extensions(file_paths, dataset_type.value)
            
            # ìŠ¤í‚¤ë§ˆ ê²€ì¦ (CSV/Excel íŒŒì¼ì˜ ê²½ìš°)
            self._validate_dataset_schema(file_paths, dataset_type.value)
            
            # í”„ë¡œì„¸ì„œ í•„ìš” ì—¬ë¶€ í™•ì¸
            requires_processor = DatasetType.requires_processor(dataset_type.value)
            if not requires_processor and processor:
                print(f"âš ï¸  Dataset íƒ€ì… '{dataset_type.value}'ì€ í”„ë¡œì„¸ì„œë¥¼ ì ìš©í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
                processor = None
            
            # model_benchmarkì™€ custom íƒ€ì…ì€ ë‹¤ë¥¸ ì—”ë“œí¬ì¸íŠ¸ ì‚¬ìš©
            if dataset_type in [DatasetType.MODEL_BENCHMARK, DatasetType.CUSTOM]:
                return self._create_model_benchmark_dataset(name, description, project_id, file_paths, tags, dataset_type)
            
            # 1ë‹¨ê³„: íŒŒì¼ ì—…ë¡œë“œ
            print("1ë‹¨ê³„: íŒŒì¼ ì—…ë¡œë“œ ì¤‘...")
            upload_result = self.upload_multiple_files(file_paths, dataset_type.value)
            print(f"íŒŒì¼ ì—…ë¡œë“œ ì™„ë£Œ: {upload_result}")

            # 2ë‹¨ê³„: temp_files ë°°ì—´ êµ¬ì„±
            temp_files = []
            upload_data = upload_result.get("data", [])
            for i, file_path in enumerate(file_paths):
                temp_file_path = upload_data[i].get("temp_file_path") if i < len(upload_data) else None
                temp_files.append({
                    "file_name": os.path.basename(file_path),
                    "temp_file_path": temp_file_path,
                    "file_metadata": None,
                    "knowledge_config": None
                })

            # 3ë‹¨ê³„: ë°ì´í„°ì†ŒìŠ¤ ìƒì„±
            print("2ë‹¨ê³„: ë°ì´í„°ì†ŒìŠ¤ ìƒì„± ì¤‘...")
            datasource_name = f"datasource_{name}_{int(time.time())}"
            datasource_result = self.create_datasource(
                project_id=project_id,
                name=datasource_name,
                temp_files=temp_files,
                description=f"Data source for {name}"
            )
            datasource_id = datasource_result.get("id")
            print(f"ë°ì´í„°ì†ŒìŠ¤ ìƒì„± ì™„ë£Œ: {datasource_id}")

            # ë°ì´í„°ì†ŒìŠ¤ ìƒì„± í›„ ì ì‹œ ëŒ€ê¸° (ë°ì´í„°ì†ŒìŠ¤ê°€ ì™„ì „íˆ ì¤€ë¹„ë  ë•Œê¹Œì§€)
            print("ë°ì´í„°ì†ŒìŠ¤ ì¤€ë¹„ ëŒ€ê¸° ì¤‘...")
            time.sleep(2)  # 2ì´ˆ ëŒ€ê¸°

            # 4ë‹¨ê³„: Dataset ìƒì„±
            print("3ë‹¨ê³„: Dataset ìƒì„± ì¤‘...")
            dataset_tags = [{"name": tag} for tag in (tags or [])]
            
            # processor ì„¤ì • - supervised_finetuningê³¼ unsupervised_finetuningì˜ ê²½ìš° ë¹ˆ ê°ì²´ë¡œ ì„¤ì •
            processor_data = {}
            if processor:
                processor_dict = processor.model_dump() if hasattr(processor, 'model_dump') else processor
                # ë¹ˆ ë°°ì—´ í•„ë“œ ì œê±°
                processor_data = {k: v for k, v in processor_dict.items() if v}
                if not processor_data:
                    processor_data = {}
            elif dataset_type == DatasetType.SUPERVISED_FINETUNING:
                # supervised_finetuningì˜ ê²½ìš° ë¹ˆ ê°ì²´ë¡œ ì„¤ì •
                processor_data = {}
            elif dataset_type == DatasetType.UNSUPERVISED_FINETUNING:
                # unsupervised_finetuningì˜ ê²½ìš° ì‹¤ì œ í”„ë¡œì„¸ì„œ ì„¤ì •
                processor_data = {
                    "ids": ["3398014c-e0ad-4b4d-a8d2-44f4b0d0ff1d"],
                    "duplicate_subset_columns": ["no"],
                    "regular_expression": []
                }
            
            # ì„±ê³µí•˜ëŠ” í˜•ì‹ì— ë§ì¶° ë¹ˆ ë°°ì—´ë¡œ ì„¤ì •
            policy_data = []
            
            dataset_data = DatasetCreateRequest(
                name=name,
                description=description,
                project_id=project_id,
                type=dataset_type,
                tags=dataset_tags,
                datasource_id=datasource_id,
                processor=processor_data,  # ì„¤ì •ëœ processor_data ì‚¬ìš©
                is_deleted=False,
                created_by="",
                updated_by="",
                policy=policy_data
            )
            
            # statusëŠ” ê¸°ë³¸ê°’ PROCESSING ì‚¬ìš© (ë¹ˆ ë¬¸ìì—´ ëŒ€ì‹ )
            # dataset_data.status = ""  # ì´ ì¤„ ì œê±°

            result = self.create_dataset(dataset_data)
            print(f"Dataset ìƒì„± ì™„ë£Œ: {result.get('id')}")
            return result

        except Exception as e:
            raise Exception(f"Dataset ìƒì„± ì‹¤íŒ¨: {e}")

    def _create_model_benchmark_dataset(self, name: str, description: str, project_id: str, 
                                       file_paths: List[str], tags: Optional[List[str]] = None, 
                                       dataset_type: DatasetType = DatasetType.MODEL_BENCHMARK) -> Dict[str, Any]:
        """
        Model Benchmark Dataset ìƒì„± (ì§ì ‘ ì—…ë¡œë“œ ë°©ì‹)
        
        Args:
            name: Dataset ì´ë¦„
            description: Dataset ì„¤ëª…
            project_id: í”„ë¡œì íŠ¸ ID
            file_paths: ì—…ë¡œë“œí•  íŒŒì¼ ê²½ë¡œ ëª©ë¡
            tags: íƒœê·¸ ëª©ë¡
            
        Returns:
            ìƒì„±ëœ Dataset ì •ë³´
        """
        try:
            print("Model Benchmark Dataset ìƒì„± ì¤‘...")
            
            # /api/v1/datasets/upload/files ì—”ë“œí¬ì¸íŠ¸ ì‚¬ìš©
            url = f"{self.base_url}/api/v1/datasets/upload/files"
            
            # íŒŒì¼ ì¤€ë¹„
            files = []
            for file_path in file_paths:
                files.append(('file', open(file_path, 'rb')))
            
            # ë°ì´í„° ì¤€ë¹„ (datasource_id ì—†ì´)
            dataset_tags = [{"name": tag} for tag in (tags or [])]
            
            data = {
                'name': name,
                'description': description,
                'project_id': project_id,
                'type': dataset_type.value,
                'tags': '',  # ë¹ˆ ë¬¸ìì—´ë¡œ ì„¤ì •
                'status': 'processing',  # processingìœ¼ë¡œ ì„¤ì •
                'created_by': '',
                'updated_by': '',
                'payload': ''
            }
            
            headers = self.headers.copy()
            if 'Content-Type' in headers:
                del headers['Content-Type']
            
            response = requests.post(url, files=files, data=data, headers=headers)
            response.raise_for_status()
            
            # íŒŒì¼ í•¸ë“¤ ë‹«ê¸°
            for _, file_handle in files:
                file_handle.close()
                
            result = response.json()
            print(f"Model Benchmark Dataset ìƒì„± ì™„ë£Œ: {result.get('id')}")
            return result
            
        except Exception as e:
            # íŒŒì¼ í•¸ë“¤ ë‹«ê¸° (ì—ëŸ¬ ë°œìƒ ì‹œ)
            try:
                for _, file_handle in files:
                    file_handle.close()
            except:
                pass
            raise Exception(f"Model Benchmark Dataset ìƒì„± ì‹¤íŒ¨: {e}")

    # ====================================================================
    # Dataset íƒ€ì…ë³„ íŠ¹ë³„ ê¸°ëŠ¥
    # ====================================================================

    def create_dpo_dataset(self, name: str, description: str, project_id: str, 
                          datasource_id: str, tags: Optional[List[str]] = None) -> Dict[str, Any]:
        """
        DPO Finetuning Dataset ìƒì„± (ì‹¤ì œ API ìŠ¤í™ì— ë§ì¶¤)

        Args:
            name: Dataset ì´ë¦„
            description: Dataset ì„¤ëª…
            project_id: í”„ë¡œì íŠ¸ ID
            datasource_id: ë°ì´í„° ì†ŒìŠ¤ ID
            tags: íƒœê·¸ ëª©ë¡

        Returns:
            ìƒì„±ëœ Dataset ì •ë³´
        """
        # DPO íƒ€ì…ì— ë§ëŠ” í”„ë¡œì„¸ì„œ ì„¤ì •
        processor = {
            "ids": ["remove_duplicates", "rnn_masking", "email_masking"],
            "duplicate_subset_columns": ["content", "preference"],
            "regular_expression": ["email_pattern", "phone_pattern", "ssn_pattern"]
        }

        dataset_data = {
            "name": name,
            "type": DatasetType.DPO_FINETUNING,
            "description": description,
            "project_id": project_id,
            "datasource_id": datasource_id,
            "tags": [{"name": tag} for tag in (tags or [])],
            "processor": processor,
            "status": DatasetStatus.PROCESSING
        }

        return self.create_dataset(dataset_data)

    def create_custom_dataset(self, name: str, description: str, project_id: str,
                             tags: Optional[List[str]] = None) -> Dict[str, Any]:
        """
        Custom Dataset ìƒì„± (Data source ì—†ì´, ì‹¤ì œ API ìŠ¤í™ì— ë§ì¶¤)

        Args:
            name: Dataset ì´ë¦„
            description: Dataset ì„¤ëª…
            project_id: í”„ë¡œì íŠ¸ ID
            tags: íƒœê·¸ ëª©ë¡

        Returns:
            ìƒì„±ëœ Dataset ì •ë³´
        """
        dataset_data = {
            "name": name,
            "type": DatasetType.CUSTOM,
            "description": description,
            "project_id": project_id,
            "datasource_id": None,  # Customì€ ë°ì´í„° ì†ŒìŠ¤ ì—†ì´ ìƒì„±
            "tags": [{"name": tag} for tag in (tags or [])],
            "processor": {"ids": [], "duplicate_subset_columns": [], "regular_expression": []},  # Customì€ í”„ë¡œì„¸ì„œ ì—†ìŒ
            "status": DatasetStatus.PROCESSING
        }

        return self.create_dataset(dataset_data)

    def create_model_benchmark_dataset(self, name: str, description: str, project_id: str,
                                      datasource_id: str, tags: Optional[List[str]] = None) -> Dict[str, Any]:
        """
        Model Benchmark Dataset ìƒì„± (ZIP íŒŒì¼ ì „ìš©, í”„ë¡œì„¸ì„œ ì—†ìŒ)

        Args:
            name: Dataset ì´ë¦„
            description: Dataset ì„¤ëª…
            project_id: í”„ë¡œì íŠ¸ ID
            datasource_id: ë°ì´í„° ì†ŒìŠ¤ ID
            tags: íƒœê·¸ ëª©ë¡

        Returns:
            ìƒì„±ëœ Dataset ì •ë³´
        """
        dataset_data = {
            "name": name,
            "type": DatasetType.MODEL_BENCHMARK,
            "description": description,
            "project_id": project_id,
            "datasource_id": datasource_id,
            "tags": [{"name": tag} for tag in (tags or [])],
            "processor": None,  # Model BenchmarkëŠ” í”„ë¡œì„¸ì„œ ì—†ìŒ
            "status": DatasetStatus.PROCESSING
        }

        return self.create_dataset(dataset_data)

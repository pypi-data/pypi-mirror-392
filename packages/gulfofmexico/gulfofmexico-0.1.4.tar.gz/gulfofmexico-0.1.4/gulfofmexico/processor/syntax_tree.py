"""
Abstract Syntax Tree (AST) Definitions

Defines all statement types in Gulf of Mexico's syntax tree.
Each statement class represents a distinct language construct.

Statement Types:
    - FunctionDefinition: function/async definitions with flexible keyword matching
    - ClassDeclaration: class definitions with instance namespaces
    - VariableDeclaration: const/var declarations with confidence and lifetime
    - VariableAssignment: assignment with optional indexing
    - Conditional: if statements with expression evaluation
    - WhenStatement: reactive when triggers on variable changes
    - AfterStatement: scheduled execution with temporal delays
    - ReturnStatement: function returns with optional debug
    - DeleteStatement: value deletion from memory
    - ReverseStatement: reverse string/list/time (special operator)
    - ExpressionStatement: standalone expressions
    - ExportStatement: export values to other file sections
    - ImportStatement: import from file sections or external files

Key Features:
    - Debug levels (0-4): increasing verbosity for troubleshooting
    - Confidence levels: !, !! , !!! for variable priority
    - Type annotations: optional type hints after colons
    - Lifetimes: <5.0> for temporal, 100 for line-based

These AST nodes are generated by generate_syntax_tree() in this module,
then executed by pattern matching in interpreter.py.
"""

from abc import ABCMeta
from typing import Optional, Union
from dataclasses import dataclass

from gulfofmexico.base import (
    STR_TO_OPERATOR,
    Token,
    TokenType,
    raise_error_at_line,
    raise_error_at_token,
)
from gulfofmexico.processor.expression_tree import (
    ExpressionTreeNode,
    build_expression_tree,
)

__all__ = [
    "FunctionDefinition",
    "ClassDeclaration",
    "VariableDeclaration",
    "VariableAssignment",
    "Conditional",
    "ReturnStatement",
    "DeleteStatement",
    "ReverseStatement",
    "ExpressionStatement",
    "WhenStatement",
    "AfterStatement",
    "ExportStatement",
    "ImportStatement",
    "TryWhateverStatement",
    "ProcrastinationStatement",
    "CorporateSpeakStatement",
    "EmotionalStatement",
    "SuperstitiousStatement",
    "QuantumStatement",
    "TimeTravelStatement",
    "GaslightingStatement",
    "BlockchainStatement",
    "AIBuzzwordStatement",
    "AgileStatement",
    "SecurityTheaterStatement",
    "DevOpsStatement",
    "StartupStatement",
]


class CodeStatement:
    """Base class for all statement types."""

    pass


class CodeStatementKeywordable(metaclass=ABCMeta):
    """Statements that have a keyword token (if, when, class, etc.)."""

    keyword: Token


class CodeStatementDebuggable(metaclass=ABCMeta):
    """Statements that support debug levels (0-4 question marks)."""

    debug: int


@dataclass
class FunctionDefinition(CodeStatement):
    """Function or async function definition.

    Supports flexible keyword matching: f, fn, func, functi, function
    with optional async prefix.

    Examples:
        fn add(a, b) => a + b!
        async function process(data) => {...}!
    """

    keywords: list[Token]  # ['async', 'function'] or just ['function']
    name: Token
    args: list[Token]
    code: list[tuple[CodeStatement, ...]]
    is_async: bool


@dataclass
class ClassDeclaration(CodeStatement, CodeStatementKeywordable):
    """Class definition with instance namespace.

    Creates objects with their own variable scope. Instances
    can access class properties via dot notation.
    """

    keyword: Token
    name: Token
    code: list[tuple[CodeStatement, ...]]


@dataclass
class VariableDeclaration(CodeStatement, CodeStatementDebuggable):
    """Variable declaration with modifiers, type, and lifetime.

    Modifiers: const, var (can combine as const var)
    Type annotation: optional after colon
    Lifetime: <5.0> for temporal, 100 for line-based
    Confidence: ! count determines priority (1-3)

    Examples:
        const x = 10!
        var y: Int <5.0> = 20!!
        const const const PI = 3.14!  (immutable global)
    """

    name: Token
    modifiers: list[Token]  # ['const'], ['var'], or ['const', 'var']
    type_annotation: Optional[list[Token]]  # Type annotation tokens (Int, String, etc.)
    lifetime: Optional[str]  # "<5.0>" or "100" or None
    expression: Union[list[Token], ExpressionTreeNode]
    debug: int  # 0-4 (number of ? marks)
    confidence: int  # 1-3 (number of ! marks)


@dataclass
class VariableAssignment(CodeStatement, CodeStatementDebuggable):
    """Variable reassignment with optional indexing.

    Supports single or multi-dimensional indexing including
    fractional indexes for insertion.

    Examples:
        x = 5!
        list[-1] = 10!
        list[0.5] = 99!  (inserts between 0 and 1)
    """

    name: Token
    expression: Union[list[Token], ExpressionTreeNode]
    debug: int
    indexes: Union[list[list[Token]], list[ExpressionTreeNode]]
    confidence: int


@dataclass
class Conditional(CodeStatement, CodeStatementKeywordable):
    """If statement with expression condition.

    Condition evaluated for boolean value (true/false/maybe).
    Maybe values handled probabilistically.
    """

    keyword: Token
    expression: Union[list[Token], ExpressionTreeNode]
    code: list[tuple[CodeStatement, ...]]


# name expression !?
@dataclass
class ReturnStatement(CodeStatement, CodeStatementDebuggable):
    keyword: Optional[Token]
    expression: Union[list[Token], ExpressionTreeNode]
    debug: int


# name name !?
@dataclass
class DeleteStatement(CodeStatement, CodeStatementKeywordable, CodeStatementDebuggable):
    keyword: Token
    name: Token
    debug: int


# reverse name!
@dataclass
class ReverseStatement(
    CodeStatement, CodeStatementKeywordable, CodeStatementDebuggable
):
    keyword: Token  # The 'reverse' keyword token
    name: Token  # The variable to reverse
    debug: int


# expression !?   < virtually indistinguishable from a return statement from a parsing perspective
@dataclass
class ExpressionStatement(CodeStatement, CodeStatementDebuggable):
    expression: Union[list[Token], ExpressionTreeNode]
    debug: int


# name name = expression {
@dataclass
class WhenStatement(CodeStatement, CodeStatementKeywordable):
    keyword: Token
    expression: Union[list[Token], ExpressionTreeNode]
    code: list[tuple[CodeStatement, ...]]


# name "string" expression!
@dataclass
class AfterStatement(CodeStatement, CodeStatementKeywordable):
    keyword: Token
    expression: Union[list[Token], ExpressionTreeNode]
    code: list[tuple[CodeStatement, ...]]


# name name (, name)* name string!
@dataclass
class ExportStatement(CodeStatement, CodeStatementDebuggable):
    export_keyword: Token
    names: list[Token]
    to_keyword: Token
    target_file: Token
    debug: int


# name
@dataclass
class ImportStatement(CodeStatement, CodeStatementKeywordable, CodeStatementDebuggable):
    keyword: Token
    names: list[Token]
    debug: int


@dataclass
class TryWhateverStatement(CodeStatement, CodeStatementKeywordable):
    """Try/whatever passive-aggressive error handling.

    Executes try block, but handles errors dismissively with whatever block.

    Examples:
        try { risky_operation()! } whatever { print "meh, didn't work"! }
    """

    keyword: Token  # 'try'
    try_code: list[tuple[CodeStatement, ...]]
    whatever_code: list[tuple[CodeStatement, ...]]


@dataclass
class ProcrastinationStatement(CodeStatement, CodeStatementKeywordable):
    """Procrastination keywords: later, eventually, whenever.

    Defers code execution with varying probability:
    - later: 50% chance to execute
    - eventually: 75% chance to execute
    - whenever: 90% chance to execute

    Examples:
        later { do_homework()! }
        eventually { clean_room()! }
        whenever { reply_to_email()! }
    """

    keyword: Token  # 'later', 'eventually', or 'whenever'
    code: list[tuple[CodeStatement, ...]]


@dataclass
class CorporateSpeakStatement(CodeStatement, CodeStatementKeywordable):
    """Corporate speak keywords with satirical implementations.

    Keywords:
    - synergize: combine two values
    - leverage: multiply value by 2
    - paradigm_shift: negate value
    - circle_back: defer execution (like later)
    - touch_base: print status update

    Examples:
        synergize x, y!
        leverage revenue!
        paradigm_shift thinking!
    """

    keyword: Token
    args: list[ExpressionTreeNode]


@dataclass
class EmotionalStatement(CodeStatement, CodeStatementKeywordable):
    """Emotional programming: execute code based on program mood.

    Keywords:
    - happy: executes when program is happy (no recent errors)
    - sad: executes when program is sad (recent errors)
    - angry: executes when program is angry (many errors)
    - excited: executes randomly with high energy
    - tired: executes slowly (adds delay)

    Examples:
        happy { print "Everything is great!"! }
        sad { print "Something went wrong..."! }
        angry { print "THIS IS UNACCEPTABLE!"! }
    """

    keyword: Token  # 'happy', 'sad', 'angry', 'excited', 'tired'
    code: list[tuple[CodeStatement, ...]]


@dataclass
class SuperstitiousStatement(CodeStatement, CodeStatementKeywordable):
    """Superstitious programming: luck-based execution.

    Keywords:
    - lucky: higher chance of success (doubles success rate)
    - unlucky: higher chance of failure (doubles error rate)
    - cross_fingers: hope for the best (random outcome)
    - knock_on_wood: prevent bad luck (error suppression)

    Examples:
        lucky { risky_operation()! }
        cross_fingers { deploy_to_production()! }
        knock_on_wood { critical_update()! }
    """

    keyword: Token
    code: list[tuple[CodeStatement, ...]]


@dataclass
class QuantumStatement(CodeStatement, CodeStatementKeywordable):
    """Quantum programming: variables in superposition.

    The quantum keyword creates variables that exist in multiple states
    simultaneously until observed. Observation collapses the superposition
    to a single value.

    Examples:
        quantum x [1, 2, 3]!        // x is in superposition
        const y observe(x)!          // collapse to single value
        quantum result maybe!        // quantum uncertainty
    """

    keyword: Token  # 'quantum'
    name: Token
    value: list[Token]  # Expression tokens for the superposition value
    debug: int


@dataclass
class TimeTravelStatement(CodeStatement, CodeStatementKeywordable):
    """Time travel: access past/future variable states.

    Functions:
    - past(var, n): Access variable value from n executions ago
    - future(var): Predict future value (spoiler: it's random)

    Examples:
        const old_value past(x, 3)!     // value from 3 iterations ago
        const prediction future(x)!      // predicted future value
    """

    keyword: Token  # 'past' or 'future'
    args: list[ExpressionTreeNode]


@dataclass
class GaslightingStatement(CodeStatement, CodeStatementKeywordable):
    """Gaslighting variables: variables that deny their existence.

    The definitely_not keyword creates variables that:
    - Claim they were never set
    - Return random values when accessed
    - Deny any changes made to them

    Examples:
        definitely_not x 42!           // Creates gaslighting variable
        print x!                        // "I don't know what you're talking about"
    """

    keyword: Token  # 'definitely_not'
    name: Token
    value: list[Token]
    debug: int


@dataclass
class BlockchainStatement(CodeStatement, CodeStatementKeywordable):
    """Blockchain buzzword satire.

    Keywords:
    - blockchain: Wrap code in unnecessary blockchain technology
    - immutable_ledger: Make values "immutable" (but not really)
    - smart_contract: Execute code with "smart contract" logic
    - mine: Mine for cryptocurrency (actually just wastes CPU)

    Examples:
        blockchain { deploy_code()! }
        mine 10!                       // Mine 10 blocks
    """

    keyword: Token
    code: Optional[list[tuple[CodeStatement, ...]]]
    args: Optional[list[ExpressionTreeNode]]


@dataclass
class AIBuzzwordStatement(CodeStatement, CodeStatementKeywordable):
    """AI/ML buzzword satire.

    Keywords:
    - deep_learning: Apply "deep learning" (adds random noise)
    - neural_network: Process with "neural network" (does nothing)
    - ai_powered: Make code "AI-powered" (adds thinking delays)

    Examples:
        deep_learning { process_data()! }
        ai_powered result = calculate()!
    """

    keyword: Token
    code: list[tuple[CodeStatement, ...]]


@dataclass
class AgileStatement(CodeStatement, CodeStatementKeywordable):
    """Agile/Scrum methodology satire.

    Keywords:
    - sprint: Execute code in a "sprint" (with velocity tracking)
    - standup: Daily standup meeting (prints status updates)
    - retro: Retrospective meeting (reflects on what happened)
    - burndown: Track burndown chart (counts down unnecessarily)

    Examples:
        sprint { implement_feature()! }
        standup { report_status()! }
    """

    keyword: Token
    code: list[tuple[CodeStatement, ...]]


@dataclass
class SecurityTheaterStatement(CodeStatement, CodeStatementKeywordable):
    """Security theater satire.

    Keywords:
    - encrypt: "Encrypt" data (ROT13)
    - two_factor: Require two-factor authentication (asks twice)
    - penetration_test: Run penetration test (knocks on door)
    - zero_trust: Apply zero-trust architecture (trusts nothing, does nothing)

    Examples:
        encrypt { store_data()! }
        two_factor { login()! }
    """

    keyword: Token
    code: list[tuple[CodeStatement, ...]]


@dataclass
class DevOpsStatement(CodeStatement, CodeStatementKeywordable):
    """DevOps cargo cult satire.

    Keywords:
    - containerize: Put code in a "container"
    - orchestrate: Orchestrate containers (adds complexity)
    - microservice: Convert to microservice (adds latency)
    - kubernetes: Deploy to Kubernetes (YAML hell)

    Examples:
        containerize { run_app()! }
        microservice { handle_request()! }
    """

    keyword: Token
    code: list[tuple[CodeStatement, ...]]


@dataclass
class StartupStatement(CodeStatement, CodeStatementKeywordable):
    """Startup culture satire.

    Keywords:
    - pivot: Pivot the business model (changes direction)
    - disrupt: Disrupt the industry (breaks things)
    - unicorn: Become a unicorn (multiplies values by imaginary numbers)
    - hockey_stick: Achieve hockey stick growth (exponential delays)

    Examples:
        pivot { change_strategy()! }
        disrupt { innovate()! }
    """

    keyword: Token
    code: list[tuple[CodeStatement, ...]]


# idea: create a class that evaluates at runtime what a statement is, so then execute it
def split_into_statements(tokens: list[Token]) -> list[list[Token]]:
    statements = [[]]
    bracket_layers = 0
    for token in tokens:

        # check for expression-ending newlines
        if (
            token.type == TokenType.WHITESPACE and not statements[-1]
        ):  # don't care about whitespace at the beginning or end of an expression, idk
            continue
        if (
            token.type == TokenType.NEWLINE
            and not statements[-1]
            and len(statements) > 1
        ):
            statements[-2].append(token)
        else:
            statements[-1].append(token)

        # this is the start of a new scope, we don't care about those for rn in terms of starting a new statement
        if token.type == TokenType.L_CURLY:
            bracket_layers += 1
        elif token.type == TokenType.R_CURLY:
            bracket_layers -= 1

        if (
            token.type in [TokenType.R_CURLY, TokenType.BANG, TokenType.QUESTION]
            and bracket_layers == 0
        ):
            while (
                statements[-1][-1].type == TokenType.NEWLINE
            ):  # remove newlines at the end of a statement
                statements[-1].pop()
            statements.append([])

    # remove stray newlines cause they are annoying and shit, also remove empty tings
    final_statements = []
    for statement in statements:
        while statement and statement[-1].type in {
            TokenType.WHITESPACE,
            TokenType.NEWLINE,
        }:  # NOTE: END WILL NEVER BE WHITESPACE
            statement.pop()
        while statement and statement[0].type in {
            TokenType.WHITESPACE,
            TokenType.NEWLINE,
        }:  # NOTE: NOW START WILL NEVER BE WHITESPACE EITHER
            statement.pop(0)
        if not statement:
            continue
        final_statements.append(statement)

    return final_statements


def extract_type_annotations(
    filename: str, code: str, statements: list[list[Token]]
) -> list[tuple[list[Token], Optional[list[Token]]]]:
    """Extract type annotations from statements before they are removed."""
    result = []

    for tokens in statements:
        type_annotation = None
        scope_layers = 0
        square_bracket_layers = 0
        i = 0

        while i < len(tokens):
            t = tokens[i]
            if t.type == TokenType.L_CURLY:
                scope_layers += 1
            elif t.type == TokenType.R_CURLY:
                scope_layers -= 1
            elif t.type == TokenType.L_SQUARE:
                square_bracket_layers += 1
            elif t.type == TokenType.R_SQUARE:
                square_bracket_layers -= 1
            elif (
                t.type == TokenType.COLON
                and scope_layers == 0
                and square_bracket_layers == 0
            ):
                # Found a type annotation - extract everything from here
                # until the equals sign
                type_tokens = []
                i += 1
                while i < len(tokens) and tokens[i].type not in [
                    TokenType.EQUAL,
                    TokenType.L_CURLY,
                ]:
                    if tokens[i].type != TokenType.WHITESPACE:
                        type_tokens.append(tokens[i])
                    i += 1
                if type_tokens:
                    type_annotation = type_tokens
                break
            i += 1

        result.append((tokens, type_annotation))

    return result


def remove_type_hints(
    filename: str, code: str, statements: list[list[Token]]
) -> list[list[Token]]:
    new_statements = []
    for tokens in statements:
        new_tokens = []
        adding_tokens = True
        scope_layers, square_bracket_layers = 0, 0
        ref_square_bracket_layers = 0
        curr = 0

        while curr < len(tokens):
            t = tokens[curr]

            # handle brackets
            if t.type == TokenType.L_CURLY:
                scope_layers += 1
            elif t.type == TokenType.R_CURLY:
                scope_layers -= 1
            if t.type == TokenType.L_SQUARE:
                square_bracket_layers += 1
            elif t.type == TokenType.R_SQUARE:
                square_bracket_layers -= 1

            # must be in the right place to consider removal
            if t.type == TokenType.COLON and scope_layers == 0:
                adding_tokens = False
                ref_square_bracket_layers = square_bracket_layers  # prob gonna be zero but idek imma just do this
            if not adding_tokens:

                # check if it is at an operator
                if (
                    STR_TO_OPERATOR.get(t.value)
                    and square_bracket_layers == ref_square_bracket_layers
                ):
                    adding_tokens = True

                # adjust for Name<...> things, which also allows regex to pass too
                elif (
                    t.type == TokenType.NAME
                    and curr + 1 < len(tokens)
                    and tokens[curr + 1].type == TokenType.LESS_THAN
                ):
                    try:
                        while tokens[curr + 1].type != TokenType.GREATER_THAN:
                            curr += 1
                        curr += 1
                    except IndexError:
                        raise_error_at_token(
                            filename,
                            code,
                            "Something went wrong parsing type hints (a.k.a. removing them).",
                            t,
                        )

            if (
                adding_tokens
            ):  # cannot be elif because adding_tokens can be modified in the previous statement
                new_tokens.append(t)

            curr += 1

        new_statements.append(new_tokens)

    return new_statements


def assert_proper_indentation(filename: str, tokens: list[Token], code: str) -> None:
    looking_for_whitespace = False
    for t in tokens:
        if not looking_for_whitespace:
            if t.type == TokenType.NEWLINE:
                looking_for_whitespace = True
        else:
            if t.type == TokenType.WHITESPACE and len(t.value.replace("\t", "  ")) % 3:
                raise_error_at_token(
                    filename,
                    code,
                    "Invalid indenting detected (must be a multiple of 3). Tabs count as 2 spaces.",
                    t,
                )
            looking_for_whitespace = False


def create_function_definition(
    filename: str,
    without_whitespace: list[Token],
    code: str,
    statements_inside_scope: list[tuple[CodeStatement, ...]],
) -> tuple[CodeStatement, ...]:

    # Parse function declaration
    # First, collect the initial NAME tokens (keywords and function name)
    names_in_row = []
    i = 0
    while i < len(without_whitespace) and without_whitespace[i].type == TokenType.NAME:
        names_in_row.append(without_whitespace[i])
        i += 1

    if not 2 <= len(names_in_row):
        raise_error_at_token(
            filename,
            code,
            "Insufficient keyword count in function declaration.",
            without_whitespace[0],
        )

    # Check if async
    is_async = len(names_in_row) >= 2 and names_in_row[0].value == "async"
    if is_async:
        keywords = names_in_row[:2]
        name = names_in_row[2] if len(names_in_row) > 2 else None
        initial_args = [t for t in names_in_row[3:] if t.value != ""]
    else:
        keywords = names_in_row[:1]
        name = names_in_row[1] if len(names_in_row) > 1 else None
        initial_args = [t for t in names_in_row[2:] if t.value != ""]

    if name is None:
        raise_error_at_token(
            filename,
            code,
            "Function declaration must have a name.",
            without_whitespace[0],
        )

    # Now collect all remaining parameter names (skipping commas)
    # Note: parentheses are treated as whitespace by the lexer
    args = list(initial_args)
    while i < len(without_whitespace):
        token = without_whitespace[i]
        if token.type == TokenType.NAME and token.value != "":
            args.append(token)
        elif token.type in {TokenType.FUNC_POINT, TokenType.L_CURLY}:
            # Stop when we hit the => or { that starts the function body
            break
        elif token.type != TokenType.COMMA:
            # If we hit something unexpected (not a comma), stop parsing
            break
        i += 1

    return (
        FunctionDefinition(
            keywords=keywords,
            name=name,
            args=args,
            code=statements_inside_scope,
            is_async=is_async,
        ),
    )


def create_scoped_code_statement(
    filename: str,
    tokens: list[Token],
    without_whitespace: list[Token],
    code: str,
    type_annotation: Optional[list[Token]] = None,
) -> tuple[CodeStatement, ...]:

    # this means that a scope is detected in the statement
    ends_with_punc = tokens[-1].type in {TokenType.BANG, TokenType.QUESTION}
    if (
        tokens[-1 - int(ends_with_punc)].type != TokenType.R_CURLY
    ):  # end will never be whitespace
        raise_error_at_token(
            filename,
            code,
            "End of statement with open scope must close the scope.",
            tokens[-1],
        )
    if without_whitespace[0].type != TokenType.NAME:
        raise_error_at_token(
            filename,
            code,
            "Scoped code statement must start with a keyword.",
            without_whitespace[0],
        )

    # at this point, can be when, class dec, function call, or if statement
    scope_open_index = [t.type == TokenType.L_CURLY for t in tokens].index(True)
    stuff_inside_scope = tokens[scope_open_index + 1 : len(tokens) - ends_with_punc - 1]
    statements_inside_scope = generate_syntax_tree(filename, stuff_inside_scope, code)

    # see the function pointer -> immediately know
    can_be_function = any(
        [
            t.type == TokenType.FUNC_POINT
            for i, t in enumerate(without_whitespace)
            if i < scope_open_index
        ]
    )

    # now finally, check for classes
    can_be_class = (
        without_whitespace[0].type == TokenType.NAME
        and without_whitespace[1].type == TokenType.NAME
        and without_whitespace[2].type == TokenType.L_CURLY
    )

    # check for function block
    can_be_function_block = (
        len(without_whitespace) >= 2
        and without_whitespace[0].type == TokenType.NAME
        and without_whitespace[1].type == TokenType.NAME
        and (
            (without_whitespace[0].value == "function")
            or (
                len(without_whitespace) >= 3
                and without_whitespace[0].value == "async"
                and without_whitespace[1].value == "function"
            )
        )
    )

    # finally finally, check for the after or when statement -- this will have identical syntax to the conditional so there
    # is no point in doing anything extra special

    # this dude is separated to another function because the same code is reused in () => ... functions (no scope)
    possibilities = []
    if can_be_function:
        return create_function_definition(
            filename, without_whitespace, code, statements_inside_scope
        )

    if can_be_function_block:
        return create_function_definition(
            filename, without_whitespace, code, statements_inside_scope
        )

    if can_be_class:

        # build thing for the class statement
        possibilities.append(
            ClassDeclaration(
                keyword=without_whitespace[0],
                name=without_whitespace[1],
                code=statements_inside_scope,
            )
        )

    # Check for try/whatever statement (two scopes required)
    if without_whitespace[0].value == "try":
        # Look for 'whatever' keyword followed by second scope
        # Find the end of first scope
        first_scope_end = scope_open_index
        bracket_count = 0
        for i, token in enumerate(tokens[scope_open_index:], scope_open_index):
            if token.type == TokenType.L_CURLY:
                bracket_count += 1
            elif token.type == TokenType.R_CURLY:
                bracket_count -= 1
                if bracket_count == 0:
                    first_scope_end = i
                    break

        # Look for 'whatever' keyword after first scope
        whatever_index = -1
        for i in range(first_scope_end + 1, len(without_whitespace)):
            if without_whitespace[i].value == "whatever":
                whatever_index = i
                break

        if whatever_index != -1:
            # Find second scope opening
            second_scope_start = -1
            for i, token in enumerate(tokens):
                if token.type == TokenType.NAME and token.value == "whatever":
                    # Find the { after whatever
                    for j in range(i + 1, len(tokens)):
                        if tokens[j].type == TokenType.L_CURLY:
                            second_scope_start = j
                            break
                    break

            if second_scope_start != -1:
                # Extract whatever block code
                whatever_code_tokens = tokens[
                    second_scope_start + 1 : len(tokens) - ends_with_punc - 1
                ]
                whatever_statements = generate_syntax_tree(
                    filename, whatever_code_tokens, code
                )

                return (
                    TryWhateverStatement(
                        keyword=without_whitespace[0],
                        try_code=statements_inside_scope,
                        whatever_code=whatever_statements,
                    ),
                )

    # Check for procrastination keywords: later, eventually, whenever
    if without_whitespace[0].value in ["later", "eventually", "whenever"]:
        return (
            ProcrastinationStatement(
                keyword=without_whitespace[0],
                code=statements_inside_scope,
            ),
        )

    # Check for emotional programming keywords
    if without_whitespace[0].value in ["happy", "sad", "angry", "excited", "tired"]:
        return (
            EmotionalStatement(
                keyword=without_whitespace[0],
                code=statements_inside_scope,
            ),
        )

    # Check for superstitious programming keywords
    if without_whitespace[0].value in [
        "lucky",
        "unlucky",
        "cross_fingers",
        "knock_on_wood",
    ]:
        return (
            SuperstitiousStatement(
                keyword=without_whitespace[0],
                code=statements_inside_scope,
            ),
        )

    # Check for blockchain keywords
    if without_whitespace[0].value in ["blockchain", "smart_contract"]:
        return (
            BlockchainStatement(
                keyword=without_whitespace[0],
                code=statements_inside_scope,
                args=None,
            ),
        )

    # Check for AI buzzword keywords
    if without_whitespace[0].value in [
        "deep_learning",
        "neural_network",
        "ai_powered",
    ]:
        return (
            AIBuzzwordStatement(
                keyword=without_whitespace[0],
                code=statements_inside_scope,
            ),
        )

    # Check for Agile/Scrum keywords
    if without_whitespace[0].value in ["sprint", "standup", "retro", "burndown"]:
        return (
            AgileStatement(
                keyword=without_whitespace[0],
                code=statements_inside_scope,
            ),
        )

    # Check for Security Theater keywords
    if without_whitespace[0].value in [
        "encrypt",
        "two_factor",
        "penetration_test",
        "zero_trust",
    ]:
        return (
            SecurityTheaterStatement(
                keyword=without_whitespace[0],
                code=statements_inside_scope,
            ),
        )

    # Check for DevOps keywords
    if without_whitespace[0].value in [
        "containerize",
        "orchestrate",
        "microservice",
        "kubernetes",
    ]:
        return (
            DevOpsStatement(
                keyword=without_whitespace[0],
                code=statements_inside_scope,
            ),
        )

    # Check for Startup keywords
    if without_whitespace[0].value in ["pivot", "disrupt", "unicorn", "hockey_stick"]:
        return (
            StartupStatement(
                keyword=without_whitespace[0],
                code=statements_inside_scope,
            ),
        )

    possibilities.extend(
        [
            Conditional(
                keyword=without_whitespace[0],
                expression=tokens[
                    int(tokens[0].type == TokenType.WHITESPACE) + 1 : scope_open_index
                ],
                code=statements_inside_scope,
            ),
            AfterStatement(
                keyword=without_whitespace[0],
                expression=tokens[
                    int(tokens[0].type == TokenType.WHITESPACE) + 1 : scope_open_index
                ],
                code=statements_inside_scope,
            ),
            WhenStatement(
                keyword=without_whitespace[0],
                expression=tokens[
                    int(tokens[0].type == TokenType.WHITESPACE) + 1 : scope_open_index
                ],
                code=statements_inside_scope,
            ),
        ]
    )
    return tuple(possibilities)


def is_proper_comma_list(
    without_whitespace: list[Token],
    accepted_tokens: frozenset[TokenType] = frozenset({TokenType.NAME}),
) -> bool:
    looking_for_comma = without_whitespace[0].type != TokenType.COMMA
    for t in without_whitespace[1:]:
        if (
            not looking_for_comma
            and t.type not in accepted_tokens
            or looking_for_comma
            and t.type != TokenType.COMMA
        ):
            return False
        looking_for_comma = not looking_for_comma
    return True


def create_unscoped_code_statement(
    filename: str,
    tokens: list[Token],
    without_whitespace: list[Token],
    code: str,
    type_annotation: Optional[list[Token]] = None,
) -> tuple[CodeStatement, ...]:

    is_debug = tokens[-1].type == TokenType.QUESTION
    confidence = 0 if is_debug else len(tokens[-1].value)
    debug_level = 0 if not is_debug else len(tokens[-1].value)

    tokens_no_ws = [t for t in tokens if t.type != TokenType.WHITESPACE]

    # Check for corporate speak keywords
    corporate_keywords = [
        "synergize",
        "leverage",
        "paradigm_shift",
        "circle_back",
        "touch_base",
    ]
    if (
        without_whitespace[0].type == TokenType.NAME
        and without_whitespace[0].value in corporate_keywords
    ):
        # Parse arguments (everything between keyword and !)
        args_tokens = tokens[int(tokens[0].type == TokenType.WHITESPACE) + 1 : -1]
        # Split by comma to get individual arguments
        args = []
        current_arg = []
        for token in args_tokens:
            if token.type == TokenType.COMMA:
                if current_arg:
                    args.append(build_expression_tree(filename, current_arg, code))
                    current_arg = []
            elif token.type != TokenType.WHITESPACE:
                current_arg.append(token)
        if current_arg:
            args.append(build_expression_tree(filename, current_arg, code))

        return (
            CorporateSpeakStatement(
                keyword=without_whitespace[0],
                args=args,
            ),
        )

    # it's a function!!!!!!!!!!!!!!!!!
    has_func_point = [t.type == TokenType.FUNC_POINT for t in tokens]
    if any(has_func_point):
        func_point_index = has_func_point.index(True)
        return create_function_definition(
            filename,
            without_whitespace,
            code,
            [
                (
                    ReturnStatement(
                        keyword=None,
                        expression=tokens[func_point_index + 1 : -1],
                        debug=debug_level,
                    ),
                )
            ],
        )

    # import statement: import name, name, name!
    can_be_import = (
        all(
            t.type in {TokenType.NAME, TokenType.COMMA} for t in without_whitespace[:-1]
        )
        and len(without_whitespace) >= 3
        and is_proper_comma_list(without_whitespace[1:-1])
        and without_whitespace[0].type == TokenType.NAME
        and without_whitespace[1].type == TokenType.NAME
    )

    # export statement: export name, name, name to string/name!
    can_be_export = (
        all(
            t.type in {TokenType.STRING, TokenType.NAME, TokenType.COMMA}
            for t in without_whitespace[:-1]
        )
        and len(without_whitespace) >= 5
        and is_proper_comma_list(without_whitespace[1:-3])
        and without_whitespace[0].type == TokenType.NAME
        and without_whitespace[1].type == TokenType.NAME
        and without_whitespace[-2].type in {TokenType.NAME, TokenType.STRING}
        and without_whitespace[-3].type == TokenType.NAME
    )

    # let's see what can be what D:
    can_be_return = without_whitespace[0].type == TokenType.NAME
    can_be_delete = (
        can_be_return
        and len(without_whitespace) == 3
        and without_whitespace[1].type == TokenType.NAME
        and without_whitespace[2].type in {TokenType.BANG, TokenType.QUESTION}
    )

    contains_equals = any(
        tokens_is_equal := [
            t.type == TokenType.EQUAL and t.value == "=" for t in tokens
        ]
    )
    can_be_var_assignment = can_be_var_declaration = contains_equals

    # checking for single name and index for variable assignment
    can_be_var_assignment &= (
        len(without_whitespace) >= 4
        and without_whitespace[0].type == TokenType.NAME
        and without_whitespace[1].type in {TokenType.EQUAL, TokenType.L_SQUARE}
    )
    var_assignment_index: list[list[Token]] = [
        []
    ]  # is list[list] to handle multiple indexes
    bracket_layers, add_to_var_assignment_index = 0, False
    for t in tokens:

        if not can_be_var_assignment:
            break

        if t.type == TokenType.L_SQUARE:
            if bracket_layers == 0:
                add_to_var_assignment_index = True
            bracket_layers += 1
            continue
        elif t.type == TokenType.R_SQUARE:
            bracket_layers -= 1
            if bracket_layers == 0:
                add_to_var_assignment_index = False
                var_assignment_index.append([])
            continue
        elif (
            bracket_layers == 0 and t.type == TokenType.EQUAL
        ):  # exit when hitting the equals
            break

        if add_to_var_assignment_index:
            var_assignment_index[-1].append(t)
    var_assignment_index.pop()  # the last one will always be empty

    # checking modifiers and lifetime for varianle declaration
    names_in_row: list[Token] = []
    looking_for_lifetime, lifetime = False, None
    for t in without_whitespace:
        if (
            not can_be_var_declaration or can_be_var_assignment
        ):  # var assignment has a single name, therefore counteracts being decl
            break
        if not looking_for_lifetime:
            if t.type != TokenType.NAME:
                if t.type == TokenType.LESS_THAN:
                    looking_for_lifetime = True
                    continue
                else:
                    break
            names_in_row.append(t)
        else:
            if (
                t.type == TokenType.GREATER_THAN
                or not 3 <= len(names_in_row) <= 4
                or not can_be_var_declaration
            ):
                break
            if not lifetime:
                lifetime = t.value

    can_be_var_declaration &= 2 <= len(names_in_row) <= 4

    # Check for quantum statement: quantum x [1,2,3]!
    can_be_quantum = (
        len(without_whitespace) >= 4
        and without_whitespace[0].type == TokenType.NAME
        and without_whitespace[0].value == "quantum"
        and without_whitespace[1].type == TokenType.NAME
    )

    # Check for gaslighting statement: definitely_not x 42!
    can_be_gaslighting = (
        len(without_whitespace) >= 4
        and without_whitespace[0].type == TokenType.NAME
        and without_whitespace[0].value == "definitely_not"
        and without_whitespace[1].type == TokenType.NAME
    )

    # Check for blockchain mine statement: mine 10!
    can_be_mine = (
        len(without_whitespace) >= 2
        and without_whitespace[0].type == TokenType.NAME
        and without_whitespace[0].value == "mine"
    )

    # Check for immutable_ledger statement: immutable_ledger x 100!
    can_be_immutable_ledger = (
        len(without_whitespace) >= 4
        and without_whitespace[0].type == TokenType.NAME
        and without_whitespace[0].value == "immutable_ledger"
        and without_whitespace[1].type == TokenType.NAME
    )

    # make a list of all possible things, starting with plain expression
    possibilities: list[CodeStatement] = [ExpressionStatement(tokens[:-1], debug_level)]
    # Consider ReverseStatement as a possibility (do not short-circuit)
    if (
        len(tokens_no_ws) == 3
        and tokens_no_ws[0].type == TokenType.NAME
        and tokens_no_ws[1].type == TokenType.NAME
        and tokens_no_ws[2].type in {TokenType.BANG, TokenType.QUESTION}
    ):
        possibilities.append(
            ReverseStatement(
                keyword=tokens_no_ws[
                    0
                ],  # candidate keyword (should resolve to 'reverse')
                name=tokens_no_ws[1],
                debug=debug_level,
            )
        )
    if can_be_return:
        possibilities.append(
            ReturnStatement(
                keyword=without_whitespace[
                    0
                ],  # should be the same as tokens[0].value but this makes me feel safe
                expression=tokens[1:-1],
                debug=debug_level,
            )
        )
    if can_be_delete:
        possibilities.append(
            DeleteStatement(
                keyword=without_whitespace[0],
                name=without_whitespace[1],
                debug=debug_level,
            )
        )
    if can_be_import:
        possibilities.append(
            ImportStatement(
                keyword=without_whitespace[0],
                names=[t for t in without_whitespace[1:-1] if t.type == TokenType.NAME],
                debug=debug_level,
            )
        )
    if can_be_export:
        possibilities.append(
            ExportStatement(
                export_keyword=without_whitespace[0],
                names=[t for t in without_whitespace[1:-3] if t.type == TokenType.NAME],
                to_keyword=without_whitespace[-3],
                target_file=without_whitespace[-2],
                debug=debug_level,
            )
        )
    if can_be_quantum:
        # Find where the variable name ends in tokens (need to skip whitespace)
        var_name_token = without_whitespace[1]
        value_start_index = 0
        for i, t in enumerate(tokens):
            if t == var_name_token:
                value_start_index = i + 1
                break

        possibilities.append(
            QuantumStatement(
                keyword=without_whitespace[0],
                name=without_whitespace[1],
                value=tokens[
                    value_start_index:-1
                ],  # After variable name until punctuation
                debug=debug_level,
            )
        )
    if can_be_gaslighting:
        # Similar to quantum - find where variable name ends
        var_name_token = without_whitespace[1]
        value_start_index = 0
        for i, t in enumerate(tokens):
            if t == var_name_token:
                value_start_index = i + 1
                break

        possibilities.append(
            GaslightingStatement(
                keyword=without_whitespace[0],
                name=without_whitespace[1],
                value=tokens[value_start_index:-1],
                debug=debug_level,
            )
        )
    if can_be_immutable_ledger:
        # Similar structure to gaslighting
        var_name_token = without_whitespace[1]
        value_start_index = 0
        for i, t in enumerate(tokens):
            if t == var_name_token:
                value_start_index = i + 1
                break

        possibilities.append(
            BlockchainStatement(
                keyword=without_whitespace[0],
                code=None,
                args=tokens[value_start_index:-1],
            )
        )
    if can_be_mine:
        possibilities.append(
            BlockchainStatement(
                keyword=without_whitespace[0],
                code=None,
                args=tokens[1:-1],  # Everything after 'mine' until punctuation
            )
        )
    if can_be_var_declaration:
        possibilities.append(
            VariableDeclaration(
                name=names_in_row[-1],
                modifiers=names_in_row[:-1],
                lifetime=lifetime,
                expression=tokens[
                    tokens_is_equal.index(True) + 1 : -1
                ],  # the end should be a puncutation
                confidence=confidence,
                debug=debug_level,
                type_annotation=type_annotation,
            )
        )
    if can_be_var_assignment:
        possibilities.append(
            VariableAssignment(
                name=without_whitespace[0],
                expression=tokens[tokens_is_equal.index(True) + 1 : -1],
                debug=debug_level,
                indexes=var_assignment_index,
                confidence=confidence,
            )
        )
    return tuple(possibilities)


def generate_syntax_tree(
    filename: str, tokens: list[Token], code: str
) -> list[tuple[CodeStatement, ...]]:
    """Split the code up into lines, which are then parsed and shit"""

    assert_proper_indentation(filename, tokens, code)
    statements = split_into_statements(tokens)
    extracted_types = extract_type_annotations(filename, code, statements)
    removed_hints = remove_type_hints(
        filename, code, [tokens for tokens, _ in extracted_types]
    )
    final_statements = []

    # now we need to perform pattern matching on each list of statements
    for (original_tokens, type_annotation), tokens in zip(
        extracted_types, removed_hints
    ):

        without_whitespace = [t for t in tokens if t.type != TokenType.WHITESPACE]

        try:
            # contains an open scope :)
            if any(t.type == TokenType.L_CURLY for t in tokens):
                final_statements.append(
                    create_scoped_code_statement(
                        filename, tokens, without_whitespace, code, type_annotation
                    )
                )

            else:
                final_statements.append(
                    create_unscoped_code_statement(
                        filename,
                        tokens,
                        without_whitespace,
                        code,
                        type_annotation,
                    )
                )

            # exit if some possiblity was found
            if final_statements[-1]:
                continue
        except (
            IndexError
        ):  # i have no idea what kind of errors are going to be rasied here
            pass
        raise_error_at_line(
            filename,
            code,
            without_whitespace[0].line,
            "Error parsing statement. I have no idea what went wrong, double check it and try again.",
        )

    return final_statements

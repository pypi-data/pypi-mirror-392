{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Advanced Features and CLI Tools\n",
    "## EPyR Tools Tutorial 05: Command-Line Interface and Automation\n",
    "\n",
    "Welcome to the final tutorial in the EPyR Tools series! This notebook covers advanced features including the command-line interface (CLI), batch processing, automation workflows, and integration capabilities.\n",
    "\n",
    "### What You'll Learn\n",
    "\n",
    "- **CLI Commands**: Complete command-line toolkit\n",
    "- **Batch Processing**: Automated analysis of multiple files\n",
    "- **FAIR Data Export**: Standards-compliant data conversion\n",
    "- **Configuration Management**: Customizing EPyR Tools behavior\n",
    "- **Performance Optimization**: Memory management and caching\n",
    "- **Plugin Architecture**: Extending functionality\n",
    "- **Integration Workflows**: Connecting with other tools\n",
    "\n",
    "### Prerequisites\n",
    "\n",
    "- Complete Tutorials 01-04\n",
    "- Command-line familiarity\n",
    "- Understanding of batch processing concepts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. EPyR Tools CLI Overview\n",
    "\n",
    "EPyR Tools provides a comprehensive command-line interface with 8 main commands. Let's explore the available tools:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, let's check our EPyR Tools installation and CLI availability\n",
    "import subprocess\n",
    "import os\n",
    "\n",
    "# Check if CLI commands are available\n",
    "cli_commands = [\n",
    "    'epyr-info',\n",
    "    'epyr-load', \n",
    "    'epyr-convert',\n",
    "    'epyr-baseline',\n",
    "    'epyr-plot',\n",
    "    'epyr-batch',\n",
    "    'epyr-config',\n",
    "    'epyr-validate'\n",
    "]\n",
    "\n",
    "print(\"EPyR Tools CLI Commands Availability:\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "for cmd in cli_commands:\n",
    "    try:\n",
    "        result = subprocess.run([cmd, '--help'], \n",
    "                              capture_output=True, text=True, timeout=10)\n",
    "        if result.returncode == 0:\n",
    "            print(f\"✅ {cmd:<15} - Available\")\n",
    "        else:\n",
    "            print(f\"❌ {cmd:<15} - Error: {result.stderr.strip()[:50]}\")\n",
    "    except FileNotFoundError:\n",
    "        print(f\"❌ {cmd:<15} - Not found in PATH\")\n",
    "    except subprocess.TimeoutExpired:\n",
    "        print(f\"⏱️ {cmd:<15} - Timeout (likely available)\")\n",
    "    except Exception as e:\n",
    "        print(f\"❓ {cmd:<15} - Unknown error: {str(e)[:30]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. System Information and Diagnostics\n",
    "\n",
    "The `epyr-info` command provides comprehensive system diagnostics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get basic system information\n",
    "try:\n",
    "    result = subprocess.run(['epyr-info'], capture_output=True, text=True, timeout=15)\n",
    "    if result.returncode == 0:\n",
    "        print(\"EPyR Tools System Information:\")\n",
    "        print(\"=\" * 40)\n",
    "        print(result.stdout)\n",
    "    else:\n",
    "        print(f\"Error running epyr-info: {result.stderr}\")\n",
    "except Exception as e:\n",
    "    print(f\"Could not run epyr-info: {e}\")\n",
    "    \n",
    "    # Fallback: get info programmatically\n",
    "    import epyr\n",
    "    import sys\n",
    "    import platform\n",
    "    \n",
    "    print(\"\\nFallback System Information:\")\n",
    "    print(\"=\" * 30)\n",
    "    print(f\"EPyR Tools Version: {epyr.__version__}\")\n",
    "    print(f\"Python Version: {sys.version.split()[0]}\")\n",
    "    print(f\"Platform: {platform.system()} {platform.machine()}\")\n",
    "    print(f\"Available Modules:\")\n",
    "    print(f\"  - Data Loading: {hasattr(epyr, 'eprload')}\")\n",
    "    print(f\"  - Baseline Correction: {hasattr(epyr, 'baseline')}\")\n",
    "    print(f\"  - Signal Processing: {hasattr(epyr, 'signalprocessing')}\")\n",
    "    print(f\"  - Lineshapes: {hasattr(epyr, 'lineshapes')}\")\n",
    "    print(f\"  - Plotting: {hasattr(epyr, 'eprplot')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Configuration Management\n",
    "\n",
    "EPyR Tools uses a hierarchical configuration system that can be managed via CLI or programmatically:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration management\n",
    "from epyr.config import config\n",
    "import json\n",
    "\n",
    "print(\"Current Configuration Settings:\")\n",
    "print(\"=\" * 35)\n",
    "\n",
    "# Get current configuration\n",
    "try:\n",
    "    # Common configuration keys\n",
    "    config_keys = [\n",
    "        ('plotting.dpi', 'Plot DPI resolution'),\n",
    "        ('plotting.figure_size', 'Default figure size'),\n",
    "        ('performance.cache_enabled', 'Data caching enabled'),\n",
    "        ('performance.cache_size_mb', 'Cache size (MB)'),\n",
    "        ('logging.level', 'Logging level'),\n",
    "        ('data.auto_baseline', 'Auto baseline correction'),\n",
    "        ('export.format', 'Default export format')\n",
    "    ]\n",
    "    \n",
    "    for key, description in config_keys:\n",
    "        try:\n",
    "            value = config.get(key, default=\"Not set\")\n",
    "            print(f\"{description:<25}: {value}\")\n",
    "        except Exception as e:\n",
    "            print(f\"{description:<25}: Error - {e}\")\n",
    "            \n",
    "except Exception as e:\n",
    "    print(f\"Configuration access error: {e}\")\n",
    "\n",
    "# Demonstrate configuration changes\n",
    "print(\"\\n\" + \"=\"*35)\n",
    "print(\"Demonstration: Changing Configuration\")\n",
    "print(\"=\"*35)\n",
    "\n",
    "# Set some configuration values for demonstration\n",
    "try:\n",
    "    config.set('plotting.dpi', 300)\n",
    "    config.set('performance.cache_enabled', True)\n",
    "    \n",
    "    print(\"Updated settings:\")\n",
    "    print(f\"Plot DPI: {config.get('plotting.dpi')}\")\n",
    "    print(f\"Cache enabled: {config.get('performance.cache_enabled')}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Could not modify configuration: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. File Format Conversion and FAIR Data Export\n",
    "\n",
    "EPyR Tools provides powerful data conversion capabilities for FAIR (Findable, Accessible, Interoperable, Reusable) data standards:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import epyr\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# First, let's load some EPR data for conversion\n",
    "data_dir = \"../data\"\n",
    "\n",
    "# Find available data files\n",
    "if os.path.exists(data_dir):\n",
    "    files = [f for f in os.listdir(data_dir) if f.endswith(('.DTA', '.dta', '.spc'))]\n",
    "    if files:\n",
    "        sample_file = os.path.join(data_dir, files[0])\n",
    "        print(f\"Using sample file: {files[0]}\")\n",
    "        \n",
    "        # Load the data\n",
    "        try:\n",
    "            x, y, params, filepath = epyr.eprload(sample_file)\n",
    "            print(f\"Loaded data: {len(y)} points\")\n",
    "            print(f\"Field range: {x[0]:.1f} to {x[-1]:.1f} G\")\n",
    "            \n",
    "            # Show available export formats\n",
    "            print(\"\\nAvailable Export Formats:\")\n",
    "            print(\"=\" * 30)\n",
    "            formats = ['CSV', 'JSON', 'HDF5', 'NPZ']\n",
    "            for fmt in formats:\n",
    "                print(f\"  - {fmt}: Standards-compliant {fmt} format\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error loading data: {e}\")\n",
    "            # Create synthetic data for demonstration\n",
    "            x = np.linspace(3400, 3500, 1000)\n",
    "            y = np.exp(-((x-3450)/10)**2) + 0.1 * np.random.normal(size=len(x))\n",
    "            params = {\n",
    "                'XMIN': x[0],\n",
    "                'XMAX': x[-1], \n",
    "                'XPTS': len(x),\n",
    "                'YMIN': np.min(y),\n",
    "                'YMAX': np.max(y),\n",
    "                'Temperature': '5 K',\n",
    "                'Microwave_Frequency': '9.5 GHz'\n",
    "            }\n",
    "            filepath = \"synthetic_demo_data\"\n",
    "            print(\"Using synthetic data for demonstration\")\n",
    "    else:\n",
    "        print(\"No EPR data files found, using synthetic data\")\n",
    "        x = np.linspace(3400, 3500, 1000)\n",
    "        y = np.exp(-((x-3450)/10)**2) + 0.1 * np.random.normal(size=len(x))\n",
    "        params = {\n",
    "            'XMIN': x[0],\n",
    "            'XMAX': x[-1],\n",
    "            'XPTS': len(x),\n",
    "            'Temperature': '5 K',\n",
    "            'Microwave_Frequency': '9.5 GHz'\n",
    "        }\n",
    "        filepath = \"synthetic_demo_data\"\nelse:\n",
    "    print(\"Data directory not found, using synthetic data\")\n",
    "    x = np.linspace(3400, 3500, 1000)\n",
    "    y = np.exp(-((x-3450)/10)**2) + 0.1 * np.random.normal(size=len(x))\n",
    "    params = {\n",
    "        'XMIN': x[0],\n",
    "        'XMAX': x[-1],\n",
    "        'XPTS': len(x),\n",
    "        'Temperature': '5 K',\n",
    "        'Microwave_Frequency': '9.5 GHz'\n",
    "    }\n",
    "    filepath = \"synthetic_demo_data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate programmatic data export\n",
    "import json\n",
    "import csv\n",
    "from datetime import datetime\n",
    "\n",
    "print(\"FAIR Data Export Demonstration:\")\n",
    "print(\"=\" * 35)\n",
    "\n",
    "# Create FAIR-compliant metadata\n",
    "fair_metadata = {\n",
    "    'data_info': {\n",
    "        'title': 'EPR Spectroscopy Data',\n",
    "        'description': 'Electron Paramagnetic Resonance spectrum',\n",
    "        'creation_date': datetime.now().isoformat(),\n",
    "        'creator': 'EPyR Tools Tutorial',\n",
    "        'data_points': len(y),\n",
    "        'x_axis_label': 'Magnetic Field (G)',\n",
    "        'y_axis_label': 'Signal Intensity (a.u.)'\n",
    "    },\n",
    "    'measurement_parameters': params,\n",
    "    'processing_info': {\n",
    "        'software': f'EPyR Tools v{epyr.__version__}',\n",
    "        'processing_date': datetime.now().isoformat(),\n",
    "        'baseline_corrected': False,\n",
    "        'normalized': False\n",
    "    },\n",
    "    'data_quality': {\n",
    "        'signal_to_noise_ratio': np.max(np.abs(y)) / np.std(y[-100:]),\n",
    "        'data_integrity_check': 'passed'\n",
    "    }\n",
    "}\n",
    "\n",
    "print(\"Generated FAIR Metadata:\")\n",
    "print(json.dumps(fair_metadata['data_info'], indent=2))\n",
    "print(f\"SNR: {fair_metadata['data_quality']['signal_to_noise_ratio']:.1f}\")\n",
    "\n",
    "# Export to CSV (FAIR format)\n",
    "csv_filename = '/tmp/epr_data_fair.csv'\n",
    "try:\n",
    "    with open(csv_filename, 'w', newline='') as csvfile:\n",
    "        writer = csv.writer(csvfile)\n",
    "        \n",
    "        # Write metadata header\n",
    "        writer.writerow(['# EPyR Tools FAIR Data Export'])\n",
    "        writer.writerow([f'# Creation Date: {fair_metadata[\"data_info\"][\"creation_date\"]}'])\n",
    "        writer.writerow([f'# Software: {fair_metadata[\"processing_info\"][\"software\"]}'])\n",
    "        writer.writerow(['# Data Format: Field (G), Intensity (a.u.)'])\n",
    "        writer.writerow([])\n",
    "        \n",
    "        # Write column headers\n",
    "        writer.writerow(['Magnetic_Field_G', 'Signal_Intensity_au'])\n",
    "        \n",
    "        # Write data\n",
    "        for xi, yi in zip(x, y):\n",
    "            writer.writerow([f'{xi:.6f}', f'{yi:.6e}'])\n",
    "    \n",
    "    print(f\"\\n✅ FAIR CSV export completed: {csv_filename}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"❌ CSV export error: {e}\")\n",
    "\n",
    "# Export metadata to JSON\n",
    "json_filename = '/tmp/epr_metadata_fair.json'\n",
    "try:\n",
    "    with open(json_filename, 'w') as jsonfile:\n",
    "        json.dump(fair_metadata, jsonfile, indent=2)\n",
    "    \n",
    "    print(f\"✅ FAIR JSON metadata completed: {json_filename}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"❌ JSON export error: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Batch Processing Workflows\n",
    "\n",
    "EPyR Tools supports efficient batch processing for analyzing multiple files:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Batch processing demonstration\n",
    "import glob\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "print(\"Batch Processing Demonstration:\")\n",
    "print(\"=\" * 35)\n",
    "\n",
    "# Find all EPR data files\n",
    "data_patterns = [\"../data/*.DTA\", \"../data/*.dta\", \"../data/*.spc\"]\n",
    "all_files = []\n",
    "\n",
    "for pattern in data_patterns:\n",
    "    all_files.extend(glob.glob(pattern))\n",
    "\n",
    "if all_files:\n",
    "    print(f\"Found {len(all_files)} EPR data files for batch processing:\")\n",
    "    for file in all_files[:5]:  # Show first 5\n",
    "        print(f\"  - {Path(file).name}\")\n",
    "    if len(all_files) > 5:\n",
    "        print(f\"  ... and {len(all_files)-5} more files\")\nelse:\n",
    "    print(\"No EPR data files found. Creating synthetic batch for demonstration.\")\n",
    "    # Create synthetic batch data\n",
    "    all_files = [\"synthetic_file_1.DTA\", \"synthetic_file_2.DTA\", \"synthetic_file_3.DTA\"]\n",
    "\n",
    "# Batch processing function\n",
    "def process_epr_file_batch(filepath, process_baseline=True, export_formats=['CSV']):\n",
    "    \"\"\"Process a single EPR file with configurable options\"\"\"\n",
    "    results = {\n",
    "        'filename': Path(filepath).name,\n",
    "        'status': 'pending',\n",
    "        'data_points': 0,\n",
    "        'field_range': (0, 0),\n",
    "        'signal_max': 0,\n",
    "        'signal_min': 0,\n",
    "        'snr_estimate': 0,\n",
    "        'processing_time': 0,\n",
    "        'exports': []\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        start_time = datetime.now()\n",
    "        \n",
    "        if filepath.startswith('synthetic'):\n",
    "            # Generate synthetic data for demonstration\n",
    "            x = np.linspace(3400, 3500, 1000) + np.random.normal(0, 50)\n",
    "            y = np.exp(-((x-3450)/15)**2) + 0.1 * np.random.normal(size=len(x))\n",
    "            params = {'Temperature': f'{np.random.randint(4,20)} K'}\n",
    "        else:\n",
    "            # Load real data\n",
    "            x, y, params, _ = epyr.eprload(filepath)\n",
    "        \n",
    "        # Basic analysis\n",
    "        results.update({\n",
    "            'status': 'success',\n",
    "            'data_points': len(y),\n",
    "            'field_range': (float(x[0]), float(x[-1])),\n",
    "            'signal_max': float(np.max(y)),\n",
    "            'signal_min': float(np.min(y)),\n",
    "            'snr_estimate': float(np.max(np.abs(y)) / np.std(y[-100:]))\n",
    "        })\n",
    "        \n",
    "        # Optional baseline correction\n",
    "        if process_baseline:\n",
    "            try:\n",
    "                y_corrected = epyr.baseline.correct_1d(x, y, order=2)\n",
    "                results['baseline_corrected'] = True\n",
    "                y = y_corrected  # Use corrected data for export\n",
    "            except:\n",
    "                results['baseline_corrected'] = False\n",
    "        \n",
    "        # Export in requested formats\n",
    "        for fmt in export_formats:\n",
    "            export_file = f\"/tmp/{Path(filepath).stem}_processed.{fmt.lower()}\"\n",
    "            try:\n",
    "                if fmt == 'CSV':\n",
    "                    np.savetxt(export_file, np.column_stack([x, y]), \n",
    "                              delimiter=',', header='Field_G,Intensity_au')\n",
    "                elif fmt == 'NPZ':\n",
    "                    np.savez(export_file, x=x, y=y, params=params)\n",
    "                results['exports'].append(export_file)\n",
    "            except Exception as e:\n",
    "                results['export_error'] = str(e)\n",
    "        \n",
    "        # Calculate processing time\n",
    "        results['processing_time'] = (datetime.now() - start_time).total_seconds()\n",
    "        \n",
    "    except Exception as e:\n",
    "        results.update({\n",
    "            'status': 'error',\n",
    "            'error_message': str(e)\n",
    "        })\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Process batch of files\n",
    "print(\"\\nProcessing files...\")\n",
    "batch_results = []\n",
    "\n",
    "for filepath in all_files[:3]:  # Process first 3 files\n",
    "    print(f\"Processing: {Path(filepath).name}\")\n",
    "    result = process_epr_file_batch(filepath, process_baseline=True, export_formats=['CSV'])\n",
    "    batch_results.append(result)\n",
    "    print(f\"  Status: {result['status']} ({result['processing_time']:.2f}s)\")\n",
    "\n",
    "# Create batch processing summary\n",
    "print(\"\\n\" + \"=\"*40)\n",
    "print(\"Batch Processing Summary\")\n",
    "print(\"=\"*40)\n",
    "\n",
    "df_results = pd.DataFrame(batch_results)\n",
    "print(f\"Total files processed: {len(batch_results)}\")\n",
    "print(f\"Successful: {sum(df_results['status'] == 'success')}\")\n",
    "print(f\"Failed: {sum(df_results['status'] == 'error')}\")\n",
    "print(f\"Average processing time: {df_results['processing_time'].mean():.2f}s\")\n",
    "print(f\"Total data points processed: {df_results['data_points'].sum()}\")\n",
    "\n",
    "if len(df_results) > 0:\n",
    "    print(\"\\nFile Summary:\")\n",
    "    for _, row in df_results.iterrows():\n",
    "        print(f\"  {row['filename']:<30} {row['data_points']:>6} pts, SNR: {row['snr_estimate']:.1f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Performance Optimization and Caching\n",
    "\n",
    "EPyR Tools includes sophisticated performance optimization features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performance optimization demonstration\n",
    "import time\n",
    "import psutil\n",
    "import gc\n",
    "\n",
    "print(\"Performance Optimization Demonstration:\")\n",
    "print(\"=\" * 45)\n",
    "\n",
    "# Memory usage monitoring\n",
    "def get_memory_usage():\n",
    "    \"\"\"Get current memory usage in MB\"\"\"\n",
    "    process = psutil.Process(os.getpid())\n",
    "    return process.memory_info().rss / 1024 / 1024\n",
    "\n",
    "initial_memory = get_memory_usage()\n",
    "print(f\"Initial memory usage: {initial_memory:.1f} MB\")\n",
    "\n",
    "# Demonstrate data caching\n",
    "print(\"\\n1. Data Loading Performance:\")\n",
    "print(\"-\" * 30)\n",
    "\n",
    "# Create large synthetic dataset for performance testing\n",
    "large_x = np.linspace(0, 1000, 100000)  # 100k points\n",
    "large_y = np.sin(large_x) + 0.1 * np.random.normal(size=len(large_x))\n",
    "\n",
    "print(f\"Created large dataset: {len(large_y):,} points\")\n",
    "print(f\"Memory after data creation: {get_memory_usage():.1f} MB (+{get_memory_usage()-initial_memory:.1f} MB)\")\n",
    "\n",
    "# Simulate data processing with timing\n",
    "processing_times = []\n",
    "memory_usage = []\n",
    "\n",
    "for i in range(3):\n",
    "    start_time = time.time()\n",
    "    start_memory = get_memory_usage()\n",
    "    \n",
    "    # Simulate expensive processing\n",
    "    processed_data = np.fft.fft(large_y)\n",
    "    smoothed_data = np.convolve(large_y, np.ones(10)/10, mode='same')\n",
    "    \n",
    "    end_time = time.time()\n",
    "    end_memory = get_memory_usage()\n",
    "    \n",
    "    processing_times.append(end_time - start_time)\n",
    "    memory_usage.append(end_memory - start_memory)\n",
    "    \n",
    "    print(f\"Processing run {i+1}: {processing_times[i]:.3f}s, +{memory_usage[i]:.1f} MB\")\n",
    "    \n",
    "    # Clean up intermediate results\n",
    "    del processed_data, smoothed_data\n",
    "    gc.collect()\n",
    "\n",
    "print(f\"Average processing time: {np.mean(processing_times):.3f}s ± {np.std(processing_times):.3f}s\")\n",
    "print(f\"Average memory overhead: {np.mean(memory_usage):.1f} MB\")\n",
    "\n",
    "# Demonstrate configuration impact on performance\n",
    "print(\"\\n2. Configuration Impact on Performance:\")\n",
    "print(\"-\" * 42)\n",
    "\n",
    "# Test different DPI settings for plotting\n",
    "dpi_settings = [72, 150, 300]\n",
    "plot_times = []\n",
    "\n",
    "for dpi in dpi_settings:\n",
    "    config.set('plotting.dpi', dpi)\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Create plot with current DPI setting\n",
    "    fig, ax = plt.subplots(figsize=(8, 6), dpi=dpi)\n",
    "    ax.plot(large_x[::100], large_y[::100])  # Subsample for plotting\n",
    "    ax.set_title(f'Plot at {dpi} DPI')\n",
    "    plt.close(fig)  # Close to save memory\n",
    "    \n",
    "    plot_time = time.time() - start_time\n",
    "    plot_times.append(plot_time)\n",
    "    \n",
    "    print(f\"Plot at {dpi:>3} DPI: {plot_time:.3f}s\")\n",
    "\n",
    "print(f\"\\nPerformance recommendation: Use DPI ≤ 150 for interactive work\")\n",
    "print(f\"High DPI (300) is {plot_times[2]/plot_times[0]:.1f}x slower than low DPI (72)\")\n",
    "\n",
    "# Clean up large dataset\n",
    "del large_x, large_y\n",
    "gc.collect()\n",
    "\n",
    "final_memory = get_memory_usage()\n",
    "print(f\"\\nFinal memory usage: {final_memory:.1f} MB (change: {final_memory-initial_memory:+.1f} MB)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Plugin Architecture and Extensibility\n",
    "\n",
    "EPyR Tools is designed with a plugin architecture for easy extensibility:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plugin architecture demonstration\n",
    "print(\"Plugin Architecture Demonstration:\")\n",
    "print(\"=\" * 38)\n",
    "\n",
    "# Show current plugin system status\n",
    "try:\n",
    "    from epyr.plugins import plugin_manager\n",
    "    \n",
    "    print(\"Plugin Manager Status:\")\n",
    "    print(f\"  Available plugin slots: File loaders, processors, exporters\")\n",
    "    print(f\"  Auto-discovery enabled: Yes\")\n",
    "    print(f\"  Entry point namespace: 'epyr_plugins'\")\n",
    "    \n",
    "except ImportError:\n",
    "    print(\"Plugin manager not available in this installation\")\n",
    "\n",
    "# Demonstrate a simple custom processing function\n",
    "print(\"\\nCustom Processing Function Example:\")\n",
    "print(\"-\" * 38)\n",
    "\n",
    "def custom_noise_analysis(x, y, params=None):\n",
    "    \"\"\"\n",
    "    Custom EPR data processing: Advanced noise analysis\n",
    "    \n",
    "    This function demonstrates how to create custom processing\n",
    "    that could be packaged as an EPyR Tools plugin.\n",
    "    \"\"\"\n",
    "    results = {\n",
    "        'function': 'custom_noise_analysis',\n",
    "        'version': '1.0.0',\n",
    "        'description': 'Advanced noise characterization for EPR data'\n",
    "    }\n",
    "    \n",
    "    # Calculate various noise metrics\n",
    "    signal = np.abs(y)\n",
    "    \n",
    "    # Estimate signal regions (above 10% of max)\n",
    "    signal_threshold = 0.1 * np.max(signal)\n",
    "    signal_region = signal > signal_threshold\n",
    "    noise_region = ~signal_region\n",
    "    \n",
    "    # Noise statistics\n",
    "    if np.any(noise_region):\n",
    "        noise_std = np.std(y[noise_region])\n",
    "        noise_mean = np.mean(y[noise_region])\n",
    "    else:\n",
    "        # Fallback: use last 10% of data as noise estimate\n",
    "        noise_end = int(0.9 * len(y))\n",
    "        noise_std = np.std(y[noise_end:])\n",
    "        noise_mean = np.mean(y[noise_end:])\n",
    "    \n",
    "    # Signal quality metrics\n",
    "    peak_signal = np.max(signal)\n",
    "    snr = peak_signal / noise_std if noise_std > 0 else np.inf\n",
    "    \n",
    "    # Frequency domain noise analysis\n",
    "    freq_spectrum = np.abs(np.fft.fft(y))\n",
    "    freq_noise_floor = np.median(freq_spectrum[len(freq_spectrum)//4:])\n",
    "    \n",
    "    # Dynamic range\n",
    "    dynamic_range = 20 * np.log10(peak_signal / noise_std) if noise_std > 0 else np.inf\n",
    "    \n",
    "    results.update({\n",
    "        'noise_statistics': {\n",
    "            'std_deviation': float(noise_std),\n",
    "            'mean_offset': float(noise_mean),\n",
    "            'freq_noise_floor': float(freq_noise_floor)\n",
    "        },\n",
    "        'signal_quality': {\n",
    "            'snr_linear': float(snr),\n",
    "            'snr_db': float(20 * np.log10(snr)) if snr > 0 else -np.inf,\n",
    "            'dynamic_range_db': float(dynamic_range),\n",
    "            'peak_signal': float(peak_signal)\n",
    "        },\n",
    "        'data_quality_grade': 'A' if snr > 50 else 'B' if snr > 20 else 'C' if snr > 10 else 'D'\n",
    "    })\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Test the custom function\n",
    "test_x = np.linspace(3400, 3500, 1000)\n",
    "test_y = np.exp(-((test_x-3450)/10)**2) + 0.05 * np.random.normal(size=len(test_x))\n",
    "\n",
    "custom_results = custom_noise_analysis(test_x, test_y)\n",
    "\n",
    "print(f\"Custom Analysis Results:\")\n",
    "print(f\"  Function: {custom_results['function']} v{custom_results['version']}\")\n",
    "print(f\"  SNR: {custom_results['signal_quality']['snr_linear']:.1f} ({custom_results['signal_quality']['snr_db']:.1f} dB)\")\n",
    "print(f\"  Dynamic Range: {custom_results['signal_quality']['dynamic_range_db']:.1f} dB\")\n",
    "print(f\"  Data Quality Grade: {custom_results['data_quality_grade']}\")\n",
    "print(f\"  Noise Statistics: σ = {custom_results['noise_statistics']['std_deviation']:.2e}\")\n",
    "\n",
    "# Show how this could be integrated into EPyR Tools workflow\n",
    "print(\"\\nIntegration Example:\")\n",
    "print(\"-------------------\")\n",
    "print(\"# To create an EPyR Tools plugin:\")\n",
    "print(\"# 1. Create a Python package with the processing function\")\n",
    "print(\"# 2. Add entry point in setup.py:\")\n",
    "print(\"#    entry_points={\")\n",
    "print(\"#        'epyr_plugins': [\")\n",
    "print(\"#            'noise_analyzer = my_plugin:custom_noise_analysis'\")\n",
    "print(\"#        ]\")\n",
    "print(\"#    }\")\n",
    "print(\"# 3. Install the plugin package\")\n",
    "print(\"# 4. EPyR Tools will auto-discover and load the plugin\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Integration Workflows\n",
    "\n",
    "EPyR Tools is designed to integrate seamlessly with other scientific software:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Integration workflow demonstration\n",
    "import json\n",
    "import subprocess\n",
    "import tempfile\n",
    "from pathlib import Path\n",
    "\n",
    "print(\"Integration Workflows Demonstration:\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "# 1. Integration with common Python scientific stack\n",
    "print(\"1. Scientific Python Stack Integration:\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "# Generate example data\n",
    "x = np.linspace(3400, 3500, 1000)\n",
    "y = np.exp(-((x-3450)/15)**2) + 0.1 * np.random.normal(size=len(x))\n",
    "\n",
    "# Integration with pandas\n",
    "try:\n",
    "    import pandas as pd\n",
    "    \n",
    "    df = pd.DataFrame({\n",
    "        'field_g': x,\n",
    "        'intensity': y,\n",
    "        'normalized_intensity': y / np.max(np.abs(y))\n",
    "    })\n",
    "    \n",
    "    print(f\"✅ Pandas integration: {len(df)} data points in DataFrame\")\n",
    "    print(f\"   Data summary: {df['intensity'].describe().to_dict()}\")\n",
    "    \n",
    "except ImportError:\n",
    "    print(\"❌ Pandas not available\")\n",
    "\n",
    "# Integration with scipy\n",
    "try:\n",
    "    from scipy import signal, optimize\n",
    "    \n",
    "    # Apply scipy filters\n",
    "    filtered_y = signal.savgol_filter(y, window_length=21, polyorder=3)\n",
    "    \n",
    "    # Find peaks using scipy\n",
    "    peaks, properties = signal.find_peaks(np.abs(filtered_y), height=0.1*np.max(np.abs(filtered_y)))\n",
    "    \n",
    "    print(f\"✅ SciPy integration: {len(peaks)} peaks detected\")\n",
    "    if len(peaks) > 0:\n",
    "        peak_fields = x[peaks]\n",
    "        print(f\"   Peak positions: {[f'{p:.1f} G' for p in peak_fields]}\")\n",
    "    \n",
    "except ImportError:\n",
    "    print(\"❌ SciPy not available\")\n",
    "\n",
    "# 2. File format conversions for other software\n",
    "print(\"\\n2. External Software Format Export:\")\n",
    "print(\"-\" * 37)\n",
    "\n",
    "# Export for Origin/OriginPro\n",
    "origin_file = '/tmp/epr_data_for_origin.dat'\n",
    "try:\n",
    "    with open(origin_file, 'w') as f:\n",
    "        f.write(\"# EPyR Tools export for Origin\\n\")\n",
    "        f.write(\"# Field(G)\\tIntensity(a.u.)\\n\")\n",
    "        for xi, yi in zip(x, y):\n",
    "            f.write(f\"{xi:.6f}\\t{yi:.6e}\\n\")\n",
    "    print(f\"✅ Origin format: {origin_file}\")\nexcept Exception as e:\n",
    "    print(f\"❌ Origin export error: {e}\")\n",
    "\n",
    "# Export for MATLAB\n",
    "matlab_file = '/tmp/epr_data_for_matlab.m'\n",
    "try:\n",
    "    with open(matlab_file, 'w') as f:\n",
    "        f.write(\"% EPyR Tools export for MATLAB\\n\")\n",
    "        f.write(\"% Generated by EPyR Tools\\n\\n\")\n",
    "        f.write(\"field_g = [\")\n",
    "        f.write(\", \".join([f\"{xi:.6f}\" for xi in x]))\n",
    "        f.write(\"];\\n\\n\")\n",
    "        f.write(\"intensity = [\")\n",
    "        f.write(\", \".join([f\"{yi:.6e}\" for yi in y]))\n",
    "        f.write(\"];\\n\\n\")\n",
    "        f.write(\"% Plot the data\\n\")\n",
    "        f.write(\"figure;\\n\")\n",
    "        f.write(\"plot(field_g, intensity);\\n\")\n",
    "        f.write(\"xlabel('Magnetic Field (G)');\\n\")\n",
    "        f.write(\"ylabel('Intensity (a.u.)');\\n\")\n",
    "        f.write(\"title('EPR Spectrum');\\n\")\n",
    "    print(f\"✅ MATLAB format: {matlab_file}\")\nexcept Exception as e:\n",
    "    print(f\"❌ MATLAB export error: {e}\")\n",
    "\n",
    "# 3. Command-line workflow integration\n",
    "print(\"\\n3. Command-Line Workflow Integration:\")\n",
    "print(\"-\" * 38)\n",
    "\n",
    "# Create a shell script for automated processing\n",
    "shell_script = '/tmp/epr_processing_pipeline.sh'\n",
    "try:\n",
    "    script_content = '''#!/bin/bash\n",
    "# EPyR Tools Automated Processing Pipeline\n",
    "# Usage: ./epr_processing_pipeline.sh <data_directory>\n",
    "\n",
    "DATA_DIR=\"${1:-../data}\"\n",
    "OUTPUT_DIR=\"/tmp/epr_batch_output\"\n",
    "\n",
    "echo \"EPyR Tools Batch Processing Pipeline\"\n",
    "echo \"====================================\"\n",
    "echo \"Input directory: $DATA_DIR\"\n",
    "echo \"Output directory: $OUTPUT_DIR\"\n",
    "\n",
    "# Create output directory\n",
    "mkdir -p \"$OUTPUT_DIR\"\n",
    "\n",
    "# Process each EPR file\n",
    "for file in \"$DATA_DIR\"/*.{DTA,dta,spc}; do\n",
    "    if [ -f \"$file\" ]; then\n",
    "        echo \"Processing: $(basename \"$file\")\"\n",
    "        \n",
    "        # Load and convert to CSV (using hypothetical CLI command)\n",
    "        # epyr-convert \"$file\" --format CSV --output \"$OUTPUT_DIR\"\n",
    "        \n",
    "        # Apply baseline correction\n",
    "        # epyr-baseline \"$file\" --order 2 --output \"$OUTPUT_DIR\"\n",
    "        \n",
    "        # Generate plot\n",
    "        # epyr-plot \"$file\" --save \"$OUTPUT_DIR/$(basename \"$file\" .DTA).png\"\n",
    "        \n",
    "        echo \"  -> Processed successfully\"\n",
    "    fi\n",
    "done\n",
    "\n",
    "echo \"\\nBatch processing completed!\"\n",
    "echo \"Results saved to: $OUTPUT_DIR\"\n",
    "'''\n",
    "    \n",
    "    with open(shell_script, 'w') as f:\n",
    "        f.write(script_content)\n",
    "    \n",
    "    # Make executable\n",
    "    os.chmod(shell_script, 0o755)\n",
    "    \n",
    "    print(f\"✅ Shell script created: {shell_script}\")\n",
    "    print(f\"   Usage: bash {shell_script} <data_directory>\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"❌ Shell script error: {e}\")\n",
    "\n",
    "# 4. Python API integration example\n",
    "print(\"\\n4. Python API Integration Example:\")\n",
    "print(\"-\" * 35)\n",
    "\n",
    "# Create a complete processing workflow function\n",
    "def complete_epr_workflow(data_file, output_dir='/tmp/epr_workflow_output'):\n",
    "    \"\"\"\n",
    "    Complete EPR data processing workflow suitable for integration\n",
    "    into larger scientific analysis pipelines.\n",
    "    \"\"\"\n",
    "    workflow_results = {\n",
    "        'input_file': data_file,\n",
    "        'output_directory': output_dir,\n",
    "        'processing_steps': [],\n",
    "        'outputs': [],\n",
    "        'status': 'success'\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        # Create output directory\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        \n",
    "        # Step 1: Load data\n",
    "        if data_file.startswith('synthetic'):\n",
    "            x, y = test_x, test_y\n",
    "            params = {'Temperature': '5 K'}\n",
    "        else:\n",
    "            x, y, params, _ = epyr.eprload(data_file)\n",
    "        \n",
    "        workflow_results['processing_steps'].append('data_loaded')\n",
    "        \n",
    "        # Step 2: Baseline correction\n",
    "        try:\n",
    "            y_corrected = epyr.baseline.correct_1d(x, y, order=2)\n",
    "            workflow_results['processing_steps'].append('baseline_corrected')\n",
    "            y = y_corrected\n",
    "        except:\n",
    "            workflow_results['processing_steps'].append('baseline_correction_failed')\n",
    "        \n",
    "        # Step 3: Export processed data\n",
    "        output_file = os.path.join(output_dir, 'processed_data.csv')\n",
    "        np.savetxt(output_file, np.column_stack([x, y]), \n",
    "                  delimiter=',', header='Field_G,Intensity_au')\n",
    "        workflow_results['outputs'].append(output_file)\n",
    "        \n",
    "        # Step 4: Generate summary plot\n",
    "        fig, ax = plt.subplots(figsize=(10, 6))\n",
    "        ax.plot(x, y, 'b-', linewidth=1.5, label='Processed Data')\n",
    "        ax.set_xlabel('Magnetic Field (G)')\n",
    "        ax.set_ylabel('Intensity (a.u.)')\n",
    "        ax.set_title('EPR Spectrum - Processed')\n",
    "        ax.grid(True, alpha=0.3)\n",
    "        ax.legend()\n",
    "        \n",
    "        plot_file = os.path.join(output_dir, 'spectrum_plot.png')\n",
    "        fig.savefig(plot_file, dpi=150, bbox_inches='tight')\n",
    "        plt.close(fig)\n",
    "        workflow_results['outputs'].append(plot_file)\n",
    "        \n",
    "        # Step 5: Generate metadata\n",
    "        metadata = {\n",
    "            'processing_info': {\n",
    "                'software': f'EPyR Tools v{epyr.__version__}',\n",
    "                'timestamp': datetime.now().isoformat(),\n",
    "                'workflow_version': '1.0'\n",
    "            },\n",
    "            'data_info': {\n",
    "                'data_points': len(y),\n",
    "                'field_range_g': [float(x[0]), float(x[-1])],\n",
    "                'signal_range': [float(np.min(y)), float(np.max(y))]\n",
    "            },\n",
    "            'parameters': params\n",
    "        }\n",
    "        \n",
    "        metadata_file = os.path.join(output_dir, 'metadata.json')\n",
    "        with open(metadata_file, 'w') as f:\n",
    "            json.dump(metadata, f, indent=2)\n",
    "        workflow_results['outputs'].append(metadata_file)\n",
    "        \n",
    "        workflow_results['processing_steps'].append('workflow_completed')\n",
    "        \n",
    "    except Exception as e:\n",
    "        workflow_results['status'] = 'error'\n",
    "        workflow_results['error_message'] = str(e)\n",
    "    \n",
    "    return workflow_results\n",
    "\n",
    "# Test the complete workflow\n",
    "workflow_result = complete_epr_workflow('synthetic_demo_file.DTA')\n",
    "\n",
    "print(f\"Workflow Status: {workflow_result['status']}\")\n",
    "print(f\"Processing Steps: {' → '.join(workflow_result['processing_steps'])}\")\n",
    "print(f\"Output Files: {len(workflow_result['outputs'])}\")\n",
    "for output in workflow_result['outputs']:\n",
    "    print(f\"  - {Path(output).name}\")\n",
    "\n",
    "print(\"\\n✅ Integration workflows demonstrated successfully!\")\n",
    "print(\"   These examples show how EPyR Tools can be integrated\")\n",
    "print(\"   into larger scientific computing workflows and pipelines.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Advanced CLI Usage Examples\n",
    "\n",
    "Here are practical examples of using EPyR Tools CLI commands for real-world scenarios:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Advanced CLI usage examples\n",
    "print(\"Advanced CLI Usage Examples:\")\n",
    "print(\"=\" * 32)\n",
    "\n",
    "# Generate example CLI command templates\n",
    "cli_examples = {\n",
    "    'Batch Data Conversion': [\n",
    "        '# Convert all EPR files in a directory to CSV format',\n",
    "        'epyr-batch convert --input-dir ./data --output-dir ./converted --format CSV',\n",
    "        '',\n",
    "        '# Convert with baseline correction applied',\n",
    "        'epyr-batch convert --input-dir ./data --baseline-order 2 --format JSON'\n",
    "    ],\n",
    "    \n",
    "    'Automated Baseline Correction': [\n",
    "        '# Apply polynomial baseline correction to all files',\n",
    "        'epyr-batch baseline --input-pattern \"*.DTA\" --order 3 --save-corrected',\n",
    "        '',\n",
    "        '# Interactive baseline correction with preview',\n",
    "        'epyr-baseline data.DTA --interactive --preview'\n",
    "    ],\n",
    "    \n",
    "    'Plotting and Visualization': [\n",
    "        '# Generate publication-quality plots',\n",
    "        'epyr-plot data.DTA --dpi 300 --format PNG --style publication',\n",
    "        '',\n",
    "        '# Create comparison plot of multiple files',\n",
    "        'epyr-plot file1.DTA file2.DTA file3.DTA --overlay --normalize'\n",
    "    ],\n",
    "    \n",
    "    'Data Validation and Quality Check': [\n",
    "        '# Validate data integrity and FAIR compliance',\n",
    "        'epyr-validate --input-dir ./data --check-integrity --fair-compliance',\n",
    "        '',\n",
    "        '# Generate data quality report',\n",
    "        'epyr-validate data.DTA --quality-report --output report.json'\n",
    "    ],\n",
    "    \n",
    "    'System Configuration': [\n",
    "        '# Show current configuration',\n",
    "        'epyr-config show',\n",
    "        '',\n",
    "        '# Set performance options',\n",
    "        'epyr-config set performance.cache_enabled true',\n",
    "        'epyr-config set plotting.dpi 150',\n",
    "        '',\n",
    "        '# Reset to defaults',\n",
    "        'epyr-config reset'\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Display CLI examples\n",
    "for category, commands in cli_examples.items():\n",
    "    print(f\"\\n{category}:\")\n",
    "    print(\"-\" * (len(category) + 1))\n",
    "    for cmd in commands:\n",
    "        if cmd.startswith('#'):\n",
    "            print(f\"  {cmd}\")\n",
    "        elif cmd.strip() == '':\n",
    "            print()\n",
    "        else:\n",
    "            print(f\"  $ {cmd}\")\n",
    "\n",
    "# Create a comprehensive CLI cheat sheet\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"EPyR Tools CLI Quick Reference\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "cheat_sheet = '''## Essential Commands\n",
    "\n",
    "**System Info & Diagnostics:**\n",
    "epyr-info                    # System information\n",
    "epyr-info --all             # Detailed diagnostics\n",
    "epyr-config show            # Current configuration\n",
    "\n",
    "**Data Loading & Inspection:**\n",
    "epyr-load data.DTA          # Load and display data info\n",
    "epyr-load data.DTA --plot   # Load and plot data\n",
    "\n",
    "**Baseline Correction:**\n",
    "epyr-baseline data.DTA --order 2        # Polynomial correction\n",
    "epyr-baseline data.DTA --interactive    # Interactive mode\n",
    "\n",
    "**Data Conversion (FAIR):**\n",
    "epyr-convert data.DTA --format CSV      # Convert to CSV\n",
    "epyr-convert data.DTA --format JSON     # Convert to JSON\n",
    "epyr-convert data.DTA --format HDF5     # Convert to HDF5\n",
    "\n",
    "**Plotting & Visualization:**\n",
    "epyr-plot data.DTA                      # Quick plot\n",
    "epyr-plot data.DTA --save plot.png     # Save plot\n",
    "epyr-plot data.DTA --dpi 300           # High resolution\n",
    "\n",
    "**Batch Processing:**\n",
    "epyr-batch process --input-dir ./data  # Batch processing\n",
    "epyr-batch convert --format CSV        # Batch conversion\n",
    "\n",
    "**Data Validation:**\n",
    "epyr-validate data.DTA                 # Basic validation\n",
    "epyr-validate --fair-compliance        # FAIR compliance check\n",
    "\n",
    "## Advanced Usage Patterns\n",
    "\n",
    "**Pipeline Processing:**\n",
    "# Load → Baseline correct → Convert → Plot\n",
    "epyr-load data.DTA | epyr-baseline --order 2 | epyr-convert --format CSV | epyr-plot\n",
    "\n",
    "**Directory Processing:**\n",
    "# Process all .DTA files in current directory\n",
    "find . -name \"*.DTA\" -exec epyr-load {} \\;\n",
    "\n",
    "**Configuration Management:**\n",
    "epyr-config set plotting.style publication  # Set publication style\n",
    "epyr-config set performance.parallel true   # Enable parallel processing\n",
    "'''\n",
    "\n",
    "print(cheat_sheet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Best Practices and Performance Tips\n",
    "\n",
    "Here are key recommendations for optimal use of EPyR Tools:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Best practices demonstration\n",
    "print(\"EPyR Tools Best Practices and Performance Tips:\")\n",
    "print(\"=\" * 48)\n",
    "\n",
    "# Performance recommendations\n",
    "performance_tips = {\n",
    "    'Memory Management': [\n",
    "        'Enable data caching for repeated analysis: config.set(\"performance.cache_enabled\", True)',\n",
    "        'Use streaming mode for large files (>100 MB): epyr.eprload(file, stream=True)',\n",
    "        'Clear cache periodically in long-running scripts: epyr.clear_cache()',\n",
    "        'Monitor memory usage with: epyr-info --memory'\n",
    "    ],\n",
    "    \n",
    "    'Data Loading Optimization': [\n",
    "        'Use format hints for faster loading: epyr.eprload(file, format=\"BES3T\")',\n",
    "        'Batch load similar files together for better cache utilization',\n",
    "        'Pre-validate file paths to avoid loading errors: epyr.validate_file(path)',\n",
    "        'Use memory mapping for very large datasets: load_options={\"mmap\": True}'\n",
    "    ],\n",
    "    \n",
    "    'Plotting Performance': [\n",
    "        'Use appropriate DPI: 72 for screen, 150 for preview, 300 for publication',\n",
    "        'Subsample large datasets for plotting: x[::10], y[::10]',\n",
    "        'Close figures after saving: plt.close(fig)',\n",
    "        'Use vector formats (SVG, PDF) for final publications'\n",
    "    ],\n",
    "    \n",
    "    'Baseline Correction': [\n",
    "        'Start with low polynomial order (1-2) and increase if needed',\n",
    "        'Use interactive mode for critical data: epyr.baseline.interactive()',\n",
    "        'Save baseline parameters for reproducibility',\n",
    "        'Validate correction quality with SNR metrics'\n",
    "    ],\n",
    "    \n",
    "    'Signal Processing': [\n",
    "        'Always remove DC offset before FFT: remove_dc=True',\n",
    "        'Choose appropriate window function: Hann for general use',\n",
    "        'Use zero padding for better frequency resolution: zero_padding=2',\n",
    "        'Consider sampling rate when interpreting frequencies'\n",
    "    ],\n",
    "    \n",
    "    'Data Export and FAIR Compliance': [\n",
    "        'Always include comprehensive metadata in exports',\n",
    "        'Use standardized units and parameter names',\n",
    "        'Include processing history and software versions',\n",
    "        'Validate exported data integrity: epyr-validate output.csv'\n",
    "    ],\n",
    "    \n",
    "    'Error Handling': [\n",
    "        'Always wrap file operations in try-except blocks',\n",
    "        'Check file formats before processing: epyr.detect_format(file)',\n",
    "        'Validate parameter ranges before analysis',\n",
    "        'Use logging for debugging: import logging; logging.basicConfig(level=logging.DEBUG)'\n",
    "    ],\n",
    "    \n",
    "    'Reproducibility': [\n",
    "        'Save configuration settings: epyr-config export settings.json',\n",
    "        'Version control analysis scripts and parameters',\n",
    "        'Document software versions: epyr-info > system_info.txt',\n",
    "        'Use deterministic random seeds for synthetic data'\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Display best practices\n",
    "for category, tips in performance_tips.items():\n",
    "    print(f\"\\n{category}:\")\n",
    "    print(\"-\" * (len(category) + 1))\n",
    "    for tip in tips:\n",
    "        print(f\"  • {tip}\")\n",
    "\n",
    "# Demonstrate configuration for optimal performance\n",
    "print(\"\\n\" + \"=\"*40)\n",
    "print(\"Recommended Configuration for Different Use Cases\")\n",
    "print(\"=\"*40)\n",
    "\n",
    "use_cases = {\n",
    "    'Interactive Analysis': {\n",
    "        'plotting.dpi': 72,\n",
    "        'performance.cache_enabled': True,\n",
    "        'performance.cache_size_mb': 256,\n",
    "        'logging.level': 'INFO'\n",
    "    },\n",
    "    \n",
    "    'Batch Processing': {\n",
    "        'plotting.dpi': 150,\n",
    "        'performance.cache_enabled': True,\n",
    "        'performance.cache_size_mb': 512,\n",
    "        'performance.parallel': True,\n",
    "        'logging.level': 'WARNING'\n",
    "    },\n",
    "    \n",
    "    'Publication Quality': {\n",
    "        'plotting.dpi': 300,\n",
    "        'plotting.style': 'publication',\n",
    "        'export.include_metadata': True,\n",
    "        'export.fair_compliance': True,\n",
    "        'logging.level': 'INFO'\n",
    "    }\n",
    "}\n",
    "\n",
    "for use_case, settings in use_cases.items():\n",
    "    print(f\"\\n{use_case}:\")\n",
    "    for key, value in settings.items():\n",
    "        print(f\"  {key}: {value}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*40)\n",
    "print(\"Performance Benchmarking\")\n",
    "print(\"=\"*40)\n",
    "\n",
    "# Simple performance benchmarking\n",
    "def benchmark_operation(operation_name, operation_func, *args, **kwargs):\n",
    "    \"\"\"Benchmark an EPyR Tools operation\"\"\"\n",
    "    start_time = time.time()\n",
    "    start_memory = get_memory_usage()\n",
    "    \n",
    "    try:\n",
    "        result = operation_func(*args, **kwargs)\n",
    "        success = True\n",
    "    except Exception as e:\n",
    "        result = str(e)\n",
    "        success = False\n",
    "    \n",
    "    end_time = time.time()\n",
    "    end_memory = get_memory_usage()\n",
    "    \n",
    "    return {\n",
    "        'operation': operation_name,\n",
    "        'success': success,\n",
    "        'duration_s': end_time - start_time,\n",
    "        'memory_change_mb': end_memory - start_memory,\n",
    "        'result_summary': str(result)[:50] if success else result\n",
    "    }\n",
    "\n",
    "# Benchmark common operations\n",
    "test_x = np.linspace(3400, 3500, 10000)  # 10k points\n",
    "test_y = np.exp(-((test_x-3450)/10)**2) + 0.05 * np.random.normal(size=len(test_x))\n",
    "\n",
    "benchmarks = []\n",
    "benchmarks.append(benchmark_operation(\n",
    "    \"Baseline Correction\", \n",
    "    lambda: epyr.baseline.correct_1d(test_x, test_y, order=2)\n",
    "))\n",
    "\n",
    "benchmarks.append(benchmark_operation(\n",
    "    \"Signal Processing FFT\", \n",
    "    lambda: epyr.signalprocessing.analyze_frequencies(test_x, test_y, plot=False)\n",
    "))\n",
    "\n",
    "benchmarks.append(benchmark_operation(\n",
    "    \"Lineshape Fitting\", \n",
    "    lambda: epyr.lineshapes.gaussian(test_x, center=3450, width=10)\n",
    "))\n",
    "\n",
    "print(\"\\nBenchmark Results:\")\n",
    "print(f\"{'Operation':<20} {'Time (s)':<10} {'Memory (MB)':<12} {'Status':<8}\")\n",
    "print(\"-\" * 55)\n",
    "\n",
    "for bench in benchmarks:\n",
    "    status = \"✅ OK\" if bench['success'] else \"❌ Error\"\n",
    "    print(f\"{bench['operation']:<20} {bench['duration_s']:<10.3f} {bench['memory_change_mb']:<12.1f} {status:<8}\")\n",
    "\n",
    "print(\"\\n✅ EPyR Tools Advanced Features Tutorial Completed!\")\n",
    "print(\"\\nYou now have comprehensive knowledge of:\")\n",
    "print(\"  • Command-line interface and automation\")\n",
    "print(\"  • Batch processing and FAIR data export\")\n",
    "print(\"  • Performance optimization techniques\")\n",
    "print(\"  • Integration with other scientific tools\")\n",
    "print(\"  • Best practices for reproducible research\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary and Next Steps\n",
    "\n",
    "Congratulations! You have completed the EPyR Tools tutorial series. You now have comprehensive knowledge of:\n",
    "\n",
    "### **Tutorial Series Review:**\n",
    "\n",
    "1. **[Tutorial 01](01_Basic_EPR_Data_Loading.ipynb)**: EPR data loading from Bruker formats (BES3T, ESP)\n",
    "2. **[Tutorial 02](02_Baseline_Correction_Guide.ipynb)**: Polynomial baseline correction techniques\n",
    "3. **[Tutorial 03](03_Signal_Processing_Analysis.ipynb)**: FFT-based signal processing and frequency analysis\n",
    "4. **[Tutorial 04](04_Lineshape_Analysis_Fitting.ipynb)**: EPR lineshape fitting and spectral analysis\n",
    "5. **[Tutorial 05](05_Advanced_Features_CLI.ipynb)**: CLI tools, automation, and integration workflows\n",
    "\n",
    "### **Key Capabilities Mastered:**\n",
    "\n",
    "- **Data Management**: Loading, validating, and exporting EPR data\n",
    "- **Signal Processing**: FFT analysis, noise characterization, frequency detection\n",
    "- **Spectral Analysis**: Baseline correction, lineshape fitting, parameter extraction\n",
    "- **Automation**: Batch processing, CLI workflows, configuration management\n",
    "- **Integration**: FAIR data compliance, external software compatibility\n",
    "- **Performance**: Memory optimization, caching, benchmarking\n",
    "\n",
    "### **Next Steps:**\n",
    "\n",
    "1. **Practice**: Apply these techniques to your own EPR data\n",
    "2. **Explore**: Experiment with different parameters and methods\n",
    "3. **Integrate**: Incorporate EPyR Tools into your research workflow\n",
    "4. **Contribute**: Share feedback and contribute to the project\n",
    "5. **Stay Updated**: Follow the project for new features and improvements\n",
    "\n",
    "### **Resources:**\n",
    "\n",
    "- **Documentation**: https://epyr-tools.readthedocs.io/\n",
    "- **GitHub**: https://github.com/BertainaS/epyrtools\n",
    "- **Issues & Support**: https://github.com/BertainaS/epyrtools/issues\n",
    "- **PyPI**: https://pypi.org/project/epyr-tools/\n",
    "\n",
    "### **Citation:**\n",
    "\n",
    "If you use EPyR Tools in your research, please cite:\n",
    "```\n",
    "EPyR Tools: A Python Package for EPR Spectroscopy Data Analysis\n",
    "Version 0.2.0+\n",
    "https://github.com/BertainaS/epyrtools\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "**Thank you for using EPyR Tools!** \n",
    "\n",
    "We hope this tutorial series has provided you with the knowledge and confidence to perform sophisticated EPR data analysis using Python. Happy analyzing! 🔬"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
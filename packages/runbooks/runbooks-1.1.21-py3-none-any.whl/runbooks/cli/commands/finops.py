"""
FinOps Commands Module - Financial Operations & Cost Optimization

KISS Principle: Focused on financial operations and cost optimization
DRY Principle: Uses centralized patterns from DRYPatternManager

Phase 2 Enhancement: Eliminates pattern duplication through reference-based access.
Context Efficiency: Reduced imports and shared instances for memory optimization.
"""

# Essential imports that can't be centralized due to decorator usage
import click

# DRY Pattern Manager - eliminates duplication across CLI modules
from runbooks.common.patterns import get_console, get_error_handlers, get_click_group_creator, get_common_decorators

# Import unified CLI decorators (v1.1.7 standardization)
from runbooks.common.cli_decorators import (
    common_aws_options,
    common_output_options,
    common_multi_account_options,
    common_filter_options,
    mcp_validation_option
)

# Single console instance shared across all modules (DRY principle)
console = get_console()

# Import additional modules for enhanced functionality
from runbooks.common.rich_utils import print_header, print_success, print_error, print_info, print_warning
from runbooks.common.output_controller import OutputController
from runbooks.common.logging_config import configure_logging

# Centralized error handlers - replaces 6 duplicate patterns in this module
error_handlers = get_error_handlers()


def _get_cost_metric_display(cost_metrics):
    """Get display string for cost metrics."""
    if len(cost_metrics) == 1:
        return cost_metrics[0]
    else:
        return " + ".join(cost_metrics)

def _check_ec2_service_attachment(instance_id: str, profile: str) -> bool:
    """
    Check if EC2 instance is attached to critical services (E5 signal).

    Returns True if attached to ASG/LB/ECS (instance should NOT be decommissioned).
    Returns False if standalone (safe to consider for decommission).

    Conservative approach: Returns True (attached) on errors to prevent false positives.
    """
    import logging
    logger = logging.getLogger(__name__)

    try:
        from runbooks.common.profile_utils import create_operational_session
        session = create_operational_session(profile)

        # Check Auto Scaling Groups
        try:
            asg_client = session.client('autoscaling')
            asgs = asg_client.describe_auto_scaling_groups()
            for group in asgs.get('AutoScalingGroups', []):
                for instance in group.get('Instances', []):
                    if instance.get('InstanceId') == instance_id:
                        logger.debug(f"E5: Instance {instance_id} attached to ASG {group.get('AutoScalingGroupName')}")
                        return True
        except Exception as e:
            logger.debug(f"E5: ASG check failed for {instance_id}: {e}")

        # Check Load Balancers (ALB/NLB/CLB)
        try:
            elbv2_client = session.client('elbv2')
            target_groups = elbv2_client.describe_target_groups().get('TargetGroups', [])
            for tg in target_groups:
                try:
                    targets = elbv2_client.describe_target_health(TargetGroupArn=tg['TargetGroupArn'])
                    for target in targets.get('TargetHealthDescriptions', []):
                        if target.get('Target', {}).get('Id') == instance_id:
                            logger.debug(f"E5: Instance {instance_id} attached to target group {tg.get('TargetGroupName')}")
                            return True
                except Exception:
                    pass  # Skip inaccessible target groups
        except Exception as e:
            logger.debug(f"E5: ELB check failed for {instance_id}: {e}")

        # Check ECS Clusters (container instances)
        try:
            ecs_client = session.client('ecs')
            clusters = ecs_client.list_clusters().get('clusterArns', [])
            for cluster_arn in clusters:
                try:
                    container_instances = ecs_client.list_container_instances(cluster=cluster_arn)
                    if instance_id in str(container_instances):  # Instance ID in container instance ARN
                        logger.debug(f"E5: Instance {instance_id} attached to ECS cluster {cluster_arn}")
                        return True
                except Exception:
                    pass  # Skip inaccessible clusters
        except Exception as e:
            logger.debug(f"E5: ECS check failed for {instance_id}: {e}")

        # No service attachment found
        logger.debug(f"E5: Instance {instance_id} not attached to critical services")
        return False

    except Exception as e:
        # Conservative: assume attached on errors (prevents false positive decommissions)
        logger.warning(f"E5: Service attachment check failed for {instance_id}: {e}. Assuming attached (safe default).")
        return True


def calculate_evidence_based_savings(
    services_data: dict,
    running_instances: int,
    stopped_instances: int,
    current_cost: float
) -> tuple:
    """
    Calculate optimization potential from actual resource analysis (evidence-based).

    Uses evidence-based calculations from AWS benchmarks and industry standards:
    - Stopped EC2: $100/month per instance (AWS cost average for EBS volumes)
    - EC2 rightsizing: 20% savings (AWS Compute Optimizer studies)
    - S3 lifecycle: 25% savings (Intelligent-Tiering + Glacier migration)
    - CloudWatch logs: 30% savings (retention optimization)
    - RDS idle: 15% savings (5-signal idle detection)

    Args:
        services_data: Cost by service mapping from Cost Explorer API
        running_instances: Count of running EC2 instances
        stopped_instances: Count of stopped EC2 instances
        current_cost: Current monthly spend

    Returns:
        Tuple of (total_monthly_savings, savings_breakdown_list)

    Example:
        >>> monthly_savings, breakdown = calculate_evidence_based_savings(
        ...     services_data={"Amazon Elastic Compute Cloud - Compute": 5000},
        ...     running_instances=10,
        ...     stopped_instances=2,
        ...     current_cost=10000
        ... )
        >>> monthly_savings
        1200
        >>> breakdown
        [("Stopped EC2 cleanup", 200), ("EC2 rightsizing", 1000)]
    """
    savings_sources = []

    # Source 1: Stopped EC2 instances (IMMEDIATE priority - ðŸ”´)
    if stopped_instances > 0:
        # $100/month per instance for EBS volumes + allocated resources
        stopped_savings = stopped_instances * 100
        savings_sources.append(("Stopped EC2 cleanup", stopped_savings))

    # Source 2: EC2 rightsizing (30-DAY priority - ðŸŸ¡)
    ec2_cost = services_data.get("Amazon Elastic Compute Cloud - Compute", 0)
    if running_instances > 5 and ec2_cost > 0:
        # 20% savings potential from AWS Compute Optimizer industry benchmarks
        rightsizing_savings = ec2_cost * 0.20
        if rightsizing_savings > 100:  # Only include if savings > $100/month
            savings_sources.append(("EC2 rightsizing", rightsizing_savings))

    # Source 3: S3 lifecycle policies (90-DAY priority - ðŸŸ¢)
    s3_cost = services_data.get("Amazon Simple Storage Service", 0)
    if s3_cost > 200:  # Meaningful S3 spend threshold
        # 25% savings from Intelligent-Tiering + Glacier Deep Archive migration
        storage_savings = s3_cost * 0.25
        savings_sources.append(("S3 lifecycle policies", storage_savings))

    # Source 4: CloudWatch log retention (60-DAY priority - ðŸŸ¡)
    cloudwatch_cost = (
        services_data.get("Amazon CloudWatch", 0) +
        services_data.get("CloudWatch", 0) +
        services_data.get("AmazonCloudWatch", 0)  # Handle name variations
    )
    if cloudwatch_cost > 50:  # Meaningful CloudWatch spend threshold
        # 30% savings from retention policy optimization (30 days â†’ 7 days for non-prod)
        log_savings = cloudwatch_cost * 0.30
        savings_sources.append(("CloudWatch log retention", log_savings))

    # Source 5: RDS idle detection (30-DAY priority - ðŸŸ¡)
    rds_cost = (
        services_data.get("Amazon Relational Database Service", 0) +
        services_data.get("Amazon RDS Service", 0) +
        services_data.get("Amazon Relational Datab", 0)  # Handle truncated names
    )
    if rds_cost > 500:  # Meaningful RDS spend threshold
        # 15% savings from idle instance detection (5 signals: connections, CPU, I/O, network, queries)
        rds_savings = rds_cost * 0.15
        savings_sources.append(("RDS idle instances", rds_savings))

    # Calculate total monthly savings from all evidence-based sources
    total_monthly_savings = sum(amount for _, amount in savings_sources)

    return total_monthly_savings, savings_sources


def _generate_table_summary(
    account_id: str,
    current_month_name: str,
    current_cost: float,
    sorted_services: list,
    monthly_savings: float,
    optimization_potential: float,
    savings_breakdown: list
) -> None:
    """Generate executive summary focused on services table data."""
    from runbooks.common.rich_utils import console

    # v1.1.20: Condensed to single row for manager preference (Improvement 1)
    # Build top 3 services inline
    top_3_display = ""
    if sorted_services:
        top_services = []
        for service, cost in sorted_services[:3]:
            pct = (cost / current_cost * 100) if current_cost > 0 else 0
            service_display = service[:30] if len(service) > 30 else service
            top_services.append(f"{service_display} ${cost:,.1f} ({pct:.1f}%)")
        top_3_display = " | Top 3: " + ", ".join(top_services)

    console.print(
        f"[bold]Executive Summary:[/bold] Account: [cyan]{account_id}[/cyan] | "
        f"Period: {current_month_name} | MTD Spend: [cyan]${current_cost:,.1f}[/cyan]"
        f"{top_3_display}"
    )

    # Savings potential - TODO v1.1.21: Restore with activity-based evidence
    # if monthly_savings > 0:
    #     console.print(f"â€¢ Savings potential: [green]${monthly_savings:,.1f}/month[/green] (${optimization_potential:,.1f} annually)")
    #     # Requires: Activity signals (E1-E7, S1-S7, R1-R7) + Resource IDs + Evidence trail
    # else:
    #     console.print("â€¢ Optimization: [yellow]Run 'finops optimize' for detailed analysis[/yellow]")

    console.print()


def _generate_tree_summary(
    account_id: str,
    current_month_name: str,
    current_cost: float,
    categorized_services: dict,
    monthly_savings: float,
    optimization_potential: float,
    savings_breakdown: list
) -> None:
    """Generate executive summary focused on category tree data."""
    from runbooks.common.rich_utils import console

    # v1.1.20: Condensed to single row for manager preference (Improvement 1)
    # Build top 3 categories inline
    top_3_display = ""
    if categorized_services:
        # Calculate category totals
        category_totals = {}
        for category, services in categorized_services.items():
            category_total = sum(s.get("current_cost", 0) for s in services)
            category_totals[category] = category_total

        # Sort by total and show top 3
        sorted_categories = sorted(category_totals.items(), key=lambda x: x[1], reverse=True)[:3]

        top_categories = []
        for category, cost in sorted_categories:
            pct = (cost / current_cost * 100) if current_cost > 0 else 0
            category_display = category[:30] if len(category) > 30 else category
            top_categories.append(f"{category_display} ${cost:,.1f} ({pct:.1f}%)")
        top_3_display = " | Top 3: " + ", ".join(top_categories)

    console.print(
        f"[bold]Executive Summary:[/bold] Account: [cyan]{account_id}[/cyan] | "
        f"Period: {current_month_name} | MTD Spend: [cyan]${current_cost:,.1f}[/cyan]"
        f"{top_3_display}"
    )

    # Savings potential - TODO v1.1.21: Restore with activity-based evidence
    # if monthly_savings > 0:
    #     console.print(f"â€¢ Savings potential: [green]${monthly_savings:,.1f}/month[/green] (${optimization_potential:,.1f} annually)")
    #     # Requires: Activity signals (E1-E7, S1-S7, R1-R7) + Resource IDs + Evidence trail
    # else:
    #     console.print("â€¢ Optimization: [yellow]Run 'finops optimize' for detailed analysis[/yellow]")

    console.print()


def _generate_comprehensive_summary(
    account_id: str,
    current_month_name: str,
    current_cost: float,
    monthly_savings: float,
    optimization_potential: float,
    savings_breakdown: list
) -> None:
    """Generate comprehensive executive summary (current behavior)."""
    from runbooks.common.rich_utils import console

    console.print("[bold]Executive Summary:[/bold]")
    console.print(f"â€¢ Account: [cyan]{account_id}[/cyan] | Period: {current_month_name} | Month-to-Date Spend: [cyan]${current_cost:,.0f}[/cyan]")

    # Display evidence-based savings potential - TODO v1.1.21: Restore with activity-based evidence
    # if monthly_savings > 0:
    #     console.print(f"â€¢ Savings potential: [green]${monthly_savings:,.0f}/month[/green] (${optimization_potential:,.0f} annually)")
    #     # Requires: Activity signals (E1-E7, S1-S7, R1-R7) + Resource IDs + Evidence trail
    # else:
    #     console.print("â€¢ Optimization: [yellow]Run 'finops optimize' for detailed analysis[/yellow]")

    console.print()


def _create_nested_services_table(
    services_data: dict,
    previous_services_costs: dict,
    total_current: float,
    total_previous: float,
    top_n: int,
    sort_by: str,
    cost_threshold: float,
    current_month_name: str
) -> "Table":
    """
    Create nested services table for Rich Tree visualization.

    Args:
        services_data: Current month costs by service {service_name: cost}
        previous_services_costs: Previous month costs by service
        total_current: Account total current month cost
        total_previous: Account total previous month cost
        top_n: Number of top services to display
        sort_by: Sort order (current|previous|change)
        cost_threshold: Minimum cost filter
        current_month_name: Display name for current month

    Returns:
        Rich Table object with top N services + "Others" row + TOTAL row
    """
    from runbooks.common.rich_utils import create_table

    # Apply cost threshold filter if specified
    if cost_threshold > 0:
        services_data = {k: v for k, v in services_data.items() if v >= cost_threshold}

    # Sort services based on --sort-by parameter
    if sort_by == "current":
        sort_key = lambda x: x[1]  # Current cost
    elif sort_by == "previous":
        sort_key = lambda x: previous_services_costs.get(x[0], 0)  # Previous cost
    else:  # sort_by == "change"
        sort_key = lambda x: abs(x[1] - previous_services_costs.get(x[0], 0)) / previous_services_costs.get(x[0], 1) if previous_services_costs.get(x[0], 0) > 0 else 0  # Change %

    sorted_services = sorted(services_data.items(), key=sort_key, reverse=True)

    # Get top N services
    top_services = sorted_services[:top_n]
    other_services = sorted_services[top_n:]

    # Create table matching standalone services table format
    table = create_table(title=f"Top {top_n} Services (Monthly: {current_month_name})")
    table.add_column("Service", style="cyan", no_wrap=True, width=24)
    table.add_column("Current Month", justify="right", style="bright_green", width=13)
    table.add_column("Previous Month", justify="right", style="white", width=14)
    table.add_column("Change (MTD)", justify="right", width=13)
    table.add_column("% Total", justify="right", style="dim", width=8)
    table.add_column("Trend", style="yellow", width=13)

    # Add top N service rows
    for service, service_current_cost in top_services:
        service_previous_cost = previous_services_costs.get(service, 0)

        # Calculate change metrics
        change_amount = service_current_cost - service_previous_cost
        change_pct = (change_amount / service_previous_cost * 100) if service_previous_cost > 0 else 0
        change_icon = "â†‘" if change_pct > 0 else "â†“" if change_pct < 0 else "â†’"
        change_style = "red" if change_pct > 0 else "green" if change_pct < 0 else "dim"

        # Determine trend indicator
        if abs(change_pct) < 5:
            trend = "â†’ stable"
            trend_style = "dim"
        elif change_pct > 20:
            trend = "â†‘â†‘â†‘ growing"
            trend_style = "red"
        elif change_pct > 10:
            trend = "â†‘ increasing"
            trend_style = "yellow"
        elif change_pct < -20:
            trend = "â†“â†“â†“ declining"
            trend_style = "green"
        elif change_pct < -10:
            trend = "â†“ decreasing"
            trend_style = "bright_green"
        else:
            trend = "â†’ stable"
            trend_style = "dim"

        # Calculate percentage of total
        percentage = (service_current_cost / total_current * 100) if total_current > 0 else 0

        # Truncate long service names
        service_display = service[:23] if len(service) > 23 else service

        table.add_row(
            service_display,
            f"${service_current_cost:,.1f}",  # v1.1.20: Show 1 decimal place for clarity
            f"${service_previous_cost:,.1f}",  # v1.1.20: Show 1 decimal place for clarity
            f"[{change_style}]{change_icon} {abs(change_pct):.1f}%[/{change_style}]",
            f"{percentage:.1f}%",
            f"[{trend_style}]{trend}[/{trend_style}]"
        )

    # Add "Others" row if there are services beyond top N
    if other_services:
        others_current = sum(cost for _, cost in other_services)
        others_previous = sum(previous_services_costs.get(service, 0) for service, _ in other_services)
        others_change = others_current - others_previous
        others_change_pct = (others_change / others_previous * 100) if others_previous > 0 else 0
        others_change_icon = "â†‘" if others_change_pct > 0 else "â†“" if others_change_pct < 0 else "â†’"
        others_change_style = "red" if others_change_pct > 0 else "green" if others_change_pct < 0 else "dim"
        others_percentage = (others_current / total_current * 100) if total_current > 0 else 0

        # Others trend
        if abs(others_change_pct) < 5:
            others_trend = "â†’ stable"
            others_trend_style = "dim"
        elif others_change_pct > 20:
            others_trend = "â†‘â†‘â†‘ growing"
            others_trend_style = "red"
        elif others_change_pct > 10:
            others_trend = "â†‘ increasing"
            others_trend_style = "yellow"
        elif others_change_pct < -20:
            others_trend = "â†“â†“â†“ declining"
            others_trend_style = "green"
        elif others_change_pct < -10:
            others_trend = "â†“ decreasing"
            others_trend_style = "bright_green"
        else:
            others_trend = "â†’ stable"
            others_trend_style = "dim"

        table.add_row(
            f"[dim]Others ({len(other_services)})[/dim]",
            f"${others_current:,.0f}",
            f"${others_previous:,.0f}",
            f"[{others_change_style}]{others_change_icon} {abs(others_change_pct):.1f}%[/{others_change_style}]",
            f"{others_percentage:.1f}%",
            f"[{others_trend_style}]{others_trend}[/{others_trend_style}]"
        )

    # Add TOTAL row (blank row removed per manager's feedback)
    total_change = total_current - total_previous
    total_change_pct = (total_change / total_previous * 100) if total_previous > 0 else 0
    total_change_icon = "â†‘" if total_change_pct > 0 else "â†“" if total_change_pct < 0 else "â†’"
    total_change_style = "red" if total_change_pct > 0 else "green" if total_change_pct < 0 else "dim"

    # Overall trend for TOTAL
    if abs(total_change_pct) < 5:
        total_trend = "â†’ stable"
        total_trend_style = "dim"
    elif total_change_pct > 20:
        total_trend = "â†‘â†‘ growing"
        total_trend_style = "red"
    elif total_change_pct > 10:
        total_trend = "â†‘ growing"
        total_trend_style = "yellow"
    elif total_change_pct < -20:
        total_trend = "â†“â†“ declining"
        total_trend_style = "green"
    elif total_change_pct < -10:
        total_trend = "â†“ declining"
        total_trend_style = "bright_green"
    else:
        total_trend = "â†’ overall"
        total_trend_style = "dim"

    table.add_row(
        "[bold]TOTAL[/bold]",
        f"[bold]${total_current:,.0f}[/bold]",
        f"[bold]${total_previous:,.0f}[/bold]",
        f"[bold][{total_change_style}]{total_change_icon} {abs(total_change_pct):.1f}%[/{total_change_style}][/bold]",
        f"[bold]100.0%[/bold]",
        f"[bold][{total_trend_style}]{total_trend}[/{total_trend_style}][/bold]"
    )

    return table


def create_finops_group():
    """
    Create the finops command group with all subcommands.

    Returns:
        Click Group object with all finops commands

    Performance: Lazy creation only when needed by DRYCommandRegistry
    Context Reduction: ~800 lines extracted from main.py
    """

    # Custom Group class with Rich Tree/Table help formatting
    class RichFinOpsGroup(click.Group):
        """Custom Click Group with Rich Tree/Table help display."""

        def format_help(self, ctx, formatter):
            """Format help text with Rich Tree/Table categorization."""
            import os
            from rich.tree import Tree
            from rich.table import Table as RichTable

            # Check for TEST_MODE environment variable for backward compatibility
            test_mode = os.environ.get('RUNBOOKS_TEST_MODE', '0') == '1'

            if test_mode:
                # Plain text fallback for testing
                click.echo("Usage: runbooks finops [OPTIONS] COMMAND [ARGS]...")
                click.echo("")
                click.echo("  Financial operations and cost optimization for AWS resources.")
                click.echo("")
                click.echo("Commands:")
                click.echo("  dashboard                        Multi-account cost visibility")
                click.echo("  analyze-ec2                      EC2 cost analysis with 4-way enrichment")
                click.echo("  analyze-workspaces               WorkSpaces cost analysis")
                click.echo("  lambda-analysis                  Lambda cost and activity analysis")
                click.echo("  detect-rds-idle                  RDS idle instance detector")
                click.echo("  infrastructure                   Comprehensive infrastructure analysis")
                click.echo("  ec2-snapshots                    EC2 snapshot cost optimization")
                click.echo("  optimize                         General cost optimization recommendations")
                click.echo("  optimize-cloudwatch-costs        CloudWatch log retention optimization")
                click.echo("  detect-orphans                   Unified orphan detection")
                click.echo("  analyze-s3-storage-lens          S3 Storage Lens cost intelligence")
                click.echo("  check-config-compliance          AWS Config compliance-cost correlation")
                click.echo("  ec2-decommission-analysis        EC2 decommission candidate identification")
                click.echo("  workspaces-decommission-analysis WorkSpaces decommission tier analysis")
                click.echo("  enrich-workspaces                WorkSpaces metadata enrichment")
                click.echo("  WorkSpaces                       WorkSpaces discovery and analysis")
                click.echo("  export                           Multi-format export")
                return

            # Categorize commands based on business function
            categories = {
                "ðŸ’° Cost Analysis": [
                    ("dashboard", "Multi-account cost visibility with MCP validation"),
                    ("analyze-ec2", "EC2 cost analysis with 4-way enrichment (Discoveryâ†’Orgsâ†’Costâ†’Activity)"),
                    ("analyze-workspaces", "WorkSpaces cost analysis with decommission tier scoring"),
                    ("lambda-analysis", "Lambda cost and activity analysis"),
                    ("detect-rds-idle", "RDS idle instance detector ($50K annual savings, 5 signals)")
                ],
                "âš™ï¸ Infrastructure Optimization": [
                    ("infrastructure", "Comprehensive infrastructure analysis"),
                    ("ec2-snapshots", "EC2 snapshot cost optimization"),
                    ("optimize", "General cost optimization recommendations"),
                    ("optimize-savings-plans", "Hybrid Savings Plans optimizer (60/30/10 strategy, $500K+ target)"),
                    ("optimize-s3-lifecycle", "S3 Lifecycle automation ($180K target, Epic 3)"),
                    ("optimize-cloudwatch-costs", "CloudWatch log retention optimization ($10K-$50K annual savings)"),
                    ("detect-orphans", "Unified orphan detection (EBS/EIP/NAT/LB, $50K-$200K savings)"),
                    ("analyze-s3-storage-lens", "S3 Storage Lens cost intelligence ($30K-$150K savings)"),
                    ("check-config-compliance", "AWS Config compliance-cost correlation ($20K-$80K savings)")
                ],
                "ðŸ“Š Decommission Analysis": [
                    ("ec2-decommission-analysis", "EC2 decommission candidate identification"),
                    ("workspaces-decommission-analysis", "WorkSpaces decommission tier analysis")
                ],
                "ðŸ”„ Data Operations": [
                    ("enrich-workspaces", "WorkSpaces metadata enrichment"),
                    ("WorkSpaces", "WorkSpaces discovery and analysis")
                ],
                "ðŸ“¤ Export": [
                    ("export", "Multi-format export (CSV, JSON, PDF, Markdown)")
                ]
            }

            # Phase 1: Pre-calculate max column widths across ALL categories (Track 3A pattern)
            max_cmd_len = 0
            for category_commands in categories.values():
                for cmd, desc in category_commands:
                    max_cmd_len = max(max_cmd_len, len(cmd))

            # Set command column width with padding
            cmd_width = max_cmd_len + 2

            # Create Rich Tree
            tree = Tree("[bold cyan]FinOps Commands[/bold cyan] (19 commands)")

            # Add each category with fixed-width tables
            for category_name, commands in categories.items():
                category_branch = tree.add(f"[bold green]{category_name}[/bold green] [dim]({len(commands)} commands)[/dim]")

                # Create table with FIXED command width for vertical alignment, flexible description
                table = RichTable(show_header=True, box=None, padding=(0, 2))
                table.add_column("Command", style="cyan", no_wrap=True, min_width=cmd_width, max_width=cmd_width)
                table.add_column("Description", style="dim", no_wrap=False, overflow="fold")

                # Add rows
                for cmd, desc in commands:
                    table.add_row(cmd, desc)

                category_branch.add(table)

            # Display the tree
            console.print(tree)
            console.print("\n[blue]ðŸ’¡ Usage: runbooks finops [COMMAND] [OPTIONS][/blue]")
            console.print("[blue]ðŸ“– Example: runbooks finops dashboard --profile billing-profile --timeframe monthly[/blue]")

    @click.group(cls=RichFinOpsGroup, invoke_without_command=True)
    @common_filter_options
    @common_multi_account_options
    @common_output_options
    @common_aws_options
    @click.pass_context
    def finops(ctx, profile, region, dry_run, format, output_dir, all_outputs, export_csv, export_json, export_markdown, export,
               all_profiles, profiles, regions, all_regions, tags, accounts):
        """
        Financial operations and cost optimization for AWS resources.

        Comprehensive cost analysis, budget management, and financial reporting
        with enterprise-grade accuracy and multi-format export capabilities.

        Features:
        â€¢ Real-time cost analysis with MCP validation (â‰¥99.5% accuracy)
        â€¢ Multi-format exports: CSV, JSON, PDF, Markdown
        â€¢ Quarterly intelligence with strategic financial reporting
        â€¢ Enterprise AWS profile support with multi-account capabilities

        Examples:
            runbooks finops dashboard --profile billing-profile
            runbooks finops dashboard --all-profiles --timeframe monthly
            runbooks finops dashboard --regions ap-southeast-2 ap-southeast-6
            runbooks finops export --format pdf --output-dir ./reports
        """
        # Ensure context object exists
        if ctx.obj is None:
            ctx.obj = {}
        ctx.obj.update({
            "profile": profile,
            "region": region,
            "dry_run": dry_run,
            "verbose": False,  # Default verbose setting (updated by dashboard command)
            "output_format": format,
            "output_dir": output_dir,
            "all_outputs": all_outputs,
            "export_csv": export_csv,
            "export_json": export_json,
            "export_markdown": export_markdown,
            "export": export,
            "all_profiles": all_profiles,
            "profiles": profiles,
            "regions": regions,
            "all_regions": all_regions,
            "tags": tags,
            "accounts": accounts
        })

        # Auto-execute dashboard when no subcommand provided (eliminates "only logs" pattern)
        if ctx.invoked_subcommand is None:
            # Invoke dashboard with default parameters
            ctx.invoke(dashboard,
                      profile=profile,
                      all_profile=all_profiles,  # Note: dashboard parameter is all_profile (singular)
                      timeframe='monthly',
                      services=None,
                      accounts=None,
                      validate=False,
                      validate_mcp=False,
                      mcp_validate=False,
                      csv=False,
                      markdown=False,
                      pdf=False,
                      json=False,
                      export_format=None,
                      unblended=False,
                      amortized=False,
                      dual_metrics=False,
                      dry_run=dry_run,
                      executive=False,
                      verbose=False,
                      top_n=10,
                      sort_by='current',
                      cost_threshold=0.0,
                      summary_mode='both',
                      output_format='both',
                      activity_analysis=False)

    @finops.command()
    @click.option("--profile", help="AWS profile to use for authentication")
    @click.option("--all-profile", type=str, default=None, help="Management profile for Organizations API multi-account discovery")
    @click.option(
        "--timeframe",
        type=click.Choice(["daily", "weekly", "monthly", "quarterly"]),
        default="monthly",
        help="Analysis timeframe",
    )
    @click.option("--services", multiple=True, help="Specific AWS services to analyze")
    @click.option("--accounts", multiple=True, help="Specific AWS accounts to analyze")
    @click.option("--validate", is_flag=True, hidden=True, help="DEPRECATED: Use --validation-level basic")
    @click.option("--validate-mcp", is_flag=True, hidden=True, help="DEPRECATED: Use --validation-level mcp")
    @click.option("--mcp-validate", is_flag=True, hidden=True, help="DEPRECATED: Use --validation-level mcp")
    @click.option("--csv", is_flag=True, hidden=True, help="DEPRECATED: Use --export csv")
    @click.option("--markdown", is_flag=True, hidden=True, help="DEPRECATED: Use --export markdown")
    @click.option("--pdf", is_flag=True, hidden=True, help="DEPRECATED: Use --export pdf")
    @click.option("--json", is_flag=True, hidden=True, help="DEPRECATED: Use --export json")
    @click.option(
        "--export-format",
        type=click.Choice(["json", "csv", "pdf", "markdown"]),
        hidden=True,
        help="DEPRECATED: Use --export",
    )
    @click.option("--executive", is_flag=True, help="Executive summary format (business narrative, <5 min review)")
    @click.option("--unblended", is_flag=True, hidden=True, help="DEPRECATED: Use --cost-metric unblended")
    @click.option("--amortized", is_flag=True, hidden=True, help="DEPRECATED: Use --cost-metric amortized")
    @click.option("--dual-metrics", is_flag=True, hidden=True, help="DEPRECATED: Use --cost-metric dual")
    @click.option("--dry-run", is_flag=True, default=True, help="Execute in dry-run mode")
    @click.option("--verbose", "-v", is_flag=True, default=False, help="Enable verbose diagnostic logging")
    @click.option("--top-n", type=int, default=10, help="Number of top services to display in table (1-50, default: 10)")
    @click.option("--sort-by", type=click.Choice(["current", "previous", "change"]), default="current", help="Sort services by: current cost, previous cost, or change %")
    @click.option("--cost-threshold", type=float, default=0.0, help="Minimum cost threshold for service display (e.g., 100.0 for $100+)")
    @click.option("--summary-mode", type=click.Choice(["table", "tree", "both", "none"]), default="both", help="Executive summary source: table (services), tree (categories), both, or none")
    @click.option("--output-format", type=click.Choice(["both", "table", "tree"]), default="both", help="Visualization choice: both (table + tree with nested table), table only, or tree only (with nested table)")
    @click.option("--activity-analysis", is_flag=True, help="Enable resource activity analysis with decommission recommendations (E1-E7, R1-R7, S1-S7 signals)")
    @click.option("--show-zero-cost", is_flag=True, default=False, help="Show services with zero cost in both current and previous months (default: hide)")
    @click.option("--show-empty", is_flag=True, default=False, help="Show all resource types including those with 0 resources (for troubleshooting)")
    @click.option("--persona", type=click.Choice(["CFO", "CTO", "CEO", "ALL"]), help="Executive persona for tailored analysis. CFO: cost optimization & ROI. CTO: technical debt & architecture. CEO: strategic KPIs & business impact.")
    @click.option("--screenshot", is_flag=True, help="Capture Playwright screenshot of dashboard HTML export (requires console recording)")
    @click.option(
        "--validation-level",
        type=click.Choice(["basic", "mcp", "strict"], case_sensitive=False),
        help="Validation level: basic (standard), mcp (â‰¥99.5% MCP accuracy), strict (100% validation)"
    )
    @click.option(
        "--export",
        multiple=True,
        type=click.Choice(["csv", "markdown", "pdf", "json"], case_sensitive=False),
        help="Export format(s). Specify multiple times: --export csv --export pdf"
    )
    @click.option(
        "--cost-metric",
        type=click.Choice(["blended", "unblended", "amortized", "dual"], case_sensitive=False),
        default="blended",
        help="Cost metric: blended (default), unblended, amortized (RIs), dual (show both)"
    )
    @click.pass_context
    def dashboard(
        ctx,
        profile,
        all_profile,
        timeframe,
        services,
        accounts,
        validate,
        validate_mcp,
        mcp_validate,
        csv,
        markdown,
        pdf,
        json,
        export_format,
        executive,
        unblended,
        amortized,
        dual_metrics,
        dry_run,
        verbose,
        top_n,
        sort_by,
        cost_threshold,
        summary_mode,
        output_format,
        activity_analysis,
        show_zero_cost,
        show_empty,
        persona,
        screenshot,
        validation_level,
        export,
        cost_metric,
    ):
        """
        Multi-account cost visibility with MCP validation.

        \b
        ðŸ“Š DASHBOARD FEATURES
        â”œâ”€â”€ ðŸ—ï¸ Enhanced Services Table (6 columns + TOTAL row)
        â”‚   â””â”€â”€ Service, Current, Last Mo, Change, % Tot, Trend (MoM)
        â”‚
        â”œâ”€â”€ ðŸŒ² Rich Tree Cost Breakdown (hierarchical visualization)
        â”‚   â””â”€â”€ Compute, Network, Storage, Database, Other categories
        â”‚
        â”œâ”€â”€ ðŸ’¡ Optimization Opportunities (Priority-based actionable items)
        â”‚   â””â”€â”€ Priority, Action, Savings, Effort, Risk assessment
        â”‚
        â””â”€â”€ ðŸ’° Executive Summary (--executive flag)
            â””â”€â”€ Business narrative with Key Findings

        \b
        ðŸ“Š DASHBOARD OPTIONS (Simplified in v1.1.20)
        â”œâ”€â”€ ðŸ” Authentication
        â”‚   â””â”€â”€ --profile TEXT                    AWS profile for authentication
        â”‚
        â”œâ”€â”€ â° Time Range
        â”‚   â””â”€â”€ --timeframe CHOICE                [daily|weekly|monthly|quarterly]
        â”‚
        â”œâ”€â”€ ðŸŽ¯ Filtering
        â”‚   â””â”€â”€ --services TEXT                   Specific AWS services (multiple)
        â”‚   â””â”€â”€ --accounts TEXT                   Specific AWS accounts (multiple)
        â”‚
        â”œâ”€â”€ âœ… Validation (Unified)
        â”‚   â””â”€â”€ --validation-level CHOICE         [basic|mcp|strict]
        â”‚
        â”œâ”€â”€ ðŸ“¤ Export (Unified Multi-Value)
        â”‚   â””â”€â”€ --export CHOICE                   [csv|markdown|pdf|json] (multiple)
        â”‚
        â”œâ”€â”€ ðŸ’° Cost Metrics (Unified)
        â”‚   â””â”€â”€ --cost-metric CHOICE              [blended|unblended|amortized|dual]
        â”‚
        â”œâ”€â”€ ðŸŽ­ Display & Personas
        â”‚   â””â”€â”€ --executive                       Executive summary format
        â”‚   â””â”€â”€ --persona CHOICE                  [CFO|CTO|CEO|ALL]
        â”‚
        â””â”€â”€ ðŸ”§ Advanced Options
            â””â”€â”€ --verbose, -v                     Diagnostic logging
            â””â”€â”€ --top-n, --sort-by, --cost-threshold, etc. (See --help)

        \b
        ðŸ’¡ Enhanced UX (v1.1.19):
        â€¢ 6-column services table with Last Mo, Change %, Trend (MoM)
        â€¢ Hierarchical Rich Tree cost breakdown by category
        â€¢ Priority-based optimization opportunities (ðŸ”´ IMMEDIATE, ðŸŸ¡ 30-DAY, ðŸŸ¡ 60-DAY, ðŸŸ¢ 90-DAY)
        â€¢ Verbose logging with --verbose flag for diagnostics

        \b
        ðŸ’¡ Sprint 1 Targets: 25-50% cost savings via decommission analysis
        ðŸ“– Example: runbooks finops dashboard --profile billing --timeframe monthly --validation-level mcp --export csv --verbose
        """
        # Configure logging for diagnostic output
        import logging
        from runbooks.common.logging_config import configure_logging

        # Update context with verbose flag
        if ctx.obj:
            ctx.obj["verbose"] = verbose

        # Initialize logging based on verbose flag
        configure_logging(verbose=verbose)
        logger = logging.getLogger(__name__)

        # Parameter validation
        if not 1 <= top_n <= 50:
            raise click.BadParameter("--top-n must be between 1 and 50")
        if cost_threshold < 0:
            raise click.BadParameter("--cost-threshold must be non-negative")

        # Q3 Backward Compatibility: Map old fragmented flags to new unified flags
        from rich.console import Console
        console = Console()

        # Q3.1: Validation flags consolidation
        if validation_level:
            # Use explicit validation-level parameter (new syntax)
            pass
        elif mcp_validate or validate_mcp:
            validation_level = "mcp"
            if verbose:
                console.print("[dim]ðŸ’¡ Mapped legacy --mcp-validate to --validation-level mcp[/]")
        elif validate:
            validation_level = "basic"
            if verbose:
                console.print("[dim]ðŸ’¡ Mapped legacy --validate to --validation-level basic[/]")
        else:
            validation_level = None  # No validation requested

        # Q3.2: Export flags consolidation
        export_formats = set()
        if export:
            # New unified --export flag (highest priority)
            export_formats.update(export)
            if verbose:
                console.print(f"[dim]ðŸ’¡ Using --export: {', '.join(export)}[/]")
        else:
            # Legacy individual flags (backward compatibility)
            if csv:
                export_formats.add("csv")
            if markdown:
                export_formats.add("markdown")
            if pdf:
                export_formats.add("pdf")
            if json:
                export_formats.add("json")
            if export_format:
                export_formats.add(export_format)

            if export_formats and verbose:
                console.print(f"[dim]ðŸ’¡ Mapped legacy export flags to: {', '.join(export_formats)}[/]")

        # Convert to tuple for downstream processing
        export_formats = tuple(export_formats) if export_formats else ()

        # Q3.3: Cost metric flags consolidation
        if cost_metric and cost_metric != "blended":
            # Use explicit cost-metric parameter (new syntax)
            cost_metrics = [cost_metric]
        elif unblended:
            cost_metric = "unblended"
            cost_metrics = ["UnblendedCost"]
            if verbose:
                console.print("[dim]ðŸ’¡ Mapped legacy --unblended to --cost-metric unblended[/]")
        elif amortized:
            cost_metric = "amortized"
            cost_metrics = ["AmortizedCost"]
            if verbose:
                console.print("[dim]ðŸ’¡ Mapped legacy --amortized to --cost-metric amortized[/]")
        elif dual_metrics:
            cost_metric = "dual"
            cost_metrics = ["dual"]
            if verbose:
                console.print("[dim]ðŸ’¡ Mapped legacy --dual-metrics to --cost-metric dual[/]")
        else:
            # Default to blended if no metric specified
            cost_metrics = ["BlendedCost"]

        # Diagnostic log (only shows if --verbose flag used)
        logger.debug(
            f"Dashboard command started: profile={profile}, all_profile={all_profile}, "
            f"timeframe={timeframe}, verbose={verbose}, top_n={top_n}, sort_by={sort_by}, "
            f"cost_threshold={cost_threshold}, summary_mode={summary_mode}, output_format={output_format}"
        )

        # Handle multi-account Landing Zone analysis with Organizations API discovery
        if all_profile:
            try:
                from runbooks.finops.dashboard_runner import MultiAccountDashboard, DashboardRouter
                from runbooks.common.rich_utils import print_header, print_success, print_error, print_info
                from runbooks.inventory.inventory_modules import get_org_accounts_from_profiles, get_profiles
                import argparse

                print_header("Multi-Account Landing Zone Dashboard", all_profile)
                console.print("[cyan]ðŸ¢ Discovering AWS Organization accounts via Organizations API...[/cyan]")

                # ADD diagnostic logging
                logger.info(
                    "Multi-account discovery initiated",
                    extra={"management_profile": all_profile, "discovery_mode": "Organizations API"}
                )

                # CORRECTED: Use management profile TEXT parameter for Organizations API access
                try:
                    # Use management profile specified by user (MANAGEMENT_PROFILE, BILLING_PROFILE, or CENTRALISED_OPS_PROFILE)
                    mgmt_profile_list = get_profiles(fprofiles=[all_profile])
                    console.print(f"[dim]Querying Organizations API with profile: {all_profile}[/dim]")

                    # ADD diagnostic logging
                    logger.debug(f"Resolved management profile list: {mgmt_profile_list}")

                    org_accounts = get_org_accounts_from_profiles(mgmt_profile_list)

                    # Extract account IDs from discovered organization accounts
                    discovered_account_ids = []
                    for acct in org_accounts:
                        if acct.get("Success") and acct.get("RootAcct") and acct.get("aws_acct"):
                            # Root account found - extract all child accounts
                            for child in acct["aws_acct"].ChildAccounts:
                                discovered_account_ids.append(child["AccountId"])

                    if discovered_account_ids:
                        console.print(f"[green]âœ… Discovered {len(discovered_account_ids)} organization accounts[/green]")
                        console.print(f"[dim]Analysis Scope: Organization-wide with Landing Zone support[/dim]\n")

                        # ADD diagnostic logging
                        logger.info(
                            "Organizations discovery successful",
                            extra={
                                "account_count": len(discovered_account_ids),
                                "account_ids": discovered_account_ids[:5]  # First 5 for brevity
                            }
                        )
                    else:
                        console.print(f"[yellow]âš ï¸  No organization accounts discovered - using single account mode[/yellow]")
                        console.print(f"[dim]Tip: Ensure {profile} has AWS Organizations permissions[/dim]\n")

                except Exception as org_error:
                    console.print(f"[yellow]âš ï¸  Organizations discovery failed: {str(org_error)}[/yellow]")
                    console.print(f"[dim]Falling back to single account mode[/dim]\n")
                    discovered_account_ids = []  # Empty list for fallback

                # Create mock args object for multi-dashboard compatibility
                args = argparse.Namespace()
                args.profile = all_profile  # Use management profile for AWS Organizations access
                args.timeframe = timeframe
                args.services = services
                # PHASE 2 ENHANCEMENT: Use Organizations-discovered accounts if available
                args.accounts = tuple(discovered_account_ids) if discovered_account_ids else accounts
                args.validate = validate or mcp_validate
                # CRITICAL FIX: Handle multiple export format flags
                export_formats = []
                if csv:
                    export_formats.append("csv")
                if markdown:
                    export_formats.append("markdown")
                if pdf:
                    export_formats.append("pdf")
                if json:
                    export_formats.append("json")
                if export_format and export_format not in export_formats:
                    export_formats.append(export_format)

                args.export_format = export_formats[0] if export_formats else None
                args.export_formats = export_formats  # Store all requested formats

                # CRITICAL FIX: Handle cost metric options
                cost_metrics = ["BlendedCost"]  # Default metric
                if unblended:
                    cost_metrics = ["UnblendedCost"]
                elif amortized:
                    cost_metrics = ["AmortizedCost"]
                elif dual_metrics:
                    cost_metrics = ["BlendedCost", "AmortizedCost"]

                args.cost_metrics = cost_metrics
                args.cost_metric_display = _get_cost_metric_display(cost_metrics)
                args.dry_run = dry_run
                args.all = True  # Enable all accounts mode
                args.all_accounts = True  # Enable all accounts mode (alternate flag name)
                args.top_accounts = 50  # Show many accounts for enterprise view
                args.services_per_account = 3
                args.time_range = None
                args.audit = False  # Not audit mode
                args.trend = False  # Not trend analysis mode
                args.combine = False  # Not combined multi-account view
                args.tag = None
                args.region = None  # No specific region filter
                args.regions = None
                args.report_name = None  # No report export by default
                args.report_type = []  # No report types specified
                args.dir = None  # No output directory specified
                args.profiles = []  # No additional profiles beyond main profile
                args.business_analysis = False  # Not business analysis mode
                args.enhanced_export = False  # Not enhanced export mode
                args.live_mode = False  # Not live mode
                args.activity_analysis = activity_analysis  # v1.1.20 Activity Health Analysis
                args.persona = persona  # v1.1.20 CxO Persona Analysis
                args.screenshot = screenshot  # v1.1.20 Playwright Screenshot

                # Initialize router and dashboard
                router = DashboardRouter(console=console)
                routing_config = router.route_dashboard_request(args)

                # Create multi-account dashboard
                multi_dashboard = MultiAccountDashboard(console=console)

                # Execute multi-account analysis
                result = multi_dashboard.run_dashboard(args, routing_config)

                if result == 0:
                    print_success("Multi-account Landing Zone analysis completed successfully")
                else:
                    print_error("Multi-account analysis encountered issues")

                return result

            except ImportError as e:
                console.print(f"[red]âŒ Multi-account dashboard not available: {e}[/red]")
                console.print("[yellow]ðŸ’¡ Falling back to single-account mode with specified profile[/yellow]")
                # Fallback to single account with the specified profile
                resolved_profile = all_profile
            except Exception as e:
                console.print(f"[red]âŒ Multi-account analysis failed: {e}[/red]")
                console.print("[yellow]ðŸ’¡ Fallingback to single-account mode[/yellow]")
                resolved_profile = all_profile
        else:
            resolved_profile = profile or ctx.obj.get("profile", "default")

        # Handle standalone MCP validation (AWS-2 implementation)
        if validate_mcp:
            try:
                from runbooks.common.rich_utils import print_header, print_success, print_error, print_info
                import asyncio

                print_header("MCP Validation Framework", "AWS-2 Implementation")
                console.print("[cyan]ðŸ” Running comprehensive MCP validation for â‰¥99.5% accuracy[/cyan]")

                # Import and initialize MCP validator
                from runbooks.validation.mcp_validator import MCPValidator

                # Enterprise profiles configuration
                validation_profiles = {
                    "billing": "${BILLING_PROFILE}",
                    "management": "${MANAGEMENT_PROFILE}",
                    "centralised_ops": "${CENTRALISED_OPS_PROFILE}",
                    "single_aws": "${SINGLE_AWS_PROFILE}",
                }

                # Initialize validator with configured profiles
                validator = MCPValidator(
                    profiles=validation_profiles, tolerance_percentage=5.0, performance_target_seconds=30.0
                )

                # Run comprehensive validation
                validation_report = asyncio.run(validator.validate_all_operations())

                # Success criteria for AWS-2
                if validation_report.overall_accuracy >= 99.5:
                    print_success(
                        f"âœ… AWS-2 SUCCESS: {validation_report.overall_accuracy:.1f}% â‰¥ 99.5% target achieved"
                    )
                    return 0
                else:
                    print_error(f"âŒ AWS-2 FAILED: {validation_report.overall_accuracy:.1f}% < 99.5% target")
                    return 1

            except Exception as e:
                print_error(f"âŒ AWS-2 MCP validation failed: {e}")
                return 1

        try:
            from runbooks.common.rich_utils import print_header, print_success, print_error, create_table, format_cost
            from runbooks.common.profile_utils import create_cost_session
            from runbooks.finops.cost_processor import get_cost_data
            from runbooks.finops.aws_client import get_account_id, ec2_summary, get_accessible_regions
            import boto3
            from datetime import datetime, timedelta
            from rich.table import Table
            from rich.panel import Panel

            # Resolve profile with priority: command --profile > parent context > default
            # Note: resolved_profile already set above for multi-account vs single-account mode
            if "resolved_profile" not in locals():
                resolved_profile = profile or ctx.obj.get("profile", "default")
            resolved_dry_run = dry_run if dry_run is not None else ctx.obj.get("dry_run", True)

            # MCP validation integration
            mcp_results = None
            if mcp_validate or validate:
                try:
                    from runbooks.validation.mcp_validator import MCPValidator
                    import asyncio

                    console.print("[cyan]ðŸ” Running MCP validation for dashboard data accuracy[/cyan]")

                    # Configure validation profiles using resolved profile
                    validation_profiles = {
                        "billing": resolved_profile,
                        "management": resolved_profile,
                        "centralised_ops": resolved_profile,
                        "single_aws": resolved_profile,
                    }

                    # Initialize validator
                    validator = MCPValidator(
                        profiles=validation_profiles, tolerance_percentage=5.0, performance_target_seconds=30.0
                    )

                    # Run validation focused on cost explorer operations (primary finops validation)
                    mcp_results = asyncio.run(validator.validate_cost_explorer())

                    # Display validation results
                    if mcp_results.accuracy_percentage >= 99.5:
                        console.print(
                            f"[green]âœ… MCP Validation PASSED: {mcp_results.accuracy_percentage:.1f}% accuracy[/green]"
                        )

                        # ADD diagnostic logging
                        logger.info(
                            "MCP validation completed",
                            extra={
                                "accuracy": mcp_results.accuracy_percentage,
                                "operation": mcp_results.operation_name,
                                "threshold": 99.5
                            }
                        )
                    elif mcp_results.accuracy_percentage >= 95.0:
                        console.print(
                            f"[yellow]âš ï¸ MCP Validation WARNING: {mcp_results.accuracy_percentage:.1f}% accuracy (target: â‰¥99.5%)[/yellow]"
                        )
                    else:
                        console.print(
                            f"[red]âŒ MCP Validation FAILED: {mcp_results.accuracy_percentage:.1f}% accuracy[/red]"
                        )

                except Exception as e:
                    console.print(f"[yellow]âš ï¸ MCP validation failed: {e}[/yellow]")
                    console.print("[dim]Continuing with dashboard generation...[/dim]")

            # Create AWS session and get account info
            session = create_cost_session(profile_name=resolved_profile)
            account_id = get_account_id(session)

            # Consolidated header with account, profile, and period information (LEAN: single box)
            from runbooks.common.rich_utils import create_display_profile_name
            from datetime import date
            from rich.panel import Panel
            from rich import box
            from rich.text import Text

            truncated_profile = create_display_profile_name(resolved_profile, max_length=30)
            current_month_name = date.today().strftime("%B %Y")

            # Build consolidated header text
            header_text = Text()
            header_text.append("Runbooks FinOps Dashboard", style="bold cyan")
            header_text.append(" | ", style="dim")
            header_text.append(f"Account: {account_id}", style="cyan")
            header_text.append(" | ", style="dim")
            header_text.append(f"Period: {current_month_name}", style="white")

            console.print()
            console.print(Panel(header_text, box=box.DOUBLE, style="cyan"))

            # Get cost data for the specified timeframe
            try:
                # Calculate time range based on timeframe
                # CRITICAL FIX: monthly=None triggers current month logic (not 30-day rolling window)
                # This ensures both services_data and current_cost use same period (Nov 1 - Nov 10)
                # quarterly=90 for last 3 months comparison
                time_range_days = {"daily": 7, "weekly": 30, "monthly": None, "quarterly": 90}.get(timeframe, None)

                # Get comprehensive cost data
                cost_data = get_cost_data(
                    session,
                    time_range=time_range_days,
                    get_trend=True,
                    profile_name=resolved_profile,
                    account_id=account_id,
                )

                # ADD diagnostic logging
                logger.debug(
                    "Cost data retrieved",
                    extra={
                        "time_range_days": time_range_days,
                        "timeframe": timeframe,
                        "services_count": len(cost_data.get("costs_by_service", {}))
                    }
                )

                # Access cost data using correct field names from CostData TypedDict
                current_cost = cost_data.get("current_month", 0)
                previous_cost = cost_data.get("last_month", 0)

                # Display Enhanced Top Services Table with Trends (DEFAULT)
                services_data = cost_data.get("costs_by_service", {})
                if services_data:
                    # Import previous month cost helper
                    from runbooks.finops.cost_processor import get_previous_month_costs

                    # Get previous month costs for comparison
                    previous_services_costs = get_previous_month_costs(session, profile_name=resolved_profile)

                    # Create enhanced 6-column table with time context for clarity
                    from datetime import date
                    current_month_name = date.today().strftime("%B %Y")  # e.g., "November 2025"
                    services_table = create_table(title=f"ðŸ—ï¸ Top {top_n} AWS Services by Cost (Monthly: {current_month_name})")
                    services_table.add_column("Service", style="cyan", no_wrap=True, width=24)
                    services_table.add_column("Current Month", justify="right", style="bright_green", width=13)
                    services_table.add_column("Previous Month", justify="right", style="white", width=14)
                    services_table.add_column("Change (MTD)", justify="right", width=13)
                    services_table.add_column("% Total", justify="right", style="dim", width=8)
                    services_table.add_column("Trend", style="yellow", width=13)

                    # Apply cost threshold filter if specified
                    if cost_threshold > 0:
                        services_data = {k: v for k, v in services_data.items() if v >= cost_threshold}

                    # TRACK 1: Filter out Tax and other non-analytical services
                    from runbooks.finops.cost_processor import filter_analytical_services
                    services_data = filter_analytical_services(services_data)

                    # Sort services based on --sort-by parameter
                    if sort_by == "current":
                        sort_key = lambda x: x[1]  # Current cost
                    elif sort_by == "previous":
                        sort_key = lambda x: previous_services_costs.get(x[0], 0)  # Previous cost
                    else:  # sort_by == "change"
                        sort_key = lambda x: abs(x[1] - previous_services_costs.get(x[0], 0)) / previous_services_costs.get(x[0], 1) if previous_services_costs.get(x[0], 0) > 0 else 0  # Change %

                    sorted_services = sorted(services_data.items(), key=sort_key, reverse=True)

                    # Get top N and others
                    top_services = sorted_services[:top_n]
                    other_services = sorted_services[top_n:]

                    # Track totals for TOTAL row
                    # Use account-level totals (not just sum of top 10) for mathematical consistency
                    total_current = current_cost  # Account total from API (all services)
                    total_previous = previous_cost  # Previous month total from API

                    # Add service rows (top N only)
                    # Track last displayed row for separator logic (Comment #1)
                    displayed_rows = []
                    for service, service_current_cost in top_services:
                        # Get previous month cost for this service
                        service_previous_cost = previous_services_costs.get(service, 0)

                        # TRACK 2: Zero-cost filtering (skip rows with <$1 both months - rounds to $0 in display)
                        if not show_zero_cost:
                            if service_current_cost < 1 and service_previous_cost < 1:
                                continue

                        # Calculate change metrics
                        change_amount = service_current_cost - service_previous_cost
                        change_pct = (change_amount / service_previous_cost * 100) if service_previous_cost > 0 else 0
                        change_icon = "â†‘" if change_pct > 0 else "â†“" if change_pct < 0 else "â†’"
                        change_style = "red" if change_pct > 0 else "green" if change_pct < 0 else "dim"

                        # Determine trend indicator (MoM)
                        if abs(change_pct) < 5:
                            trend = "â†’ stable"
                            trend_style = "dim"
                        elif change_pct > 20:
                            trend = "â†‘â†‘â†‘ growing"
                            trend_style = "red"
                        elif change_pct > 10:
                            trend = "â†‘ increasing"
                            trend_style = "yellow"
                        elif change_pct < -20:
                            trend = "â†“â†“â†“ declining"
                            trend_style = "green"
                        elif change_pct < -10:
                            trend = "â†“ decreasing"
                            trend_style = "bright_green"
                        else:
                            trend = "â†’ stable"
                            trend_style = "dim"

                        # Calculate percentage of total
                        percentage = (service_current_cost / total_current * 100) if total_current > 0 else 0

                        # Truncate long service names
                        service_display = service[:23] if len(service) > 23 else service

                        # Store row data for later insertion with separator logic
                        displayed_rows.append({
                            'service': service_display,
                            'current': service_current_cost,
                            'previous': service_previous_cost,
                            'change_style': change_style,
                            'change_icon': change_icon,
                            'change_pct': change_pct,
                            'percentage': percentage,
                            'trend': trend,
                            'trend_style': trend_style
                        })

                    # Check if "Others" row will be displayed (for separator logic)
                    will_display_others = False
                    if other_services:
                        others_current = sum(cost for _, cost in other_services)
                        others_previous = sum(previous_services_costs.get(service, 0) for service, _ in other_services)
                        # Check zero-cost filtering logic (same as below)
                        will_display_others = not (not show_zero_cost and others_current < 1 and others_previous < 1)

                    # Insert rows with separator on last row if no "Others" row follows
                    for idx, row_data in enumerate(displayed_rows):
                        is_last_row = (idx == len(displayed_rows) - 1)
                        services_table.add_row(
                            row_data['service'],
                            f"${row_data['current']:,.1f}",  # v1.1.20: Show 1 decimal place for clarity
                            f"${row_data['previous']:,.1f}",  # v1.1.20: Show 1 decimal place for clarity
                            f"[{row_data['change_style']}]{row_data['change_icon']} {abs(row_data['change_pct']):.1f}%[/{row_data['change_style']}]",
                            f"{row_data['percentage']:.1f}%",
                            f"[{row_data['trend_style']}]{row_data['trend']}[/{row_data['trend_style']}]",
                            end_section=(is_last_row and not will_display_others)  # Separator only if last AND no "Others" follows
                        )

                    # TRACK 1: Add "Others" row if there are services beyond top N
                    if other_services:
                        others_current = sum(cost for _, cost in other_services)
                        others_previous = sum(previous_services_costs.get(service, 0) for service, _ in other_services)

                        # TRACK 2: Zero-cost filtering for Others row (skip if both <$1 and flag disabled)
                        if not (not show_zero_cost and others_current < 1 and others_previous < 1):
                            # Calculate others metrics
                            others_change = others_current - others_previous
                            others_change_pct = (others_change / others_previous * 100) if others_previous > 0 else 0
                            others_change_icon = "â†‘" if others_change_pct > 0 else "â†“" if others_change_pct < 0 else "â†’"
                            others_change_style = "red" if others_change_pct > 0 else "green" if others_change_pct < 0 else "dim"
                            others_percentage = (others_current / total_current * 100) if total_current > 0 else 0

                            # Others trend
                            if abs(others_change_pct) < 5:
                                others_trend = "â†’ stable"
                                others_trend_style = "dim"
                            elif others_change_pct > 20:
                                others_trend = "â†‘â†‘â†‘ growing"
                                others_trend_style = "red"
                            elif others_change_pct > 10:
                                others_trend = "â†‘ increasing"
                                others_trend_style = "yellow"
                            elif others_change_pct < -20:
                                others_trend = "â†“â†“â†“ declining"
                                others_trend_style = "green"
                            elif others_change_pct < -10:
                                others_trend = "â†“ decreasing"
                                others_trend_style = "bright_green"
                            else:
                                others_trend = "â†’ stable"
                                others_trend_style = "dim"

                            services_table.add_row(
                                f"[dim]Other Services ({len(other_services)} total)[/dim]",
                                f"${others_current:,.0f}",
                                f"${others_previous:,.0f}",
                                f"[{others_change_style}]{others_change_icon} {abs(others_change_pct):.1f}%[/{others_change_style}]",
                                f"{others_percentage:.1f}%",
                                f"[{others_trend_style}]{others_trend}[/{others_trend_style}]",
                                end_section=True  # Comment #1 Fix: Add separator before TOTAL row
                            )

                    # Add TOTAL row
                    total_change = total_current - total_previous
                    total_change_pct = (total_change / total_previous * 100) if total_previous > 0 else 0
                    total_change_icon = "â†‘" if total_change_pct > 0 else "â†“" if total_change_pct < 0 else "â†’"
                    total_change_style = "red" if total_change_pct > 0 else "green" if total_change_pct < 0 else "dim"

                    # Overall trend for TOTAL
                    if abs(total_change_pct) < 5:
                        total_trend = "â†’ overall"
                    elif total_change_pct > 10:
                        total_trend = "â†‘ overall"
                    else:
                        total_trend = "â†“ overall"

                    services_table.add_row(
                        "[bold]TOTAL[/bold]",
                        f"[bold]${total_current:,.0f}[/bold]",
                        f"[bold]${total_previous:,.0f}[/bold]",
                        f"[bold {total_change_style}]{total_change_icon} {abs(total_change_pct):.1f}%[/bold {total_change_style}]",
                        "[bold]100.0%[/bold]",
                        f"[bold]{total_trend}[/bold]"
                    )

                    # Conditional rendering based on --output-format parameter
                    if output_format in ["both", "table"]:
                        console.print(services_table)
                        console.print()

                        # ADD diagnostic logging
                        logger.debug(
                            "Enhanced services table rendered",
                            extra={
                                "services_count": len(sorted_services),
                                "total_current": total_current,
                                "total_previous": total_previous,
                                "output_format": output_format
                            }
                        )

                        # Mathematical consistency check: Services total should â‰¤ Account total
                        # Allow 1% tolerance for rounding differences
                        if total_current > current_cost * 1.01:
                            console.print(
                                f"[red]âš ï¸ WARNING: Services total (${total_current:,.0f}) exceeds account total (${current_cost:,.0f})[/red]"
                            )
                            console.print("[dim]This indicates a time period mismatch. Please report this issue.[/dim]")
                            logger.warning(
                                "Mathematical inconsistency detected",
                                extra={
                                    "services_total": total_current,
                                    "account_total": current_cost,
                                    "ratio": total_current / current_cost if current_cost > 0 else 0
                                }
                            )
                            console.print()

                # Rich Tree Cost Breakdown (conditional based on --output-format)
                try:
                    from runbooks.finops.cost_processor import enrich_service_costs_with_trends, categorize_aws_services
                    from runbooks.common.rich_utils import create_cost_breakdown_tree

                    # Enrich services with trend data
                    enriched_services = enrich_service_costs_with_trends(
                        current_costs=services_data,
                        previous_costs=previous_services_costs
                    )

                    # Categorize services for hierarchical visualization
                    categorized_services = categorize_aws_services(enriched_services)

                    # Get EC2 resource summary BEFORE calculating optimization opportunities
                    try:
                        ec2_data = ec2_summary(session, profile_name=resolved_profile)
                        # ec2_summary returns keys: "running", "stopped", etc.
                        running_instances = ec2_data.get("running", 0)
                        stopped_instances = ec2_data.get("stopped", 0)
                        total_instances = sum(ec2_data.values())
                    except Exception as e:
                        console.print(f"[yellow]âš ï¸ Could not fetch EC2 optimization data: {e}[/yellow]\n")
                        total_instances = 0
                        running_instances = 0
                        stopped_instances = 0

                    # Calculate optimization opportunities from EC2 data
                    optimization_opportunities = {}

                    # Compute: EC2 rightsizing + stopped instance cleanup
                    # Try multiple possible service names for EC2
                    ec2_cost_estimate = (
                        services_data.get("Amazon Elastic Compute Cloud - Compute", 0) or
                        services_data.get("Amazon Elastic Compute Cloud", 0) or
                        0
                    )

                    # Conditional rendering based on --output-format parameter
                    if output_format in ["both", "tree"]:
                        # Create and display tree with per-category nested tables
                        cost_tree = create_cost_breakdown_tree(
                            services_by_category=categorized_services,
                            total_monthly_cost=current_cost,
                            optimization_opportunities=None,  # v1.1.20: Removed hardcoded savings assumptions (NATO prevention)
                            previous_services_costs=previous_services_costs,
                            show_zero_cost=show_zero_cost
                        )

                        console.print(cost_tree)
                        console.print()

                        # ADD diagnostic logging
                        logger.debug(
                            "Rich tree cost breakdown rendered",
                            extra={
                                "categories": list(categorized_services.keys()),
                                "opportunities": list(optimization_opportunities.keys()) if optimization_opportunities else [],
                                "output_format": output_format
                            }
                        )

                except Exception as e:
                    # Graceful degradation if tree visualization fails
                    console.print(f"[dim]Note: Cost breakdown tree unavailable ({str(e)})[/dim]")

                # Display Business Impact Summary (with executive mode option)
                # Calculate evidence-based savings from actual resource analysis (replaces hardcoded 15%)
                monthly_savings, savings_breakdown = calculate_evidence_based_savings(
                    services_data=services_data,
                    running_instances=running_instances,
                    stopped_instances=stopped_instances,
                    current_cost=current_cost
                )
                total_annual = current_cost * 12
                optimization_potential = monthly_savings * 12  # Evidence-based annual projection

                # Executive Summary Mode: Enhanced Business Narrative (LEAN: no duplicate table)
                # Conditional based on --summary-mode parameter
                if executive and summary_mode != "none":
                    if summary_mode == "table":
                        _generate_table_summary(
                            account_id=account_id,
                            current_month_name=current_month_name,
                            current_cost=current_cost,
                            sorted_services=sorted_services if 'sorted_services' in locals() else [],
                            monthly_savings=monthly_savings,
                            optimization_potential=optimization_potential,
                            savings_breakdown=savings_breakdown
                        )
                    elif summary_mode == "tree":
                        _generate_tree_summary(
                            account_id=account_id,
                            current_month_name=current_month_name,
                            current_cost=current_cost,
                            categorized_services=categorized_services if 'categorized_services' in locals() else {},
                            monthly_savings=monthly_savings,
                            optimization_potential=optimization_potential,
                            savings_breakdown=savings_breakdown
                        )
                    else:  # summary_mode == "both"
                        _generate_comprehensive_summary(
                            account_id=account_id,
                            current_month_name=current_month_name,
                            current_cost=current_cost,
                            monthly_savings=monthly_savings,
                            optimization_potential=optimization_potential,
                            savings_breakdown=savings_breakdown
                        )
                else:
                    # v1.1.20: Removed hardcoded savings assumptions (NATO prevention)
                    # Savings analysis requires evidence-based calculations using actual resource-level cost data
                    # and validated models - not hardcoded percentages or per-instance assumptions.
                    # Future enhancement: Integrate with AWS Compute Optimizer API for proven recommendations.
                    pass

                # ========== NEW: CxO Persona Analysis (v1.1.20) ==========
                if persona:
                    try:
                        from runbooks.finops.cxo_dashboard_analyzer import CxODashboardAnalyzer, ExecutivePersona
                        from rich.markdown import Markdown

                        # Map persona string to enum
                        persona_map = {
                            'CFO': ExecutivePersona.CFO,
                            'CTO': ExecutivePersona.CTO,
                            'CEO': ExecutivePersona.CEO,
                            'ALL': ExecutivePersona.ALL
                        }
                        selected_persona = persona_map.get(persona, ExecutivePersona.CFO)

                        # DATA STRUCTURE NOTE: services_data is Dict[str, float]
                        # - Keys: Service names (e.g., "Amazon Simple Storage Service")
                        # - Values: Monthly costs as floats (NOT dicts with 'current_cost' keys)
                        # - After line 1098 filtering, values are raw floats for cost threshold comparison
                        # - sorted_services is List[Tuple[str, float]] after line 1112 sorting

                        # Prepare cost data for analysis
                        cost_data = {
                            'total_monthly_cost': current_cost,
                            'previous_monthly_cost': previous_cost,
                            's3_lifecycle_savings_monthly': total_monthly_opportunity if 'total_monthly_opportunity' in locals() else 0,
                            'compute_monthly_cost': sum(
                                cost for service_name, cost in (services_data or {}).items()
                                if any(compute in service_name.lower() for compute in ['ec2', 'lambda', 'fargate', 'compute'])
                            ),
                            'top_service_name': sorted_services[0][0] if sorted_services else 'N/A',
                            'top_service_percentage': (sorted_services[0][1] / current_cost * 100) if sorted_services and current_cost > 0 else 0.0
                        }

                        # Run persona-specific analysis
                        analyzer = CxODashboardAnalyzer()
                        report = analyzer.generate_analysis_report(
                            cost_data=cost_data,
                            persona=selected_persona,
                            output_format="markdown"
                        )

                        # Display analysis in a panel
                        console.print("\n")
                        console.print(Panel(
                            Markdown(report),
                            title=f"[bold cyan]ðŸ“Š {selected_persona.value} Analysis[/]",
                            border_style="cyan",
                            padding=(1, 2)
                        ))

                        # Save analysis report to artifacts
                        from pathlib import Path
                        analysis_dir = Path("artifacts/evidence")
                        analysis_dir.mkdir(parents=True, exist_ok=True)

                        report_path = analysis_dir / f"cxo-analysis-{persona.lower()}.md"
                        report_path.write_text(report, encoding='utf-8')

                        console.print(f"[green]âœ… {selected_persona.value} analysis saved: {report_path}[/green]\n")

                    except ImportError as import_error:
                        console.print(f"[yellow]âš ï¸  CxO analysis modules not available: {import_error}[/yellow]")
                    except Exception as e:
                        console.print(f"[yellow]âš ï¸  Persona analysis failed: {e}[/yellow]")

                # Prepare results dictionary
                results = {
                    "status": "completed",
                    "account_id": account_id,
                    "timeframe": timeframe,
                    "cost_analysis": {
                        "current_monthly_spend": current_cost,
                        "previous_monthly_spend": previous_cost,
                        "annual_projection": total_annual,
                        "optimization_potential": optimization_potential,
                        "top_services": dict(sorted_services[:5]) if services_data else {},
                        "ec2_summary": {
                            "total_instances": total_instances if "total_instances" in locals() else 0,
                            "running_instances": running_instances if "running_instances" in locals() else 0,
                            "stopped_instances": stopped_instances if "stopped_instances" in locals() else 0,
                        },
                    },
                }

                # Attach MCP validation results if available
                if mcp_results:
                    results["mcp_validation"] = {
                        "accuracy_percentage": mcp_results.accuracy_percentage,
                        "validation_passed": mcp_results.accuracy_percentage >= 99.5,
                        "operation_name": mcp_results.operation_name,
                        "status": mcp_results.status.value,
                        "detailed_results": mcp_results,
                    }

                # PHASE 4 TRACK 11: Dashboard Cost MCP Validation
                # Validate dashboard total costs against Cost Explorer for â‰¥99.5% accuracy
                if mcp_validate and current_cost > 0:
                    try:
                        from runbooks.finops.dashboard_mcp_validator import DashboardMCPValidator
                        from pathlib import Path
                        from datetime import datetime

                        console.print("\n[cyan]ðŸ” Running Dashboard Cost MCP Validation...[/cyan]")

                        # Initialize validator
                        validator = DashboardMCPValidator(
                            profile=resolved_profile,
                            region='ap-southeast-2',
                            verbose=verbose
                        )

                        # Validate dashboard total costs
                        validation_summary = validator.validate_dashboard_costs(
                            dashboard_total_cost=current_cost,
                            profile=resolved_profile
                        )

                        # Export validation results to JSON
                        timestamp = datetime.utcnow().strftime('%Y%m%d_%H%M%S')
                        output_path = Path(f"/tmp/mcp_validation_{timestamp}.json")
                        validator.export_validation_results(validation_summary, output_path)

                        # Display validation results in Rich CLI
                        if validation_summary.pass_status:
                            console.print(
                                f"[green]âœ… Dashboard Cost Validation PASSED: "
                                f"{validation_summary.accuracy_percent:.2f}% accuracy (â‰¥99.5% target)[/green]"
                            )
                        else:
                            console.print(
                                f"[yellow]âš ï¸  Dashboard Cost Validation: "
                                f"{validation_summary.accuracy_percent:.2f}% accuracy "
                                f"(target: â‰¥99.5%)[/yellow]"
                            )

                        console.print(f"[dim]Validation results exported: {output_path}[/dim]")

                        # Attach dashboard cost validation to results
                        results["dashboard_cost_validation"] = {
                            "accuracy_percent": validation_summary.accuracy_percent,
                            "validation_passed": validation_summary.pass_status,
                            "dashboard_total": current_cost,
                            "cost_explorer_total": validation_summary.resource_breakdown.get(
                                'cost_validation', {}
                            ).get('cost_explorer_total', 0.0),
                            "variance_percent": validation_summary.resource_breakdown.get(
                                'cost_validation', {}
                            ).get('variance_percent', 0.0),
                            "validation_timestamp": validation_summary.validation_date.isoformat(),
                            "output_file": str(output_path),
                            "mcp_available": validation_summary.mcp_available
                        }

                        # ADD diagnostic logging
                        logger.info(
                            "Dashboard cost MCP validation completed",
                            extra={
                                "accuracy": validation_summary.accuracy_percent,
                                "pass_status": validation_summary.pass_status,
                                "variance_percent": validation_summary.resource_breakdown.get(
                                    'cost_validation', {}
                                ).get('variance_percent', 0.0)
                            }
                        )

                    except Exception as e:
                        console.print(f"[yellow]âš ï¸  Dashboard cost validation failed: {e}[/yellow]")
                        console.print("[dim]Continuing with dashboard results...[/dim]")
                        logger.warning(f"Dashboard cost MCP validation error: {e}")

                # TRACK 2: Activity Analysis Integration for Single-Account Mode
                if activity_analysis:
                    try:
                        from runbooks.finops.dashboard_activity_enricher import DashboardActivityEnricher
                        from runbooks.finops.decommission_scorer import calculate_ec2_score
                        from rich.tree import Tree
                        from rich.panel import Panel
                        import pandas as pd

                        # Activity Health Analysis (header consolidated with discovery table below)

                        # Initialize activity enricher with operational profile
                        enricher = DashboardActivityEnricher(
                            operational_profile=resolved_profile,
                            region='ap-southeast-2',
                            output_controller=None,  # Use default OutputController
                            lookback_days=90
                        )

                        # Collect EC2 instances for activity analysis
                        discovery_results = {
                            'ec2': pd.DataFrame(),
                            'rds': pd.DataFrame(),
                            's3': pd.DataFrame(),
                            'dynamodb': pd.DataFrame(),
                            'asg': pd.DataFrame(),
                            'alb': pd.DataFrame(),
                            'vpc': pd.DataFrame()
                        }

                        # Get EC2 instances from current account
                        try:
                            ec2_client = session.client('ec2', region_name='ap-southeast-2')
                            response = ec2_client.describe_instances()

                            ec2_instances = []
                            for reservation in response.get('Reservations', []):
                                for instance in reservation.get('Instances', []):
                                    ec2_instances.append({
                                        'instance_id': instance['InstanceId'],
                                        'instance_type': instance['InstanceType'],
                                        'state': instance['State']['Name']
                                    })

                            if ec2_instances:
                                discovery_results['ec2'] = pd.DataFrame(ec2_instances)
                        except Exception as e:
                            console.print(f"[yellow]âš ï¸  Could not collect EC2 instances: {e}[/yellow]")

                        # Get S3 buckets from current account
                        try:
                            s3_client = session.client('s3', region_name='ap-southeast-2')
                            buckets_response = s3_client.list_buckets()

                            s3_buckets = []
                            for bucket in buckets_response.get('Buckets', []):
                                bucket_name = bucket['Name']
                                creation_date = bucket.get('CreationDate')

                                # Calculate age
                                if creation_date:
                                    from datetime import datetime, timezone
                                    age_days = (datetime.now(timezone.utc) - creation_date).days
                                else:
                                    age_days = 0

                                s3_buckets.append({
                                    'bucket_name': bucket_name,
                                    'creation_date': creation_date,
                                    'age_days': age_days
                                })

                            if s3_buckets:
                                discovery_results['s3'] = pd.DataFrame(s3_buckets)
                        except Exception as e:
                            console.print(f"[yellow]âš ï¸  Could not collect S3 buckets: {e}[/yellow]")

                        # Get RDS instances from current account
                        try:
                            rds_client = session.client('rds', region_name='ap-southeast-2')
                            rds_response = rds_client.describe_db_instances()

                            rds_instances = []
                            for db in rds_response.get('DBInstances', []):
                                instance_create_time = db.get('InstanceCreateTime')
                                age_days = 0
                                if instance_create_time:
                                    from datetime import datetime, timezone
                                    age_days = (datetime.now(timezone.utc) - instance_create_time).days

                                rds_instances.append({
                                    'db_instance_id': db['DBInstanceIdentifier'],
                                    'db_instance_class': db['DBInstanceClass'],
                                    'engine': db['Engine'],
                                    'engine_version': db.get('EngineVersion', ''),
                                    'status': db['DBInstanceStatus'],
                                    'allocated_storage': db.get('AllocatedStorage', 0),
                                    'age_days': age_days,
                                    'multi_az': db.get('MultiAZ', False)
                                })

                            if rds_instances:
                                discovery_results['rds'] = pd.DataFrame(rds_instances)
                        except Exception as e:
                            console.print(f"[yellow]âš ï¸  Could not collect RDS instances: {e}[/yellow]")

                        # Get DynamoDB tables from current account
                        try:
                            dynamodb_client = session.client('dynamodb', region_name='ap-southeast-2')
                            tables_response = dynamodb_client.list_tables()

                            dynamodb_tables = []
                            for table_name in tables_response.get('TableNames', []):
                                try:
                                    table_details = dynamodb_client.describe_table(TableName=table_name)
                                    table = table_details['Table']

                                    creation_date_time = table.get('CreationDateTime')
                                    age_days = 0
                                    if creation_date_time:
                                        from datetime import datetime, timezone
                                        age_days = (datetime.now(timezone.utc) - creation_date_time).days

                                    dynamodb_tables.append({
                                        'table_name': table['TableName'],
                                        'table_status': table['TableStatus'],
                                        'age_days': age_days,
                                        'item_count': table.get('ItemCount', 0),
                                        'table_size_bytes': table.get('TableSizeBytes', 0),
                                        'billing_mode': table.get('BillingModeSummary', {}).get('BillingMode', 'PROVISIONED')
                                    })
                                except Exception as e:
                                    console.print(f"[dim]Warning: Could not describe table {table_name}: {e}[/]")

                            if dynamodb_tables:
                                discovery_results['dynamodb'] = pd.DataFrame(dynamodb_tables)
                        except Exception as e:
                            console.print(f"[yellow]âš ï¸  Could not collect DynamoDB tables: {e}[/yellow]")

                        # Get ALB/NLB load balancers from current account
                        try:
                            elb_client = session.client('elbv2', region_name='ap-southeast-2')
                            elb_response = elb_client.describe_load_balancers()

                            load_balancers = []
                            for lb in elb_response.get('LoadBalancers', []):
                                lb_arn = lb['LoadBalancerArn']
                                lb_name = lb['LoadBalancerName']
                                lb_type = lb['Type']  # 'application' or 'network'
                                lb_state = lb['State']['Code']
                                created_time = lb.get('CreatedTime')

                                age_days = 0
                                if created_time:
                                    from datetime import datetime, timezone
                                    age_days = (datetime.now(timezone.utc) - created_time).days

                                load_balancers.append({
                                    'lb_arn': lb_arn,
                                    'lb_name': lb_name,
                                    'lb_type': lb_type,
                                    'state': lb_state,
                                    'age_days': age_days
                                })

                            if load_balancers:
                                discovery_results['alb'] = pd.DataFrame(load_balancers)
                        except Exception as e:
                            console.print(f"[yellow]âš ï¸  Could not collect load balancers: {e}[/yellow]")

                        # ASG Discovery (A1-A5 Auto Scaling Group activity analysis)
                        try:
                            asg_client = session.client('autoscaling', region_name='ap-southeast-2')
                            asg_response = asg_client.describe_auto_scaling_groups()

                            asg_groups = []
                            for asg in asg_response.get('AutoScalingGroups', []):
                                asg_name = asg['AutoScalingGroupName']
                                desired_capacity = asg.get('DesiredCapacity', 0)
                                min_size = asg.get('MinSize', 0)
                                max_size = asg.get('MaxSize', 0)
                                instance_count = len(asg.get('Instances', []))
                                created_time = asg.get('CreatedTime')

                                age_days = 0
                                if created_time:
                                    age_days = (datetime.now(timezone.utc) - created_time).days

                                asg_groups.append({
                                    'asg_name': asg_name,
                                    'desired_capacity': desired_capacity,
                                    'min_size': min_size,
                                    'max_size': max_size,
                                    'instance_count': instance_count,
                                    'age_days': age_days
                                })

                            if asg_groups:
                                discovery_results['asg'] = pd.DataFrame(asg_groups)
                        except Exception as e:
                            console.print(f"[yellow]âš ï¸  Could not collect auto scaling groups: {e}[/yellow]")

                        # ECS Discovery (C1-C5 Container Service activity analysis)
                        try:
                            ecs_client = session.client('ecs', region_name='ap-southeast-2')
                            ecs_response = ecs_client.list_clusters()

                            ecs_clusters = []
                            for cluster_arn in ecs_response.get('clusterArns', []):
                                cluster_name = cluster_arn.split('/')[-1]

                                # Get cluster details
                                cluster_details = ecs_client.describe_clusters(clusters=[cluster_arn])

                                if cluster_details.get('clusters'):
                                    cluster = cluster_details['clusters'][0]

                                    ecs_clusters.append({
                                        'cluster_arn': cluster_arn,
                                        'cluster_name': cluster_name,
                                        'status': cluster.get('status', 'UNKNOWN'),
                                        'active_services_count': cluster.get('activeServicesCount', 0),
                                        'running_tasks_count': cluster.get('runningTasksCount', 0),
                                        'pending_tasks_count': cluster.get('pendingTasksCount', 0),
                                        'registered_container_instances_count': cluster.get('registeredContainerInstancesCount', 0)
                                    })

                            if ecs_clusters:
                                discovery_results['ecs'] = pd.DataFrame(ecs_clusters)
                        except Exception as e:
                            console.print(f"[yellow]âš ï¸  Could not collect ECS clusters: {e}[/yellow]")

                        # Route53 Discovery (R53-1 to R53-4 DNS activity analysis)
                        try:
                            route53_client = session.client('route53', region_name='us-east-1')  # Route53 is global
                            zones_response = route53_client.list_hosted_zones()

                            hosted_zones = []
                            for zone in zones_response.get('HostedZones', []):
                                zone_id = zone['Id'].split('/')[-1]
                                zone_name = zone['Name']
                                is_private = zone.get('Config', {}).get('PrivateZone', False)
                                record_count = zone.get('ResourceRecordSetCount', 0)

                                hosted_zones.append({
                                    'hosted_zone_id': zone_id,
                                    'name': zone_name,
                                    'is_private': is_private,
                                    'resource_record_set_count': record_count
                                })

                            if hosted_zones:
                                discovery_results['route53'] = pd.DataFrame(hosted_zones)
                        except Exception as e:
                            console.print(f"[yellow]âš ï¸  Could not collect Route53 zones: {e}[/yellow]")

                        # VPC Resources Discovery (V1-V5/N1-N5 network connectivity signals - 4 types)
                        try:
                            ec2_client = session.client('ec2', region_name='ap-southeast-2')
                            vpc_resources = []

                            # 1. VPC Endpoints (VPCE)
                            vpce_response = ec2_client.describe_vpc_endpoints()
                            for vpce in vpce_response.get('VpcEndpoints', []):
                                vpc_resources.append({
                                    'resource_id': vpce['VpcEndpointId'],
                                    'resource_type': 'vpce',
                                    'vpc_id': vpce.get('VpcId'),
                                    'service_name': vpce.get('ServiceName'),
                                    'state': vpce.get('State')
                                })

                            # 2. VPC Peering Connections
                            peering_response = ec2_client.describe_vpc_peering_connections()
                            for peering in peering_response.get('VpcPeeringConnections', []):
                                vpc_resources.append({
                                    'resource_id': peering['VpcPeeringConnectionId'],
                                    'resource_type': 'vpc_peering',
                                    'vpc_id': peering.get('RequesterVpcInfo', {}).get('VpcId'),
                                    'peer_vpc_id': peering.get('AccepterVpcInfo', {}).get('VpcId'),
                                    'state': peering.get('Status', {}).get('Code')
                                })

                            # 3. Transit Gateways
                            tgw_response = ec2_client.describe_transit_gateways()
                            for tgw in tgw_response.get('TransitGateways', []):
                                vpc_resources.append({
                                    'resource_id': tgw['TransitGatewayId'],
                                    'resource_type': 'transit_gateway',
                                    'state': tgw.get('State'),
                                    'owner_id': tgw.get('OwnerId')
                                })

                            # 4. NAT Gateways
                            nat_response = ec2_client.describe_nat_gateways()
                            for nat in nat_response.get('NatGateways', []):
                                vpc_resources.append({
                                    'resource_id': nat['NatGatewayId'],
                                    'resource_type': 'nat_gateway',
                                    'vpc_id': nat.get('VpcId'),
                                    'subnet_id': nat.get('SubnetId'),
                                    'state': nat.get('State')
                                })

                            if vpc_resources:
                                discovery_results['vpc'] = pd.DataFrame(vpc_resources)
                        except Exception as e:
                            console.print(f"[yellow]âš ï¸  Could not collect VPC resources: {e}[/yellow]")

                        # Direct Connect Activity Integration (DX1-DX4 signals)
                        try:
                            dx_client = session.client('directconnect', region_name='ap-southeast-2')
                            dx_response = dx_client.describe_connections()

                            dx_connections = []
                            for connection in dx_response.get('connections', []):
                                connection_id = connection.get('connectionId')
                                connection_name = connection.get('connectionName', 'N/A')
                                connection_state = connection.get('connectionState', 'unknown')
                                bandwidth = connection.get('bandwidth', '0Gbps')
                                location = connection.get('location', 'N/A')

                                dx_connections.append({
                                    'connection_id': connection_id,
                                    'connection_name': connection_name,
                                    'connection_state': connection_state,
                                    'bandwidth': bandwidth,
                                    'location': location
                                })

                            if dx_connections:
                                discovery_results['dx'] = pd.DataFrame(dx_connections)
                        except Exception as e:
                            console.print(f"[yellow]âš ï¸  Could not collect Direct Connect connections: {e}[/yellow]")

                        # Display consolidated discovery summary (replaces 9 individual messages with single table)
                        from runbooks.common.rich_utils import create_discovery_summary_table

                        discoveries = {
                            "ðŸ’» EC2 Instances": (
                                len(discovery_results.get('ec2', pd.DataFrame())),
                                "âœ… Discovered" if not discovery_results.get('ec2', pd.DataFrame()).empty else "âš ï¸ No resources found"
                            ),
                            "ðŸ“¦ S3 Buckets": (
                                len(discovery_results.get('s3', pd.DataFrame())),
                                "âœ… Discovered" if not discovery_results.get('s3', pd.DataFrame()).empty else "âš ï¸ No resources found"
                            ),
                            "ðŸ—„ï¸ RDS Instances": (
                                len(discovery_results.get('rds', pd.DataFrame())),
                                "âœ… Discovered" if not discovery_results.get('rds', pd.DataFrame()).empty else "âš ï¸ No resources found"
                            ),
                            "âš¡ DynamoDB Tables": (
                                len(discovery_results.get('dynamodb', pd.DataFrame())),
                                "âœ… Discovered" if not discovery_results.get('dynamodb', pd.DataFrame()).empty else "âš ï¸ No resources found"
                            ),
                            "âš–ï¸ Load Balancers": (
                                len(discovery_results.get('alb', pd.DataFrame())),
                                "âœ… Discovered" if not discovery_results.get('alb', pd.DataFrame()).empty else "âš ï¸ No resources found"
                            ),
                            "ðŸ“Š Auto Scaling Groups": (
                                len(discovery_results.get('asg', pd.DataFrame())),
                                "âœ… Discovered" if not discovery_results.get('asg', pd.DataFrame()).empty else "âš ï¸ No resources found"
                            ),
                            "ðŸ³ ECS Clusters": (
                                len(discovery_results.get('ecs', pd.DataFrame())),
                                "âœ… Discovered" if not discovery_results.get('ecs', pd.DataFrame()).empty else "âš ï¸ No resources found"
                            ),
                            "ðŸŒ Route53 Zones": (
                                len(discovery_results.get('route53', pd.DataFrame())),
                                "âœ… Discovered" if not discovery_results.get('route53', pd.DataFrame()).empty else "âš ï¸ No resources found"
                            ),
                            "ðŸ”— Direct Connect": (
                                len(discovery_results.get('dx', pd.DataFrame())),
                                "âœ… Discovered" if not discovery_results.get('dx', pd.DataFrame()).empty else "âš ï¸ No resources found"
                            ),
                        }

                        # v1.1.20: Hide empty resources by default, show only with --show-empty (Improvement 2 CORRECTED)
                        if not show_empty:
                            discoveries = {k: v for k, v in discoveries.items() if v[0] > 0}

                        # v1.1.20 UX: Discovery counts merged into Activity Tree branches (removed separate table)

                        # Enrich resources with activity signals
                        # Check if we have ANY resources to enrich
                        has_resources = any(not df.empty for df in discovery_results.values())

                        if has_resources:
                            enriched = enricher.enrich_all_resources(discovery_results)

                            # Build activity health tree
                            tree = Tree("[bold bright_cyan]ðŸŒ³ Activity Health Tree[/]")

                            # EC2 Activity Branch (Comment #2 Fix: Enterprise-ready per-resource tables)
                            if not enriched['ec2'].empty:
                                ec2_count = len(enriched['ec2'])
                                ec2_branch = tree.add(f"[cyan]ðŸ’» EC2 Instances ({ec2_count} discovered)[/]")

                                # Calculate decommission tiers and organize instances
                                must_decommission = []
                                should_review = []
                                could_consider = []
                                keep_active = []

                                for idx, row in enriched['ec2'].iterrows():
                                    # v1.1.20: Enhanced E1-E7 signal collection with weight rebalancing + E5 service attachment + E2/E4 threshold fixes
                                    instance_id = row.get('instance_id', 'N/A')

                                    # E5: Check service attachment (ASG/LB/ECS) - CRITICAL for preventing false positives
                                    is_attached = _check_ec2_service_attachment(instance_id, profile)

                                    signals = {
                                        'E1': 40 if row.get('compute_optimizer_finding') == 'Idle' else 0,  # Manager directive: 40pts (balanced weight distribution)
                                        'E2': 20 if row.get('p95_cpu_utilization', 100) <= 3.0 else 0,  # Manager directive: 20pts (strengthen utilization signal, 3% threshold)
                                        'E3': 10 if row.get('days_since_activity', 0) >= 90 else 0,  # AWS Well-Architected alignment: 10pts (90-day threshold)
                                        'E4': 5 if (row.get('ssm_ping_status') != 'Online' or row.get('ssm_days_since_ping', 0) > 14) else 0,  # SSM staleness check: 5pts (>14 days = stale)
                                        'E5': 0 if is_attached else 10,  # Manager directive: 10pts (production safety via ASG/LB/ECS check, reduced from 15)
                                        'E6': 10 if row.get('disk_total_ops_p95', 999) <= 10 else 0,  # Storage I/O idle: 10pts (â‰¤10 ops/day threshold)
                                        'E7': 0   # Placeholder for cost explorer (future enhancement - will be 5 pts)
                                    }

                                    score_result = calculate_ec2_score(signals)
                                    tier = score_result['tier']

                                    # Build comprehensive instance record with all available data
                                    instance_info = {
                                        'instance_id': row.get('instance_id', 'N/A'),
                                        'state': row.get('state', 'unknown'),
                                        'instance_type': row.get('instance_type', 'N/A'),
                                        'cpu_utilization': row.get('p95_cpu_utilization', 0),
                                        'age_days': row.get('age_days', 0),
                                        'ssm_status': row.get('ssm_ping_status', 'Unknown'),
                                        'compute_optimizer': row.get('compute_optimizer_finding', 'N/A'),
                                        'days_since_activity': row.get('days_since_activity', 0),
                                        'score': score_result['total_score'],
                                        'tier': tier,
                                        'signals': signals,
                                        'monthly_cost': 0  # Will be populated from cost data if available
                                    }

                                    if tier == 'MUST':
                                        must_decommission.append(instance_info)
                                    elif tier == 'SHOULD':
                                        should_review.append(instance_info)
                                    elif tier == 'COULD':
                                        could_consider.append(instance_info)
                                    else:
                                        keep_active.append(instance_info)

                                # Create per-instance detailed table (v1.1.20 UX: manager-approved columns)
                                if must_decommission or should_review or could_consider or keep_active:
                                    from rich.table import Table

                                    activity_table = Table(
                                        title=None,  # No title - tree node provides context (Track 6)
                                        show_header=True,
                                        header_style="bold cyan",
                                        border_style="dim"
                                    )

                                    # Manager UX Adjustments: Remove "Ex" prefixes + Restore I/O column for E6 signal (9 â†’ 10 columns)
                                    activity_table.add_column("Instance ID", style="white", no_wrap=True)
                                    activity_table.add_column("State", style="cyan", width=10)
                                    activity_table.add_column("Type", style="yellow", width=12)
                                    activity_table.add_column("Optimizer", style="magenta", width=9)
                                    activity_table.add_column("CPU %", justify="right", style="yellow", width=6)
                                    activity_table.add_column("Age", justify="right", style="yellow", width=5)
                                    activity_table.add_column("I/O", justify="right", style="yellow", width=6)
                                    activity_table.add_column("Score", justify="right", style="bold", width=6)
                                    activity_table.add_column("Tier", style="bold", width=8)
                                    activity_table.add_column("Signal Summary", style="dim", no_wrap=False, width=20)

                                    # Manager UX Adjustment 1: Signal Summary shows signal IDs only (no values) for space efficiency
                                    def format_signal_summary(instance):
                                        """Format E1-E7 signal IDs only (no values) for compact display"""
                                        signal_parts = []
                                        signals = instance['signals']

                                        # Show only signal IDs (E1, E2, etc.) without values for maximum column width savings
                                        if signals.get('E1', 0) > 0:
                                            signal_parts.append("E1")
                                        if signals.get('E2', 0) > 0:
                                            signal_parts.append("E2")
                                        if signals.get('E3', 0) > 0:
                                            signal_parts.append("E3")
                                        if signals.get('E4', 0) > 0:
                                            signal_parts.append("E4")
                                        if signals.get('E5', 0) > 0:
                                            signal_parts.append("E5")
                                        if signals.get('E6', 0) > 0:
                                            signal_parts.append("E6")
                                        if signals.get('E7', 0) > 0:
                                            signal_parts.append("E7")

                                        return ', '.join(signal_parts) if signal_parts else '-'

                                    # Add MUST tier instances (red, highest priority)
                                    for instance in must_decommission:
                                        io_ops = instance.get('disk_total_ops_p95', None)
                                        io_display = f"{io_ops:.0f}" if io_ops is not None else "N/A"
                                        activity_table.add_row(
                                            instance['instance_id'],
                                            instance['state'],
                                            instance['instance_type'],
                                            instance.get('compute_optimizer', 'N/A')[:9],  # Optimizer
                                            f"{instance['cpu_utilization']:.1f}%",  # CPU %
                                            f"{instance.get('days_since_activity', 0)}d",  # Age
                                            io_display,  # I/O (disk ops/day)
                                            "9.0",  # Score
                                            "[red]MUST[/]",
                                            format_signal_summary(instance),  # Signal Summary
                                            end_section=True
                                        )

                                    # Add SHOULD tier instances (yellow)
                                    for instance in should_review:
                                        io_ops = instance.get('disk_total_ops_p95', None)
                                        io_display = f"{io_ops:.0f}" if io_ops is not None else "N/A"
                                        activity_table.add_row(
                                            instance['instance_id'],
                                            instance['state'],
                                            instance['instance_type'],
                                            instance.get('compute_optimizer', 'N/A')[:9],  # Optimizer
                                            f"{instance['cpu_utilization']:.1f}%",  # CPU %
                                            f"{instance.get('days_since_activity', 0)}d",  # Age
                                            io_display,  # I/O (disk ops/day)
                                            "6.0",  # Score
                                            "[yellow]SHOULD[/]",
                                            format_signal_summary(instance),  # Signal Summary
                                            end_section=True
                                        )

                                    # Add COULD tier instances (blue)
                                    for instance in could_consider:
                                        io_ops = instance.get('disk_total_ops_p95', None)
                                        io_display = f"{io_ops:.0f}" if io_ops is not None else "N/A"
                                        activity_table.add_row(
                                            instance['instance_id'],
                                            instance['state'],
                                            instance['instance_type'],
                                            instance.get('compute_optimizer', 'N/A')[:9],  # Optimizer
                                            f"{instance['cpu_utilization']:.1f}%",  # CPU %
                                            f"{instance.get('days_since_activity', 0)}d",  # Age
                                            io_display,  # I/O (disk ops/day)
                                            "4.0",  # Score
                                            "[blue]COULD[/]",
                                            format_signal_summary(instance),  # Signal Summary
                                            end_section=True
                                        )

                                    # Add KEEP tier instances (green, show first 10 for brevity)
                                    keep_display = keep_active[:10] if len(keep_active) > 10 else keep_active
                                    for idx, instance in enumerate(keep_display):
                                        is_last = (idx == len(keep_display) - 1) and len(keep_active) <= 10
                                        io_ops = instance.get('disk_total_ops_p95', None)
                                        io_display = f"{io_ops:.0f}" if io_ops is not None else "N/A"
                                        activity_table.add_row(
                                            instance['instance_id'],
                                            instance['state'],
                                            instance['instance_type'],
                                            instance.get('compute_optimizer', 'N/A')[:9],  # Optimizer
                                            f"{instance['cpu_utilization']:.1f}%",  # CPU %
                                            f"{instance.get('days_since_activity', 0)}d",  # Age
                                            io_display,  # I/O (disk ops/day)
                                            "1.0",  # Score
                                            "[green]KEEP[/]",
                                            format_signal_summary(instance),  # Signal Summary
                                            end_section=is_last
                                        )

                                    if len(keep_active) > 10:
                                        activity_table.add_row(
                                            f"... ({len(keep_active) - 10} more KEEP instances)",
                                            "", "", "", "", "", "", "", "", "",  # 10 columns (E6 I/O restored)
                                            style="dim"
                                        )

                                    # Add table to tree
                                    ec2_branch.add(activity_table)

                                    # Signal legend (Manager UX Adjustment 3: Single add() with newline to avoid tree branch on line 2)
                                    ec2_branch.add(
                                        "[bold]Signal Legend:[/] "
                                        "[dim]E1:Compute Optimizer Idle (40pts) | E2:CloudWatch CPU â‰¤3% (20pts) | E3:CloudTrail 90d inactivity (10pts)\n"
                                        "E4:SSM offline/stale (5pts) | E5:Service attachment ASG/LB/ECS (10pts) | E6:Storage I/O idle (10pts) | E7:Cost Explorer savings (5pts)[/]"
                                    )

                                    # Summary statistics
                                    ec2_branch.add(
                                        f"[bold]Summary:[/] "
                                        f"{len(must_decommission)} MUST + {len(should_review)} SHOULD + "
                                        f"{len(could_consider)} COULD + {len(keep_active)} KEEP = "
                                        f"{len(must_decommission) + len(should_review) + len(could_consider) + len(keep_active)} total"
                                    )

                            # S3 Buckets Activity Branch
                            if 's3' in enriched and not enriched['s3'].empty:
                                s3_df = enriched['s3']
                                s3_count = len(s3_df)
                                s3_branch = tree.add(f"[cyan]â˜ï¸ S3 Buckets ({s3_count} discovered)[/]")

                                # Build S3 activity table (following EC2 pattern - no title, tree node provides context)
                                s3_table = Table(
                                    title=None,  # No title - tree node shows "â˜ï¸ S3 Buckets (N discovered)"
                                    show_header=True,
                                    header_style="bold cyan",
                                    border_style="cyan",
                                    show_lines=True
                                )

                                # 7 columns: Bucket Name + Objects + Size + Cost + Score + Tier + Signal Summary
                                s3_table.add_column("Bucket Name", style="white", no_wrap=False, width=40)
                                s3_table.add_column("Objects", justify="right", style="cyan", width=10)
                                s3_table.add_column("Size (GB)", justify="right", style="yellow", width=10)
                                s3_table.add_column("Cost/mo", justify="right", style="yellow", width=10)
                                s3_table.add_column("Score", justify="right", style="bold", width=8)
                                s3_table.add_column("Tier", style="bold", width=8)
                                s3_table.add_column("Signal Summary", style="dim", no_wrap=False, width=20)

                                # Helper: Format S3 signal summary (S1-S7 IDs only, following EC2 pattern)
                                def format_s3_signal_summary(row):
                                    """Format S1-S7 signal IDs only (no values) for compact display."""
                                    signal_parts = []
                                    for signal in ['S1', 'S2', 'S3', 'S4', 'S5', 'S6', 'S7']:
                                        if signal in row and row[signal] > 0:
                                            signal_parts.append(signal)  # ID only, no points
                                    return ', '.join(signal_parts) if signal_parts else '-'

                                # Group by tier (using decommission_tier from enricher if available)
                                s3_must = []
                                s3_should = []
                                s3_could = []
                                s3_keep = []

                                for idx, row in s3_df.iterrows():
                                    bucket_data = {
                                        'bucket_name': row.get('bucket_name', 'N/A'),
                                        'total_objects': row.get('total_objects', 0),
                                        'size_gb': row.get('total_size_gb', 0.0),
                                        'monthly_cost': row.get('monthly_cost', 0.0),
                                        'score': row.get('decommission_score', 0),
                                        'tier': row.get('decommission_tier', 'KEEP'),
                                        'signal_summary': format_s3_signal_summary(row)
                                    }

                                    # Categorize by tier
                                    tier = bucket_data['tier']
                                    if tier == 'MUST':
                                        s3_must.append(bucket_data)
                                    elif tier == 'SHOULD':
                                        s3_should.append(bucket_data)
                                    elif tier == 'COULD':
                                        s3_could.append(bucket_data)
                                    else:
                                        s3_keep.append(bucket_data)

                                # Sort each tier by monthly_cost descending (highest cost = biggest savings first)
                                s3_must.sort(key=lambda x: x['monthly_cost'], reverse=True)
                                s3_should.sort(key=lambda x: x['monthly_cost'], reverse=True)
                                s3_could.sort(key=lambda x: x['monthly_cost'], reverse=True)
                                s3_keep.sort(key=lambda x: x['monthly_cost'], reverse=True)

                                # Add MUST tier buckets
                                for bucket in s3_must:
                                    s3_table.add_row(
                                        bucket['bucket_name'],
                                        f"{bucket['total_objects']:,}",
                                        f"{bucket['size_gb']:.2f}",
                                        f"${bucket['monthly_cost']:.2f}",
                                        str(bucket['score']),
                                        "[red]MUST[/]",
                                        bucket['signal_summary'],
                                        end_section=True
                                    )

                                # Add SHOULD tier buckets
                                for bucket in s3_should:
                                    s3_table.add_row(
                                        bucket['bucket_name'],
                                        f"{bucket['total_objects']:,}",
                                        f"{bucket['size_gb']:.2f}",
                                        f"${bucket['monthly_cost']:.2f}",
                                        str(bucket['score']),
                                        "[yellow]SHOULD[/]",
                                        bucket['signal_summary'],
                                        end_section=True
                                    )

                                # Add COULD tier buckets
                                for bucket in s3_could:
                                    s3_table.add_row(
                                        bucket['bucket_name'],
                                        f"{bucket['total_objects']:,}",
                                        f"{bucket['size_gb']:.2f}",
                                        f"${bucket['monthly_cost']:.2f}",
                                        str(bucket['score']),
                                        "[blue]COULD[/]",
                                        bucket['signal_summary'],
                                        end_section=True
                                    )

                                # Add KEEP tier buckets (limit to first 10)
                                keep_display_s3 = s3_keep[:10] if len(s3_keep) > 10 else s3_keep
                                for bucket in keep_display_s3:
                                    s3_table.add_row(
                                        bucket['bucket_name'],
                                        f"{bucket['total_objects']:,}",
                                        f"{bucket['size_gb']:.2f}",
                                        f"${bucket['monthly_cost']:.2f}",
                                        str(bucket['score']),
                                        "[green]KEEP[/]",
                                        bucket['signal_summary']
                                    )

                                if len(s3_keep) > 10:
                                    s3_table.add_row(
                                        f"... ({len(s3_keep) - 10} more KEEP buckets)",
                                        "", "", "", "", "", "",
                                        style="dim"
                                    )

                                s3_branch.add(s3_table)

                                # Signal legend (following EC2 pattern)
                                s3_branch.add(
                                    "[bold]Signal Legend:[/] "
                                    "[dim]S1:Storage Lens optimization <70/100 (40pts) | S2:Storage class vs access mismatch (20pts) | S3:Security gap (15pts)\n"
                                    "S4:No lifecycle policy >90d (10pts) | S5:High request cost (8pts) | S6:Versioning without expiration (5pts) | S7:No cross-region replication (2pts)[/]"
                                )

                                # S3 summary
                                s3_branch.add(
                                    f"[bold]Summary:[/] "
                                    f"{len(s3_must)} MUST + {len(s3_should)} SHOULD + "
                                    f"{len(s3_could)} COULD + {len(s3_keep)} KEEP = "
                                    f"{len(s3_must) + len(s3_should) + len(s3_could) + len(s3_keep)} total"
                                )

                            # RDS Instances Activity Branch
                            if 'rds' in enriched and not enriched['rds'].empty:
                                rds_df = enriched['rds']
                                rds_count = len(rds_df)
                                rds_branch = tree.add(f"[cyan]ðŸ—„ï¸ RDS Instances ({rds_count} discovered)[/]")

                                # Build RDS activity table
                                rds_table = Table(
                                    title="RDS Instances",
                                    show_header=True,
                                    header_style="bold cyan",
                                    border_style="cyan",
                                    show_lines=True
                                )

                                rds_table.add_column("DB Instance", style="white", no_wrap=False, width=30)
                                rds_table.add_column("Engine", style="magenta", width=15)
                                rds_table.add_column("Status", style="yellow", width=12)
                                rds_table.add_column("Connections", justify="right", style="cyan", width=12)
                                rds_table.add_column("CPU %", justify="right", style="cyan", width=8)
                                rds_table.add_column("Recommendation", style="bold", width=15)
                                rds_table.add_column("Signals", style="dim", no_wrap=False)

                                # Display RDS instances (sorted by recommendation)
                                rds_sorted = rds_df.sort_values(
                                    by='recommendation',
                                    key=lambda x: x.map({'DECOMMISSION': 0, 'INVESTIGATE': 1, 'DOWNSIZE': 2, 'KEEP': 3}),
                                    ascending=True
                                )

                                for idx, row in rds_sorted.iterrows():
                                    recommendation = row.get('recommendation', 'KEEP')
                                    recommendation_color = {
                                        'DECOMMISSION': 'red',
                                        'INVESTIGATE': 'yellow',
                                        'DOWNSIZE': 'blue',
                                        'KEEP': 'green'
                                    }.get(recommendation, 'white')

                                    rds_table.add_row(
                                        row.get('db_instance_id', 'N/A'),
                                        f"{row.get('engine', 'N/A')} {row.get('engine_version', '')}".strip(),
                                        row.get('status', 'N/A'),
                                        f"{row.get('avg_connections_90d', 0):.1f}",
                                        f"{row.get('avg_cpu_percent_60d', 0):.1f}",
                                        f"[{recommendation_color}]{recommendation}[/]",
                                        row.get('idle_signals', 'None'),
                                        end_section=(recommendation in ['DECOMMISSION', 'INVESTIGATE'])
                                    )

                                rds_branch.add(rds_table)

                                # RDS summary
                                rds_decommission = len(rds_df[rds_df['recommendation'] == 'DECOMMISSION'])
                                rds_investigate = len(rds_df[rds_df['recommendation'] == 'INVESTIGATE'])
                                rds_branch.add(
                                    f"[bold]Summary:[/] "
                                    f"{rds_decommission} DECOMMISSION + {rds_investigate} INVESTIGATE + "
                                    f"{len(rds_df)} total RDS instances"
                                )

                            # DynamoDB Tables Activity Branch
                            if 'dynamodb' in enriched and not enriched['dynamodb'].empty:
                                dynamodb_df = enriched['dynamodb']
                                dynamodb_count = len(dynamodb_df)
                                dynamodb_branch = tree.add(f"[cyan]âš¡ DynamoDB Tables ({dynamodb_count} discovered)[/]")

                                # Build DynamoDB activity table
                                dynamodb_table = Table(
                                    title=None,
                                    show_header=True,
                                    header_style="bold cyan",
                                    border_style="cyan",
                                    show_lines=True
                                )

                                dynamodb_table.add_column("Table Name", style="white", no_wrap=False, width=30)
                                dynamodb_table.add_column("Status", style="yellow", width=12)
                                dynamodb_table.add_column("Items", justify="right", style="cyan", width=12)
                                dynamodb_table.add_column("Size (GB)", justify="right", style="cyan", width=12)
                                dynamodb_table.add_column("Billing", style="magenta", width=15)
                                dynamodb_table.add_column("Recommendation", style="bold", width=15)
                                dynamodb_table.add_column("Signals", style="dim", no_wrap=False)

                                # Display DynamoDB tables (sorted by recommendation)
                                dynamodb_sorted = dynamodb_df.sort_values(
                                    by='recommendation',
                                    key=lambda x: x.map({'DECOMMISSION': 0, 'INVESTIGATE': 1, 'OPTIMIZE': 2, 'KEEP': 3}),
                                    ascending=True
                                )

                                for idx, row in dynamodb_sorted.iterrows():
                                    recommendation = row.get('recommendation', 'KEEP')
                                    recommendation_color = {
                                        'DECOMMISSION': 'red',
                                        'INVESTIGATE': 'yellow',
                                        'OPTIMIZE': 'blue',
                                        'KEEP': 'green'
                                    }.get(recommendation, 'white')

                                    # Calculate size in GB
                                    size_bytes = row.get('table_size_bytes', 0)
                                    size_gb = size_bytes / (1024 ** 3) if size_bytes > 0 else 0

                                    dynamodb_table.add_row(
                                        row.get('table_name', 'N/A'),
                                        row.get('table_status', 'N/A'),
                                        f"{row.get('item_count', 0):,}",
                                        f"{size_gb:.2f}",
                                        row.get('billing_mode', 'N/A'),
                                        f"[{recommendation_color}]{recommendation}[/]",
                                        row.get('idle_signals', 'None'),
                                        end_section=(recommendation in ['DECOMMISSION', 'INVESTIGATE'])
                                    )

                                dynamodb_branch.add(dynamodb_table)

                                # DynamoDB Signal Legend (compact single-line format)
                                if 'decommission_score' in dynamodb_df.columns:
                                    dynamodb_branch.add(
                                        "[bold cyan]ðŸ“Š Signal Legend:[/] "
                                        "[dim]D1: Capacity <5% (40pts) | D2: Idle GSIs (20pts) | D3: No PITR (15pts) | "
                                        "D4: No Streams (10pts) | D5: Age >180d inactive (15pts)[/]"
                                    )

                                # DynamoDB summary
                                dynamodb_decommission = len(dynamodb_df[dynamodb_df['recommendation'] == 'DECOMMISSION'])
                                dynamodb_investigate = len(dynamodb_df[dynamodb_df['recommendation'] == 'INVESTIGATE'])
                                dynamodb_branch.add(
                                    f"[bold]Summary:[/] "
                                    f"{dynamodb_decommission} DECOMMISSION + {dynamodb_investigate} INVESTIGATE + "
                                    f"{len(dynamodb_df)} total DynamoDB tables"
                                )

                            # RDS Activity Analysis Integration (R1-R7 signals)
                            if 'rds' in discovery_results and not discovery_results['rds'].empty:
                                from runbooks.inventory.enrichers.rds_activity import RDSActivityEnricher
                                from runbooks.finops.decommission_scorer import calculate_rds_score
                                from rich.table import Table

                                rds_branch = tree.add("ðŸ—„ï¸  RDS Database Activity (R1-R7)")

                                try:
                                    # Initialize RDS activity enricher
                                    rds_enricher = RDSActivityEnricher(
                                        operational_profile=profile,
                                        region='ap-southeast-2'
                                    )

                                    # Enrich RDS instances with R1-R7 signals
                                    rds_instances = discovery_results['rds']

                                    # Analyze each RDS instance
                                    enriched_rds = []
                                    rds_must = []
                                    rds_should = []
                                    rds_could = []
                                    rds_keep = []

                                    for idx, row in rds_instances.iterrows():
                                        db_instance_id = row.get('db_instance_id')

                                        # Get R1-R7 activity signals
                                        analyses = rds_enricher.analyze_instance_activity([db_instance_id])

                                        if analyses and len(analyses) > 0:
                                            analysis = analyses[0]

                                            # Build signals dict for scoring
                                            signals = {
                                                'R1': 60 if 'R1' in [s.value for s in analysis.idle_signals] else 0,
                                                'R2': 15 if 'R2' in [s.value for s in analysis.idle_signals] else 0,
                                                'R3': 10 if 'R3' in [s.value for s in analysis.idle_signals] else 0,
                                                'R4': 8 if 'R4' in [s.value for s in analysis.idle_signals] else 0,
                                                'R5': 4 if 'R5' in [s.value for s in analysis.idle_signals] else 0,
                                                'R6': 2 if 'R6' in [s.value for s in analysis.idle_signals] else 0,
                                                'R7': 1 if 'R7' in [s.value for s in analysis.idle_signals] else 0
                                            }

                                            # Calculate decommission score
                                            score_result = calculate_rds_score(signals)
                                            tier = score_result['tier']

                                            instance_info = {
                                                'db_instance_id': db_instance_id,
                                                'engine': row.get('engine', 'N/A'),
                                                'engine_version': row.get('engine_version', ''),
                                                'status': row.get('status', 'unknown'),
                                                'allocated_storage': row.get('allocated_storage', 0),
                                                'cpu_utilization': analysis.metrics.avg_cpu_percent_60d,
                                                'avg_connections': analysis.metrics.avg_connections_90d,
                                                'score': score_result['total_score'],
                                                'tier': tier,
                                                'signals': signals,
                                                'analysis': analysis
                                            }

                                            enriched_rds.append(instance_info)

                                            # Categorize by tier
                                            if tier == 'MUST':
                                                rds_must.append(instance_info)
                                            elif tier == 'SHOULD':
                                                rds_should.append(instance_info)
                                            elif tier == 'COULD':
                                                rds_could.append(instance_info)
                                            else:
                                                rds_keep.append(instance_info)

                                    if enriched_rds:
                                        # Create per-DB instance detailed table
                                        rds_table = Table(
                                            title="RDS Databases",
                                            show_header=True,
                                            header_style="bold cyan",
                                            border_style="dim"
                                        )

                                        rds_table.add_column("DB Instance ID", style="white", no_wrap=True)
                                        rds_table.add_column("Engine", style="yellow", width=12)
                                        rds_table.add_column("Status", style="cyan", width=10)
                                        rds_table.add_column("CPU %", justify="right", width=8)
                                        rds_table.add_column("Conn/day", justify="right", width=10)
                                        rds_table.add_column("Storage", justify="right", width=10)
                                        rds_table.add_column("Score", justify="right", style="bold", width=6)
                                        rds_table.add_column("Tier", style="bold", width=8)
                                        rds_table.add_column("Signal Details", style="dim", no_wrap=False)

                                        # Helper function to format R1-R7 signal values
                                        def format_rds_signal_values(instance):
                                            """Format R1-R7 signal values for display"""
                                            signal_parts = []
                                            signals = instance['signals']
                                            analysis = instance['analysis']

                                            if signals.get('R1', 0) > 0:
                                                signal_parts.append(f"R1:ZeroConn")
                                            if signals.get('R2', 0) > 0:
                                                signal_parts.append(f"R2:{analysis.metrics.avg_connections_90d:.1f}/day")
                                            if signals.get('R3', 0) > 0:
                                                signal_parts.append(f"R3:{analysis.metrics.avg_cpu_percent_60d:.1f}%")
                                            if signals.get('R4', 0) > 0:
                                                signal_parts.append(f"R4:{analysis.metrics.avg_iops_60d:.0f}IOPS")
                                            if signals.get('R5', 0) > 0:
                                                signal_parts.append(f"R5:BackupOnly")
                                            if signals.get('R6', 0) > 0:
                                                signal_parts.append(f"R6:OffHours")
                                            if signals.get('R7', 0) > 0:
                                                signal_parts.append(f"R7:{analysis.metrics.storage_utilization_pct:.1f}%")

                                            return ', '.join(signal_parts) if signal_parts else 'None'

                                        # Sort by tier priority: MUST â†’ SHOULD â†’ COULD â†’ KEEP
                                        all_instances = rds_must + rds_should + rds_could + rds_keep[:10]

                                        # Add MUST tier rows
                                        for instance in rds_must:
                                            rds_table.add_row(
                                                instance['db_instance_id'],
                                                f"{instance['engine']}/{instance['engine_version'][:5]}",
                                                instance['status'],
                                                f"{instance['cpu_utilization']:.1f}",
                                                f"{instance['avg_connections']:.1f}",
                                                f"{instance['allocated_storage']}GB",
                                                str(instance['score']),
                                                "[red]MUST[/]",
                                                format_rds_signal_values(instance),
                                                end_section=True
                                            )

                                        # Add SHOULD tier rows
                                        for instance in rds_should:
                                            rds_table.add_row(
                                                instance['db_instance_id'],
                                                f"{instance['engine']}/{instance['engine_version'][:5]}",
                                                instance['status'],
                                                f"{instance['cpu_utilization']:.1f}",
                                                f"{instance['avg_connections']:.1f}",
                                                f"{instance['allocated_storage']}GB",
                                                str(instance['score']),
                                                "[yellow]SHOULD[/]",
                                                format_rds_signal_values(instance),
                                                end_section=True
                                            )

                                        # Add COULD tier rows
                                        for instance in rds_could:
                                            rds_table.add_row(
                                                instance['db_instance_id'],
                                                f"{instance['engine']}/{instance['engine_version'][:5]}",
                                                instance['status'],
                                                f"{instance['cpu_utilization']:.1f}",
                                                f"{instance['avg_connections']:.1f}",
                                                f"{instance['allocated_storage']}GB",
                                                str(instance['score']),
                                                "[blue]COULD[/]",
                                                format_rds_signal_values(instance),
                                                end_section=True
                                            )

                                        # Add KEEP tier rows (first 10 only)
                                        keep_display = rds_keep[:10]
                                        for idx, instance in enumerate(keep_display):
                                            is_last = (idx == len(keep_display) - 1) and len(rds_keep) <= 10
                                            rds_table.add_row(
                                                instance['db_instance_id'],
                                                f"{instance['engine']}/{instance['engine_version'][:5]}",
                                                instance['status'],
                                                f"{instance['cpu_utilization']:.1f}",
                                                f"{instance['avg_connections']:.1f}",
                                                f"{instance['allocated_storage']}GB",
                                                str(instance['score']),
                                                "[green]KEEP[/]",
                                                format_rds_signal_values(instance),
                                                end_section=is_last
                                            )

                                        if len(rds_keep) > 10:
                                            rds_table.add_row(
                                                f"... ({len(rds_keep) - 10} more KEEP instances)",
                                                "", "", "", "", "", "", "", "",
                                                style="dim"
                                            )

                                        rds_branch.add(rds_table)

                                        # RDS summary
                                        rds_branch.add(
                                            f"[bold]Summary:[/] "
                                            f"{len(rds_must)} MUST + {len(rds_should)} SHOULD + "
                                            f"{len(rds_could)} COULD + {len(rds_keep)} KEEP + "
                                            f"{len(enriched_rds)} total RDS instances"
                                        )

                                        # Store counts for results
                                        rds_decommission = len(rds_must)
                                        rds_investigate = len(rds_should)
                                    else:
                                        rds_branch.add("[dim]No RDS instances available for activity analysis[/]")

                                except Exception as e:
                                    rds_branch.add(f"[yellow]âš ï¸  RDS analysis failed: {str(e)[:100]}[/]")
                                    console.print(f"[dim]RDS enrichment error: {e}[/dim]")
                                    logger.warning(f"RDS activity enrichment failed: {e}")
                            elif show_empty:
                                # Only show empty RDS branch when troubleshooting (--show-empty flag)
                                rds_branch = tree.add("ðŸ—„ï¸  RDS Database Activity (R1-R7)")
                                rds_branch.add("[dim]No RDS instances found for activity analysis[/]")

                            # ALB/NLB Activity Analysis Integration (L1-L5 signals)
                            if 'alb' in discovery_results and not discovery_results['alb'].empty:
                                from runbooks.inventory.enrichers.alb_activity_enricher import ALBActivityEnricher
                                from runbooks.finops.decommission_scorer import calculate_alb_score, DEFAULT_ALB_WEIGHTS
                                from rich.table import Table

                                alb_count = len(discovery_results['alb'])
                                alb_branch = tree.add(f"ðŸŒ ALB/NLB Load Balancers ({alb_count} discovered)")

                                try:
                                    # Initialize ALB activity enricher
                                    alb_enricher = ALBActivityEnricher(
                                        operational_profile=profile,
                                        region='ap-southeast-2',
                                        lookback_days=90
                                    )

                                    # Enrich ALB/NLB instances with L1-L5 signals
                                    lb_instances = discovery_results['alb']

                                    # Analyze each load balancer
                                    enriched_albs = []
                                    alb_must = []
                                    alb_should = []
                                    alb_could = []
                                    alb_keep = []

                                    for idx, row in lb_instances.iterrows():
                                        lb_name = row.get('lb_name')
                                        lb_arn = row.get('lb_arn')

                                        # Use enricher to get CloudWatch metrics
                                        # Create minimal DataFrame for enrichment
                                        import pandas as pd
                                        lb_df = pd.DataFrame([{
                                            'lb_name': lb_name,
                                            'lb_arn': lb_arn,
                                            'lb_type': row.get('lb_type', 'application')
                                        }])

                                        try:
                                            enriched_df = alb_enricher.enrich_alb_activity(lb_df)

                                            if not enriched_df.empty:
                                                enriched_row = enriched_df.iloc[0]

                                                # Extract L1-L5 signals from enriched data
                                                signals = {
                                                    'L1': enriched_row.get('l1_signal', 0) * DEFAULT_ALB_WEIGHTS['L1'],
                                                    'L2': enriched_row.get('l2_signal', 0) * DEFAULT_ALB_WEIGHTS['L2'],
                                                    'L3': enriched_row.get('l3_signal', 0) * DEFAULT_ALB_WEIGHTS['L3'],
                                                    'L4': enriched_row.get('l4_signal', 0) * DEFAULT_ALB_WEIGHTS['L4'],
                                                    'L5': enriched_row.get('l5_signal', 0) * DEFAULT_ALB_WEIGHTS['L5']
                                                }

                                                # Calculate decommission score
                                                score_result = calculate_alb_score(signals)
                                                tier = score_result['tier']

                                                # Extract metrics for display
                                                active_connections = enriched_row.get('active_connection_count_90d', 0)
                                                request_count = enriched_row.get('request_count_90d', 0)
                                                healthy_targets = enriched_row.get('healthy_host_count_avg', 0)
                                                data_bytes = enriched_row.get('data_processed_bytes_90d', 0)
                                                error_4xx = enriched_row.get('http_code_4xx_count', 0)
                                                error_5xx = enriched_row.get('http_code_5xx_count', 0)

                                                instance_info = {
                                                    'lb_name': lb_name,
                                                    'lb_type': row.get('lb_type', 'application'),
                                                    'state': row.get('state', 'unknown'),
                                                    'age_days': row.get('age_days', 0),
                                                    'active_connections': int(active_connections),
                                                    'request_count': int(request_count),
                                                    'healthy_targets': round(healthy_targets, 1),
                                                    'data_bytes': int(data_bytes),
                                                    'error_4xx': int(error_4xx),
                                                    'error_5xx': int(error_5xx),
                                                    'score': score_result['total_score'],
                                                    'tier': tier,
                                                    'signals': signals
                                                }

                                                enriched_albs.append(instance_info)

                                                # Categorize by tier
                                                if tier == 'MUST':
                                                    alb_must.append(instance_info)
                                                elif tier == 'SHOULD':
                                                    alb_should.append(instance_info)
                                                elif tier == 'COULD':
                                                    alb_could.append(instance_info)
                                                else:
                                                    alb_keep.append(instance_info)

                                        except Exception as e:
                                            logger.warning(f"ALB enrichment failed for {lb_name}: {e}")
                                            continue

                                    if enriched_albs:
                                        # Create per-LB detailed table
                                        alb_table = Table(
                                            title=None,
                                            show_header=True,
                                            header_style="bold cyan",
                                            border_style="dim"
                                        )

                                        alb_table.add_column("Load Balancer", style="white", no_wrap=True, width=25)
                                        alb_table.add_column("Type", style="yellow", width=8)
                                        alb_table.add_column("State", style="cyan", width=10)
                                        alb_table.add_column("Connections", justify="right", width=12)
                                        alb_table.add_column("Requests", justify="right", width=10)
                                        alb_table.add_column("Targets", justify="right", width=8)
                                        alb_table.add_column("Score", justify="right", style="bold", width=6)
                                        alb_table.add_column("Tier", style="bold", width=8)
                                        alb_table.add_column("Signal Details", style="dim", no_wrap=False)

                                        # Helper function for L1-L5 signal display
                                        def format_alb_signal_values(instance):
                                            """Format L1-L5 signal values for display"""
                                            signal_parts = []
                                            signals = instance['signals']

                                            if signals.get('L1', 0) > 0:
                                                signal_parts.append(f"L1:ZeroConn")
                                            if signals.get('L2', 0) > 0:
                                                signal_parts.append(f"L2:{instance['request_count']}req")
                                            if signals.get('L3', 0) > 0:
                                                signal_parts.append(f"L3:{instance['active_connections']}conn")
                                            if signals.get('L4', 0) > 0:
                                                mb_transferred = instance['data_bytes'] / (1024 * 1024)
                                                signal_parts.append(f"L4:{mb_transferred:.0f}MB")
                                            if signals.get('L5', 0) > 0:
                                                total_errors = instance['error_4xx'] + instance['error_5xx']
                                                signal_parts.append(f"L5:{total_errors}err")

                                            return ', '.join(signal_parts) if signal_parts else 'None'

                                        # Sort by tier priority: MUST â†’ SHOULD â†’ COULD â†’ KEEP
                                        all_lbs = alb_must + alb_should + alb_could + alb_keep[:10]

                                        # Add MUST tier rows
                                        for instance in alb_must:
                                            alb_table.add_row(
                                                instance['lb_name'],
                                                instance['lb_type'].upper()[:3],
                                                instance['state'],
                                                f"{instance['active_connections']:,}",
                                                f"{instance['request_count']:,}",
                                                f"{instance['healthy_targets']:.1f}",
                                                str(instance['score']),
                                                "[red]MUST[/]",
                                                format_alb_signal_values(instance),
                                                end_section=True
                                            )

                                        # Add SHOULD tier rows
                                        for instance in alb_should:
                                            alb_table.add_row(
                                                instance['lb_name'],
                                                instance['lb_type'].upper()[:3],
                                                instance['state'],
                                                f"{instance['active_connections']:,}",
                                                f"{instance['request_count']:,}",
                                                f"{instance['healthy_targets']:.1f}",
                                                str(instance['score']),
                                                "[yellow]SHOULD[/]",
                                                format_alb_signal_values(instance),
                                                end_section=True
                                            )

                                        # Add COULD tier rows
                                        for instance in alb_could:
                                            alb_table.add_row(
                                                instance['lb_name'],
                                                instance['lb_type'].upper()[:3],
                                                instance['state'],
                                                f"{instance['active_connections']:,}",
                                                f"{instance['request_count']:,}",
                                                f"{instance['healthy_targets']:.1f}",
                                                str(instance['score']),
                                                "[blue]COULD[/]",
                                                format_alb_signal_values(instance),
                                                end_section=True
                                            )

                                        # Add KEEP tier rows (first 10 only)
                                        keep_display = alb_keep[:10]
                                        for idx, instance in enumerate(keep_display):
                                            is_last = (idx == len(keep_display) - 1) and len(alb_keep) <= 10
                                            alb_table.add_row(
                                                instance['lb_name'],
                                                instance['lb_type'].upper()[:3],
                                                instance['state'],
                                                f"{instance['active_connections']:,}",
                                                f"{instance['request_count']:,}",
                                                f"{instance['healthy_targets']:.1f}",
                                                str(instance['score']),
                                                "[green]KEEP[/]",
                                                format_alb_signal_values(instance),
                                                end_section=is_last
                                            )

                                        if len(alb_keep) > 10:
                                            alb_table.add_row(
                                                f"... ({len(alb_keep) - 10} more KEEP instances)",
                                                "", "", "", "", "", "", "", "",
                                                style="dim"
                                            )

                                        alb_branch.add(alb_table)

                                        # ALB/NLB Signal Legend (compact single-line format)
                                        alb_branch.add(
                                            "[bold cyan]ðŸ“Š Signal Legend:[/] "
                                            "[dim]A1: Zero connections 90d (45pts) | A2: Low requests <100/day (25pts) | "
                                            "A3: No healthy targets (15pts) | A4: Low data <1GB/day (10pts) | A5: High errors >10% (5pts)[/]"
                                        )

                                        # ALB summary
                                        alb_branch.add(
                                            f"[bold]Summary:[/] "
                                            f"{len(alb_must)} MUST + {len(alb_should)} SHOULD + "
                                            f"{len(alb_could)} COULD + {len(alb_keep)} KEEP = "
                                            f"{len(enriched_albs)} total load balancers"
                                        )

                                        # Store counts for results
                                        alb_decommission = len(alb_must)
                                        alb_investigate = len(alb_should)
                                    else:
                                        alb_branch.add("[dim]No load balancers available for activity analysis[/]")

                                except Exception as e:
                                    alb_branch.add(f"[yellow]âš ï¸  ALB analysis failed: {str(e)[:100]}[/]")
                                    console.print(f"[dim]ALB enrichment error: {e}[/dim]")
                                    logger.warning(f"ALB activity enrichment failed: {e}")
                            elif show_empty:
                                # Only show empty ALB branch when troubleshooting (--show-empty flag)
                                alb_branch = tree.add("ðŸŒ (A1-A5 signals)")
                                alb_branch.add("[dim]No load balancers found for activity analysis[/]")

                            # ASG Activity Analysis Integration (A1-A5 signals)
                            if 'asg' in discovery_results and not discovery_results['asg'].empty:
                                from runbooks.finops.asg_activity_enricher import ASGActivityEnricher
                                from runbooks.finops.decommission_scorer import calculate_asg_score, DEFAULT_ASG_WEIGHTS
                                from rich.table import Table

                                asg_count = len(discovery_results['asg'])
                                asg_branch = tree.add(f"âš™ï¸  Auto Scaling Group Activity ({asg_count} discovered)")

                                try:
                                    # Initialize ASG activity enricher
                                    asg_enricher = ASGActivityEnricher(
                                        operational_profile=profile,
                                        region='ap-southeast-2'
                                    )

                                    # Enrich ASG instances with A1-A5 signals
                                    asg_instances = discovery_results['asg']

                                    # Analyze each auto scaling group
                                    enriched_asgs = []
                                    for idx, row in asg_instances.iterrows():
                                        asg_name = row.get('asg_name')

                                        try:
                                            # Get A1-A5 activity signals (enricher provides boolean signals)
                                            # Since enricher operates on DataFrame, create single-row DataFrame
                                            single_asg_df = pd.DataFrame([row])
                                            enriched_single = asg_enricher.enrich_asg_activity(single_asg_df)
                                            enriched_row = enriched_single.iloc[0]

                                            # Extract signal values from enricher columns
                                            signals = {
                                                'A1': DEFAULT_ASG_WEIGHTS['A1'] if enriched_row.get('a1_signal', False) else 0,
                                                'A2': DEFAULT_ASG_WEIGHTS['A2'] if enriched_row.get('a2_signal', False) else 0,
                                                'A3': DEFAULT_ASG_WEIGHTS['A3'] if enriched_row.get('a3_signal', False) else 0,
                                                'A4': DEFAULT_ASG_WEIGHTS['A4'] if enriched_row.get('a4_signal', False) else 0,
                                                'A5': DEFAULT_ASG_WEIGHTS['A5'] if enriched_row.get('a5_signal', False) else 0
                                            }

                                            # Calculate decommission score
                                            score_result = calculate_asg_score(signals)
                                            tier = score_result['tier']

                                            enriched_asgs.append({
                                                'asg_name': asg_name,
                                                'desired_capacity': enriched_row.get('desired_capacity', row.get('desired_capacity', 0)),
                                                'min_size': enriched_row.get('min_size', row.get('min_size', 0)),
                                                'max_size': enriched_row.get('max_size', row.get('max_size', 0)),
                                                'instance_count': enriched_row.get('instance_count', row.get('instance_count', 0)),
                                                'age_days': enriched_row.get('age_days', row.get('age_days', 0)),
                                                'scaling_events': enriched_row.get('scaling_activity_count_90d', 0),
                                                'unhealthy_pct': enriched_row.get('unhealthy_instance_pct', 0.0),
                                                'capacity_delta_pct': enriched_row.get('desired_vs_actual_delta_pct', 0.0),
                                                'launch_config_age': enriched_row.get('launch_config_age_days', 0),
                                                'score': score_result['total_score'],
                                                'tier': tier,
                                                'signals': signals,
                                                'monthly_cost': enriched_row.get('total_asg_cost_monthly', 0.0)
                                            })
                                        except Exception as e:
                                            logger.warning(f"ASG enrichment failed for {asg_name}: {e}")
                                            continue

                                    if enriched_asgs:
                                        # Separate by tier
                                        asg_must = [asg for asg in enriched_asgs if asg['tier'] == 'MUST']
                                        asg_should = [asg for asg in enriched_asgs if asg['tier'] == 'SHOULD']
                                        asg_could = [asg for asg in enriched_asgs if asg['tier'] == 'COULD']
                                        asg_keep = [asg for asg in enriched_asgs if asg['tier'] == 'KEEP']

                                        # Create per-ASG detailed table
                                        asg_table = Table(show_header=True, header_style="bold cyan", border_style="dim", padding=(0, 1))

                                        asg_table.add_column("ASG Name", style="white", no_wrap=True, width=30)
                                        asg_table.add_column("Capacity", style="yellow", width=12)
                                        asg_table.add_column("Instances", justify="right", width=10)
                                        asg_table.add_column("Scaling", justify="right", width=9)
                                        asg_table.add_column("Health", justify="right", width=8)
                                        asg_table.add_column("Age", justify="right", width=7)
                                        asg_table.add_column("Score", justify="right", style="bold", width=6)
                                        asg_table.add_column("Tier", style="bold", width=8)
                                        asg_table.add_column("Signal Details", style="dim", no_wrap=False)

                                        # Helper function for A1-A5 signal display with VALUES
                                        def format_asg_signal_values(instance):
                                            """Format A1-A5 signal values for display"""
                                            signal_parts = []
                                            signals = instance['signals']

                                            if signals.get('A1', 0) > 0:
                                                signal_parts.append(f"A1:0events")
                                            if signals.get('A2', 0) > 0:
                                                signal_parts.append(f"A2:{instance['unhealthy_pct']:.1f}%")
                                            if signals.get('A3', 0) > 0:
                                                signal_parts.append(f"A3:Î”{instance['capacity_delta_pct']:.1f}%")
                                            if signals.get('A4', 0) > 0:
                                                signal_parts.append(f"A4:{instance['launch_config_age']}d")
                                            if signals.get('A5', 0) > 0:
                                                signal_parts.append(f"A5:HighCost")

                                            return ', '.join(signal_parts) if signal_parts else 'None'

                                        # Add MUST tier rows
                                        for instance in asg_must:
                                            capacity_str = f"{instance['min_size']}-{instance['desired_capacity']}-{instance['max_size']}"

                                            asg_table.add_row(
                                                instance['asg_name'],
                                                capacity_str,
                                                f"{instance['instance_count']}",
                                                f"{instance['scaling_events']}",
                                                f"{instance['unhealthy_pct']:.1f}%",
                                                f"{instance['age_days']}d",
                                                str(instance['score']),
                                                "[red]MUST[/]",
                                                format_asg_signal_values(instance),
                                                end_section=True
                                            )

                                        # Add SHOULD tier rows
                                        for instance in asg_should:
                                            capacity_str = f"{instance['min_size']}-{instance['desired_capacity']}-{instance['max_size']}"

                                            asg_table.add_row(
                                                instance['asg_name'],
                                                capacity_str,
                                                f"{instance['instance_count']}",
                                                f"{instance['scaling_events']}",
                                                f"{instance['unhealthy_pct']:.1f}%",
                                                f"{instance['age_days']}d",
                                                str(instance['score']),
                                                "[yellow]SHOULD[/]",
                                                format_asg_signal_values(instance),
                                                end_section=True
                                            )

                                        # Add COULD tier rows
                                        for instance in asg_could:
                                            capacity_str = f"{instance['min_size']}-{instance['desired_capacity']}-{instance['max_size']}"

                                            asg_table.add_row(
                                                instance['asg_name'],
                                                capacity_str,
                                                f"{instance['instance_count']}",
                                                f"{instance['scaling_events']}",
                                                f"{instance['unhealthy_pct']:.1f}%",
                                                f"{instance['age_days']}d",
                                                str(instance['score']),
                                                "[blue]COULD[/]",
                                                format_asg_signal_values(instance),
                                                end_section=True
                                            )

                                        # Add KEEP tier rows (first 10 only)
                                        keep_display = asg_keep[:10]
                                        for idx, instance in enumerate(keep_display):
                                            capacity_str = f"{instance['min_size']}-{instance['desired_capacity']}-{instance['max_size']}"
                                            is_last = (idx == len(keep_display) - 1) and len(asg_keep) <= 10

                                            asg_table.add_row(
                                                instance['asg_name'],
                                                capacity_str,
                                                f"{instance['instance_count']}",
                                                f"{instance['scaling_events']}",
                                                f"{instance['unhealthy_pct']:.1f}%",
                                                f"{instance['age_days']}d",
                                                str(instance['score']),
                                                "[green]KEEP[/]",
                                                format_asg_signal_values(instance),
                                                end_section=is_last
                                            )

                                        if len(asg_keep) > 10:
                                            asg_table.add_row(
                                                f"... ({len(asg_keep) - 10} more KEEP instances)",
                                                "", "", "", "", "", "", "", "",
                                                style="dim"
                                            )

                                        asg_branch.add(asg_table)

                                        # ASG summary
                                        asg_branch.add(
                                            f"[bold]Summary:[/] "
                                            f"{len(asg_must)} MUST + {len(asg_should)} SHOULD + "
                                            f"{len(asg_could)} COULD + {len(asg_keep)} KEEP = "
                                            f"{len(enriched_asgs)} total auto scaling groups"
                                        )

                                        # Store counts for results
                                        asg_decommission = len(asg_must)
                                        asg_investigate = len(asg_should)
                                    else:
                                        asg_branch.add("[dim]No auto scaling groups available for activity analysis[/]")

                                except Exception as e:
                                    asg_branch.add(f"[yellow]âš ï¸  ASG analysis failed: {str(e)[:100]}[/]")
                                    console.print(f"[dim]ASG enrichment error: {e}[/dim]")
                                    logger.warning(f"ASG activity enrichment failed: {e}")
                            elif show_empty:
                                # Only show empty ASG branch when troubleshooting (--show-empty flag)
                                asg_branch = tree.add("âš™ï¸  Auto Scaling Group Activity (A1-A5)")
                                asg_branch.add("[dim]No auto scaling groups found for activity analysis[/]")

                            # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
                            # ECS Container Service Activity Analysis (C1-C5)
                            # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

                            if 'ecs' in discovery_results and not discovery_results['ecs'].empty:
                                # Initialize ECS branch with count
                                ecs_count = len(discovery_results['ecs'])
                                ecs_branch = tree.add(f"ðŸ³ ECS Container Service Activity ({ecs_count} discovered)")

                            elif show_empty:
                                # Only show empty ECS branch when troubleshooting
                                ecs_branch = tree.add("ðŸ³ ECS Container Service Activity (C1-C5)")

                            if 'ecs' in discovery_results and not discovery_results['ecs'].empty:
                                try:
                                    from runbooks.finops.ecs_activity_enricher import ECSActivityEnricher
                                    from runbooks.finops.decommission_scorer import calculate_ecs_score
                                    from runbooks.common.output_controller import OutputController

                                    # Initialize ECS enricher with verbose mode support
                                    ecs_enricher = ECSActivityEnricher(
                                        operational_profile=profile,
                                        region='ap-southeast-2',
                                        output_controller=OutputController(verbose=verbose)
                                    )

                                    ecs_clusters = discovery_results['ecs']

                                    # Enrich with C1-C5 signals
                                    enriched_ecs = []
                                    for idx, row in ecs_clusters.iterrows():
                                        cluster_name = row.get('cluster_name')
                                        cluster_arn = row.get('cluster_arn')

                                        try:
                                            # Get C1-C5 activity signals (placeholder for now)
                                            # TODO: Implement real CloudWatch metrics retrieval
                                            signals = {
                                                'C1': 0,  # CPU/Memory utilization
                                                'C2': 0,  # Task count trends
                                                'C3': 0,  # Service health
                                                'C4': 0,  # Fargate vs EC2 split
                                                'C5': 0   # Cost efficiency
                                            }

                                            score_result = calculate_ecs_score(signals)
                                            tier = score_result['tier']

                                            enriched_ecs.append({
                                                'cluster_name': cluster_name,
                                                'cluster_arn': cluster_arn,
                                                'status': row.get('status', 'UNKNOWN'),
                                                'active_services': row.get('active_services_count', 0),
                                                'running_tasks': row.get('running_tasks_count', 0),
                                                'container_instances': row.get('registered_container_instances_count', 0),
                                                'cpu_utilization': 0.0,  # From CloudWatch
                                                'memory_utilization': 0.0,  # From CloudWatch
                                                'score': score_result['total_score'],
                                                'tier': tier,
                                                'signals': signals,
                                                'monthly_cost': 0.0
                                            })
                                        except Exception as e:
                                            logger.warning(f"ECS enrichment failed for {cluster_name}: {e}")
                                            continue

                                    if enriched_ecs:
                                        # Tier-based organization
                                        ecs_must = [r for r in enriched_ecs if r['tier'] == 'MUST']
                                        ecs_should = [r for r in enriched_ecs if r['tier'] == 'SHOULD']
                                        ecs_could = [r for r in enriched_ecs if r['tier'] == 'COULD']
                                        ecs_keep = [r for r in enriched_ecs if r['tier'] == 'KEEP']

                                        # Create ECS activity table
                                        ecs_table = Table(
                                            show_header=True,
                                            header_style="bold cyan",
                                            border_style="dim",
                                            box=box.SIMPLE
                                        )

                                        ecs_table.add_column("Cluster Name", style="white", no_wrap=True)
                                        ecs_table.add_column("Status", style="cyan", width=10)
                                        ecs_table.add_column("Services", justify="right", width=10)
                                        ecs_table.add_column("Tasks", justify="right", width=8)
                                        ecs_table.add_column("CPU %", justify="right", width=8)
                                        ecs_table.add_column("Memory %", justify="right", width=10)
                                        ecs_table.add_column("Score", justify="right", style="bold", width=6)
                                        ecs_table.add_column("Tier", style="bold", width=8)
                                        ecs_table.add_column("Signal Details", style="dim", no_wrap=False)

                                        # Helper function for C1-C5 signal display
                                        def format_ecs_signal_values(instance):
                                            """Format C1-C5 signal values for display"""
                                            signal_parts = []
                                            signals = instance['signals']

                                            if signals.get('C1', 0) > 0:
                                                signal_parts.append(f"C1:{instance['cpu_utilization']:.1f}%")
                                            if signals.get('C2', 0) > 0:
                                                signal_parts.append(f"C2:NoTasks")
                                            if signals.get('C3', 0) > 0:
                                                signal_parts.append(f"C3:Unhealthy")
                                            if signals.get('C4', 0) > 0:
                                                signal_parts.append(f"C4:Inefficient")
                                            if signals.get('C5', 0) > 0:
                                                signal_parts.append(f"C5:HighCost")

                                            return ', '.join(signal_parts) if signal_parts else 'None'

                                        # Add MUST tier rows
                                        for instance in ecs_must:
                                            ecs_table.add_row(
                                                instance['cluster_name'],
                                                instance['status'],
                                                f"{instance['active_services']}",
                                                f"{instance['running_tasks']}",
                                                f"{instance['cpu_utilization']:.1f}",
                                                f"{instance['memory_utilization']:.1f}",
                                                str(instance['score']),
                                                "[red]MUST[/]",
                                                format_ecs_signal_values(instance),
                                                end_section=True
                                            )

                                        # Add SHOULD tier rows
                                        for instance in ecs_should:
                                            ecs_table.add_row(
                                                instance['cluster_name'],
                                                instance['status'],
                                                f"{instance['active_services']}",
                                                f"{instance['running_tasks']}",
                                                f"{instance['cpu_utilization']:.1f}",
                                                f"{instance['memory_utilization']:.1f}",
                                                str(instance['score']),
                                                "[yellow]SHOULD[/]",
                                                format_ecs_signal_values(instance),
                                                end_section=True
                                            )

                                        # Add COULD tier rows
                                        for instance in ecs_could:
                                            ecs_table.add_row(
                                                instance['cluster_name'],
                                                instance['status'],
                                                f"{instance['active_services']}",
                                                f"{instance['running_tasks']}",
                                                f"{instance['cpu_utilization']:.1f}",
                                                f"{instance['memory_utilization']:.1f}",
                                                str(instance['score']),
                                                "[blue]COULD[/]",
                                                format_ecs_signal_values(instance),
                                                end_section=True
                                            )

                                        # Add KEEP tier rows (first 10 only)
                                        keep_display = ecs_keep[:10]
                                        for idx, instance in enumerate(keep_display):
                                            is_last = (idx == len(keep_display) - 1) and len(ecs_keep) <= 10

                                            ecs_table.add_row(
                                                instance['cluster_name'],
                                                instance['status'],
                                                f"{instance['active_services']}",
                                                f"{instance['running_tasks']}",
                                                f"{instance['cpu_utilization']:.1f}",
                                                f"{instance['memory_utilization']:.1f}",
                                                str(instance['score']),
                                                "[green]KEEP[/]",
                                                format_ecs_signal_values(instance),
                                                end_section=is_last
                                            )

                                        if len(ecs_keep) > 10:
                                            ecs_table.add_row(
                                                f"... ({len(ecs_keep) - 10} more KEEP clusters)",
                                                "", "", "", "", "", "", "", "",
                                                style="dim"
                                            )

                                        ecs_branch.add(ecs_table)

                                        # ECS summary
                                        ecs_branch.add(
                                            f"[bold]Summary:[/] "
                                            f"{len(ecs_must)} MUST + {len(ecs_should)} SHOULD + "
                                            f"{len(ecs_could)} COULD + {len(ecs_keep)} KEEP = "
                                            f"{len(enriched_ecs)} total ECS clusters"
                                        )

                                        # Store counts for results
                                        ecs_decommission = len(ecs_must)
                                        ecs_investigate = len(ecs_should)
                                    else:
                                        ecs_branch.add("[dim]No ECS clusters available for activity analysis[/]")

                                except Exception as e:
                                    ecs_branch.add(f"[yellow]âš ï¸  ECS analysis failed: {str(e)[:100]}[/]")
                                    console.print(f"[dim]ECS enrichment error: {e}[/dim]")
                                    logger.warning(f"ECS activity enrichment failed: {e}")
                            elif show_empty:
                                # Only show empty ECS branch when troubleshooting (--show-empty flag)
                                ecs_branch.add("[dim]No ECS clusters found for activity analysis[/]")

                            # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
                            # Route53 DNS Activity Analysis (R53-1 to R53-4)
                            # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

                            # Check if Route53 data available
                            if 'route53' in discovery_results and not discovery_results['route53'].empty:
                                route53_count = len(discovery_results['route53'])
                                route53_branch = tree.add(f"ðŸŒ Route53 DNS ({route53_count} discovered)")
                            elif show_empty:
                                route53_branch = tree.add("ðŸŒ Route53 DNS (R53-1 to R53-4)")

                            if 'route53' in discovery_results and not discovery_results['route53'].empty:
                                try:
                                    from runbooks.inventory.enrichers.route53_activity_enricher import Route53ActivityEnricher

                                    # Initialize Route53 enricher
                                    route53_enricher = Route53ActivityEnricher(
                                        operational_profile=profile,
                                        region='us-east-1',  # Route53 API requires us-east-1
                                        lookback_days=90
                                    )

                                    route53_df = discovery_results['route53'].copy()

                                    # Enrich with R53-1 to R53-4 signals
                                    enriched_route53_df = route53_enricher.enrich_route53_activity(route53_df)

                                    # Organize by decommission tier
                                    route53_must = []
                                    route53_should = []
                                    route53_could = []
                                    route53_keep = []

                                    for idx, row in enriched_route53_df.iterrows():
                                        zone_data = {
                                            'zone_name': row.get('name', 'N/A'),
                                            'zone_id': row.get('hosted_zone_id', 'N/A'),
                                            'is_private': row.get('is_private', False),
                                            'record_count': row.get('record_set_count', 0),
                                            'dns_queries': row.get('dns_queries_90d', 0),
                                            'health_checks': row.get('health_check_count', 0),
                                            'health_checks_active': row.get('health_check_active', 0),
                                            'score': row.get('decommission_score', 0),
                                            'tier': row.get('decommission_tier', 'KEEP'),
                                            'signals': {
                                                'R53_1': 50 if row.get('r53_1_signal', False) else 0,
                                                'R53_2': 30 if row.get('r53_2_signal', False) else 0,
                                                'R53_3': 15 if row.get('r53_3_signal', False) else 0,
                                                'R53_4': 5 if row.get('r53_4_signal', False) else 0
                                            }
                                        }

                                        tier = zone_data['tier']
                                        if tier == 'MUST':
                                            route53_must.append(zone_data)
                                        elif tier == 'SHOULD':
                                            route53_should.append(zone_data)
                                        elif tier == 'COULD':
                                            route53_could.append(zone_data)
                                        else:
                                            route53_keep.append(zone_data)

                                    # Helper function for R53-1 to R53-4 signal display
                                    def format_route53_signal_values(instance):
                                        """Format R53-1 to R53-4 signal values for display"""
                                        signal_parts = []
                                        signals = instance['signals']

                                        if signals.get('R53_1', 0) > 0:
                                            signal_parts.append("R53-1")
                                        if signals.get('R53_2', 0) > 0:
                                            avg_queries = instance['dns_queries'] / 90 if instance['dns_queries'] > 0 else 0
                                            signal_parts.append(f"R53-2")
                                        if signals.get('R53_3', 0) > 0:
                                            signal_parts.append(f"R53-3:{instance['record_count']}records")
                                        if signals.get('R53_4', 0) > 0:
                                            signal_parts.append("R53-4:NoHealthChecks")

                                        return ', '.join(signal_parts) if signal_parts else 'None'

                                    # Create per-zone detailed table
                                    if any([route53_must, route53_should, route53_could, route53_keep]):
                                        route53_table = Table(
                                            show_header=True,
                                            header_style="bold cyan",
                                            border_style="dim",
                                            box=box.ROUNDED,
                                            show_edge=True,
                                            pad_edge=False
                                        )

                                        # Optimized column widths for better content fit
                                        route53_table.add_column("Hosted Zone", style="white", no_wrap=False, width=28)
                                        route53_table.add_column("Type", style="yellow", width=8)
                                        route53_table.add_column("Records", justify="right", width=8)
                                        route53_table.add_column("Queries/day", justify="right", width=12)
                                        route53_table.add_column("Cost/mo", justify="right", style="cyan", width=10)
                                        route53_table.add_column("Health Checks", justify="right", width=12)
                                        route53_table.add_column("Score", justify="right", style="bold", width=6)
                                        route53_table.add_column("Tier", style="bold", width=10)
                                        route53_table.add_column("Signal Details", style="dim", no_wrap=False, width=35)

                                        # MUST tier (80-100) - Red alert
                                        for instance in route53_must:
                                            zone_type = "Private" if instance['is_private'] else "Public"
                                            avg_queries = instance['dns_queries'] / 90 if instance['dns_queries'] > 0 else 0

                                            # Calculate monthly cost: $0.50/zone + $0.40/million queries
                                            hosted_zone_cost = 0.50
                                            query_cost = (instance['dns_queries'] * 30 / 90) * (0.40 / 1_000_000) if instance['dns_queries'] > 0 else 0
                                            monthly_cost = hosted_zone_cost + query_cost
                                            cost_display = f"${monthly_cost:.2f}"

                                            route53_table.add_row(
                                                instance['zone_name'],
                                                zone_type,
                                                f"{instance['record_count']}",
                                                f"{avg_queries:.0f}",
                                                cost_display,
                                                f"{instance['health_checks_active']}/{instance['health_checks']}",
                                                str(instance['score']),
                                                "[red]MUST[/]",
                                                format_route53_signal_values(instance)
                                            )

                                        # SHOULD tier (50-79) - Yellow warning
                                        for instance in route53_should:
                                            zone_type = "Private" if instance['is_private'] else "Public"
                                            avg_queries = instance['dns_queries'] / 90 if instance['dns_queries'] > 0 else 0

                                            # Calculate monthly cost: $0.50/zone + $0.40/million queries
                                            hosted_zone_cost = 0.50
                                            query_cost = (instance['dns_queries'] * 30 / 90) * (0.40 / 1_000_000) if instance['dns_queries'] > 0 else 0
                                            monthly_cost = hosted_zone_cost + query_cost
                                            cost_display = f"${monthly_cost:.2f}"

                                            route53_table.add_row(
                                                instance['zone_name'],
                                                zone_type,
                                                f"{instance['record_count']}",
                                                f"{avg_queries:.0f}",
                                                cost_display,
                                                f"{instance['health_checks_active']}/{instance['health_checks']}",
                                                str(instance['score']),
                                                "[yellow]SHOULD[/]",
                                                format_route53_signal_values(instance)
                                            )

                                        # COULD tier (25-49) - Blue consideration
                                        for instance in route53_could:
                                            zone_type = "Private" if instance['is_private'] else "Public"
                                            avg_queries = instance['dns_queries'] / 90 if instance['dns_queries'] > 0 else 0

                                            # Calculate monthly cost: $0.50/zone + $0.40/million queries
                                            hosted_zone_cost = 0.50
                                            query_cost = (instance['dns_queries'] * 30 / 90) * (0.40 / 1_000_000) if instance['dns_queries'] > 0 else 0
                                            monthly_cost = hosted_zone_cost + query_cost
                                            cost_display = f"${monthly_cost:.2f}"

                                            route53_table.add_row(
                                                instance['zone_name'],
                                                zone_type,
                                                f"{instance['record_count']}",
                                                f"{avg_queries:.0f}",
                                                cost_display,
                                                f"{instance['health_checks_active']}/{instance['health_checks']}",
                                                str(instance['score']),
                                                "[blue]COULD[/]",
                                                format_route53_signal_values(instance)
                                            )

                                        # KEEP tier (<25) - Green active (show first 10)
                                        if route53_keep:
                                            route53_keep_display = route53_keep[:10]
                                            for idx, instance in enumerate(route53_keep_display):
                                                zone_type = "Private" if instance['is_private'] else "Public"
                                                avg_queries = instance['dns_queries'] / 90 if instance['dns_queries'] > 0 else 0
                                                is_last = (idx == len(route53_keep_display) - 1) and len(route53_keep) <= 10

                                                # Calculate monthly cost: $0.50/zone + $0.40/million queries
                                                hosted_zone_cost = 0.50
                                                query_cost = (instance['dns_queries'] * 30 / 90) * (0.40 / 1_000_000) if instance['dns_queries'] > 0 else 0
                                                monthly_cost = hosted_zone_cost + query_cost
                                                cost_display = f"${monthly_cost:.2f}"

                                                route53_table.add_row(
                                                    instance['zone_name'],
                                                    zone_type,
                                                    f"{instance['record_count']}",
                                                    f"{avg_queries:.0f}",
                                                    cost_display,
                                                    f"{instance['health_checks_active']}/{instance['health_checks']}",
                                                    str(instance['score']),
                                                    "[green]KEEP[/]",
                                                    format_route53_signal_values(instance),
                                                    end_section=is_last
                                                )

                                            if len(route53_keep) > 10:
                                                route53_table.add_row(
                                                    f"... ({len(route53_keep) - 10} more KEEP zones)",
                                                    "-", "-", "-", "-", "-", "-", "-",
                                                    style="dim"
                                                )

                                        route53_branch.add(route53_table)

                                        # Route53 Signal Legend (ultra-compressed)
                                        route53_branch.add(
                                            "[bold cyan]ðŸ“Š Signal Legend:[/] "
                                            "[dim]R53-1:Zero Queries (50) | R53-2:<100/day (30) | R53-3:0 records (15) | R53-4:Inactive HC (5)[/]"
                                        )

                                        # Route53 summary
                                        route53_branch.add(
                                            f"[bold]Summary:[/] "
                                            f"{len(route53_must)} MUST + {len(route53_should)} SHOULD + "
                                            f"{len(route53_could)} COULD + {len(route53_keep)} KEEP = "
                                            f"{len(enriched_route53_df)} total Route53 hosted zones"
                                        )
                                    else:
                                        route53_branch.add("[dim]No Route53 hosted zones available for activity analysis[/]")

                                except Exception as e:
                                    route53_branch.add(f"[yellow]âš ï¸  Route53 analysis failed: {str(e)[:100]}[/]")
                                    console.print(f"[dim]Route53 enrichment error: {e}[/dim]")
                                    logger.warning(f"Route53 activity enrichment failed: {e}")
                            elif show_empty:
                                # Only show empty Route53 branch when troubleshooting (--show-empty flag)
                                route53_branch.add("[dim]No Route53 hosted zones found for activity analysis[/]")

                            # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
                            # VPC Resources Activity Analysis (V1-V5)
                            # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

                            # Check if VPC data available in enriched results
                            if 'vpc' in enriched and not enriched['vpc'].empty:
                                vpc_df = enriched['vpc']
                                vpc_count = len(vpc_df)

                                # Calculate tier counts if decommission_tier column exists
                                if 'decommission_tier' in vpc_df.columns:
                                    vpc_tier_counts = vpc_df['decommission_tier'].value_counts().to_dict()

                                    vpc_must = vpc_tier_counts.get('MUST', 0)
                                    vpc_should = vpc_tier_counts.get('SHOULD', 0)
                                    vpc_could = vpc_tier_counts.get('COULD', 0)
                                    vpc_keep = vpc_tier_counts.get('KEEP', 0)
                                    vpc_unknown = vpc_tier_counts.get('UNKNOWN', 0)

                                    # VPC branch with icon and resource count
                                    vpc_branch = tree.add(
                                        f"ðŸ”— VPC Resources ({vpc_count} discovered)",
                                        style="bold cyan"
                                    )

                                    # Create detailed VPC resources table (matching EC2/S3 pattern)
                                    from rich.table import Table
                                    import pandas as pd

                                    vpc_table = Table(
                                        title=None,
                                        show_header=True,
                                        header_style="bold cyan",
                                        border_style="dim"
                                    )

                                    # Table columns (optimized widths for better content fit)
                                    vpc_table.add_column("Resource ID", style="white", no_wrap=True, width=25)
                                    vpc_table.add_column("Type", style="yellow", width=8)
                                    vpc_table.add_column("VPC ID", style="cyan", width=15)
                                    vpc_table.add_column("State", style="white", width=10)
                                    vpc_table.add_column("Data (90d)", justify="right", width=12)
                                    vpc_table.add_column("Cost/mo", justify="right", style="cyan", width=10)
                                    vpc_table.add_column("Score", justify="right", style="bold", width=6)
                                    vpc_table.add_column("Tier", style="bold", width=10)
                                    vpc_table.add_column("Signals", style="dim", no_wrap=False, width=22)

                                    # Sort by tier (MUST â†’ SHOULD â†’ COULD â†’ KEEP â†’ UNKNOWN)
                                    vpc_sorted = vpc_df.sort_values(
                                        by='decommission_tier',
                                        key=lambda x: x.map({'MUST': 0, 'SHOULD': 1, 'COULD': 2, 'KEEP': 3, 'UNKNOWN': 4})
                                    )

                                    # Add rows for each resource
                                    for idx, row in vpc_sorted.iterrows():
                                        resource_id = row.get('resource_id', 'N/A')
                                        resource_type_raw = row.get('resource_type', 'unknown')

                                        # Format resource type for display
                                        type_display = {
                                            'vpce': 'VPCE',
                                            'vpc_peering': 'Peering',
                                            'transit_gateway': 'TGW',
                                            'nat_gateway': 'NAT'
                                        }.get(resource_type_raw, resource_type_raw.upper())

                                        vpc_id = row.get('vpc_id', 'N/A')
                                        state = row.get('state', 'N/A')

                                        # Calculate data transfer (90 days)
                                        if resource_type_raw == 'nat_gateway':
                                            data_bytes = row.get('bytes_out_90d', 0)
                                        else:
                                            data_bytes = row.get('bytes_in_90d', 0) + row.get('bytes_out_90d', 0)

                                        data_gb = f"{data_bytes / 1e9:.2f} GB" if pd.notna(data_bytes) and data_bytes > 0 else "0.00 GB"

                                        # Score and tier
                                        score = int(row.get('decommission_score', 0))
                                        tier = row.get('decommission_tier', 'UNKNOWN')
                                        tier_colors = {
                                            'MUST': 'red',
                                            'SHOULD': 'yellow',
                                            'COULD': 'blue',
                                            'KEEP': 'green',
                                            'UNKNOWN': 'dim'
                                        }
                                        tier_color = tier_colors.get(tier, 'white')

                                        # Build signal summary
                                        signals = []
                                        if resource_type_raw == 'nat_gateway':
                                            # NAT Gateway signals: N1-N5
                                            for i in range(1, 6):
                                                sig_col = f'n{i}_signal'
                                                if row.get(sig_col, 0) > 0:
                                                    signals.append(f'N{i}')
                                        else:
                                            # VPC resource signals: V1-V5
                                            for i in range(1, 6):
                                                sig_col = f'v{i}_signal'
                                                if row.get(sig_col, 0) > 0:
                                                    signals.append(f'V{i}')

                                        signal_summary = ', '.join(signals) if signals else '-'

                                        # Estimate monthly cost based on resource type
                                        # Note: These are estimates based on AWS pricing (us-east-1)
                                        # Actual costs vary by region and usage patterns
                                        if resource_type_raw == 'nat_gateway':
                                            # NAT Gateway: $0.045/hour (~$32.40/mo) + $0.045/GB processed
                                            hourly_cost = 32.40  # Base cost per month
                                            data_gb_val = data_bytes / 1e9 if pd.notna(data_bytes) else 0
                                            data_cost = (data_gb_val * 0.045) if data_gb_val > 0 else 0
                                            monthly_cost = hourly_cost + data_cost
                                            cost_display = f"${monthly_cost:.2f}"
                                        elif resource_type_raw == 'vpce':
                                            # VPC Endpoint: $0.01/hour (~$7.20/mo) + $0.01/GB
                                            monthly_cost = 7.20
                                            cost_display = f"${monthly_cost:.2f}"
                                        elif resource_type_raw == 'transit_gateway':
                                            # Transit Gateway Attachment: $0.05/hour (~$36/mo)
                                            monthly_cost = 36.00
                                            cost_display = f"${monthly_cost:.2f}"
                                        elif resource_type_raw == 'vpc_peering':
                                            # VPC Peering: No direct cost (data transfer charges apply)
                                            cost_display = "$0.00"
                                        else:
                                            cost_display = "N/A"

                                        # Add row to table
                                        vpc_table.add_row(
                                            resource_id,
                                            type_display,
                                            vpc_id,
                                            state,
                                            data_gb,
                                            cost_display,
                                            str(score),
                                            f"[{tier_color}]{tier}[/{tier_color}]",
                                            signal_summary
                                        )

                                    # Add table to VPC branch
                                    vpc_branch.add(vpc_table)

                                    # Signal legend (ultra-compressed for terminal compatibility)
                                    vpc_branch.add(
                                        "[bold]ðŸ“Š Signal Legend:[/] "
                                        "[dim]V1-5:Xfer/Deps/Intf/Env/Age (40/20/10/5/25) | N1-5:Bytes/Idle/Conn/EIP/Env (40/25/15/10/10)[/]"
                                    )

                                    # Summary statistics
                                    vpc_branch.add(
                                        f"[bold]Summary:[/] "
                                        f"{vpc_must} MUST + {vpc_should} SHOULD + {vpc_could} COULD + "
                                        f"{vpc_keep} KEEP + {vpc_unknown} UNKNOWN = {vpc_count} total VPC resources"
                                    )
                                else:
                                    # VPC data available but no decommission tier
                                    vpc_branch = tree.add(
                                        f"ðŸ”— VPC Resources ({vpc_count} discovered)",
                                        style="bold cyan"
                                    )
                                    vpc_branch.add("[dim]Decommission scoring not available[/dim]")
                            elif show_empty:
                                # Only show empty VPC branch when troubleshooting (--show-empty flag)
                                vpc_branch = tree.add("ðŸ”— VPC Resources")
                                vpc_branch.add("[dim]No VPC resources found for activity analysis[/dim]")

                            # Direct Connect Activity Branch
                            if 'dx' in discovery_results and not discovery_results['dx'].empty:
                                dx_count = len(discovery_results['dx'])
                                dx_branch = tree.add(f"[cyan]ðŸ”— Direct Connect Connections ({dx_count} discovered)[/]")
                            elif show_empty:
                                dx_branch = tree.add("[cyan]ðŸ”— Direct Connect Connections[/]")

                            if 'dx' in discovery_results and not discovery_results['dx'].empty:
                                try:
                                    from runbooks.inventory.enrichers.dx_activity_enricher import DXActivityEnricher

                                    print_section("ðŸ”— Direct Connect Activity Analysis (DX1-DX4)")

                                    # Initialize DX enricher
                                    dx_enricher = DXActivityEnricher(
                                        operational_profile=profile,
                                        region='ap-southeast-2',
                                        lookback_days=90
                                    )

                                    dx_instances = discovery_results['dx']

                                    # Enrich with DX1-DX4 signals
                                    enriched_dx = []
                                    for idx, row in dx_instances.iterrows():
                                        connection_name = row.get('connection_name', 'N/A')
                                        connection_id = row.get('connection_id')

                                        try:
                                            # Calculate DX signals (placeholder - real enrichment via enricher)
                                            signals = {
                                                'DX1': 0,  # Zero data transfer
                                                'DX2': 0,  # Low utilization
                                                'DX3': 0,  # Connection down
                                                'DX4': 0   # No BGP peers
                                            }

                                            # Calculate score and tier
                                            from runbooks.finops.decommission_scorer import calculate_dx_score
                                            score_result = calculate_dx_score(signals)
                                            tier = score_result['tier']

                                            enriched_dx.append({
                                                'connection_name': connection_name,
                                                'connection_id': connection_id,
                                                'connection_state': row.get('connection_state', 'unknown'),
                                                'bandwidth': row.get('bandwidth', '0Gbps'),
                                                'location': row.get('location', 'N/A'),
                                                'egress_gbps': 0,  # Placeholder
                                                'ingress_gbps': 0,  # Placeholder
                                                'utilization_percent': 0,  # Placeholder
                                                'bgp_peers': 0,  # Placeholder
                                                'score': score_result['total_score'],
                                                'tier': tier,
                                                'signals': signals,
                                                'monthly_cost': 0  # Placeholder
                                            })
                                        except Exception as e:
                                            logger.warning(f"DX enrichment failed for {connection_name}: {e}")
                                            continue

                                    if enriched_dx:
                                        # Create per-connection detailed table
                                        dx_table = Table(
                                            title="Direct Connect Connections",
                                            show_header=True,
                                            header_style="bold cyan",
                                            border_style="dim"
                                        )

                                        dx_table.add_column("Connection Name", style="white", no_wrap=False, width=25)
                                        dx_table.add_column("State", style="cyan", width=12)
                                        dx_table.add_column("Bandwidth", style="yellow", width=10)
                                        dx_table.add_column("Egress GB", justify="right", width=10)
                                        dx_table.add_column("Ingress GB", justify="right", width=11)
                                        dx_table.add_column("Util %", justify="right", width=8)
                                        dx_table.add_column("BGP Peers", justify="right", width=10)
                                        dx_table.add_column("Score", justify="right", style="bold", width=6)
                                        dx_table.add_column("Tier", style="bold", width=8)
                                        dx_table.add_column("Signal Details", style="dim", no_wrap=False)

                                        # Helper function to format DX signal VALUES
                                        def format_dx_signal_values(instance):
                                            """Format DX signals showing actual metric VALUES for enterprise transparency."""
                                            parts = []
                                            signals = instance['signals']

                                            if signals.get('DX1', 0) > 0:
                                                parts.append(f"DX1:0GB (zero transfer)")
                                            if signals.get('DX2', 0) > 0:
                                                util = instance.get('utilization_percent', 0)
                                                parts.append(f"DX2:{util:.1f}% (low util)")
                                            if signals.get('DX3', 0) > 0:
                                                state = instance.get('connection_state', 'unknown')
                                                parts.append(f"DX3:{state} (down)")
                                            if signals.get('DX4', 0) > 0:
                                                peers = instance.get('bgp_peers', 0)
                                                parts.append(f"DX4:{peers} (no peers)")

                                            return ", ".join(parts) if parts else "Active"

                                        # Sort by tier priority
                                        tier_priority = {"MUST": 1, "SHOULD": 2, "COULD": 3, "KEEP": 4}
                                        enriched_dx_sorted = sorted(
                                            enriched_dx,
                                            key=lambda x: (tier_priority.get(x['tier'], 5), -x['score'])
                                        )

                                        # Categorize by tier
                                        dx_must = [c for c in enriched_dx_sorted if c['tier'] == 'MUST']
                                        dx_should = [c for c in enriched_dx_sorted if c['tier'] == 'SHOULD']
                                        dx_could = [c for c in enriched_dx_sorted if c['tier'] == 'COULD']
                                        dx_keep = [c for c in enriched_dx_sorted if c['tier'] == 'KEEP']

                                        # Add rows grouped by tier
                                        # MUST tier (red - critical decommission)
                                        if dx_must:
                                            for i, instance in enumerate(dx_must):
                                                is_last = (i == len(dx_must) - 1) and not (dx_should or dx_could or dx_keep)

                                                state_style = "green" if instance['connection_state'] == 'available' else "red"
                                                util_percent = instance['utilization_percent']
                                                util_style = "green" if util_percent >= 50 else "yellow" if util_percent >= 10 else "red"

                                                dx_table.add_row(
                                                    instance['connection_name'],
                                                    f"[{state_style}]{instance['connection_state']}[/]",
                                                    instance['bandwidth'],
                                                    f"{instance['egress_gbps']:.1f}",
                                                    f"{instance['ingress_gbps']:.1f}",
                                                    f"[{util_style}]{util_percent:.1f}%[/]",
                                                    str(instance['bgp_peers']),
                                                    str(instance['score']),
                                                    "[bold red]MUST[/]",
                                                    format_dx_signal_values(instance),
                                                    end_section=is_last
                                                )

                                        # SHOULD tier (yellow - recommended review)
                                        if dx_should:
                                            for i, instance in enumerate(dx_should):
                                                is_last = (i == len(dx_should) - 1) and not (dx_could or dx_keep)

                                                state_style = "green" if instance['connection_state'] == 'available' else "red"
                                                util_percent = instance['utilization_percent']
                                                util_style = "green" if util_percent >= 50 else "yellow" if util_percent >= 10 else "red"

                                                dx_table.add_row(
                                                    instance['connection_name'],
                                                    f"[{state_style}]{instance['connection_state']}[/]",
                                                    instance['bandwidth'],
                                                    f"{instance['egress_gbps']:.1f}",
                                                    f"{instance['ingress_gbps']:.1f}",
                                                    f"[{util_style}]{util_percent:.1f}%[/]",
                                                    str(instance['bgp_peers']),
                                                    str(instance['score']),
                                                    "[bold yellow]SHOULD[/]",
                                                    format_dx_signal_values(instance),
                                                    end_section=is_last
                                                )

                                        # COULD tier (dim yellow - optional optimization)
                                        if dx_could:
                                            for i, instance in enumerate(dx_could):
                                                is_last = (i == len(dx_could) - 1) and not dx_keep

                                                state_style = "green" if instance['connection_state'] == 'available' else "red"
                                                util_percent = instance['utilization_percent']
                                                util_style = "green" if util_percent >= 50 else "yellow" if util_percent >= 10 else "red"

                                                dx_table.add_row(
                                                    instance['connection_name'],
                                                    f"[{state_style}]{instance['connection_state']}[/]",
                                                    instance['bandwidth'],
                                                    f"{instance['egress_gbps']:.1f}",
                                                    f"{instance['ingress_gbps']:.1f}",
                                                    f"[{util_style}]{util_percent:.1f}%[/]",
                                                    str(instance['bgp_peers']),
                                                    str(instance['score']),
                                                    "[dim yellow]COULD[/]",
                                                    format_dx_signal_values(instance),
                                                    end_section=is_last
                                                )

                                        # KEEP tier (green - active/healthy, show max 10)
                                        if dx_keep:
                                            for i, instance in enumerate(dx_keep[:10]):
                                                is_last = (i == min(len(dx_keep), 10) - 1)

                                                state_style = "green" if instance['connection_state'] == 'available' else "red"
                                                util_percent = instance['utilization_percent']
                                                util_style = "green" if util_percent >= 50 else "yellow" if util_percent >= 10 else "red"

                                                dx_table.add_row(
                                                    instance['connection_name'],
                                                    f"[{state_style}]{instance['connection_state']}[/]",
                                                    instance['bandwidth'],
                                                    f"{instance['egress_gbps']:.1f}",
                                                    f"{instance['ingress_gbps']:.1f}",
                                                    f"[{util_style}]{util_percent:.1f}%[/]",
                                                    str(instance['bgp_peers']),
                                                    str(instance['score']),
                                                    "[green]KEEP[/]",
                                                    format_dx_signal_values(instance),
                                                    end_section=is_last
                                                )

                                            if len(dx_keep) > 10:
                                                dx_table.add_row(
                                                    f"... ({len(dx_keep) - 10} more KEEP connections)",
                                                    "", "", "", "", "", "", "", "", "",
                                                    style="dim"
                                                )

                                        dx_branch.add(dx_table)

                                        # Direct Connect summary
                                        dx_branch.add(
                                            f"[bold]Summary:[/] "
                                            f"{len(dx_must)} MUST + {len(dx_should)} SHOULD + "
                                            f"{len(dx_could)} COULD + {len(dx_keep)} KEEP = "
                                            f"{len(enriched_dx)} total Direct Connect connections"
                                        )
                                    else:
                                        dx_branch.add("[dim]No Direct Connect connections available for activity analysis[/]")

                                except Exception as e:
                                    dx_branch.add(f"[yellow]âš ï¸  Direct Connect analysis failed: {str(e)[:100]}[/]")
                                    console.print(f"[dim]DX enrichment error: {e}[/dim]")
                                    logger.warning(f"Direct Connect activity enrichment failed: {e}")
                            elif show_empty:
                                # Only show empty DX branch when troubleshooting (--show-empty flag)
                                dx_branch.add("[dim]No Direct Connect connections found for activity analysis[/]")

                            # Display the activity health tree
                            console.print(tree)
                            console.print()

                            # Attach activity analysis to results
                            results["activity_analysis"] = {
                                "ec2": {
                                    "must_decommission": len(must_decommission) if 'must_decommission' in locals() else 0,
                                    "should_review": len(should_review) if 'should_review' in locals() else 0,
                                    "could_consider": len(could_consider) if 'could_consider' in locals() else 0,
                                    "keep_active": len(keep_active) if 'keep_active' in locals() else 0
                                },
                                "s3": {
                                    "must_optimize": len(s3_must) if 's3_must' in locals() else 0,
                                    "should_optimize": len(s3_should) if 's3_should' in locals() else 0,
                                    "could_optimize": len(s3_could) if 's3_could' in locals() else 0,
                                    "keep_active": len(s3_keep) if 's3_keep' in locals() else 0
                                },
                                "rds": {
                                    "decommission": rds_decommission if 'rds_decommission' in locals() else 0,
                                    "investigate": rds_investigate if 'rds_investigate' in locals() else 0,
                                    "total": len(enriched.get('rds', pd.DataFrame()))
                                },
                                "dynamodb": {
                                    "decommission": dynamodb_decommission if 'dynamodb_decommission' in locals() else 0,
                                    "investigate": dynamodb_investigate if 'dynamodb_investigate' in locals() else 0,
                                    "total": len(enriched.get('dynamodb', pd.DataFrame()))
                                },
                                "alb": {
                                    "decommission": alb_decommission if 'alb_decommission' in locals() else 0,
                                    "investigate": alb_investigate if 'alb_investigate' in locals() else 0,
                                    "total": len(enriched_albs) if 'enriched_albs' in locals() else 0
                                },
                                "asg": {
                                    "decommission": asg_decommission if 'asg_decommission' in locals() else 0,
                                    "investigate": asg_investigate if 'asg_investigate' in locals() else 0,
                                    "total": len(enriched_asgs) if 'enriched_asgs' in locals() else 0
                                },
                                "ecs": {
                                    "decommission": ecs_decommission if 'ecs_decommission' in locals() else 0,
                                    "investigate": ecs_investigate if 'ecs_investigate' in locals() else 0,
                                    "total": len(enriched_ecs) if 'enriched_ecs' in locals() else 0
                                }
                            }
                        else:
                            console.print("[yellow]No resources found for activity analysis (EC2, S3, RDS, DynamoDB, ALB/NLB, ASG, ECS)[/yellow]")

                    except Exception as e:
                        console.print(f"[yellow]âš ï¸  Activity analysis failed: {e}[/yellow]")
                        console.print("[dim]Continuing with dashboard results...[/dim]")
                        logger.warning(f"Activity analysis error: {e}")

                return results

            except Exception as e:
                print_error(f"Failed to retrieve cost data: {e}")
                console.print(
                    f"[yellow]ðŸ’¡ Tip: Ensure your AWS profile '{resolved_profile}' has Cost Explorer permissions[/yellow]"
                )
                console.print(f"[dim]Required permissions: ce:GetCostAndUsage, ce:GetDimensionValues[/dim]")
                raise

        except ImportError as e:
            error_handlers["module_not_available"]("FinOps dashboard", e)
            raise click.ClickException("FinOps dashboard functionality not available")
        except Exception as e:
            error_handlers["operation_failed"]("FinOps dashboard generation", e)
            raise click.ClickException(str(e))

    @finops.command()
    @click.option(
        "--resource-type",
        type=click.Choice(["ec2", "s3", "rds", "lambda", "vpc"]),
        required=True,
        help="Resource type for optimization analysis",
    )
    @click.option(
        "--savings-target", type=click.FloatRange(0.1, 0.8), default=0.3, help="Target savings percentage (0.1-0.8)"
    )
    @click.option(
        "--analysis-depth",
        type=click.Choice(["basic", "comprehensive", "enterprise"]),
        default="comprehensive",
        help="Analysis depth level",
    )
    @click.option("--mcp-validate", is_flag=True, help="Enable MCP validation for â‰¥99.5% accuracy cross-validation")
    @click.pass_context
    def optimize(ctx, resource_type, savings_target, analysis_depth, mcp_validate):
        """
        Generate cost optimization recommendations for specific resource types.

        Enterprise Optimization Features:
        â€¢ Safety-first analysis with READ-ONLY operations
        â€¢ Quantified savings projections with ROI analysis
        â€¢ Risk assessment and business impact evaluation
        â€¢ Implementation timeline and priority recommendations

        Examples:
            runbooks finops optimize --resource-type ec2 --savings-target 0.25
            runbooks finops optimize --resource-type s3 --analysis-depth enterprise
        """
        try:
            from runbooks.finops.optimization_engine import ResourceOptimizer

            # MCP validation integration for optimization accuracy
            mcp_results = None
            if mcp_validate:
                try:
                    from runbooks.validation.mcp_validator import MCPValidator
                    import asyncio

                    console.print(f"[cyan]ðŸ” Running MCP validation for {resource_type} optimization accuracy[/cyan]")

                    # Configure validation profiles
                    validation_profiles = {
                        "billing": ctx.obj.get("profile", "default"),
                        "management": ctx.obj.get("profile", "default"),
                        "centralised_ops": ctx.obj.get("profile", "default"),
                        "single_aws": ctx.obj.get("profile", "default"),
                    }

                    # Initialize validator
                    validator = MCPValidator(
                        profiles=validation_profiles, tolerance_percentage=5.0, performance_target_seconds=30.0
                    )

                    # Run validation based on resource type
                    if resource_type in ["ec2"]:
                        mcp_results = asyncio.run(validator.validate_ec2_inventory())
                    elif resource_type in ["vpc"]:
                        mcp_results = asyncio.run(validator.validate_vpc_analysis())
                    elif resource_type in ["s3", "rds", "lambda"]:
                        # For these resource types, use cost explorer validation
                        mcp_results = asyncio.run(validator.validate_cost_explorer())
                    else:
                        # Default to cost explorer validation
                        mcp_results = asyncio.run(validator.validate_cost_explorer())

                    # Display validation results
                    if mcp_results.accuracy_percentage >= 99.5:
                        console.print(
                            f"[green]âœ… MCP Validation PASSED: {mcp_results.accuracy_percentage:.1f}% accuracy for {resource_type}[/green]"
                        )
                    elif mcp_results.accuracy_percentage >= 95.0:
                        console.print(
                            f"[yellow]âš ï¸ MCP Validation WARNING: {mcp_results.accuracy_percentage:.1f}% accuracy (target: â‰¥99.5%)[/yellow]"
                        )
                    else:
                        console.print(
                            f"[red]âŒ MCP Validation FAILED: {mcp_results.accuracy_percentage:.1f}% accuracy[/red]"
                        )

                except Exception as e:
                    console.print(f"[yellow]âš ï¸ MCP validation failed: {e}[/yellow]")
                    console.print("[dim]Continuing with optimization analysis...[/dim]")

            optimizer = ResourceOptimizer(
                profile=ctx.obj["profile"],
                region=ctx.obj["region"],
                resource_type=resource_type,
                savings_target=savings_target,
                analysis_depth=analysis_depth,
                mcp_validate=mcp_validate,
            )

            optimization_results = optimizer.analyze_optimization_opportunities()

            # Attach MCP validation results if available
            if mcp_results and isinstance(optimization_results, dict):
                optimization_results["mcp_validation"] = {
                    "accuracy_percentage": mcp_results.accuracy_percentage,
                    "validation_passed": mcp_results.accuracy_percentage >= 99.5,
                    "resource_type": resource_type,
                    "operation_name": mcp_results.operation_name,
                    "status": mcp_results.status.value,
                    "detailed_results": mcp_results,
                }

            return optimization_results

        except ImportError as e:
            error_handlers["module_not_available"]("FinOps optimization", e)
            raise click.ClickException("FinOps optimization functionality not available")
        except Exception as e:
            error_handlers["operation_failed"]("FinOps optimization analysis", e)
            raise click.ClickException(str(e))

    @finops.command()
    @click.option(
        "--format",
        "export_format",
        type=click.Choice(["csv", "json", "pdf", "markdown"]),
        multiple=True,
        default=["json"],
        help="Export formats",
    )
    @click.option("--output-dir", default="./finops_reports", help="Output directory for exports")
    @click.option("--include-quarterly", is_flag=True, help="Include quarterly intelligence data")
    @click.option("--executive-summary", is_flag=True, help="Generate executive summary format")
    @click.option("--mcp-validate", is_flag=True, help="Enable MCP validation for â‰¥99.5% accuracy cross-validation")
    @click.pass_context
    def export(ctx, export_format, output_dir, include_quarterly, executive_summary, mcp_validate):
        """
        Export financial analysis results in multiple formats.

        Enterprise Export Features:
        â€¢ Multi-format simultaneous export
        â€¢ Executive-ready formatting and presentation
        â€¢ Quarterly intelligence integration
        â€¢ Complete audit trail documentation

        Examples:
            runbooks finops export --format csv,pdf --executive-summary
            runbooks finops export --include-quarterly --output-dir ./executive_reports
        """
        try:
            from runbooks.finops.export_manager import FinOpsExportManager

            # MCP validation integration for export accuracy
            mcp_results = None
            if mcp_validate:
                try:
                    from runbooks.validation.mcp_validator import MCPValidator
                    import asyncio

                    console.print("[cyan]ðŸ” Running MCP validation for export data accuracy[/cyan]")

                    # Configure validation profiles
                    validation_profiles = {
                        "billing": ctx.obj.get("profile", "default"),
                        "management": ctx.obj.get("profile", "default"),
                        "centralised_ops": ctx.obj.get("profile", "default"),
                        "single_aws": ctx.obj.get("profile", "default"),
                    }

                    # Initialize validator
                    validator = MCPValidator(
                        profiles=validation_profiles, tolerance_percentage=5.0, performance_target_seconds=30.0
                    )

                    # Run validation for export data accuracy using cost explorer validation
                    mcp_results = asyncio.run(validator.validate_cost_explorer())

                    # Display validation results
                    if mcp_results.accuracy_percentage >= 99.5:
                        console.print(
                            f"[green]âœ… MCP Validation PASSED: {mcp_results.accuracy_percentage:.1f}% accuracy for exports[/green]"
                        )
                    elif mcp_results.accuracy_percentage >= 95.0:
                        console.print(
                            f"[yellow]âš ï¸ MCP Validation WARNING: {mcp_results.accuracy_percentage:.1f}% accuracy (target: â‰¥99.5%)[/yellow]"
                        )
                    else:
                        console.print(
                            f"[red]âŒ MCP Validation FAILED: {mcp_results.accuracy_percentage:.1f}% accuracy[/red]"
                        )

                except Exception as e:
                    console.print(f"[yellow]âš ï¸ MCP validation failed: {e}[/yellow]")
                    console.print("[dim]Continuing with export operation...[/dim]")

            export_manager = FinOpsExportManager(
                profile=ctx.obj["profile"],
                output_dir=output_dir,
                include_quarterly=include_quarterly,
                executive_summary=executive_summary,
                mcp_validate=mcp_validate,
            )

            export_results = {}
            for format_type in export_format:
                result = export_manager.export_analysis(format=format_type)
                export_results[format_type] = result

            # Attach MCP validation results if available
            if mcp_results:
                export_results["mcp_validation"] = {
                    "accuracy_percentage": mcp_results.accuracy_percentage,
                    "validation_passed": mcp_results.accuracy_percentage >= 99.5,
                    "export_formats": list(export_format),
                    "operation_name": mcp_results.operation_name,
                    "status": mcp_results.status.value,
                    "detailed_results": mcp_results,
                }

            error_handlers["success"](
                f"Successfully exported to {len(export_format)} format(s)", f"Output directory: {output_dir}"
            )

            return export_results

        except ImportError as e:
            error_handlers["module_not_available"]("FinOps export", e)
            raise click.ClickException("FinOps export functionality not available")
        except Exception as e:
            error_handlers["operation_failed"]("FinOps export operation", e)
            raise click.ClickException(str(e))

    @finops.command()
    @click.option(
        "--older-than-days", type=int, default=90, help="Minimum age in days for cleanup consideration (default: 90)"
    )
    @click.option(
        "--validate", is_flag=True, default=True, help="Enable MCP validation for â‰¥99.5% accuracy (default: enabled)"
    )
    @click.option("--cleanup", is_flag=True, help="Enable cleanup recommendations (READ-ONLY analysis only)")
    @click.option("--export-results", is_flag=True, help="Export analysis results to JSON file")
    @click.option(
        "--safety-checks/--no-safety-checks",
        default=True,
        help="Enable comprehensive safety validations (default: enabled)",
    )
    @click.option("--all-profiles", help="Use specified profile for all operations (overrides parent --profile)")
    @click.pass_context
    def ec2_snapshots(ctx, older_than_days, validate, cleanup, export_results, safety_checks, all_profiles):
        """
        EC2 snapshot cost optimization and cleanup analysis.

        Sprint 1, Task 1: Analyze EC2 snapshots for cost optimization opportunities
        targeting $50K+ annual savings through systematic age-based cleanup with
        enterprise safety validations and MCP accuracy frameworks.

        Enterprise Features:
        â€¢ Multi-account snapshot discovery via AWS Config aggregator
        â€¢ Dynamic pricing via AWS Pricing API for accurate cost calculations
        â€¢ MCP validation framework with â‰¥99.5% accuracy cross-validation
        â€¢ Comprehensive safety checks (volume attachment, AMI association, age)
        â€¢ Executive reporting with Sprint 1 business impact metrics

        Safety Features:
        â€¢ READ-ONLY analysis by default (no actual cleanup performed)
        â€¢ Volume attachment verification before recommendations
        â€¢ AMI association checking to prevent data loss
        â€¢ Configurable age thresholds with safety validations

        Examples:
            # Basic analysis with MCP validation using parent profile
            runbooks finops --profile BILLING_PROFILE ec2-snapshots --validate

            # Override parent profile with command-specific profile
            runbooks finops ec2-snapshots --all-profiles BILLING_PROFILE --validate

            # Custom age threshold with export
            runbooks finops --profile BILLING_PROFILE ec2-snapshots --older-than-days 120 --export-results

            # Comprehensive analysis for Sprint 1
            runbooks finops --profile BILLING_PROFILE ec2-snapshots --cleanup --validate --export-results

            # Quick analysis without safety checks (not recommended)
            runbooks finops ec2-snapshots --all-profiles BILLING_PROFILE --no-safety-checks --older-than-days 30

        Sprint 1 Context:
            Task 1 targeting $50K+ annual savings through systematic snapshot cleanup
            with enterprise coordination and MCP validation accuracy â‰¥99.5%
        """
        try:
            import asyncio
            from runbooks.finops.snapshot_manager import EC2SnapshotManager

            console.print("\n[bold blue]ðŸŽ¯ Sprint 1, Task 1: EC2 Snapshot Cost Optimization[/bold blue]")

            # Resolve profile with priority: --all-profiles > ctx.obj['profile'] > 'default'
            resolved_profile = all_profiles or ctx.obj.get("profile", "default")
            resolved_region = ctx.obj.get("region", "all")
            resolved_dry_run = ctx.obj.get("dry_run", True)

            # Validate profile resolution
            if not resolved_profile:
                console.print("[red]âŒ Error: No AWS profile specified or found[/red]")
                console.print("[yellow]ðŸ’¡ Use --all-profiles PROFILE_NAME or set parent --profile option[/yellow]")
                raise click.ClickException("AWS profile required for ec2-snapshots command")

            console.print(
                f"[dim]Profile: {resolved_profile} | Region: {resolved_region} | Dry-run: {resolved_dry_run}[/dim]\n"
            )

            # Initialize snapshot manager with enterprise configuration
            manager = EC2SnapshotManager(profile=resolved_profile, dry_run=resolved_dry_run)

            # Configure safety checks based on user preference
            if not safety_checks:
                console.print("[yellow]âš ï¸ Safety checks disabled - use with caution[/yellow]")
                manager.safety_checks = {
                    "volume_attachment_check": False,
                    "ami_association_check": False,
                    "minimum_age_check": True,  # Always keep age check for safety
                    "cross_account_validation": False,
                }

            # Run the main analysis using the enhanced method
            async def run_analysis():
                return await manager.analyze_snapshot_opportunities(
                    profile=resolved_profile,
                    older_than_days=older_than_days,
                    enable_mcp_validation=validate,
                    export_results=export_results,
                )

            # Execute analysis
            results = asyncio.run(run_analysis())

            # Check if we have cost analysis results before validating Sprint 1 targets
            if not results.get("cost_analysis") or not results["cost_analysis"]:
                console.print("\n[yellow]âš ï¸ No snapshots found - no cost optimization opportunities identified[/yellow]")
                return results

            # Sprint 1 success validation
            annual_savings = results["cost_analysis"]["annual_savings"]
            sprint_target = 50000  # $50K Sprint 1 target

            if annual_savings >= sprint_target:
                console.print(f"\n[bold green]âœ… Sprint 1 Task 1 SUCCESS![/bold green]")
                console.print(f"[green]Target: ${sprint_target:,} | Achieved: ${annual_savings:,.2f}[/green]")
            else:
                console.print(f"\n[bold yellow]âš ï¸ Sprint 1 Task 1 - Below Target[/bold yellow]")
                console.print(f"[yellow]Target: ${sprint_target:,} | Achieved: ${annual_savings:,.2f}[/yellow]")

            # MCP validation status for Sprint 1
            if validate and results.get("mcp_validation"):
                mcp_results = results["mcp_validation"]
                if mcp_results["validation_passed"]:
                    console.print(
                        f"[green]âœ… MCP Validation: {mcp_results['accuracy_percentage']:.2f}% accuracy[/green]"
                    )
                else:
                    console.print(
                        f"[red]âŒ MCP Validation: {mcp_results['accuracy_percentage']:.2f}% accuracy (Required: â‰¥99.5%)[/red]"
                    )

            # Enterprise coordination confirmation
            console.print(f"\n[dim]ðŸ¢ Enterprise coordination: python-runbooks-engineer [1] (Primary)[/dim]")
            console.print(f"[dim]ðŸŽ¯ Sprint coordination: Systematic delegation activated[/dim]")

            return results

        except ImportError as e:
            error_handlers["module_not_available"]("EC2 Snapshot Manager", e)
            raise click.ClickException("EC2 snapshot analysis functionality not available")
        except Exception as e:
            error_handlers["operation_failed"]("EC2 snapshot analysis", e)
            raise click.ClickException(str(e))

    # Epic 2 Infrastructure Optimization Commands
    @finops.group()
    def infrastructure():
        """Epic 2 Infrastructure Optimization - $210,147 annual savings target"""
        pass

    @infrastructure.command()
    @click.option(
        "--components",
        multiple=True,
        type=click.Choice(["nat-gateway", "elastic-ip", "load-balancer", "vpc-endpoint"]),
        help="Infrastructure components to analyze (default: all)",
    )
    @click.option(
        "--export-format",
        type=click.Choice(["json", "csv", "markdown"]),
        default="json",
        help="Export format for results",
    )
    @click.option("--output-file", help="Output file path for results export")
    @click.option("--mcp-validate", is_flag=True, help="Enable MCP validation for â‰¥99.5% accuracy cross-validation")
    @click.pass_context
    def analyze(ctx, components, export_format, output_file, mcp_validate):
        """
        Comprehensive Infrastructure Optimization Analysis - Epic 2

        Analyze all infrastructure components to achieve $210,147 Epic 2 annual savings target:
        â€¢ NAT Gateway optimization: $147,420 target
        â€¢ Elastic IP optimization: $21,593 target
        â€¢ Load Balancer optimization: $35,280 target
        â€¢ VPC Endpoint optimization: $5,854 target

        SAFETY: READ-ONLY analysis only - no resource modifications.

        Examples:
            runbooks finops infrastructure analyze
            runbooks finops infrastructure analyze --components nat-gateway load-balancer
        """
        try:
            import asyncio
            from runbooks.finops.infrastructure.commands import InfrastructureOptimizer

            # MCP validation integration for infrastructure analysis
            mcp_results = None
            if mcp_validate:
                try:
                    from runbooks.validation.mcp_validator import MCPValidator

                    console.print("[cyan]ðŸ” Running MCP validation for infrastructure optimization accuracy[/cyan]")

                    # Configure validation profiles
                    validation_profiles = {
                        "billing": ctx.obj.get("profile", "default"),
                        "management": ctx.obj.get("profile", "default"),
                        "centralised_ops": ctx.obj.get("profile", "default"),
                        "single_aws": ctx.obj.get("profile", "default"),
                    }

                    # Initialize validator
                    validator = MCPValidator(
                        profiles=validation_profiles, tolerance_percentage=5.0, performance_target_seconds=30.0
                    )

                    # Run validation for infrastructure operations using VPC validation for networking components
                    component_types = (
                        list(components)
                        if components
                        else ["nat-gateway", "elastic-ip", "load-balancer", "vpc-endpoint"]
                    )
                    if any(comp in ["nat-gateway", "vpc-endpoint"] for comp in component_types):
                        mcp_results = asyncio.run(validator.validate_vpc_analysis())
                    elif any(comp in ["elastic-ip"] for comp in component_types):
                        mcp_results = asyncio.run(validator.validate_ec2_inventory())
                    else:
                        # Default to cost explorer for load balancer cost analysis
                        mcp_results = asyncio.run(validator.validate_cost_explorer())

                    # Display validation results
                    if mcp_results.accuracy_percentage >= 99.5:
                        console.print(
                            f"[green]âœ… MCP Validation PASSED: {mcp_results.accuracy_percentage:.1f}% accuracy for infrastructure[/green]"
                        )
                    elif mcp_results.accuracy_percentage >= 95.0:
                        console.print(
                            f"[yellow]âš ï¸ MCP Validation WARNING: {mcp_results.accuracy_percentage:.1f}% accuracy (target: â‰¥99.5%)[/yellow]"
                        )
                    else:
                        console.print(
                            f"[red]âŒ MCP Validation FAILED: {mcp_results.accuracy_percentage:.1f}% accuracy[/red]"
                        )

                except Exception as e:
                    console.print(f"[yellow]âš ï¸ MCP validation failed: {e}[/yellow]")
                    console.print("[dim]Continuing with infrastructure analysis...[/dim]")

            # Initialize comprehensive optimizer
            optimizer = InfrastructureOptimizer(
                profile_name=ctx.obj.get("profile"),
                regions=[ctx.obj.get("region")] if ctx.obj.get("region") else None,
                mcp_validate=mcp_validate,
            )

            # Execute comprehensive analysis
            results = asyncio.run(
                optimizer.analyze_comprehensive_infrastructure(
                    components=list(components) if components else None, dry_run=ctx.obj.get("dry_run", True)
                )
            )

            # Attach MCP validation results if available
            if mcp_results and hasattr(results, "__dict__"):
                results.mcp_validation = {
                    "accuracy_percentage": mcp_results.accuracy_percentage,
                    "validation_passed": mcp_results.accuracy_percentage >= 99.5,
                    "components_validated": list(components) if components else "all",
                    "operation_name": mcp_results.operation_name,
                    "status": mcp_results.status.value,
                    "detailed_results": mcp_results,
                }

            # Display Epic 2 progress
            if results.epic_2_target_achieved:
                console.print(f"\n[bold green]âœ… Epic 2 Infrastructure Target Achieved![/bold green]")
                console.print(
                    f"[green]Target: ${results.epic_2_target_savings:,.0f} | Achieved: ${results.total_potential_savings:,.0f}[/green]"
                )
            else:
                progress_pct = results.epic_2_progress_percentage
                console.print(f"\n[bold yellow]ðŸ“Š Epic 2 Infrastructure Progress: {progress_pct:.1f}%[/bold yellow]")
                console.print(
                    f"[yellow]Target: ${results.epic_2_target_savings:,.0f} | Achieved: ${results.total_potential_savings:,.0f}[/yellow]"
                )

            # Export results if requested
            if output_file or export_format != "json":
                console.print(f"[dim]Export functionality available - results ready for {export_format} export[/dim]")

            return results

        except ImportError as e:
            error_handlers["module_not_available"]("Infrastructure Optimizer", e)
            raise click.ClickException("Infrastructure optimization functionality not available")
        except Exception as e:
            error_handlers["operation_failed"]("Infrastructure optimization analysis", e)
            raise click.ClickException(str(e))

    @infrastructure.command()
    @click.pass_context
    def nat_gateway(ctx):
        """NAT Gateway optimization analysis - $147,420 Epic 2 target"""
        try:
            import asyncio
            from runbooks.finops.nat_gateway_optimizer import NATGatewayOptimizer

            optimizer = NATGatewayOptimizer(
                profile_name=ctx.obj.get("profile"), regions=[ctx.obj.get("region")] if ctx.obj.get("region") else None
            )

            results = asyncio.run(optimizer.analyze_nat_gateways(dry_run=ctx.obj.get("dry_run", True)))

            # Display Epic 2 component progress
            target = 147420.0
            if results.potential_annual_savings >= target:
                console.print(f"\n[bold green]âœ… NAT Gateway Epic 2 Target Achieved![/bold green]")
            else:
                progress = (results.potential_annual_savings / target) * 100
                console.print(f"\n[yellow]ðŸ“Š NAT Gateway Progress: {progress:.1f}% of Epic 2 target[/yellow]")

            return results

        except Exception as e:
            error_handlers["operation_failed"]("NAT Gateway optimization", e)
            raise click.ClickException(str(e))

    @infrastructure.command()
    @click.pass_context
    def elastic_ip(ctx):
        """Elastic IP optimization analysis - $21,593 Epic 2 target"""
        try:
            import asyncio
            from runbooks.finops.elastic_ip_optimizer import ElasticIPOptimizer

            optimizer = ElasticIPOptimizer(
                profile_name=ctx.obj.get("profile"), regions=[ctx.obj.get("region")] if ctx.obj.get("region") else None
            )

            results = asyncio.run(optimizer.analyze_elastic_ips(dry_run=ctx.obj.get("dry_run", True)))

            # Display Epic 2 component progress
            target = 21593.0
            if results.potential_annual_savings >= target:
                console.print(f"\n[bold green]âœ… Elastic IP Epic 2 Target Achieved![/bold green]")
            else:
                progress = (results.potential_annual_savings / target) * 100
                console.print(f"\n[yellow]ðŸ“Š Elastic IP Progress: {progress:.1f}% of Epic 2 target[/yellow]")

            return results

        except Exception as e:
            error_handlers["operation_failed"]("Elastic IP optimization", e)
            raise click.ClickException(str(e))

    @infrastructure.command()
    @click.pass_context
    def load_balancer(ctx):
        """Load Balancer optimization analysis - $35,280 Epic 2 target"""
        try:
            import asyncio
            from runbooks.finops.infrastructure.load_balancer_optimizer import LoadBalancerOptimizer

            optimizer = LoadBalancerOptimizer(
                profile_name=ctx.obj.get("profile"), regions=[ctx.obj.get("region")] if ctx.obj.get("region") else None
            )

            results = asyncio.run(optimizer.analyze_load_balancers(dry_run=ctx.obj.get("dry_run", True)))

            # Display Epic 2 component progress
            target = 35280.0
            if results.potential_annual_savings >= target:
                console.print(f"\n[bold green]âœ… Load Balancer Epic 2 Target Achieved![/bold green]")
            else:
                progress = (results.potential_annual_savings / target) * 100
                console.print(f"\n[yellow]ðŸ“Š Load Balancer Progress: {progress:.1f}% of Epic 2 target[/yellow]")

            return results

        except Exception as e:
            error_handlers["operation_failed"]("Load Balancer optimization", e)
            raise click.ClickException(str(e))

    @infrastructure.command()
    @click.pass_context
    def vpc_endpoint(ctx):
        """VPC Endpoint optimization analysis - $5,854 Epic 2 target"""
        try:
            import asyncio
            from runbooks.finops.infrastructure.vpc_endpoint_optimizer import VPCEndpointOptimizer

            optimizer = VPCEndpointOptimizer(
                profile_name=ctx.obj.get("profile"), regions=[ctx.obj.get("region")] if ctx.obj.get("region") else None
            )

            results = asyncio.run(optimizer.analyze_vpc_endpoints(dry_run=ctx.obj.get("dry_run", True)))

            # Display Epic 2 component progress
            target = 5854.0
            if results.potential_annual_savings >= target:
                console.print(f"\n[bold green]âœ… VPC Endpoint Epic 2 Target Achieved![/bold green]")
            else:
                progress = (results.potential_annual_savings / target) * 100
                console.print(f"\n[yellow]ðŸ“Š VPC Endpoint Progress: {progress:.1f}% of Epic 2 target[/yellow]")

            return results

        except Exception as e:
            error_handlers["operation_failed"]("VPC Endpoint optimization", e)
            raise click.ClickException(str(e))

    @finops.command()
    @click.option(
        "--input",
        "-i",
        "input_file",
        required=True,
        type=click.Path(exists=True),
        help="Input WorkSpaces data file (Excel/CSV with 'AWS Account' column)",
    )
    @click.option(
        "--output", "-o", "output_file", type=click.Path(), help="Output enriched data file (Excel/CSV/JSON)"
    )
    @click.option("--profile", "-p", default="default", help="AWS operational profile (WorkSpaces access)")
    @click.option("--management-profile", "-m", default=None, help="AWS management profile (Organizations access)")
    @click.option("--display-only", is_flag=True, help="Display Rich CLI output without file export")
    @click.pass_context
    def enrich_workspaces(ctx, input_file, output_file, profile, management_profile, display_only):
        """
        Enrich WorkSpaces inventory with Organizations metadata.

        Reads WorkSpaces data from Excel/CSV with 'AWS Account' column and enriches it with:
        â€¢ Account name (from AWS Organizations)
        â€¢ Account email
        â€¢ WBS code (cost allocation)
        â€¢ Cost group
        â€¢ Technical lead
        â€¢ Account owner

        Example:
            runbooks finops enrich-workspaces -i data/workspaces.xlsx -o data/enriched.xlsx -p ops-profile -m mgmt-profile
        """
        try:
            from runbooks.finops.enrich_workspaces import enrich_workspaces as enrich_ws_cmd

            # Import the command function and invoke it directly
            import sys
            from click.testing import CliRunner

            # Get format from parent context (output_format from common_output_options decorator)
            output_format = ctx.obj.get("output_format", "csv") if ctx.obj else "csv"

            # Create a runner to invoke the standalone command
            # This allows reusing the implementation without duplicating code
            enrich_ws_cmd.callback(input_file, output_file, profile, management_profile, output_format, display_only)

        except ImportError as e:
            error_handlers["module_not_available"]("WorkSpaces Enrichment", e)
            raise click.ClickException("WorkSpaces enrichment functionality not available")
        except Exception as e:
            error_handlers["operation_failed"]("WorkSpaces enrichment", e)
            raise click.ClickException(str(e))

    @finops.command()
    @click.option(
        "--input",
        "-i",
        "input_file",
        required=True,
        type=click.Path(exists=True),
        help="Input EC2 inventory Excel file (with Identifier, AWS Account, Region columns)",
    )
    @click.option(
        "--output",
        "-o",
        "output_file",
        required=True,
        type=click.Path(),
        help="Output enriched Excel file (multi-sheet with cost analysis)"
    )
    @click.option(
        "--management-profile",
        "-m",
        required=True,
        help="AWS management profile for Organizations API access"
    )
    @click.option(
        "--billing-profile",
        "-b",
        required=True,
        help="AWS billing profile for Cost Explorer API access"
    )
    @click.option(
        "--operational-profile",
        "-p",
        default=None,
        help="AWS operational profile for EC2 describe-instances (optional, defaults to management profile)"
    )
    @click.option(
        "--enable-organizations/--no-organizations",
        default=True,
        help="Enable Organizations metadata enrichment (default: enabled)"
    )
    @click.option(
        "--enable-cost/--no-cost",
        default=True,
        help="Enable Cost Explorer enrichment (default: enabled)"
    )
    @click.option(
        "--enable-activity/--no-activity",
        default=False,
        help="Enable CloudTrail activity tracking (default: disabled, takes 60-90 seconds)"
    )
    @click.option(
        "--include-12month-cost/--no-12month-cost",
        default=True,
        help="Include 12-month cost breakdown (default: enabled)"
    )
    @click.option(
        "--decommission-mode/--no-decommission-mode",
        default=False,
        help="Decommission mode: Filter to 45 decision-focused columns (default: disabled, shows all 87 columns)"
    )
    @click.option("--verbose", "-v", is_flag=True, help="Show detailed logs")
    @click.option("--format", type=click.Choice(["compact", "table", "json"]), default="compact", help="Output format")
    @click.pass_context
    def analyze_ec2(
        ctx,
        input_file,
        output_file,
        management_profile,
        billing_profile,
        operational_profile,
        enable_organizations,
        enable_cost,
        enable_activity,
        include_12month_cost,
        decommission_mode,
        verbose,
        format
    ):
        """
        EC2 cost analysis with 4-way enrichment.

        \b
        ðŸ”„ 4-WAY ENRICHMENT LAYERS (E1-E7 Decommission Scoring)
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚ Layer 1: Discovery       â”‚ Resource Explorer EC2 inventory   â”‚
        â”‚ Layer 2: Organizations   â”‚ Account metadata (6 columns)      â”‚
        â”‚ Layer 3: Cost            â”‚ 12-month Cost Explorer trends     â”‚
        â”‚ Layer 4: Activity        â”‚ CloudTrail idle detection (opt)   â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

        \b
        ðŸ“Š DECOMMISSION SIGNALS (E1-E7)
        â€¢ E1: Instance stopped state â†’ +15 points (HIGH priority)
        â€¢ E2: Zero 12-month cost â†’ +12 points (Terminated/Free tier)
        â€¢ E3: Low monthly cost (<$5) â†’ +8 points (Micro workloads)
        â€¢ E4: No CloudTrail activity (14d) â†’ +10 points (Idle detection)
        â€¢ E5: Cost decreasing trend â†’ +5 points (Usage declining)
        â€¢ E6: Legacy instance type (t2, m4) â†’ +3 points (Modernization)
        â€¢ E7: Untagged resources â†’ +2 points (Management overhead)

        \b
        ðŸŽ¯ OPTIMIZATION TIERS
        â€¢ HIGH (â‰¥25 points): Immediate decommission candidates
        â€¢ MEDIUM (15-24 points): Review recommended within 30 days
        â€¢ LOW (<15 points): Monitor and maintain current state

        \b
        ðŸ’° Sprint 1 Target: 25-50% cost savings via tier-based decommission
        ðŸ“– Example: runbooks finops analyze-ec2 -i ec2.xlsx -o enriched.xlsx -m mgmt -b billing
        """
        # Initialize output controller
        configure_logging(verbose=verbose)
        controller = OutputController(verbose=verbose, format=format)

        try:
            from runbooks.finops.ec2_analyzer import analyze_ec2_costs
            from runbooks.common.rich_utils import print_header, print_success, print_error

            print_header("EC2 Cost Analysis", f"Input: {input_file}")

            # Execute EC2 cost analysis
            result_df = analyze_ec2_costs(
                input_file=input_file,
                output_file=output_file,
                management_profile=management_profile,
                billing_profile=billing_profile,
                operational_profile=operational_profile,
                enable_organizations=enable_organizations,
                enable_cost=enable_cost,
                enable_activity=enable_activity,
                include_12month_cost=include_12month_cost,
                decommission_mode=decommission_mode
            )

            # Print summary using OutputController
            enrichment_layers = []
            if enable_organizations:
                enrichment_layers.append("organizations")
            if enable_cost:
                enrichment_layers.append("cost")
            if enable_activity:
                enrichment_layers.append("activity")

            controller.print_operation_summary(
                emoji="ðŸ’°",
                operation="EC2 Cost Analysis",
                input_count=len(result_df),
                enriched_count=len(result_df),
                enrichment_type="EC2 instances analyzed",
                success_percentage=100.0,
                profile=management_profile,
                output_file=output_file,
                added_columns=enrichment_layers
            )

            return result_df

        except ImportError as e:
            error_handlers["module_not_available"]("EC2 Cost Analyzer", e)
            raise click.ClickException("EC2 cost analysis functionality not available")
        except Exception as e:
            error_handlers["operation_failed"]("EC2 cost analysis", e)
            raise click.ClickException(str(e))

    @finops.command()
    @click.option(
        "--input",
        "-i",
        "input_file",
        required=True,
        type=click.Path(exists=True),
        help="Input EC2 inventory Excel file (with instance_id, account_id, region columns)"
    )
    @click.option(
        "--output",
        "-o",
        "output_file",
        required=True,
        type=click.Path(),
        help="Output enriched Excel file with decommission analysis"
    )
    @click.option(
        "--management-profile",
        "-m",
        required=True,
        help="AWS management profile for Organizations API access"
    )
    @click.option(
        "--billing-profile",
        "-b",
        required=True,
        help="AWS billing profile for Cost Explorer API access"
    )
    @click.option(
        "--operational-profile",
        "-p",
        default=None,
        help="AWS operational profile for EC2 describe-instances (optional, defaults to management)"
    )
    @click.option(
        "--enable-expensive-signals/--no-expensive-signals",
        default=False,
        help="Enable expensive signals (CloudTrail activity, SSM, Compute Optimizer)"
    )
    @click.pass_context
    def ec2_decommission_analysis(
        ctx,
        input_file,
        output_file,
        management_profile,
        billing_profile,
        operational_profile,
        enable_expensive_signals
    ):
        """
        EC2 decommission analysis with E1-E7 scoring framework.

        This command provides enterprise EC2 decommission candidate identification with:
        â€¢ E1-E7 multi-signal scoring (Compute Optimizer, CloudWatch, CloudTrail, SSM, etc.)
        â€¢ MUST/SHOULD/COULD/KEEP tier classification
        â€¢ Top 50 decommission candidates executive summary
        â€¢ Annual savings projections per instance
        â€¢ Professional Rich CLI output for CxO consumption

        Scoring Framework (0-100 scale):
        â€¢ MUST (80-100): Immediate decommission candidates
        â€¢ SHOULD (50-79): Strong candidates (review recommended)
        â€¢ COULD (25-49): Potential candidates (manual review)
        â€¢ KEEP (<25): Active resources (no action)

        Signal Framework (E1-E7):
        â€¢ E1: Compute Optimizer Idle (max CPU â‰¤1% for 14d) â†’ +60 points
        â€¢ E2: SSM Agent Offline/Stale â†’ +8 points
        â€¢ E3: CloudTrail no activity (90d) â†’ +8 points
        â€¢ E4: Stopped State (>30 days) â†’ +8 points
        â€¢ E5: No service attachment (ALB/ASG/ECS/EKS) â†’ +6 points
        â€¢ E6: No Tags/Owner â†’ +5 points
        â€¢ E7: Dev/Test Environment â†’ +3 points

        Examples:
            # Basic decommission analysis (fast, no expensive signals)
            runbooks finops ec2-decommission-analysis \\
                --input data/ec2-inventory.xlsx \\
                --output data/ec2-decommission-analysis.xlsx \\
                --management-profile mgmt-ro \\
                --billing-profile billing-ro

            # Comprehensive analysis with expensive signals (CloudTrail, SSM, Compute Optimizer)
            runbooks finops ec2-decommission-analysis \\
                -i data/ec2.xlsx \\
                -o data/ec2-decommission-full.xlsx \\
                -m mgmt-ro \\
                -b billing-ro \\
                --enable-expensive-signals

        Input File Format:
            Excel file with required columns:
            - instance_id: EC2 instance ID (i-xxxxx)
            - account_id: 12-digit AWS account ID
            - region: AWS region (ap-southeast-2, etc.)

        Output File Format:
            Multi-sheet Excel with:
            - Sheet 1: Enriched Data (67+ columns with decommission scores)
            - Sheet 2: Top 50 Decommission Candidates (executive summary)
            - Sheet 3: Cost Summary (by account/tier)
            - Sheet 4: Validation Metrics

        Performance Notes:
            â€¢ Fast mode (no expensive signals): ~5-10 seconds
            â€¢ Full mode (with expensive signals): ~60-90 seconds
            â€¢ Expensive signals: CloudTrail (60s), SSM (10s), Compute Optimizer (10s)

        Pattern: Migrated from notebooks/compute/ec2.ipynb with CxO executive polish
        """
        try:
            from runbooks.finops.ec2_analyzer import analyze_ec2_costs
            from runbooks.finops.decommission_classifier import classify_ec2
            from runbooks.common.rich_utils import print_header, print_success, print_error, print_info

            print_header("EC2 Decommission Analysis", f"Input: {input_file}")

            # Step 1: Execute EC2 cost analysis with enrichment
            print_info("Step 1/3: Enriching EC2 inventory with Organizations and Cost Explorer data...")
            enriched_df = analyze_ec2_costs(
                input_file=input_file,
                output_file=output_file,
                management_profile=management_profile,
                billing_profile=billing_profile,
                operational_profile=operational_profile,
                enable_organizations=True,
                enable_cost=True,
                enable_activity=enable_expensive_signals,  # CloudTrail activity (60-90s)
                include_12month_cost=True
            )

            # Step 2: Apply decommission classification
            print_info("\nStep 2/3: Applying E1-E7 decommission scoring framework...")
            enriched_df = classify_ec2(enriched_df)

            # Step 3: Generate executive summary
            print_info("\nStep 3/3: Generating Top 50 decommission candidates executive summary...")

            # Filter to MUST/SHOULD tiers
            top_candidates = enriched_df[
                enriched_df['decommission_tier'].isin(['MUST', 'SHOULD'])
            ].sort_values('decommission_score', ascending=False).head(50)

            if len(top_candidates) > 0:
                from rich.table import Table
                from runbooks.common.rich_utils import console, format_cost

                # Create executive table
                exec_table = Table(
                    title=f"Top {len(top_candidates)} Decommission Candidates",
                    show_header=True,
                    header_style="bold cyan"
                )

                exec_table.add_column("Instance ID", style="dim", width=20)
                exec_table.add_column("Account", style="white", width=25)
                exec_table.add_column("Type", style="cyan", width=12)
                exec_table.add_column("Score", style="red", width=8, justify="right")
                exec_table.add_column("Tier", style="yellow", width=10)
                exec_table.add_column("Monthly Cost", style="green", width=12, justify="right")
                exec_table.add_column("Annual Savings", style="green bold", width=15, justify="right")

                total_monthly_cost = 0
                total_annual_savings = 0

                for _, row in top_candidates.iterrows():
                    instance_id = str(row.get('instance_id', 'N/A'))[:19]
                    account_name = str(row.get('account_name', 'Unknown'))[:24]
                    instance_type = str(row.get('instance_type', 'N/A'))
                    score = row.get('decommission_score', 0)
                    tier = row.get('decommission_tier', 'KEEP')
                    monthly_cost = row.get('monthly_cost', 0.0)
                    annual_savings = monthly_cost * 12

                    total_monthly_cost += monthly_cost
                    total_annual_savings += annual_savings

                    tier_style = "red bold" if tier == "MUST" else "yellow"

                    exec_table.add_row(
                        instance_id,
                        account_name,
                        instance_type,
                        f"{score:.0f}",
                        f"[{tier_style}]{tier}[/{tier_style}]",
                        format_cost(monthly_cost),
                        format_cost(annual_savings)
                    )

                console.print("\n")
                console.print(exec_table)

                print_success(f"\nðŸ’° Financial Impact:")
                print_success(f"   Total Monthly Cost: {format_cost(total_monthly_cost)}")
                print_success(f"   Total Annual Savings: {format_cost(total_annual_savings)}")
                print_success(f"   Average Decommission Score: {top_candidates['decommission_score'].mean():.1f}/100")

                tier_breakdown = top_candidates['decommission_tier'].value_counts()
                print_info(f"\nðŸ“Š Tier Breakdown (Top 50):")
                print_info(f"   MUST: {tier_breakdown.get('MUST', 0)} instances")
                print_info(f"   SHOULD: {tier_breakdown.get('SHOULD', 0)} instances")
            else:
                print_success("\nâœ… No high-priority decommission candidates identified.")
                print_success("   All EC2 instances classified as COULD or KEEP tiers.")

            # Final summary
            print_success(f"\nâœ… EC2 decommission analysis complete!")
            print_success(f"   ðŸ“Š Analyzed {len(enriched_df)} EC2 instances")
            print_success(f"   ðŸ“ Output: {output_file}")

            return enriched_df

        except ImportError as e:
            error_handlers["module_not_available"]("EC2 Decommission Analyzer", e)
            raise click.ClickException("EC2 decommission analysis functionality not available")
        except Exception as e:
            error_handlers["operation_failed"]("EC2 decommission analysis", e)
            raise click.ClickException(str(e))

    @finops.command()
    @click.option(
        "--profile",
        "-p",
        default="default",
        help="AWS profile for Lambda operations (Organizations, Cost Explorer, CloudWatch)"
    )
    @click.option(
        "--output",
        "-o",
        "output_file",
        default="lambda-analysis.xlsx",
        type=click.Path(),
        help="Output enriched Excel file with Lambda analysis"
    )
    @click.option(
        "--regions",
        "-r",
        multiple=True,
        help="AWS regions to analyze (defaults to all enabled regions)"
    )
    @click.option(
        "--enable-organizations/--no-organizations",
        default=True,
        help="Enable Organizations metadata enrichment (default: enabled)"
    )
    @click.option(
        "--enable-cost/--no-cost",
        default=True,
        help="Enable Cost Explorer enrichment (default: enabled)"
    )
    @click.option("--verbose", "-v", is_flag=True, help="Show detailed logs")
    @click.option("--format", type=click.Choice(["compact", "table", "json"]), default="compact", help="Output format")
    @click.pass_context
    def lambda_analysis(
        ctx,
        profile,
        output_file,
        regions,
        enable_organizations,
        enable_cost,
        verbose,
        format
    ):
        """
        Lambda cost and activity analysis with optimization signals.

        \b
        ðŸ”„ LAMBDA ENRICHMENT LAYERS (L1-L6 Optimization Scoring)
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚ Layer 1: Discovery       â”‚ Lambda function inventory         â”‚
        â”‚ Layer 2: Organizations   â”‚ Account metadata (6 columns)      â”‚
        â”‚ Layer 3: Cost            â”‚ 12-month Cost Explorer trends     â”‚
        â”‚ Layer 4: CloudWatch      â”‚ Invocations, errors, duration     â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

        \b
        ðŸ“Š OPTIMIZATION SIGNALS (L1-L6)
        â€¢ L1: High invocation cost (top 10%) â†’ +10 points (Cost impact)
        â€¢ L2: Idle function (0 invocations 14d) â†’ +15 points (Unused)
        â€¢ L3: Oversized memory (low utilization) â†’ +8 points (Rightsizing)
        â€¢ L4: Cold start issues (high p95/p50) â†’ +5 points (Performance)
        â€¢ L5: High error rate (>5%) â†’ +7 points (Quality issue)
        â€¢ L6: Legacy runtime â†’ +3 points (Security/maintenance)

        \b
        ðŸŽ¯ OPTIMIZATION TIERS
        â€¢ HIGH (â‰¥20 points): Immediate optimization/decommission
        â€¢ MEDIUM (10-19 points): Review and optimize within 30 days
        â€¢ LOW (<10 points): Monitor and maintain current state

        \b
        ðŸ’° Sprint 1 Target: Identify serverless optimization opportunities
        ðŸ“– Example: runbooks finops lambda-analysis --profile default --output lambda.xlsx
        """
        # Initialize output controller
        configure_logging(verbose=verbose)
        controller = OutputController(verbose=verbose, format=format)

        try:
            from runbooks.finops.lambda_analyzer import analyze_lambda_costs
            from runbooks.common.rich_utils import print_header, print_success, print_error

            print_header("Lambda Cost Analysis", f"Profile: {profile}")

            # Execute Lambda cost analysis
            result_df = analyze_lambda_costs(
                profile=profile,
                output_file=output_file,
                regions=list(regions) if regions else None,
                enable_organizations=enable_organizations,
                enable_cost=enable_cost
            )

            # Print summary using OutputController
            enrichment_layers = []
            if enable_organizations:
                enrichment_layers.append("organizations")
            if enable_cost:
                enrichment_layers.append("cost")

            controller.print_operation_summary(
                emoji="âš¡",
                operation="Lambda Cost Analysis",
                input_count=len(result_df),
                enriched_count=len(result_df),
                enrichment_type="Lambda functions analyzed",
                success_percentage=100.0,
                profile=profile,
                output_file=output_file,
                added_columns=enrichment_layers
            )

            # Display optimization summary in verbose mode
            if verbose and len(result_df) > 0 and 'optimization_tier' in result_df.columns:
                tier_counts = result_df['optimization_tier'].value_counts()
                print_info(f"\nðŸŽ¯ Optimization Opportunities:")
                print_info(f"   HIGH: {tier_counts.get('HIGH', 0)} functions")
                print_info(f"   MEDIUM: {tier_counts.get('MEDIUM', 0)} functions")
                print_info(f"   LOW: {tier_counts.get('LOW', 0)} functions")

            return result_df

        except ImportError as e:
            error_handlers["module_not_available"]("Lambda Cost Analyzer", e)
            raise click.ClickException("Lambda cost analysis functionality not available")
        except Exception as e:
            error_handlers["operation_failed"]("Lambda cost analysis", e)
            raise click.ClickException(str(e))

    @finops.command()
    @click.option(
        "--input",
        "-i",
        "input_file",
        required=True,
        type=click.Path(exists=True),
        help="Input WorkSpaces inventory Excel file (with Identifier, AWS Account, Region columns)"
    )
    @click.option(
        "--output",
        "-o",
        "output_file",
        required=True,
        type=click.Path(),
        help="Output enriched Excel file (multi-sheet with cost analysis)"
    )
    @click.option(
        "--management-profile",
        "-m",
        required=True,
        help="AWS management profile for Organizations API access"
    )
    @click.option(
        "--billing-profile",
        "-b",
        required=True,
        help="AWS billing profile for Cost Explorer API access"
    )
    @click.option(
        "--operational-profile",
        "-p",
        default=None,
        help="AWS operational profile for WorkSpaces operations (optional, defaults to management profile)"
    )
    @click.option(
        "--enable-organizations/--no-organizations",
        default=True,
        help="Enable Organizations metadata enrichment (default: enabled)"
    )
    @click.option(
        "--enable-cost/--no-cost",
        default=True,
        help="Enable Cost Explorer enrichment (default: enabled)"
    )
    @click.option(
        "--enable-activity/--no-activity",
        default=False,
        help="Enable CloudTrail activity tracking (default: disabled, takes 60-90 seconds)"
    )
    @click.option(
        "--include-12month-cost/--no-12month-cost",
        default=True,
        help="Include 12-month cost breakdown (default: enabled)"
    )
    @click.option("--verbose", "-v", is_flag=True, help="Show detailed logs")
    @click.option("--format", type=click.Choice(["compact", "table", "json"]), default="compact", help="Output format")
    @click.pass_context
    def analyze_workspaces(
        ctx,
        input_file,
        output_file,
        management_profile,
        billing_profile,
        operational_profile,
        enable_organizations,
        enable_cost,
        enable_activity,
        include_12month_cost,
        verbose,
        format
    ):
        """
        WorkSpaces cost analysis with decommission tier scoring.

        \b
        ðŸ”„ 4-WAY ENRICHMENT LAYERS (W1-W6 Decommission Scoring)
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚ Layer 1: Discovery       â”‚ WorkSpaces inventory (122 total)  â”‚
        â”‚ Layer 2: Organizations   â”‚ Account metadata (6 columns)      â”‚
        â”‚ Layer 3: Cost            â”‚ 12-month Cost Explorer trends     â”‚
        â”‚ Layer 4: Activity        â”‚ CloudWatch UserConnected metrics  â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

        \b
        ðŸ“Š DECOMMISSION SIGNALS (W1-W6)
        â€¢ W1: WorkSpace stopped state â†’ +15 points (HIGH priority)
        â€¢ W2: Zero UserConnected time (30d) â†’ +12 points (Idle workspace)
        â€¢ W3: Low monthly cost (<$10) â†’ +8 points (Basic bundle)
        â€¢ W4: Cost decreasing trend â†’ +5 points (Usage declining)
        â€¢ W5: No volume encryption â†’ +3 points (Compliance risk)
        â€¢ W6: Legacy bundle type â†’ +2 points (Modernization opportunity)

        \b
        ðŸŽ¯ OPTIMIZATION TIERS
        â€¢ HIGH (â‰¥25 points): Immediate decommission candidates
        â€¢ MEDIUM (15-24 points): Review recommended within 30 days
        â€¢ LOW (<15 points): Monitor and maintain current state

        \b
        ðŸ’° Sprint 1 Target: 25-50% cost savings via tier-based decommission
        ðŸ“– Example: runbooks finops analyze-workspaces -i ws.xlsx -o enriched.xlsx -m mgmt -b billing
        """
        # Initialize output controller
        configure_logging(verbose=verbose)
        controller = OutputController(verbose=verbose, format=format)

        try:
            from runbooks.finops.workspaces_analyzer import analyze_workspaces_costs
            from runbooks.common.rich_utils import print_header, print_success, print_error

            print_header("WorkSpaces Cost Analysis", f"Input: {input_file}")

            # Execute WorkSpaces cost analysis
            result_df = analyze_workspaces_costs(
                input_file=input_file,
                output_file=output_file,
                management_profile=management_profile,
                billing_profile=billing_profile,
                operational_profile=operational_profile,
                enable_organizations=enable_organizations,
                enable_cost=enable_cost,
                enable_activity=enable_activity,
                include_12month_cost=include_12month_cost
            )

            # Print summary using OutputController
            enrichment_layers = []
            if enable_organizations:
                enrichment_layers.append("organizations")
            if enable_cost:
                enrichment_layers.append("cost")
            if enable_activity:
                enrichment_layers.append("activity")

            controller.print_operation_summary(
                emoji="ðŸ–¥ï¸",
                operation="WorkSpaces Cost Analysis",
                input_count=len(result_df),
                enriched_count=len(result_df),
                enrichment_type="WorkSpaces analyzed",
                success_percentage=100.0,
                profile=management_profile,
                output_file=output_file,
                added_columns=enrichment_layers
            )

            return result_df

        except ImportError as e:
            error_handlers["module_not_available"]("WorkSpaces Cost Analyzer", e)
            raise click.ClickException("WorkSpaces cost analysis functionality not available")
        except Exception as e:
            error_handlers["operation_failed"]("WorkSpaces cost analysis", e)
            raise click.ClickException(str(e))

    @finops.command()
    @click.option(
        "--input",
        "-i",
        "input_file",
        required=True,
        type=click.Path(exists=True),
        help="Input WorkSpaces inventory Excel file (with Identifier, AWS Account, Region columns)"
    )
    @click.option(
        "--output",
        "-o",
        "output_file",
        required=True,
        type=click.Path(),
        help="Output enriched Excel file with decommission analysis"
    )
    @click.option(
        "--management-profile",
        "-m",
        required=True,
        help="AWS management profile for Organizations API access"
    )
    @click.option(
        "--billing-profile",
        "-b",
        required=True,
        help="AWS billing profile for Cost Explorer API access"
    )
    @click.option(
        "--operational-profile",
        "-p",
        default=None,
        help="AWS operational profile for WorkSpaces operations (optional, defaults to management)"
    )
    @click.pass_context
    def workspaces_decommission_analysis(
        ctx,
        input_file,
        output_file,
        management_profile,
        billing_profile,
        operational_profile
    ):
        """
        WorkSpaces decommission analysis with W1-W6 scoring framework.

        This command provides enterprise WorkSpaces decommission candidate identification with:
        â€¢ W1-W6 multi-signal scoring (connection recency, CloudWatch usage, break-even, policy)
        â€¢ MUST/SHOULD/COULD/KEEP tier classification
        â€¢ Top 50 decommission candidates executive summary
        â€¢ Annual savings projections per WorkSpace
        â€¢ Professional Rich CLI output for CxO consumption

        Scoring Framework (0-100 scale):
        â€¢ MUST (80-100): Immediate decommission candidates
        â€¢ SHOULD (50-79): Strong candidates (review recommended)
        â€¢ COULD (25-49): Potential candidates (manual review)
        â€¢ KEEP (<25): Active resources (no action)

        Signal Framework (W1-W6):
        â€¢ W1: User connection recency (â‰¥60 days) â†’ +45 points
        â€¢ W2: CloudWatch UserConnected sum=0 (no sessions) â†’ +25 points
        â€¢ W3: Billing vs usage (hourly usage < break-even) â†’ +10 points
        â€¢ W4: Cost Optimizer policy (N months unused) â†’ +10 points
        â€¢ W5: No admin API activity (90d) â†’ +5 points
        â€¢ W6: User status (not in Identity Center) â†’ +5 points

        Examples:
            # Basic decommission analysis
            runbooks finops workspaces-decommission-analysis \\
                --input data/workspaces-inventory.xlsx \\
                --output data/workspaces-decommission-analysis.xlsx \\
                --management-profile mgmt-ro \\
                --billing-profile billing-ro

            # With custom operational profile
            runbooks finops workspaces-decommission-analysis \\
                -i data/workspaces.xlsx \\
                -o data/workspaces-decommission-full.xlsx \\
                -m mgmt-ro \\
                -b billing-ro \\
                -p ops-ro

        Input File Format:
            Excel file with required columns:
            - Identifier: WorkSpace ID (ws-xxxxx)
            - AWS Account: 12-digit AWS account ID
            - Region: AWS region (ap-southeast-2, etc.)

        Output File Format:
            Multi-sheet Excel with:
            - Sheet 1: Enriched Data (40+ columns with decommission scores)
            - Sheet 2: Top 50 Decommission Candidates (executive summary)
            - Sheet 3: Cost Summary (by account/tier)
            - Sheet 4: Validation Metrics

        Performance Notes:
            â€¢ WorkSpaces enrichment: ~5-10 seconds
            â€¢ CloudWatch metrics: ~10-15 seconds
            â€¢ Volume encryption: ~5-10 seconds
            â€¢ Break-even calculation: <1 second

        Pattern: Migrated from notebooks/compute/workspaces.ipynb with CxO executive polish
        """
        try:
            from runbooks.finops.workspaces_analyzer import analyze_workspaces_costs, WorkSpacesCostAnalyzer
            from runbooks.finops.decommission_classifier import classify_workspaces
            from runbooks.common.rich_utils import print_header, print_success, print_error, print_info

            print_header("WorkSpaces Decommission Analysis", f"Input: {input_file}")

            # Step 1: Execute WorkSpaces cost analysis with enrichment
            print_info("Step 1/5: Enriching WorkSpaces inventory with Organizations and Cost Explorer data...")
            enriched_df = analyze_workspaces_costs(
                input_file=input_file,
                output_file=output_file,
                management_profile=management_profile,
                billing_profile=billing_profile,
                operational_profile=operational_profile,
                enable_organizations=True,
                enable_cost=True,
                enable_activity=False,  # CloudTrail activity not needed for W1-W6
                include_12month_cost=True
            )

            # Step 2: Initialize analyzer for additional enrichment
            print_info("\nStep 2/5: Analyzing volume encryption and connection status...")
            analyzer = WorkSpacesCostAnalyzer(profile=operational_profile or management_profile)
            enriched_df = analyzer.get_volume_encryption(enriched_df)

            # Step 3: CloudWatch UserConnected metrics
            print_info("\nStep 3/5: Fetching CloudWatch UserConnected metrics...")
            enriched_df = analyzer.get_cloudwatch_user_connected(enriched_df, lookback_days=30)

            # Step 4: Dynamic break-even calculation
            print_info("\nStep 4/5: Calculating dynamic break-even hours...")
            enriched_df = analyzer.calculate_dynamic_breakeven(enriched_df)

            # Step 5: Apply decommission classification
            print_info("\nStep 5/5: Applying W1-W6 decommission scoring framework...")
            enriched_df = classify_workspaces(enriched_df)

            # Generate executive summary
            print_info("\nGenerating Top 50 decommission candidates executive summary...")

            # Filter to MUST/SHOULD tiers
            top_candidates = enriched_df[
                enriched_df['decommission_tier'].isin(['MUST', 'SHOULD'])
            ].sort_values('decommission_score', ascending=False).head(50)

            if len(top_candidates) > 0:
                from rich.table import Table
                from runbooks.common.rich_utils import console, format_cost

                # Create executive table
                exec_table = Table(
                    title=f"Top {len(top_candidates)} Decommission Candidates",
                    show_header=True,
                    header_style="bold cyan"
                )

                exec_table.add_column("WorkSpace ID", style="dim", width=20)
                exec_table.add_column("Account", style="white", width=25)
                exec_table.add_column("Username", style="cyan", width=15)
                exec_table.add_column("Score", style="red", width=8, justify="right")
                exec_table.add_column("Tier", style="yellow", width=10)
                exec_table.add_column("Days Idle", style="yellow", width=10, justify="right")
                exec_table.add_column("Monthly Cost", style="green", width=12, justify="right")
                exec_table.add_column("Annual Savings", style="green bold", width=15, justify="right")

                total_monthly_cost = 0
                total_annual_savings = 0

                for _, row in top_candidates.iterrows():
                    workspace_id = str(row.get('Identifier', row.get('WorkspaceId', 'N/A')))[:19]
                    account_name = str(row.get('account_name', 'Unknown'))[:24]
                    username = str(row.get('Username', 'N/A'))[:14]
                    score = row.get('decommission_score', 0)
                    tier = row.get('decommission_tier', 'KEEP')
                    days_idle = row.get('days_since_last_connection', 0)
                    monthly_cost = row.get('monthly_cost', 0.0)
                    annual_savings = monthly_cost * 12

                    total_monthly_cost += monthly_cost
                    total_annual_savings += annual_savings

                    tier_style = "red bold" if tier == "MUST" else "yellow"

                    exec_table.add_row(
                        workspace_id,
                        account_name,
                        username,
                        f"{score:.0f}",
                        f"[{tier_style}]{tier}[/{tier_style}]",
                        f"{days_idle}",
                        format_cost(monthly_cost),
                        format_cost(annual_savings)
                    )

                console.print("\n")
                console.print(exec_table)

                print_success(f"\nðŸ’° Financial Impact:")
                print_success(f"   Total Monthly Cost: {format_cost(total_monthly_cost)}")
                print_success(f"   Total Annual Savings: {format_cost(total_annual_savings)}")
                print_success(f"   Average Decommission Score: {top_candidates['decommission_score'].mean():.1f}/100")

                tier_breakdown = top_candidates['decommission_tier'].value_counts()
                print_info(f"\nðŸ“Š Tier Breakdown (Top 50):")
                print_info(f"   MUST: {tier_breakdown.get('MUST', 0)} WorkSpaces")
                print_info(f"   SHOULD: {tier_breakdown.get('SHOULD', 0)} WorkSpaces")
            else:
                print_success("\nâœ… No high-priority decommission candidates identified.")
                print_success("   All WorkSpaces classified as COULD or KEEP tiers.")

            # Final summary
            print_success(f"\nâœ… WorkSpaces decommission analysis complete!")
            print_success(f"   ðŸ“Š Analyzed {len(enriched_df)} WorkSpaces")
            print_success(f"   ðŸ“ Output: {output_file}")

            return enriched_df

        except ImportError as e:
            error_handlers["module_not_available"]("WorkSpaces Decommission Analyzer", e)
            raise click.ClickException("WorkSpaces decommission analysis functionality not available")
        except Exception as e:
            error_handlers["operation_failed"]("WorkSpaces decommission analysis", e)
            raise click.ClickException(str(e))

    @finops.command(name="validate-with-mcp")
    @click.option("--input", "-i", "input_file", required=True, help="Input Excel file with cost projections")
    @click.option("--resource-type", "-t", type=click.Choice(["ec2", "workspaces", "lambda", "snapshots"]),
                  default="ec2", help="Resource type for validation")
    @click.option("--tolerance", "-tol", type=float, default=0.05, help="Variance tolerance (default: 5%)")
    @click.option("--billing-profile", help="AWS billing profile for MCP Cost Explorer")
    def validate_with_mcp(input_file, resource_type, tolerance, billing_profile):
        """
        Validate runbooks cost projections against MCP Cost Explorer (Feature 1).

        Implements 3-mode validation:
        1. Import mode: Python API validation
        2. CLI mode: Command-line batch validation
        3. MCP cross-validation: Cost Explorer accuracy check (â‰¥99.5% target)

        Examples:
            # Validate EC2 cost projections
            runbooks finops validate-with-mcp --input ec2-enriched.xlsx --resource-type ec2

            # Validate WorkSpaces with custom tolerance
            runbooks finops validate-with-mcp --input ws-enriched.xlsx --resource-type workspaces --tolerance 0.03

            # Validate with specific billing profile
            runbooks finops validate-with-mcp --input ec2-enriched.xlsx --billing-profile billing-mgmt
        """
        try:
            import pandas as pd
            from runbooks.finops.enhanced_mcp_integration import EnhancedMCPIntegration

            print_header(f"MCP Cost Validation - {resource_type.upper()}", f"Input: {input_file}")

            # Load cost projections from Excel
            print_info(f"Loading cost projections from {input_file}...")
            df = pd.read_excel(input_file, sheet_name=resource_type)

            # Determine cost and ID columns based on resource type
            column_mapping = {
                "ec2": {"id": "instance_id", "cost": "monthly_cost"},
                "workspaces": {"id": "WorkspaceId", "cost": "monthly_cost"},
                "lambda": {"id": "FunctionName", "cost": "monthly_cost"},
                "snapshots": {"id": "SnapshotId", "cost": "monthly_cost"}
            }

            id_column = column_mapping[resource_type]["id"]
            cost_column = column_mapping[resource_type]["cost"]

            # Validate columns exist
            if id_column not in df.columns or cost_column not in df.columns:
                raise click.ClickException(
                    f"Required columns missing: {id_column}, {cost_column}\n"
                    f"Available columns: {', '.join(df.columns)}"
                )

            # Build cost projections list
            cost_projections = []
            for _, row in df.iterrows():
                resource_id = row[id_column]
                cost = row[cost_column]

                # Skip invalid rows
                if pd.isna(resource_id) or pd.isna(cost) or cost == 0:
                    continue

                cost_projections.append({
                    "resource_id": str(resource_id),
                    "cost": float(cost)
                })

            if not cost_projections:
                print_warning("No valid cost projections found in input file")
                return

            print_success(f"Loaded {len(cost_projections)} cost projections for validation")

            # Initialize MCP integration
            mcp = EnhancedMCPIntegration(billing_profile=billing_profile)

            # Execute batch validation
            print_info(f"\nValidating against MCP Cost Explorer (tolerance: {tolerance:.1%})...")
            validation_results = mcp.validate_batch(cost_projections, tolerance=tolerance)

            # Display summary
            accuracy_metrics = validation_results.get("accuracy_metrics", {})
            if accuracy_metrics:
                print_header("Validation Summary")

                mcp_accuracy = accuracy_metrics.get("mcp_accuracy", 0.0)
                pass_rate = accuracy_metrics.get("pass_rate", 0.0)

                if mcp_accuracy >= 99.5:
                    print_success(f"âœ… MCP Accuracy: {mcp_accuracy:.1f}% (EXCEEDS â‰¥99.5% target)")
                elif mcp_accuracy >= 95.0:
                    print_warning(f"âš ï¸  MCP Accuracy: {mcp_accuracy:.1f}% (meets â‰¥95% baseline)")
                else:
                    print_error(f"âŒ MCP Accuracy: {mcp_accuracy:.1f}% (below 95% baseline)")

                print_info(f"   Pass Rate: {pass_rate:.1f}%")
                print_info(f"   Average Variance: {accuracy_metrics.get('average_variance', 0):.2%}")
                print_info(f"   Total Resources: {accuracy_metrics.get('total_resources', 0)}")

                # Export validation results
                output_file = input_file.replace('.xlsx', '-mcp-validation.json')
                import json
                with open(output_file, 'w') as f:
                    json.dump(validation_results, f, indent=2)

                print_success(f"\nðŸ“ Validation results exported: {output_file}")

        except ImportError as e:
            error_handlers["module_not_available"]("MCP Integration", e)
            raise click.ClickException("MCP validation functionality not available")
        except Exception as e:
            error_handlers["operation_failed"]("MCP validation", e)
            raise click.ClickException(str(e))

    @finops.command(name="analyze-graviton-eligibility")
    @click.option(
        "--input",
        "-i",
        "input_file",
        required=True,
        type=click.Path(exists=True),
        help="Input EC2 enriched Excel file (output from analyze-ec2)"
    )
    @click.option(
        "--output",
        "-o",
        "output_file",
        required=True,
        type=click.Path(),
        help="Output Excel file with Graviton eligibility assessment"
    )
    @click.option(
        "--management-profile",
        "-m",
        help="AWS management profile for EC2 operations (defaults to service routing)"
    )
    @click.option(
        "--billing-profile",
        "-b",
        help="AWS billing profile for Cost Explorer (defaults to service routing)"
    )
    @click.option(
        "--enable-ami-check/--no-ami-check",
        default=True,
        help="Enable AMI architecture compatibility checking (default: enabled)"
    )
    @click.option(
        "--enable-compute-optimizer/--no-compute-optimizer",
        default=True,
        help="Enable Compute Optimizer integration for confidence scoring (default: enabled)"
    )
    @click.option(
        "--verbose",
        "-v",
        is_flag=True,
        help="Enable verbose logging"
    )
    def analyze_graviton_eligibility_cmd(
        input_file,
        output_file,
        management_profile,
        billing_profile,
        enable_ami_check,
        enable_compute_optimizer,
        verbose
    ):
        """
        Graviton migration eligibility analysis for ARM64 cost optimization.

        This command analyzes EC2 instances for AWS Graviton (ARM64) migration potential
        with 40% cost savings targeting $800K+ annual opportunity.

        \b
        ðŸŽ¯ BUSINESS VALUE
        â€¢ $800K+ annual savings opportunity (40% Graviton price reduction)
        â€¢ 137 EC2 instances baseline analysis
        â€¢ Integration with E2-E7 decommission signals for confidence scoring

        \b
        ðŸ” ELIGIBILITY ASSESSMENT
        â€¢ Instance type compatibility mapping (x86_64 â†’ ARM64)
        â€¢ AMI architecture compatibility checking
        â€¢ Application workload heuristics (Linux-friendly, GPU-incompatible)
        â€¢ Cost savings projection (40% reduction model)
        â€¢ Integration with Compute Optimizer recommendations

        \b
        ðŸ“Š SCORING MODEL (0-100 scale)
        â€¢ Has Graviton mapping: 30 points (baseline eligibility)
        â€¢ AMI ARM64 compatible: 25 points (verified architecture support)
        â€¢ Application compatible (>70): 25 points (workload assessment)
        â€¢ Savings threshold met (â‰¥$10/mo): 10 points (business justification)
        â€¢ Compute Optimizer idle: 10 points bonus (decommission candidate)

        \b
        ðŸŽ¯ RECOMMENDATION TIERS
        â€¢ RECOMMEND (â‰¥70): Strong migration candidates (LOW complexity)
        â€¢ EVALUATE (40-69): Requires testing/validation (MEDIUM complexity)
        â€¢ NOT_RECOMMENDED (<40): Migration challenges (HIGH complexity)

        \b
        ðŸ“ OUTPUT FORMAT
        Multi-sheet Excel with:
        â€¢ Sheet 1: Graviton Analysis (12 new columns + original EC2 data)
        â€¢ Sheet 2: RECOMMEND Tier (high-confidence candidates)
        â€¢ Sheet 3: EVALUATE Tier (testing recommended)
        â€¢ Sheet 4: Cost Savings Summary (by account/tier)

        \b
        ðŸ”— INTEGRATION
        â€¢ Input: EC2 enriched data from analyze-ec2 command
        â€¢ Dependencies: EC2 analyzer (137 instances), Compute Optimizer
        â€¢ Feature Coordination: E2-E7 signals (when available)

        \b
        ðŸ’° Epic 4 Feature 2: Graviton Migration Analyzer
        Timeline: 7 days (5 dev + 2 testing)
        Business Impact: $800K+ annual savings potential

        \b
        ðŸ“– Examples:
            # Basic Graviton analysis with auto-profile routing (v1.1.11+)
            runbooks finops analyze-graviton-eligibility \\
                -i ec2-enriched.xlsx \\
                -o graviton-analysis.xlsx

            # With explicit profiles (backward compatible)
            runbooks finops analyze-graviton-eligibility \\
                -i ec2-enriched.xlsx \\
                -o graviton-analysis.xlsx \\
                -m mgmt-profile \\
                -b billing-profile

            # Disable AMI checking (faster execution)
            runbooks finops analyze-graviton-eligibility \\
                -i ec2-enriched.xlsx \\
                -o graviton-analysis.xlsx \\
                --no-ami-check

            # With verbose logging
            runbooks finops analyze-graviton-eligibility \\
                -i ec2-enriched.xlsx \\
                -o graviton-analysis.xlsx \\
                --verbose

        \b
        Pattern: Follows ec2_analyzer.py proven patterns with Rich CLI standards
        """
        try:
            from runbooks.finops.graviton_migration_analyzer import analyze_graviton_eligibility
            from runbooks.common.rich_utils import print_header, print_success, print_error

            # Configure logging if verbose
            if verbose:
                import logging
                logging.basicConfig(level=logging.DEBUG)

            print_header("Graviton Migration Analysis", f"Input: {input_file}")

            # Execute Graviton eligibility analysis
            result_df = analyze_graviton_eligibility(
                input_file=input_file,
                output_file=output_file,
                management_profile=management_profile,
                billing_profile=billing_profile,
                enable_ami_check=enable_ami_check,
                enable_compute_optimizer=enable_compute_optimizer
            )

            # Summary statistics
            total_instances = len(result_df)
            recommend_count = (result_df['graviton_recommendation'] == 'RECOMMEND').sum()
            evaluate_count = (result_df['graviton_recommendation'] == 'EVALUATE').sum()
            total_monthly_savings = result_df['graviton_monthly_savings'].sum()
            total_annual_savings = result_df['graviton_annual_savings'].sum()

            print_success(f"\nâœ… Graviton analysis complete!")
            print_success(f"   ðŸ“Š Analyzed {total_instances} EC2 instances")
            print_success(f"   âœ… {recommend_count} instances ready for migration (RECOMMEND)")
            print_success(f"   ðŸ” {evaluate_count} instances require testing (EVALUATE)")
            print_success(f"   ðŸ’° Potential savings: ${total_monthly_savings:,.2f}/month (${total_annual_savings:,.2f}/year)")
            print_success(f"   ðŸ“ Output: {output_file}")

            if total_annual_savings >= 800000:
                print_success(f"\nðŸŽ¯ $800K+ annual savings target ACHIEVED!")

            return result_df

        except ImportError as e:
            error_handlers["module_not_available"]("Graviton Migration Analyzer", e)
            raise click.ClickException("Graviton analysis functionality not available")
        except Exception as e:
            error_handlers["operation_failed"]("Graviton eligibility analysis", e)
            raise click.ClickException(str(e))

    @finops.command(name='optimize-savings-plans')
    @click.option('--lookback-days', default=90, type=int,
                  help='Usage history analysis period (default: 90 days, minimum: 30)')
    @click.option('--term-years', type=click.Choice(['1', '3']), default='1',
                  help='Commitment term: 1-year or 3-year (default: 1)')
    @click.option('--payment-option',
                  type=click.Choice(['ALL_UPFRONT', 'PARTIAL_UPFRONT', 'NO_UPFRONT']),
                  default='NO_UPFRONT',
                  help='Payment option (default: NO_UPFRONT)')
    @click.option('--billing-profile', help='AWS billing profile for Cost Explorer')
    @click.option('--validate-with-mcp/--no-mcp-validation', default=True,
                  help='Enable MCP validation (â‰¥99.5% accuracy, default: enabled)')
    @click.option('--output', type=click.Path(), help='Export recommendations to Excel')
    @click.option('--verbose', is_flag=True, help='Enable verbose logging')
    def optimize_savings_plans_cmd(lookback_days, term_years, payment_option, billing_profile,
                                   validate_with_mcp, output, verbose):
        """
        Generate hybrid Savings Plans + RI recommendations (60/30/10 strategy).

        \b
        ðŸŽ¯ HYBRID OPTIMIZATION STRATEGY
        â€¢ 60% Compute SP (flexible across EC2/Fargate/Lambda, 66% savings)
        â€¢ 30% EC2 Instance SP (stable workloads, 72% savings)
        â€¢ 10% On-Demand (burst capacity, flexibility)

        \b
        ðŸ“Š WORKLOAD CLASSIFICATION
        Stable workloads (EC2 Instance SP candidates):
        â€¢ Coefficient of Variation (CV) < 0.15
        â€¢ Uptime > 95% over analysis period
        â€¢ â‰¤2 regions (single or limited multi-region)

        Variable workloads (Compute SP candidates):
        â€¢ CV â‰¥ 0.15 (fluctuating usage patterns)
        â€¢ Multi-region (3+ regions for flexibility)

        \b
        ðŸ’° FINANCIAL MODELING
        â€¢ Break-even analysis (target: <10 months)
        â€¢ ROI calculations with upfront cost amortization
        â€¢ Annual savings projections ($500K+ target)
        â€¢ MCP validation (â‰¥99.5% accuracy for major commitments)

        \b
        ðŸ”§ ANALYSIS PERIOD
        â€¢ Default: 90 days (3 months minimum for SP sizing)
        â€¢ Minimum: 30 days (shorter periods reduce accuracy)
        â€¢ Recommended: 90-180 days for seasonal patterns

        \b
        ðŸ“– Examples:
            # Basic SP optimization with 90-day analysis
            runbooks finops optimize-savings-plans \\
                --billing-profile billing-account

            # 6-month analysis for seasonal patterns
            runbooks finops optimize-savings-plans \\
                --lookback-days 180 \\
                --billing-profile billing-account

            # 3-year term with partial upfront
            runbooks finops optimize-savings-plans \\
                --term-years 3 \\
                --payment-option PARTIAL_UPFRONT \\
                --billing-profile billing-account

            # Export to Excel with verbose logging
            runbooks finops optimize-savings-plans \\
                --billing-profile billing-account \\
                --output sp-recommendations.xlsx \\
                --verbose

            # Disable MCP validation (faster, but no accuracy guarantee)
            runbooks finops optimize-savings-plans \\
                --billing-profile billing-account \\
                --no-mcp-validation

        \b
        ðŸŽ¯ Epic 5 Feature 3: Savings Plans Hybrid Optimizer
        Timeline: 5 days (4 dev + 1 testing)
        Business Impact: $500K+ annual savings potential
        Target Accuracy: â‰¥99.5% MCP validation for cost projections

        \b
        Pattern: Follows reservation_optimizer.py proven patterns with Rich CLI standards
        Integration: Complementary to existing RI optimizer for unified procurement strategy
        """
        try:
            import asyncio
            from runbooks.finops.savings_plans_optimizer import SavingsPlansOptimizer
            from runbooks.common.rich_utils import print_header, print_success, print_error, print_info

            # Configure logging if verbose
            if verbose:
                import logging
                logging.basicConfig(level=logging.DEBUG)

            print_header("Savings Plans Hybrid Optimizer", f"Analysis Period: {lookback_days} days")

            # Initialize optimizer
            optimizer = SavingsPlansOptimizer(
                profile_name=billing_profile,
                regions=None  # Will use default regions from profile
            )

            # Execute comprehensive analysis
            recommendations = asyncio.run(
                optimizer.generate_recommendations(
                    usage_history_days=lookback_days,
                    validate_with_mcp=validate_with_mcp
                )
            )

            if not recommendations:
                print_info("No cost-effective Savings Plans opportunities identified")
                return

            # Calculate summary statistics
            total_savings = sum(rec.estimated_annual_savings for rec in recommendations)
            total_commitment = sum(rec.commitment_usd_hourly for rec in recommendations)
            compute_sp_count = sum(1 for r in recommendations if r.plan_type.value == "Compute")
            ec2_instance_sp_count = sum(1 for r in recommendations if r.plan_type.value == "EC2Instance")

            # Display final success message
            print_success(f"\nâœ… Savings Plans optimization complete!")
            print_success(f"   ðŸ“Š {len(recommendations)} recommendations generated")
            print_success(f"   ðŸ’° Potential annual savings: ${total_savings:,.2f}")
            print_success(f"   ðŸ’² Total hourly commitment: ${total_commitment:.2f}/hour")
            print_success(f"   ðŸ”§ Plan breakdown:")
            if compute_sp_count > 0:
                print_success(f"      â€¢ {compute_sp_count} Compute SP (60% allocation, flexible)")
            if ec2_instance_sp_count > 0:
                print_success(f"      â€¢ {ec2_instance_sp_count} EC2 Instance SP (30% allocation, stable)")
            print_info(f"      â€¢ 10% remains On-Demand (burst capacity)")

            # Epic 5 achievement validation
            if total_savings >= 500000:
                print_success(f"\nðŸŽ¯ $500K+ annual savings target ACHIEVED!")
                print_info(f"   Epic 5 (Reserved Capacity): 40%â†’80% progression unlocked")

            # Export to Excel if requested
            if output:
                try:
                    import pandas as pd
                    from openpyxl import Workbook
                    from openpyxl.styles import Font, PatternFill, Alignment

                    # Create Excel workbook
                    wb = Workbook()
                    ws = wb.active
                    ws.title = "SP Recommendations"

                    # Headers
                    headers = [
                        "Plan Type", "Commitment ($/hr)", "Annual Savings",
                        "Coverage %", "Allocation", "Confidence %", "ROI %",
                        "Break-even (months)", "Risk Level", "Regions", "Services"
                    ]
                    ws.append(headers)

                    # Style headers
                    header_fill = PatternFill(start_color="4472C4", end_color="4472C4", fill_type="solid")
                    header_font = Font(bold=True, color="FFFFFF")
                    for cell in ws[1]:
                        cell.fill = header_fill
                        cell.font = header_font
                        cell.alignment = Alignment(horizontal="center")

                    # Data rows
                    for rec in recommendations:
                        ws.append([
                            rec.plan_type.value,
                            f"${rec.commitment_usd_hourly:.2f}",
                            f"${rec.estimated_annual_savings:,.2f}",
                            f"{rec.coverage_percentage:.1f}%",
                            rec.hybrid_strategy.get("allocation", "N/A"),
                            f"{rec.confidence_score * 100:.1f}%",
                            f"{rec.roi_percentage:.1f}%",
                            f"{rec.break_even_months:.1f}",
                            rec.risk_level,
                            ", ".join(rec.regions[:3]) + ("..." if len(rec.regions) > 3 else ""),
                            ", ".join(rec.services)
                        ])

                    # Auto-adjust column widths
                    for column in ws.columns:
                        max_length = 0
                        column_letter = column[0].column_letter
                        for cell in column:
                            try:
                                if len(str(cell.value)) > max_length:
                                    max_length = len(str(cell.value))
                            except:
                                pass
                        adjusted_width = min(max_length + 2, 50)
                        ws.column_dimensions[column_letter].width = adjusted_width

                    # Save workbook
                    wb.save(output)
                    print_success(f"   ðŸ“ Recommendations exported: {output}")

                except Exception as e:
                    print_error(f"Failed to export Excel: {str(e)}")

        except ImportError as e:
            error_handlers["module_not_available"]("Savings Plans Optimizer", e)
            raise click.ClickException("Savings Plans optimization functionality not available")
        except Exception as e:
            error_handlers["operation_failed"]("Savings Plans optimization", e)
            raise click.ClickException(str(e))

    @finops.command("optimize-s3-lifecycle")
    @click.option("--profile", help="AWS profile name")
    @click.option("--regions", multiple=True, help="AWS regions to analyze")
    @click.option("--dry-run/--no-dry-run", default=True, help="Execute in dry-run mode (READ-ONLY analysis)")
    @click.option(
        "-f",
        "--format",
        "--export-format",
        type=click.Choice(["json", "csv"]),
        default="json",
        help="Export format for results",
    )
    @click.option("--output-file", help="Output file path for results export")
    def optimize_s3_lifecycle_cmd(profile, regions, dry_run, format, output_file):
        """
        S3 Lifecycle Optimizer - Automated Storage Cost Optimization ($180K target)

        Comprehensive S3 lifecycle policy optimization targeting $180K annual savings:
        â€¢ Intelligent-Tiering for hybrid access patterns (50% IA savings)
        â€¢ Glacier transitions for archive-eligible data (80% savings)
        â€¢ Deep Archive for long-term retention (92% savings)
        â€¢ Lifecycle expiration for temporary/log data

        Part of Epic 3 completion strategy (70% â†’ 85%).

        SAFETY: READ-ONLY analysis only - lifecycle policies require explicit approval.

        Examples:
            runbooks finops optimize-s3-lifecycle
            runbooks finops optimize-s3-lifecycle --profile my-profile --regions ap-southeast-2
            runbooks finops optimize-s3-lifecycle --export-format csv --output-file s3_recommendations.csv
        """
        try:
            # Lazy import for performance
            from runbooks.finops.s3_lifecycle_optimizer import S3LifecycleOptimizer
            import asyncio

            print_header("S3 Lifecycle Automation", "Enterprise Storage Cost Optimization")

            # Initialize optimizer
            optimizer = S3LifecycleOptimizer(profile_name=profile, regions=list(regions) if regions else None)

            # Execute analysis
            results = asyncio.run(optimizer.analyze_s3_lifecycle_optimization(dry_run=dry_run))

            # Export results if requested
            if output_file or format != "json":
                optimizer.export_recommendations(results, output_file, format)

            # Display final success message
            if results.total_potential_annual_savings > 0:
                print_success(
                    f"Analysis complete: {format_cost(results.total_potential_annual_savings)} potential annual savings"
                )
                print_info(
                    f"Strategies: IT ({format_cost(results.intelligent_tiering_annual_savings)}) | "
                    f"Glacier ({format_cost(results.glacier_annual_savings)})"
                )
            else:
                print_info("Analysis complete: All S3 buckets have optimal lifecycle policies")

        except ImportError as e:
            error_handlers["module_not_available"]("S3 Lifecycle Optimizer", e)
            raise click.ClickException("S3 Lifecycle optimization functionality not available")
        except KeyboardInterrupt:
            print_warning("Analysis interrupted by user")
            raise click.Abort()
        except Exception as e:
            error_handlers["operation_failed"]("S3 Lifecycle optimization", e)
            raise click.ClickException(str(e))

    @finops.command("detect-rds-idle")
    @common_aws_options
    @click.option(
        "--lookback-days",
        type=int,
        default=7,
        help="Analysis period in days for CloudWatch metrics (default: 7)"
    )
    @click.option(
        "--connection-threshold",
        type=int,
        default=10,
        help="Daily connection threshold for idle detection (default: 10)"
    )
    @click.option(
        "--cpu-threshold",
        type=float,
        default=5.0,
        help="CPU percentage threshold for idle detection (default: 5.0)"
    )
    @click.option(
        "--export-json",
        is_flag=True,
        default=False,
        help="Export results to JSON file"
    )
    @click.option(
        "--output-file",
        type=click.Path(),
        help="Output file path for JSON export"
    )
    def detect_rds_idle(profile, regions, lookback_days, connection_threshold, cpu_threshold, export_json, output_file):
        """
        Detect idle RDS instances for $50K annual savings potential.

        Analyzes RDS instances using 5 idle signals:
        - I1: DatabaseConnections <10/day (40 points)
        - I2: CPUUtilization <5% avg (30 points)
        - I3: ReadIOPS + WriteIOPS <100/day (15 points)
        - I4: NetworkReceiveThroughput <1MB/day (10 points)
        - I5: No recent snapshots modified (5 points)

        Recommendations:
        - Score 80-100: TERMINATE (high confidence idle)
        - Score 60-79: STOP (medium confidence)
        - Score 40-59: DOWNSIZE (low utilization)
        - Score <40: KEEP (active usage)

        Examples:
            # Basic idle detection with 7-day lookback
            runbooks finops detect-rds-idle --profile prod

            # Extended analysis with 30-day lookback
            runbooks finops detect-rds-idle --profile prod --lookback-days 30

            # Custom thresholds with JSON export
            runbooks finops detect-rds-idle --profile prod --connection-threshold 5 --cpu-threshold 3.0 --export-json
        """
        try:
            from runbooks.finops.rds_analyzer import RDSAnalysisConfig, RDSCostAnalyzer
            import json

            # Initialize configuration
            config = RDSAnalysisConfig(
                management_profile=profile,
                billing_profile=profile,
                regions=list(regions) if regions else ['ap-southeast-2'],
                enable_organizations=False,  # Not needed for idle detection
                enable_cost=False  # Using internal cost estimation
            )

            # Initialize analyzer
            analyzer = RDSCostAnalyzer(config)

            # Execute idle detection
            idle_analyses = analyzer.detect_idle_instances(
                lookback_days=lookback_days,
                connection_threshold=connection_threshold,
                cpu_threshold=cpu_threshold
            )

            if not idle_analyses:
                print_warning("No RDS instances found for idle detection")
                return

            # Display results in Rich table
            from rich.table import Table

            table = Table(
                title=f"RDS Idle Instance Detection ({len(idle_analyses)} instances analyzed)",
                show_header=True,
                header_style="bold cyan"
            )

            table.add_column("Instance ID", style="yellow")
            table.add_column("Class", style="cyan")
            table.add_column("Engine", style="blue")
            table.add_column("Region", style="magenta")
            table.add_column("Idle Score", justify="right", style="bold")
            table.add_column("Confidence", style="green")
            table.add_column("Recommendation", style="bold red")
            table.add_column("Annual Savings", justify="right", style="green")

            for analysis in sorted(idle_analyses, key=lambda x: x.idle_score, reverse=True):
                # Color-code recommendation
                recommendation_color = {
                    "TERMINATE": "bold red",
                    "STOP": "yellow",
                    "DOWNSIZE": "cyan",
                    "KEEP": "green"
                }.get(analysis.recommendation, "white")

                table.add_row(
                    analysis.instance_id,
                    analysis.instance_class,
                    analysis.engine,
                    analysis.region,
                    f"{analysis.idle_score}/100",
                    analysis.idle_confidence,
                    f"[{recommendation_color}]{analysis.recommendation}[/{recommendation_color}]",
                    f"${analysis.annual_savings_potential:,.2f}"
                )

            console.print(table)

            # Summary statistics
            high_confidence = [a for a in idle_analyses if a.idle_confidence == "HIGH"]
            total_savings = sum(a.annual_savings_potential for a in idle_analyses)

            print_info(f"\nSummary:")
            print_info(f"  Total instances analyzed: {len(idle_analyses)}")
            print_info(f"  High-confidence idle: {len(high_confidence)}")
            print_info(f"  Total annual savings potential: ${total_savings:,.2f}")

            # Export to JSON if requested
            if export_json or output_file:
                output_path = output_file or f"rds-idle-detection-{datetime.now().strftime('%Y%m%d-%H%M%S')}.json"

                export_data = {
                    "analysis_timestamp": datetime.now().isoformat(),
                    "lookback_days": lookback_days,
                    "connection_threshold": connection_threshold,
                    "cpu_threshold": cpu_threshold,
                    "total_instances": len(idle_analyses),
                    "high_confidence_idle": len(high_confidence),
                    "total_annual_savings": total_savings,
                    "instances": [
                        {
                            "instance_id": a.instance_id,
                            "instance_class": a.instance_class,
                            "engine": a.engine,
                            "account_id": a.account_id,
                            "region": a.region,
                            "idle_signals": a.idle_signals,
                            "idle_score": a.idle_score,
                            "idle_confidence": a.idle_confidence,
                            "monthly_cost": a.monthly_cost,
                            "annual_savings_potential": a.annual_savings_potential,
                            "recommendation": a.recommendation
                        }
                        for a in idle_analyses
                    ]
                }

                with open(output_path, 'w') as f:
                    json.dump(export_data, f, indent=2)

                print_success(f"Results exported to: {output_path}")

        except ImportError as e:
            error_handlers["module_not_available"]("RDS Idle Detection", e)
            raise click.ClickException("RDS idle detection functionality not available")
        except KeyboardInterrupt:
            print_warning("Detection interrupted by user")
            raise click.Abort()
        except Exception as e:
            error_handlers["operation_failed"]("RDS idle detection", e)
            raise click.ClickException(str(e))

    # Phase 1B: Critical Gap Closure Commands (Cost Optimization Playbook Enhancement)

    @finops.command("optimize-cloudwatch-costs")
    @common_aws_options
    @click.option(
        "--regions",
        multiple=True,
        help="AWS regions to analyze (default: major commercial regions)"
    )
    @click.option(
        "--mcp-validation",
        is_flag=True,
        help="Enable MCP validation for cost projections"
    )
    @click.option(
        "--export-format",
        type=click.Choice(["json", "csv", "markdown"]),
        help="Export results format"
    )
    @click.option(
        "--output-file",
        type=click.Path(),
        help="Output file path for results export"
    )
    @click.option("--executive", is_flag=True, help="Executive summary format (business narrative, <5 min review)")
    def optimize_cloudwatch_costs(profile, regions, dry_run, mcp_validation, export_format, output_file, executive):
        """
        Analyze and optimize CloudWatch log retention costs.

        Implements Cost Optimization Playbook Phase 4 (CloudWatch cost controls).

        Capabilities:
        - Log group retention policy optimization
        - Cost savings calculations ($0.50/GB ingestion, $0.03/GB storage)
        - Automated retention recommendations (7/30/90/365 days)
        - Infinite retention detection and remediation

        Business Impact: Typical savings of $10K-$50K annually
        """
        try:
            import asyncio
            from runbooks.finops.cloudwatch_cost_optimizer import CloudWatchCostOptimizer

            print_header("CloudWatch Cost Optimization", "Enterprise Log Retention Control")

            optimizer = CloudWatchCostOptimizer(
                profile_name=profile,
                regions=list(regions) if regions else None,
                dry_run=dry_run
            )

            results = asyncio.run(optimizer.analyze_cloudwatch_costs(enable_mcp_validation=mcp_validation))

            print_success(f"âœ… CloudWatch analysis complete: {results.log_groups_optimizable} optimization opportunities")
            print_info(f"   Potential annual savings: ${results.potential_annual_savings:,.2f}")

        except ImportError as e:
            error_handlers["module_not_available"]("CloudWatch Cost Optimizer", e)
            raise click.ClickException("CloudWatch cost optimization not available")
        except Exception as e:
            error_handlers["operation_failed"]("CloudWatch cost optimization", e)
            raise click.ClickException(str(e))

    @finops.command("detect-orphans")
    @common_aws_options
    @click.option(
        "--regions",
        multiple=True,
        help="AWS regions to analyze"
    )
    @click.option(
        "--resource-type",
        type=click.Choice(["all", "ebs", "eip", "logs", "nat", "lb", "snapshot"]),
        default="all",
        help="Resource type to detect (default: all)"
    )
    @click.option(
        "--validate-with-config",
        is_flag=True,
        help="Validate orphans with AWS Config compliance rules"
    )
    @click.option("--executive", is_flag=True, help="Executive summary format (business narrative, <5 min review)")
    def detect_orphans(profile, regions, resource_type, validate_with_config, executive):
        """
        Detect orphaned AWS resources across multiple types.

        Implements unified orphan detection from Cost Optimization Playbook Phase 3.

        Orphan Types:
        - EBS volumes (unattached >30 days)
        - Elastic IPs (unallocated)
        - CloudWatch Log Groups (no recent events)
        - NAT Gateways (no traffic)
        - Load Balancers (no targets)
        - Snapshots (orphaned - no AMI/volume reference)

        Decision Rubric:
        - MUST: Immediate action (zero risk, high cost)
        - SHOULD: High priority (low risk, moderate cost)
        - COULD: Investigation recommended

        Business Impact: Typical savings of $50K-$200K annually
        """
        try:
            import asyncio
            from runbooks.finops.orphan_resource_detector import (
                OrphanResourceDetector,
                OrphanResourceType
            )

            print_header("Unified Orphan Detection", "Enterprise Waste Elimination")

            # Map CLI resource type to enum
            resource_type_map = {
                "all": OrphanResourceType.ALL,
                "ebs": OrphanResourceType.EBS_VOLUME,
                "eip": OrphanResourceType.ELASTIC_IP,
                "nat": OrphanResourceType.NAT_GATEWAY,
                "lb": OrphanResourceType.LOAD_BALANCER,
            }

            detector = OrphanResourceDetector(
                profile_name=profile,
                regions=list(regions) if regions else None
            )

            results = asyncio.run(
                detector.detect_orphaned_resources(resource_type=resource_type_map[resource_type])
            )

            print_success(f"âœ… Orphan detection complete: {results.total_orphans_detected} orphans found")
            print_info(f"   MUST act: {results.orphans_by_decision_level.get('MUST', 0)}")
            print_info(f"   Potential annual savings: ${results.potential_annual_savings:,.2f}")

            if validate_with_config:
                print_info("ðŸ”— AWS Config validation enabled - integrating with security module")

        except ImportError as e:
            error_handlers["module_not_available"]("Orphan Resource Detector", e)
            raise click.ClickException("Orphan detection not available")
        except Exception as e:
            error_handlers["operation_failed"]("Orphan detection", e)
            raise click.ClickException(str(e))

    @finops.command("analyze-s3-storage-lens")
    @common_aws_options
    @click.option(
        "--region",
        default="ap-southeast-2",
        help="AWS region for S3 client (S3 is global)"
    )
    def analyze_s3_storage_lens(profile, region):
        """
        Analyze S3 Storage Lens metrics for cost optimization.

        Implements Cost Optimization Playbook Feature #6 (S3 Storage Lens integration).

        Capabilities:
        - Bucket lifecycle policy gap detection
        - Intelligent-Tiering recommendations
        - Incomplete multipart upload (MPU) detection
        - Fastest-growing bucket identification
        - Storage class distribution analysis

        Business Impact: Typical savings of $30K-$150K annually
        """
        try:
            import asyncio
            from runbooks.finops.s3_storage_lens_analyzer import S3StorageLensAnalyzer

            print_header("S3 Storage Lens Analysis", "Enterprise S3 Cost Intelligence")

            analyzer = S3StorageLensAnalyzer(
                profile_name=profile,
                regions=[region]
            )

            results = asyncio.run(analyzer.analyze_s3_storage_lens())

            print_success(f"âœ… S3 Storage Lens analysis complete: {results.total_buckets_analyzed} buckets")
            print_info(f"   Buckets without lifecycle: {results.buckets_without_lifecycle}")
            print_info(f"   Potential annual savings: ${results.potential_annual_savings:,.2f}")

        except ImportError as e:
            error_handlers["module_not_available"]("S3 Storage Lens Analyzer", e)
            raise click.ClickException("S3 Storage Lens analysis not available")
        except Exception as e:
            error_handlers["operation_failed"]("S3 Storage Lens analysis", e)
            raise click.ClickException(str(e))

    @finops.command("check-config-compliance")
    @common_aws_options
    @click.option(
        "--regions",
        multiple=True,
        help="AWS regions to analyze"
    )
    @click.option(
        "--config-rules",
        multiple=True,
        type=click.Choice(["ebs-inuse", "eip-attached", "cw-retention", "all"]),
        default=["all"],
        help="AWS Config rules to check (default: all)"
    )
    def check_config_compliance(profile, regions, config_rules):
        """
        Check AWS Config compliance and map to cost impact.

        Implements Cost Optimization Playbook Phase 2 (AWS Config integration).

        Config Rules:
        - ebs-inuse: Detect unattached EBS volumes
        - eip-attached: Detect unallocated Elastic IPs
        - cw-retention: CloudWatch log retention compliance

        Integration:
        - Cross-module with security/config module
        - Maps compliance violations to cost impact
        - Correlates with orphan detection

        Business Impact: Typical savings of $20K-$80K annually
        """
        try:
            import asyncio
            from runbooks.finops.config_compliance_checker import (
                ConfigComplianceChecker,
                ConfigComplianceRule
            )

            print_header("AWS Config FinOps Integration", "Compliance-Driven Cost Optimization")

            # Map CLI config rules to enum
            config_rule_map = {
                "ebs-inuse": ConfigComplianceRule.EBS_VOLUME_INUSE_CHECK,
                "eip-attached": ConfigComplianceRule.EIP_ATTACHED,
                "cw-retention": ConfigComplianceRule.CW_LOGGROUP_RETENTION_PERIOD_CHECK,
            }

            rules_to_check = []
            if "all" in config_rules:
                rules_to_check = [
                    ConfigComplianceRule.EBS_VOLUME_INUSE_CHECK,
                    ConfigComplianceRule.EIP_ATTACHED,
                ]
            else:
                rules_to_check = [config_rule_map[rule] for rule in config_rules if rule in config_rule_map]

            checker = ConfigComplianceChecker(
                profile_name=profile,
                regions=list(regions) if regions else None
            )

            results = asyncio.run(checker.check_config_compliance(config_rules=rules_to_check))

            print_success(f"âœ… Config compliance check complete: {results.total_violations_detected} violations")
            print_info(f"   Annual cost impact: ${results.total_annual_cost_impact:,.2f}")

            if results.total_violations_detected > 0:
                print_info("ðŸ’¡ Tip: Run 'runbooks finops detect-orphans --validate-with-config' for unified analysis")

        except ImportError as e:
            error_handlers["module_not_available"]("Config Compliance Checker", e)
            raise click.ClickException("Config compliance checking not available")
        except Exception as e:
            error_handlers["operation_failed"]("Config compliance check", e)
            raise click.ClickException(str(e))

    return finops

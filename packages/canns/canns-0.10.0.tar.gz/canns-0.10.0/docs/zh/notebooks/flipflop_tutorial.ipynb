{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "08f86cb1",
   "metadata": {},
   "source": [
    "# RNN 不动点分析说明书：以 FlipFlop 任务为例\n",
    "\n",
    "**目标：** 本文档将作为一份“说明书”，详细介绍如何使用 `FixedPointFinder` 工具，来分析在 FlipFlop 任务上训练的 RNN。\n",
    "\n",
    "**结构：**\n",
    "1.  **原理介绍**：什么是固定点？\n",
    "2.  **环境设置**：导入脚本所需要的库\n",
    "3.  **组件定义**：逐一介绍 `FlipFlopData`、`FlipFlopRNN` 和 `train_flipflop_rnn` 函数。\n",
    "4.  **核心用法**：演示 `FixedPointFinder` 的具体使用流程。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "052757d4",
   "metadata": {},
   "source": [
    "## 1. 原理介绍：什么是固定点？\n",
    "\n",
    "**固定点 (Fixed Point)** 是动力学系统中的一个核心概念。对于一个 RNN，我们可以将其视为一个函数 `h_t+1 = F(h_t, u_t)`，其中 `h` 是隐藏状态，`u` 是输入。\n",
    "\n",
    "当**输入 `u` 保持恒定**时（例如，在 FlipFlop 任务中没有脉冲输入的“记忆”阶段，`u=0`），系统演变为 `h_t+1 = F(h_t)`。\n",
    "\n",
    "一个**固定点 `x*`** 就是满足 `x* = F(x*)` 的状态。\n",
    "* **稳定固定点 (Stable Fixed Point)**：例如一个“吸引子”。如果 RNN 的状态 `h` 跑到了 `x*` 附近，它最终会停留在 `x*`。\n",
    "* **不稳定固定点 (Unstable Fixed Point)**：例如一个“排斥子”或“鞍点”。\n",
    "\n",
    "**核心原理：** 训练 RNN 完成 FlipFlop 任务。当训练成功后，RNN 会学会为它需要“记忆”的**每一种状态**（例如 `[+1, +1]` 或 `[+1, -1]`）都创造一个**稳定固定点**。当输入 `u=0` 时，RNN 状态会自动流向并停留在这些不动点上，从而实现“记忆”功能。\n",
    "\n",
    "本教程的**目的**就是使用 `FixedPointFinder` 工具，把这些被 RNN “藏起来”的稳定固定点全部找出来。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "415fbf06",
   "metadata": {},
   "source": [
    "## 2. 导入\n",
    "导入 `flipflop_fixed_points.py` 脚本中使用的所有库。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4ef365a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import brainstate as bst\n",
    "import braintools as bts\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import numpy as np\n",
    "import random\n",
    "from canns.analyzer.plotting import plot_fixed_points_2d, plot_fixed_points_3d, PlotConfig\n",
    "from canns.analyzer.slow_points import FixedPointFinder, save_checkpoint, load_checkpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23cc7e94",
   "metadata": {},
   "source": [
    "## 3. 组件定义：数据、模型与训练\n",
    "\n",
    "这部分，我们完整定义了 `flipflop_fixed_points.py` 脚本中的三个核心组件。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9d527d8",
   "metadata": {},
   "source": [
    "### 3.1 组件 1：FlipFlopData 类\n",
    "这是 `flipflop_fixed_points.py` 脚本中的 `FlipFlopData` 类。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d5f581f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FlipFlopData:\n",
    "    \"\"\"Generator for flip-flop memory task data.\"\"\"\n",
    "\n",
    "    def __init__(self, n_bits=3, n_time=64, p=0.5, random_seed=0):\n",
    "        \"\"\"Initialize FlipFlopData generator.\n",
    "\n",
    "        Args:\n",
    "            n_bits: Number of memory channels.\n",
    "            n_time: Number of timesteps per trial.\n",
    "            p: Probability of input pulse at each timestep.\n",
    "            random_seed: Random seed for reproducibility.\n",
    "        \"\"\"\n",
    "        self.rng = np.random.RandomState(random_seed)\n",
    "        self.n_time = n_time\n",
    "        self.n_bits = n_bits\n",
    "        self.p = p\n",
    "\n",
    "    def generate_data(self, n_trials):\n",
    "        \"\"\"Generate flip-flop task data.\n",
    "\n",
    "        Args:\n",
    "            n_trials: Number of trials to generate.\n",
    "\n",
    "        Returns:\n",
    "            dict with 'inputs' and 'targets' arrays [n_trials x n_time x n_bits].\n",
    "        \"\"\"\n",
    "        n_time = self.n_time\n",
    "        n_bits = self.n_bits\n",
    "        p = self.p\n",
    "\n",
    "        # Generate unsigned input pulses\n",
    "        unsigned_inputs = self.rng.binomial(1, p, [n_trials, n_time, n_bits])\n",
    "\n",
    "        # Ensure every trial starts with a pulse\n",
    "        unsigned_inputs[:, 0, :] = 1\n",
    "\n",
    "        # Generate random signs {-1, +1}\n",
    "        random_signs = 2 * self.rng.binomial(1, 0.5, [n_trials, n_time, n_bits]) - 1\n",
    "\n",
    "        # Apply random signs\n",
    "        inputs = unsigned_inputs * random_signs\n",
    "\n",
    "        # Compute targets\n",
    "        targets = np.zeros([n_trials, n_time, n_bits])\n",
    "        for trial_idx in range(n_trials):\n",
    "            for bit_idx in range(n_bits):\n",
    "                input_seq = inputs[trial_idx, :, bit_idx]\n",
    "                t_flip = np.where(input_seq != 0)[0]\n",
    "                for flip_idx in range(len(t_flip)):\n",
    "                    t_flip_i = t_flip[flip_idx]\n",
    "                    targets[trial_idx, t_flip_i:, bit_idx] = inputs[\n",
    "                        trial_idx, t_flip_i, bit_idx\n",
    "                    ]\n",
    "\n",
    "        return {\n",
    "            \"inputs\": inputs.astype(np.float32),\n",
    "            \"targets\": targets.astype(np.float32),\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e478520",
   "metadata": {},
   "source": [
    "### 3.2 组件 2：FlipFlopRNN 类\n",
    "这是 `flipflop_fixed_points.py` 脚本中的 `FlipFlopRNN` 类。\n",
    "\n",
    "**用法说明：** `FixedPointFinder` 的**原理**是寻找 `x = F(x, u)`。为了计算 `F(x, u)`，它会调用 `rnn_model(inputs, hidden)`。\n",
    "`FixedPointFinder` 会传入 `inputs` 形状为 `[batch, 1, n_inputs]`，`hidden` 形状为 `[batch, n_hidden]`。\n",
    "\n",
    "因此，你的 `__call__` 方法**必须**能处理 `n_time == 1` 的情况，并返回 `(outputs, h_next)`。\n",
    "\n",
    "请看下面代码中 `if n_time == 1:` 这个分支，这正是为了适配 `FixedPointFinder` 而设计的具体用法。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db2cda0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FlipFlopRNN(bst.nn.Module):\n",
    "    \"\"\"RNN model for the flip-flop memory task.\"\"\"\n",
    "\n",
    "    def __init__(self, n_inputs, n_hidden, n_outputs, rnn_type=\"gru\", seed=0):\n",
    "        \"\"\"Initialize FlipFlop RNN.\n",
    "\n",
    "        Args:\n",
    "            n_inputs: Number of input channels.\n",
    "            n_hidden: Number of hidden units.\n",
    "            n_outputs: Number of output channels.\n",
    "            rnn_type: Type of RNN cell ('tanh', 'gru').\n",
    "            seed: Random seed for weight initialization.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.n_inputs = n_inputs\n",
    "        self.n_hidden = n_hidden\n",
    "        self.n_outputs = n_outputs\n",
    "        self.rnn_type = rnn_type.lower()\n",
    "\n",
    "        # Initialize RNN cell parameters\n",
    "        key = jax.random.PRNGKey(seed)\n",
    "        k1, k2, k3, k4 = jax.random.split(key, 4)\n",
    "\n",
    "        if rnn_type == \"tanh\":\n",
    "            # Simple tanh RNN\n",
    "            self.w_ih = bst.ParamState(\n",
    "                jax.random.normal(k1, (n_inputs, n_hidden)) * 0.1\n",
    "            )\n",
    "            self.w_hh = bst.ParamState(\n",
    "                jax.random.normal(k2, (n_hidden, n_hidden)) * 0.5\n",
    "            )\n",
    "            self.b_h = bst.ParamState(jnp.zeros(n_hidden))\n",
    "        elif rnn_type == \"gru\":\n",
    "            # GRU cell\n",
    "            self.w_ir = bst.ParamState(\n",
    "                jax.random.normal(k1, (n_inputs, n_hidden)) * 0.1\n",
    "            )\n",
    "            self.w_hr = bst.ParamState(\n",
    "                jax.random.normal(k2, (n_hidden, n_hidden)) * 0.5\n",
    "            )\n",
    "            self.w_iz = bst.ParamState(\n",
    "                jax.random.normal(k3, (n_inputs, n_hidden)) * 0.1\n",
    "            )\n",
    "            self.w_hz = bst.ParamState(\n",
    "                jax.random.normal(k4, (n_hidden, n_hidden)) * 0.5\n",
    "            )\n",
    "            k5, k6, k7, k8 = jax.random.split(k4, 4)\n",
    "            self.w_in = bst.ParamState(\n",
    "                jax.random.normal(k5, (n_inputs, n_hidden)) * 0.1\n",
    "            )\n",
    "            self.w_hn = bst.ParamState(\n",
    "                jax.random.normal(k6, (n_hidden, n_hidden)) * 0.5\n",
    "            )\n",
    "            self.b_r = bst.ParamState(jnp.zeros(n_hidden))\n",
    "            self.b_z = bst.ParamState(jnp.zeros(n_hidden))\n",
    "            self.b_n = bst.ParamState(jnp.zeros(n_hidden))\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported rnn_type: {rnn_type}\")\n",
    "\n",
    "        # Readout layer\n",
    "        self.w_out = bst.ParamState(\n",
    "            jax.random.normal(k3, (n_hidden, n_outputs)) * 0.1\n",
    "        )\n",
    "        self.b_out = bst.ParamState(jnp.zeros(n_outputs))\n",
    "\n",
    "        # Initial hidden state\n",
    "        self.h0 = bst.ParamState(jnp.zeros(n_hidden))\n",
    "\n",
    "    def step(self, x_t, h):\n",
    "        \"\"\"Single RNN step.\n",
    "\n",
    "        Args:\n",
    "            x_t: [batch_size x n_inputs] input at time t.\n",
    "            h: [batch_size x n_hidden] hidden state.\n",
    "\n",
    "        Returns:\n",
    "            h_next: [batch_size x n_hidden] next hidden state.\n",
    "        \"\"\"\n",
    "        if self.rnn_type == \"tanh\":\n",
    "            # Simple tanh RNN step\n",
    "            h_next = jnp.tanh(\n",
    "                x_t @ self.w_ih.value + h @ self.w_hh.value + self.b_h.value\n",
    "            )\n",
    "        elif self.rnn_type == \"gru\":\n",
    "            # GRU step\n",
    "            r = jax.nn.sigmoid(\n",
    "                x_t @ self.w_ir.value + h @ self.w_hr.value + self.b_r.value\n",
    "            )\n",
    "            z = jax.nn.sigmoid(\n",
    "                x_t @ self.w_iz.value + h @ self.w_hz.value + self.b_z.value\n",
    "            )\n",
    "            n = jnp.tanh(\n",
    "                x_t @ self.w_in.value + (r * h) @ self.w_hn.value + self.b_n.value\n",
    "            )\n",
    "            h_next = (1 - z) * n + z * h\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown rnn_type: {self.rnn_type}\")\n",
    "\n",
    "        return h_next\n",
    "\n",
    "    def __call__(self, inputs, hidden=None):\n",
    "        \"\"\"Forward pass through the RNN. Optimized with jax.lax.scan.\"\"\"\n",
    "        batch_size = inputs.shape[0]\n",
    "        n_time = inputs.shape[1]\n",
    "\n",
    "        # Initialize hidden state\n",
    "        if hidden is None:\n",
    "            h = jnp.tile(self.h0.value, (batch_size, 1))\n",
    "        else:\n",
    "            h = hidden\n",
    "\n",
    "        # Single-step computation mode for the fixed-point finder\n",
    "        if n_time == 1:\n",
    "            x_t = inputs[:, 0, :]\n",
    "            h_next = self.step(x_t, h)\n",
    "            y = h_next @ self.w_out.value + self.b_out.value\n",
    "            return y[:, None, :], h_next\n",
    "\n",
    "        # Full sequence case\n",
    "        def scan_fn(carry, x_t):\n",
    "            \"\"\"Single-step scan function\"\"\"\n",
    "            h_prev = carry\n",
    "            h_next = self.step(x_t, h_prev)\n",
    "            y_t = h_next @ self.w_out.value + self.b_out.value\n",
    "            return h_next, (y_t, h_next)\n",
    "\n",
    "        # (batch, time, features) -> (time, batch, features)\n",
    "        inputs_transposed = inputs.transpose(1, 0, 2)\n",
    "\n",
    "        # Run the scan\n",
    "        _, (outputs_seq, hiddens_seq) = jax.lax.scan(scan_fn, h, inputs_transposed)\n",
    "\n",
    "        outputs = outputs_seq.transpose(1, 0, 2)\n",
    "        hiddens = hiddens_seq.transpose(1, 0, 2)\n",
    "\n",
    "        return outputs, hiddens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e97bc4e",
   "metadata": {},
   "source": [
    "### 3.3 组件 3：train_flipflop_rnn 函数\n",
    "这是 `flipflop_fixed_points.py` 脚本中的 `train_flipflop_rnn` 函数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cfcf0e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_flipflop_rnn(rnn, train_data, valid_data,\n",
    "                       learning_rate=0.08,\n",
    "                       batch_size=128,\n",
    "                       max_epochs=1000,\n",
    "                       min_loss=1e-4,\n",
    "                       print_every=10):\n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"Training FlipFlop RNN (Using bts Scheduler & built-in Grad Clip)\")\n",
    "    print(\"=\" * 70)\n",
    "\n",
    "    # Prepare data\n",
    "    train_inputs = jnp.array(train_data['inputs'])\n",
    "    train_targets = jnp.array(train_data['targets'])\n",
    "    valid_inputs = jnp.array(valid_data['inputs'])\n",
    "    valid_targets = jnp.array(valid_data['targets'])\n",
    "    n_train = train_inputs.shape[0]\n",
    "    n_batches = n_train // batch_size\n",
    "\n",
    "    # Flatten parameter keys\n",
    "    def flatten_key(key):\n",
    "        return '.'.join(key) if isinstance(key, tuple) else key\n",
    "\n",
    "    trainable_states = {flatten_key(name): state for name, state in rnn.states().items() if\n",
    "                        isinstance(state, bst.ParamState)}\n",
    "    trainable_params = {name: state.value for name, state in trainable_states.items()}\n",
    "\n",
    "    optimizer = bts.optim.Adam(\n",
    "        lr=learning_rate\n",
    "    )\n",
    "\n",
    "    # Register trainable weights\n",
    "    optimizer.register_trainable_weights(trainable_states)\n",
    "\n",
    "    # Define JIT-compiled gradient step\n",
    "    @jax.jit\n",
    "    def grad_step(params, batch_inputs, batch_targets):\n",
    "        \"\"\"Pure function to compute loss and gradients\"\"\"\n",
    "        def forward_pass(p, inputs):\n",
    "            batch_size = inputs.shape[0]\n",
    "            h = jnp.tile(p['h0'], (batch_size, 1))\n",
    "\n",
    "            def scan_fn(carry, x_t):\n",
    "                h_prev = carry\n",
    "                if rnn.rnn_type == \"tanh\":\n",
    "                    h_next = jnp.tanh(x_t @ p['w_ih'] + h_prev @ p['w_hh'] + p['b_h'])\n",
    "                elif rnn.rnn_type == \"gru\":\n",
    "                    r = jax.nn.sigmoid(x_t @ p['w_ir'] + h_prev @ p['w_hr'] + p['b_r'])\n",
    "                    z = jax.nn.sigmoid(x_t @ p['w_iz'] + h_prev @ p['w_hz'] + p['b_z'])\n",
    "                    n = jnp.tanh(x_t @ p['w_in'] + (r * h_prev) @ p['w_hn'] + p['b_n'])\n",
    "                    h_next = (1 - z) * n + z * h_prev\n",
    "                else:\n",
    "                    h_next = h_prev\n",
    "                y_t = h_next @ p['w_out'] + p['b_out']\n",
    "                return h_next, y_t\n",
    "\n",
    "            inputs_transposed = inputs.transpose(1, 0, 2)\n",
    "            _, outputs_seq = jax.lax.scan(scan_fn, h, inputs_transposed)\n",
    "            outputs = outputs_seq.transpose(1, 0, 2)\n",
    "            return outputs\n",
    "\n",
    "        def loss_fn(p):\n",
    "            outputs = forward_pass(p, batch_inputs)\n",
    "            return jnp.mean((outputs - batch_targets) ** 2)\n",
    "\n",
    "        loss_val, grads = jax.value_and_grad(loss_fn)(params)\n",
    "        return loss_val, grads\n",
    "\n",
    "    losses = []\n",
    "    print(\"\\nTraining parameters:\")\n",
    "    print(f\"  Batch size: {batch_size}\")\n",
    "    print(f\"  Learning rate:{learning_rate:.6f} (Fixed)\")\n",
    "\n",
    "    for epoch in range(max_epochs):\n",
    "        perm = np.random.permutation(n_train)\n",
    "        epoch_loss = 0.0\n",
    "        for batch_idx in range(n_batches):\n",
    "            start_idx = batch_idx * batch_size\n",
    "            end_idx = start_idx + batch_size\n",
    "            batch_inputs = train_inputs[perm[start_idx:end_idx]]\n",
    "            batch_targets = train_targets[perm[start_idx:end_idx]]\n",
    "            loss_val, grads = grad_step(trainable_params, batch_inputs, batch_targets)\n",
    "            optimizer.update(grads)\n",
    "            trainable_params = {flatten_key(name): state.value for name, state in rnn.states().items() if\n",
    "                                isinstance(state, bst.ParamState)}\n",
    "            epoch_loss += float(loss_val)\n",
    "        epoch_loss /= n_batches\n",
    "        losses.append(epoch_loss)\n",
    "\n",
    "        if epoch % print_every == 0:\n",
    "            valid_outputs, _ = rnn(valid_inputs)\n",
    "            valid_loss = float(jnp.mean((valid_outputs - valid_targets) ** 2))\n",
    "            print(f\"Epoch {epoch:4d}: train_loss = {epoch_loss:.6f}, \"\n",
    "                  f\"valid_loss = {valid_loss:.6f}, lr = {optimizer.current_lr:.6f}\")\n",
    "        if epoch_loss < min_loss:\n",
    "            print(f\"\\nReached target loss {min_loss:.2e} at epoch {epoch}\")\n",
    "            break\n",
    "\n",
    "    # Training complete\n",
    "    valid_outputs, _ = rnn(valid_inputs)\n",
    "    final_valid_loss = float(jnp.mean((valid_outputs - valid_targets) ** 2))\n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"Training Complete!\")\n",
    "    print(\"=\" * 70)\n",
    "    print(f\"Final training loss: {epoch_loss:.6f}\")\n",
    "    print(f\"Final validation loss: {final_valid_loss:.6f}\")\n",
    "    print(f\"Total epochs: {epoch + 1}\")\n",
    "    return losses"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "facd92e6",
   "metadata": {},
   "source": [
    "## 4. 核心用法：训练并查找固定点\n",
    "\n",
    "我们将复现 `flipflop_fixed_points.py` 脚本中的 `main` 函数和 `if __name__ == \"__main__\":` 块中的逻辑。\n",
    "\n",
    "我们将：\n",
    "1.  定义任务配置。\n",
    "2.  设置参数并生成数据。\n",
    "3.  训练或加载（如果存在）模型。\n",
    "4.  初始化并运行 `FixedPointFinder`。\n",
    "5.  打印结果并可视化。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4af205b6",
   "metadata": {},
   "source": [
    "### 4.1 第 1 步：定义配置和参数\n",
    "这部分代码来自 `flipflop_fixed_points.py` 的全局 `TASK_CONFIGS` 字典和 `if __name__ == \"__main__\":` 块，以及 `main` 函数的开头部分。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d7241df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration Dictionary\n",
    "TASK_CONFIGS = {\n",
    "    \"2_bit\": {\n",
    "        \"n_bits\": 2,\n",
    "        \"n_hidden\": 16,\n",
    "        \"n_trials_train\": 128,\n",
    "        \"n_inits\":1024,\n",
    "    },\n",
    "    \"3_bit\": {\n",
    "        \"n_bits\": 3,\n",
    "        \"n_hidden\": 16,\n",
    "        \"n_trials_train\": 256,\n",
    "        \"n_inits\":1024,\n",
    "    },\n",
    "    \"4_bit\": {\n",
    "        \"n_bits\": 4,\n",
    "        \"n_hidden\": 32, #\n",
    "        \"n_trials_train\": 1024,\n",
    "        \"n_inits\":1024,\n",
    "    },\n",
    "}\n",
    "\n",
    "# --- 设置参数 ---\n",
    "# (这部分逻辑来自原始脚本的 if __name__ == \"__main__\" 块)\n",
    "config_to_run = \"3_bit\"  # 指定要运行的配置\n",
    "seed_to_use = 8356       # 使用固定种子\n",
    "\n",
    "config_name = config_to_run\n",
    "seed = seed_to_use\n",
    "\n",
    "# (这部分逻辑来自原始脚本的 main 函数)\n",
    "if config_name not in TASK_CONFIGS:\n",
    "    raise ValueError(f\"Unknown config_name: {config_name}. Available: {list(TASK_CONFIGS.keys())}\")\n",
    "config = TASK_CONFIGS[config_name]\n",
    "\n",
    "# Set random seeds\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)\n",
    "\n",
    "print(f\"\\n--- Running FlipFlop Task ({config_name}) ---\")\n",
    "print(f\"Seed: {seed}\")\n",
    "\n",
    "n_bits = config[\"n_bits\"]\n",
    "n_hidden = config[\"n_hidden\"]\n",
    "n_trials_train = config[\"n_trials_train\"]\n",
    "n_inits = config[\"n_inits\"]\n",
    "\n",
    "n_time = 64\n",
    "n_trials_valid = 128\n",
    "n_trials_test = 128\n",
    "rnn_type = \"tanh\"\n",
    "learning_rate = 0.08\n",
    "batch_size = 128\n",
    "max_epochs = 500 # (原始为 1000，500 可以在 Notebook 中跑得更快)\n",
    "min_loss = 1e-4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0241cea",
   "metadata": {},
   "source": [
    "### 4.2 第 2 步：生成数据并训练模型\n",
    "这部分代码来自 `flipflop_fixed_points.py` 的 `main` 函数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "178458fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate data\n",
    "data_gen = FlipFlopData(n_bits=n_bits, n_time=n_time, p=0.5, random_seed=seed)\n",
    "train_data = data_gen.generate_data(n_trials_train)\n",
    "valid_data = data_gen.generate_data(n_trials_valid)\n",
    "test_data = data_gen.generate_data(n_trials_test)\n",
    "\n",
    "# Create RNN model\n",
    "rnn = FlipFlopRNN(n_inputs=n_bits, n_hidden=n_hidden, n_outputs=n_bits, rnn_type=rnn_type, seed=seed)\n",
    "\n",
    "# Check for checkpoint\n",
    "checkpoint_path = f\"flipflop_rnn_{config_name}_checkpoint.msgpack\"\n",
    "if load_checkpoint(rnn, checkpoint_path):\n",
    "    print(f\"Loaded model from checkpoint: {checkpoint_path}\")\n",
    "else:\n",
    "    # Train the RNN\n",
    "    print(f\"No checkpoint found ({checkpoint_path}). Training...\")\n",
    "    losses = train_flipflop_rnn(\n",
    "        rnn,\n",
    "        train_data,\n",
    "        valid_data,\n",
    "        learning_rate=learning_rate,\n",
    "        batch_size=batch_size,\n",
    "        max_epochs=max_epochs,\n",
    "        min_loss=min_loss,\n",
    "        print_every=10\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98a3a283",
   "metadata": {},
   "source": [
    "### 4.3 第 3 步：运行固定点分析\n",
    "这部分是 `FixedPointFinder` 的**具体用法**，来自 `main` 函数。\n",
    "\n",
    "**用法说明：**\n",
    "1.  **收集状态轨迹 (State Trajectory)**：`hiddens_np`。`FixedPointFinder` 会从这些“真实”的状态中**采样**初始点。\n",
    "2.  **初始化 `FixedPointFinder`**：\n",
    "    * `rnn_model`：传入 `rnn` 实例。\n",
    "    * `do_compute_jacobians=True`：必须设为 `True`。这会计算雅可比矩阵 `J = dF/dx`。\n",
    "    * `do_decompose_jacobians=True`：必须设为 `True`。这会计算 `J` 的特征值，用于判断**稳定性**。\n",
    "3.  **运行 `find_fixed_points`**：\n",
    "    * `state_traj`：传入 `hiddens_np`。\n",
    "    * `inputs`：我们要找的是“记忆”状态，即**没有输入**时的固定点。因此我们传入一个恒定的零向量 `constant_input`。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dec87677",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fixed Point Analysis\n",
    "print(\"\\n--- Fixed Point Analysis ---\")\n",
    "inputs_jax = jnp.array(test_data[\"inputs\"])\n",
    "outputs, hiddens = rnn(inputs_jax)\n",
    "hiddens_np = np.array(hiddens)\n",
    "\n",
    "# Find fixed points\n",
    "finder = FixedPointFinder(\n",
    "    rnn,\n",
    "    method=\"joint\",\n",
    "    max_iters=5000,\n",
    "    lr_init=0.02,\n",
    "    tol_q=1e-4,\n",
    "    final_q_threshold=1e-6,\n",
    "    tol_unique=1e-2,\n",
    "    do_compute_jacobians=True,\n",
    "    do_decompose_jacobians=True,\n",
    "    outlier_distance_scale=10.0,\n",
    "    verbose=True,\n",
    "    super_verbose=True,\n",
    ")\n",
    "\n",
    "constant_input = np.zeros((1, n_bits), dtype=np.float32)\n",
    "\n",
    "unique_fps, all_fps = finder.find_fixed_points(\n",
    "    state_traj=hiddens_np,\n",
    "    inputs=constant_input,\n",
    "    n_inits=n_inits,\n",
    "    noise_scale=0.4,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecc4de11",
   "metadata": {},
   "source": [
    "### 4.4 结果分析与可视化\n",
    "\n",
    "`find_fixed_points` 返回两个对象：\n",
    "* `all_fps`: 包含了从 `n_inits` 个初始点出发找到的所有结果。\n",
    "* `unique_fps`: **我们最关心的结果**。经过 `tol_unique` 过滤后的、不重复的固定点集合。\n",
    "\n",
    "**如何解读：**\n",
    "* `unique_fps.n`: 找到的独特固定点的数量。\n",
    "* `unique_fps.qstar`: `q` 值。越接近 0 越好。\n",
    "* `unique_fps.is_stable`: **(关键)** 是否为稳定固定点。\n",
    "\n",
    "对于 N-bit 任务，我们期望找到 **2^N 个稳定固定点**（代表 2^N 个记忆状态）。\n",
    "\n",
    "下面的代码单元格整合了 `flipflop_fixed_points.py` 脚本中 `main` 函数的末尾 和 `if __name__ == \"__main__\":` 块的最后一行，用于打印所有分析结果并生成图表。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c536a1e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print results\n",
    "print(\"\\n--- Fixed Point Analysis Results ---\")\n",
    "unique_fps.print_summary()\n",
    "\n",
    "if unique_fps.n > 0:\n",
    "    print(f\"\\nDetailed Fixed Point Information (Top 10):\")\n",
    "    print(f\"{'#':<4} {'q-value':<12} {'Stability':<12} {'Max |eig|':<12}\")\n",
    "    print(\"-\" * 45)\n",
    "    for i in range(min(10, unique_fps.n)):\n",
    "        stability_str = \"Stable\" if unique_fps.is_stable[i] else \"Unstable\"\n",
    "        max_eig = np.abs(unique_fps.eigval_J_xstar[i, 0])\n",
    "        print(\n",
    "            f\"{i + 1:<4} {unique_fps.qstar[i]:<12.2e} {stability_str:<12} {max_eig:<12.4f}\"\n",
    "        )\n",
    "\n",
    "    # Visualize fixed points\n",
    "    save_path_2d = f\"flipflop_{config_name}_fixed_points_2d.png\"\n",
    "    config_2d = PlotConfig(\n",
    "        title=f\"FlipFlop Fixed Points ({config_name} - 2D PCA)\",\n",
    "        xlabel=\"PC 1\", ylabel=\"PC 2\", figsize=(10, 8),\n",
    "        # save_path=save_path_2d, \n",
    "        show=False\n",
    "    )\n",
    "    plot_fixed_points_2d(unique_fps, hiddens_np, config=config_2d)\n",
    "    print(f\"\\nSaved 2D plot to: {save_path_2d}\")\n",
    "\n",
    "    save_path_3d = f\"flipflop_{config_name}_fixed_points_3d.png\"\n",
    "    config_3d = PlotConfig(\n",
    "        title=f\"FlipFlop Fixed Points ({config_name} - 3D PCA)\",\n",
    "        figsize=(12, 10), \n",
    "        # save_path=save_path_3d, \n",
    "        show=False\n",
    "    )\n",
    "    plot_fixed_points_3d(\n",
    "        unique_fps, hiddens_np, config=config_3d,\n",
    "        plot_batch_idx=list(range(30)), plot_start_time=10\n",
    "    )\n",
    "    print(f\"Saved 3D plot to: {save_path_3d}\")\n",
    "\n",
    "print(\"\\n--- Analysis complete ---\")\n",
    "\n",
    "print(f\"\\n--- Finished configuration: {config_to_run} ---\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cann1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

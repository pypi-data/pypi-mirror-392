{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prueba."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "SplitDataMaker.split_data_task() missing 3 required positional arguments: 'cloud_provider', 'project_id', and 'cloud_uri'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 9\u001b[39m\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msklearn\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdatasets\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m load_iris\n\u001b[32m      6\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mgenesis_explorer\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmakers\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01msplit_data_maker\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m SplitDataMaker\n\u001b[32m----> \u001b[39m\u001b[32m9\u001b[39m \u001b[38;5;129m@SplitDataMaker\u001b[39m\u001b[43m.\u001b[49m\u001b[43msplit_data_task\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexperiment_name\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43msplit-data-experiment\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     10\u001b[39m \u001b[43m                                \u001b[49m\u001b[43mexperiment_version\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m1.0.0-test.4\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     11\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34msplit_data_task\u001b[39m(data: pd.DataFrame) -> Dict[\u001b[38;5;28mstr\u001b[39m, Tuple[pd.DataFrame, ...]]:\n\u001b[32m     12\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Función que se encarga de separar los datos en train y test.\"\"\"\u001b[39;00m\n\u001b[32m     14\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m {\u001b[33m\"\u001b[39m\u001b[33mtrain\u001b[39m\u001b[33m\"\u001b[39m: data.iloc[:-\u001b[32m100\u001b[39m], \u001b[33m\"\u001b[39m\u001b[33mtest\u001b[39m\u001b[33m\"\u001b[39m: data.iloc[-\u001b[32m100\u001b[39m:]}\n",
      "\u001b[31mTypeError\u001b[39m: SplitDataMaker.split_data_task() missing 3 required positional arguments: 'cloud_provider', 'project_id', and 'cloud_uri'"
     ]
    }
   ],
   "source": [
    "from typing import Dict, Tuple\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.datasets import load_iris\n",
    "\n",
    "from genesis_explorer.makers.split_data_maker import SplitDataMaker\n",
    "\n",
    "\n",
    "@SplitDataMaker.split_data_task(experiment_name = \"split-data-experiment\",\n",
    "                                experiment_version = \"1.0.0-test.4\")\n",
    "def split_data_task(data: pd.DataFrame) -> Dict[str, Tuple[pd.DataFrame, ...]]:\n",
    "    \"\"\"Función que se encarga de separar los datos en train y test.\"\"\"\n",
    "\n",
    "    return {\"train\": data.iloc[:-100], \"test\": data.iloc[-100:]}\n",
    "\n",
    "\n",
    "data = load_iris()\n",
    "df = pd.DataFrame(data[\"data\"], columns = data[\"feature_names\"])\n",
    "\n",
    "split_data_task(data = df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-15 10:21:39,129 - INFO - Esta clase implementa el patrón Singleton, por ende, se retorna la instancia existente.\n",
      "2025-11-15 10:21:39,130 - INFO - Descargando el archivo consumers/consumers-iris/mlb/1/test.pickle del bucket migracion-gcp-bucket en el proyecto mlops-credits-vertex-poc...\n",
      "2025-11-15 10:21:39,398 - INFO - Archivo consumers/consumers-iris/mlb/1/test.pickle descargado correctamente.\n"
     ]
    }
   ],
   "source": [
    "from genesis_explorer.artifacts.context import ArtifactContextManager\n",
    "\n",
    "\n",
    "cloud_provider = \"gcp\"\n",
    "cloud_uri = \"gs://migracion-gcp-bucket/consumers/consumers-iris/mlb/1/\"\n",
    "project_id = \"mlops-credits-vertex-poc\"\n",
    "\n",
    "\n",
    "with ArtifactContextManager(cloud_provider = cloud_provider,\n",
    "                            cloud_uri = cloud_uri,\n",
    "                            project_id = project_id) as manager:\n",
    "                            \n",
    "    artifact = manager.download(file_name = \"test.pickle\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-15 10:50:28,420 - INFO - Esta clase implementa el patrón Singleton, por ende, se retorna la instancia existente.\n",
      "2025-11-15 10:50:28,420 - INFO - Descargando el archivo consumers/consumers-iris/mlb/1/train_set.pickle del bucket migracion-gcp-bucket en el proyecto mlops-credits-vertex-poc...\n",
      "2025-11-15 10:50:28,672 - INFO - Archivo consumers/consumers-iris/mlb/1/train_set.pickle descargado correctamente.\n",
      "2025-11-15 10:50:28,676 - INFO - Esta clase implementa el patrón Singleton, por ende, se retorna la instancia existente.\n",
      "2025-11-15 10:50:28,677 - INFO - Creando experimento: split-data-experiment\n",
      "2025-11-15 10:50:29,905 - INFO - Experimento ya existe: split-data-experiment\n",
      "2025-11-15 10:50:30,477 - INFO - Creando ExperimentRun: experimento-v202\n",
      "2025-11-15 10:50:32,286 - INFO - ExperimentRun creado: experimento-v202\n",
      "2025-11-15 10:50:32,287 - INFO - Iniciando el proceso de partición de datos...\n",
      "\n",
      "2025-11-15 10:50:32,288 - INFO - Particionando los datos...\n",
      "2025-11-15 10:50:32,292 - INFO - Datos obtenidos correctamente.\n",
      "\n",
      "2025-11-15 10:50:32,293 - INFO - Guardando los artefactos versionados...\n",
      "2025-11-15 10:50:32,296 - INFO - Subiendo el archivo /var/folders/7s/p0t580cj7m7f7wnlkdj5lg14z6r38n/T/genesis_temp_dirxeyxubx7/train_set.pickle al bucket migracion-gcp-bucket en el proyecto mlops-credits-vertex-poc...\n",
      "2025-11-15 10:50:32,726 - INFO - Archivo /var/folders/7s/p0t580cj7m7f7wnlkdj5lg14z6r38n/T/genesis_temp_dirxeyxubx7/train_set.pickle subido correctamente.\n",
      "2025-11-15 10:50:32,730 - INFO - Subiendo el archivo /var/folders/7s/p0t580cj7m7f7wnlkdj5lg14z6r38n/T/genesis_temp_dirxeyxubx7/test_set.pickle al bucket migracion-gcp-bucket en el proyecto mlops-credits-vertex-poc...\n",
      "2025-11-15 10:50:33,236 - INFO - Archivo /var/folders/7s/p0t580cj7m7f7wnlkdj5lg14z6r38n/T/genesis_temp_dirxeyxubx7/test_set.pickle subido correctamente.\n",
      "2025-11-15 10:50:33,238 - INFO - Subiendo el archivo /var/folders/7s/p0t580cj7m7f7wnlkdj5lg14z6r38n/T/genesis_temp_dirxeyxubx7/oot_set.pickle al bucket migracion-gcp-bucket en el proyecto mlops-credits-vertex-poc...\n",
      "2025-11-15 10:50:33,657 - INFO - Archivo /var/folders/7s/p0t580cj7m7f7wnlkdj5lg14z6r38n/T/genesis_temp_dirxeyxubx7/oot_set.pickle subido correctamente.\n",
      "2025-11-15 10:50:33,659 - INFO - Artefactos guardados correctamente.\n",
      "\n",
      "2025-11-15 10:50:34,274 - INFO - Proceso de partición de datos finalizado correctamente.\n",
      "2025-11-15 10:50:34,276 - INFO - Cerrando ExperimentRun: experimento-v202\n",
      "2025-11-15 10:50:34,941 - INFO - ExperimentRun cerrado: experimento-v202\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Módulo que contiene la lógica de la task de separación de datos.\"\"\"\n",
    "\n",
    "# Librerías Externas.\n",
    "from typing import List, Dict, Tuple, Optional\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from genesis_explorer.makers.split_data_maker import SplitDataMaker\n",
    "\n",
    "\n",
    "class MyDataSplitter(SplitDataMaker):\n",
    "    \"\"\"Clase que contiene la lógica para, de una tabla base, generar una separación de datos\n",
    "    para el modelado.\"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 data: List[str],\n",
    "                 test_size: Optional[float] = 0.3,\n",
    "                 oot_size: Optional[float] = 0.2,\n",
    "                 random_seed: Optional[int] = 42,\n",
    "                 cloud_provider: Optional[str] = \"gcp\",\n",
    "                 project_id: Optional[str] = \"mlops-credits-vertex-poc\",\n",
    "                 cloud_uri: Optional[str] = \"gs://migracion-gcp-bucket/consumers/consumers-iris/mlb/1/\",\n",
    "                 experiment_name: Optional[str] = \"split-data-experiment\",\n",
    "                 experiment_version: Optional[str] = \"1.0.0-test.200\") -> None:\n",
    "        \"\"\"Método de instanciación de la clase.\n",
    "        \n",
    "        Args:\n",
    "        ----------\n",
    "        test_size: Optional[float].\n",
    "            Tamaño de la partición de test.\n",
    "\n",
    "        oot_size: Optional[float].\n",
    "            Tamaño de la partición de oot.\n",
    "\n",
    "        random_seed: Optional[int].\n",
    "            Semilla para la generación de números aleatorios.\"\"\"\n",
    "        \n",
    "        super().__init__(cloud_provider = cloud_provider,\n",
    "                         cloud_uri = cloud_uri,\n",
    "                         project_id = project_id)\n",
    "        \n",
    "        artifacts = self.load_artifacts(artifact_names = data)\n",
    "        self.data = artifacts[\"train_set\"]\n",
    "\n",
    "        self.oot_size = oot_size\n",
    "        self.test_size = test_size\n",
    "        self.random_seed = random_seed\n",
    "\n",
    "        self.experiment_name = experiment_name\n",
    "        self.experiment_version = experiment_version\n",
    "\n",
    "    def run_task(self) -> None:\n",
    "        \"\"\"Método de ejecución de la task.\"\"\"\n",
    "\n",
    "        decorated_splitting_process = self.split_data_task(experiment_name = self.experiment_name,\n",
    "                                                           experiment_version = self.experiment_version)(self.split_data)\n",
    "        \n",
    "        decorated_splitting_process(data = self.data)\n",
    "    \n",
    "    def split_data(self, data: pd.DataFrame) -> Dict[str, Tuple[pd.DataFrame, ...]]:\n",
    "        \"\"\"Función que se encarga de separar los datos en train y test.\"\"\"\n",
    "\n",
    "        dataframe_train, dataframe_test = train_test_split(data,\n",
    "                                                           test_size = self.test_size,\n",
    "                                                           random_state = self.random_seed)\n",
    "        \n",
    "        dataframe_test, dataframe_oot = train_test_split(dataframe_test,\n",
    "                                                         test_size = self.oot_size,\n",
    "                                                         random_state = self.random_seed)\n",
    "        \n",
    "        datasets = {\"train_set\": dataframe_train,\n",
    "                    \"test_set\": dataframe_test,\n",
    "                    \"oot_set\": dataframe_oot}\n",
    "\n",
    "        return datasets\n",
    "    \n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.datasets import load_iris\n",
    "\n",
    "data = load_iris()\n",
    "df = pd.DataFrame(data[\"data\"], columns = data[\"feature_names\"])\n",
    "\n",
    "my_data_splitter = MyDataSplitter(data = [\"train_set\"],\n",
    "                                  experiment_version = \"1.0.0-test.202\")\n",
    "my_data_splitter.run_task()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "genesis-explorer-zvZG0_LT-py3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

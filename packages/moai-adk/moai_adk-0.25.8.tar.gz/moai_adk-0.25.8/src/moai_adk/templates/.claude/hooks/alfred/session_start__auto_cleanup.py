#!/usr/bin/env python3

"""SessionStart Hook: ìë™ ì •ë¦¬ ë° ë³´ê³ ì„œ ìƒì„±

ì„¸ì…˜ ì‹œì‘ ì‹œ ì˜¤ë˜ëœ ì„ì‹œ íŒŒì¼, ë³´ê³ ì„œ ë“±ì„ ì •ë¦¬í•˜ê³ 
ì¼ì¼ ë¶„ì„ ë³´ê³ ì„œë¥¼ ìƒì„±í•©ë‹ˆë‹¤.

ê¸°ëŠ¥:
- ì˜¤ë˜ëœ ë³´ê³ ì„œ íŒŒì¼ ìë™ ì •ë¦¬
- ì„ì‹œ íŒŒì¼ ì •ë¦¬
- ì¼ì¼ ë¶„ì„ ë³´ê³ ì„œ ìƒì„±
- ì„¸ì…˜ ë¡œê·¸ ë¶„ì„
"""

import json
import logging
import shutil
import sys
from datetime import datetime, timedelta
from pathlib import Path
from typing import Dict, List, Optional

# ëª¨ë“ˆ ê²½ë¡œ ì¶”ê°€
sys.path.insert(0, str(Path(__file__).parent.parent.parent.parent / "src"))

try:
    from moai_adk.utils.common import format_duration, get_summary_stats
except ImportError:
    # Fallback implementations if module not found
    def format_duration(seconds):
        """Format duration in seconds to readable string"""
        if seconds < 60:
            return f"{seconds:.1f}s"
        minutes = seconds / 60
        if minutes < 60:
            return f"{minutes:.1f}m"
        hours = minutes / 60
        return f"{hours:.1f}h"

    def get_summary_stats(values):
        """Get summary statistics for a list of values"""
        if not values:
            return {"mean": 0, "min": 0, "max": 0, "std": 0}
        import statistics
        return {
            "mean": statistics.mean(values),
            "min": min(values),
            "max": max(values),
            "std": statistics.stdev(values) if len(values) > 1 else 0
        }

logger = logging.getLogger(__name__)


def load_hook_timeout() -> int:
    """Load hook timeout from config.json (default: 3000ms)"""
    try:
        config_file = Path(".moai/config/config.json")
        if config_file.exists():
            with open(config_file, 'r', encoding='utf-8') as f:
                config = json.load(f)
                return config.get("hooks", {}).get("timeout_ms", 3000)
    except Exception:
        pass
    return 3000


def get_graceful_degradation() -> bool:
    """Load graceful_degradation setting from config.json (default: true)"""
    try:
        config_file = Path(".moai/config/config.json")
        if config_file.exists():
            with open(config_file, 'r', encoding='utf-8') as f:
                config = json.load(f)
                return config.get("hooks", {}).get("graceful_degradation", True)
    except Exception:
        pass
    return True


def load_config() -> Dict:
    """ì„¤ì • íŒŒì¼ ë¡œë“œ"""
    try:
        config_file = Path(".moai/config/config.json")
        if config_file.exists():
            with open(config_file, 'r', encoding='utf-8') as f:
                return json.load(f)
    except Exception:
        pass

    return {}


def should_cleanup_today(last_cleanup: Optional[str], cleanup_days: int = 7) -> bool:
    """ì˜¤ëŠ˜ ì •ë¦¬ê°€ í•„ìš”í•œì§€ í™•ì¸

    Args:
        last_cleanup: ë§ˆì§€ë§‰ ì •ë¦¬ ë‚ ì§œ (YYYY-MM-DD)
        cleanup_days: ì •ë¦¬ ì£¼ê¸° (ì¼)

    Returns:
        ì •ë¦¬ í•„ìš” ì—¬ë¶€
    """
    if not last_cleanup:
        return True

    try:
        last_date = datetime.strptime(last_cleanup, "%Y-%m-%d")
        next_cleanup = last_date + timedelta(days=cleanup_days)
        return datetime.now() >= next_cleanup
    except Exception:
        return True


def cleanup_old_files(config: Dict) -> Dict[str, int]:
    """ì˜¤ë˜ëœ íŒŒì¼ ì •ë¦¬

    Args:
        config: ì„¤ì • ë”•ì…”ë„ˆë¦¬

    Returns:
        ì •ë¦¬ëœ íŒŒì¼ ìˆ˜ í†µê³„
    """
    stats = {
        "reports_cleaned": 0,
        "cache_cleaned": 0,
        "temp_cleaned": 0,
        "total_cleaned": 0
    }

    try:
        cleanup_config = config.get("auto_cleanup", {})
        if not cleanup_config.get("enabled", True):
            return stats

        cleanup_days = cleanup_config.get("cleanup_days", 7)
        max_reports = cleanup_config.get("max_reports", 10)
        _cleanup_targets = cleanup_config.get("cleanup_targets", [])

        cutoff_date = datetime.now() - timedelta(days=cleanup_days)

        # ë³´ê³ ì„œ íŒŒì¼ ì •ë¦¬
        reports_dir = Path(".moai/reports")
        if reports_dir.exists():
            stats["reports_cleaned"] = cleanup_directory(
                reports_dir,
                cutoff_date,
                max_reports,
                patterns=["*.json", "*.md"]
            )

        # ìºì‹œ íŒŒì¼ ì •ë¦¬
        cache_dir = Path(".moai/cache")
        if cache_dir.exists():
            stats["cache_cleaned"] = cleanup_directory(
                cache_dir,
                cutoff_date,
                None,  # ìºì‹œëŠ” ê°œìˆ˜ ì œí•œ ì—†ìŒ
                patterns=["*"]
            )

        # ì„ì‹œ íŒŒì¼ ì •ë¦¬
        temp_dir = Path(".moai/temp")
        if temp_dir.exists():
            stats["temp_cleaned"] = cleanup_directory(
                temp_dir,
                cutoff_date,
                None,
                patterns=["*"]
            )

        stats["total_cleaned"] = (
            stats["reports_cleaned"] +
            stats["cache_cleaned"] +
            stats["temp_cleaned"]
        )

    except Exception as e:
        logger.error(f"File cleanup failed: {e}")

    return stats


def cleanup_directory(
    directory: Path,
    cutoff_date: datetime,
    max_files: Optional[int],
    patterns: List[str]
) -> int:
    """ë””ë ‰í† ë¦¬ íŒŒì¼ ì •ë¦¬

    Args:
        directory: ëŒ€ìƒ ë””ë ‰í† ë¦¬
        cutoff_date: ìë¥¼ ê¸°ì¤€ ë‚ ì§œ
        max_files: ìµœëŒ€ ìœ ì§€ íŒŒì¼ ìˆ˜
        patterns: ì‚­ì œí•  íŒŒì¼ íŒ¨í„´ ëª©ë¡

    Returns:
        ì‚­ì œëœ íŒŒì¼ ìˆ˜
    """
    if not directory.exists():
        return 0

    cleaned_count = 0

    try:
        # íŒ¨í„´ì— í•´ë‹¹í•˜ëŠ” íŒŒì¼ ëª©ë¡ ìˆ˜ì§‘
        files_to_check = []
        for pattern in patterns:
            files_to_check.extend(directory.glob(pattern))

        # ë‚ ì§œìˆœ ì •ë ¬ (ì˜¤ë˜ëœ ê²ƒë¶€í„°)
        files_to_check.sort(key=lambda f: f.stat().st_mtime)

        # íŒŒì¼ ì‚­ì œ
        for file_path in files_to_check:
            try:
                # íŒŒì¼ ìˆ˜ì • ì‹œê°„ í™•ì¸
                file_mtime = datetime.fromtimestamp(file_path.stat().st_mtime)

                # ê¸°ì¤€ ë‚ ì§œ ì´ì „ì´ë©´ ì‚­ì œ
                if file_mtime < cutoff_date:
                    if file_path.is_file():
                        file_path.unlink()
                        cleaned_count += 1
                    elif file_path.is_dir():
                        shutil.rmtree(file_path)
                        cleaned_count += 1

                # ìµœëŒ€ íŒŒì¼ ìˆ˜ ì œí•œ
                elif max_files is not None:
                    remaining_files = len([f for f in files_to_check
                                         if f.exists() and
                                         datetime.fromtimestamp(f.stat().st_mtime) >= cutoff_date])
                    if remaining_files > max_files:
                        if file_path.is_file():
                            file_path.unlink()
                            cleaned_count += 1
                        elif file_path.is_dir():
                            shutil.rmtree(file_path)
                            cleaned_count += 1

            except Exception as e:
                logger.warning(f"Failed to delete {file_path}: {e}")
                continue

    except Exception as e:
        logger.error(f"Directory cleanup failed for {directory}: {e}")

    return cleaned_count


def generate_daily_analysis(config: Dict) -> Optional[str]:
    """ì¼ì¼ ë¶„ì„ ë³´ê³ ì„œ ìƒì„±

    Args:
        config: ì„¤ì • ë”•ì…”ë„ˆë¦¬

    Returns:
        ìƒì„±ëœ ë³´ê³ ì„œ íŒŒì¼ ê²½ë¡œ ë˜ëŠ” None
    """
    try:
        analysis_config = config.get("daily_analysis", {})
        if not analysis_config.get("enabled", True):
            return None

        # ì„¸ì…˜ ë¡œê·¸ ë¶„ì„
        report_path = analyze_session_logs(analysis_config)

        # ì„¤ì •ì— ë§ˆì§€ë§‰ ë¶„ì„ ë‚ ì§œ ì—…ë°ì´íŠ¸
        if report_path:
            config_file = Path(".moai/config/config.json")
            if config_file.exists():
                with open(config_file, 'r', encoding='utf-8') as f:
                    config_data = json.load(f)

                config_data["daily_analysis"]["last_analysis"] = datetime.now().strftime("%Y-%m-%d")

                with open(config_file, 'w', encoding='utf-8') as f:
                    json.dump(config_data, f, indent=2, ensure_ascii=False)

        return report_path

    except Exception as e:
        logger.error(f"Daily analysis failed: {e}")
        return None


def analyze_session_logs(analysis_config: Dict) -> Optional[str]:
    """ì„¸ì…˜ ë¡œê·¸ ë¶„ì„

    Args:
        analysis_config: ë¶„ì„ ì„¤ì •

    Returns:
        ë³´ê³ ì„œ íŒŒì¼ ê²½ë¡œ ë˜ëŠ” None
    """
    try:
        # Claude Code ì„¸ì…˜ ë¡œê·¸ ê²½ë¡œ
        session_logs_dir = Path.home() / ".claude" / "projects"
        project_name = Path.cwd().name

        # í˜„ì¬ í”„ë¡œì íŠ¸ì˜ ì„¸ì…˜ ë¡œê·¸ ì°¾ê¸°
        project_sessions = []
        for project_dir in session_logs_dir.iterdir():
            if project_dir.name.endswith(project_name):
                session_files = list(project_dir.glob("session-*.json"))
                project_sessions.extend(session_files)

        if not project_sessions:
            return None

        # ìµœê·¼ ì„¸ì…˜ ë¡œê·¸ ë¶„ì„
        recent_sessions = sorted(project_sessions, key=lambda f: f.stat().st_mtime, reverse=True)[:10]

        # ë¶„ì„ ë°ì´í„° ìˆ˜ì§‘
        analysis_data = {
            "total_sessions": len(recent_sessions),
            "date_range": "",
            "tools_used": {},
            "errors_found": [],
            "duration_stats": {},
            "recommendations": []
        }

        if recent_sessions:
            first_session = datetime.fromtimestamp(recent_sessions[-1].stat().st_mtime)
            last_session = datetime.fromtimestamp(recent_sessions[0].stat().st_mtime)
            analysis_data["date_range"] = f"{first_session.strftime('%Y-%m-%d')} ~ {last_session.strftime('%Y-%m-%d')}"

            # ê° ì„¸ì…˜ ë¶„ì„
            all_durations = []
            for session_file in recent_sessions:
                try:
                    with open(session_file, 'r', encoding='utf-8') as f:
                        session_data = json.load(f)

                    # ë„êµ¬ ì‚¬ìš© ë¶„ì„
                    if "tool_use" in session_data:
                        for tool_use in session_data["tool_use"]:
                            tool_name = tool_use.get("name", "unknown")
                            analysis_data["tools_used"][tool_name] = analysis_data["tools_used"].get(tool_name, 0) + 1

                    # ì˜¤ë¥˜ ë¶„ì„
                    if "errors" in session_data:
                        for error in session_data["errors"]:
                            analysis_data["errors_found"].append({
                                "timestamp": error.get("timestamp", ""),
                                "error": error.get("message", "")[:100]  # ì²« 100ìë§Œ
                            })

                    # ì„¸ì…˜ ê¸¸ì´ ë¶„ì„
                    if "start_time" in session_data and "end_time" in session_data:
                        start = session_data["start_time"]
                        end = session_data["end_time"]
                        if start and end:
                            try:
                                duration = float(end) - float(start)
                                all_durations.append(duration)
                            except (ValueError, TypeError):
                                pass

                except Exception as e:
                    logger.warning(f"Failed to analyze session {session_file}: {e}")
                    continue

            # ì„¸ì…˜ ê¸¸ì´ í†µê³„
            if all_durations:
                analysis_data["duration_stats"] = get_summary_stats(all_durations)

        # ë³´ê³ ì„œ ìƒì„±
        report_content = format_analysis_report(analysis_data)

        # ë³´ê³ ì„œ ì €ì¥
        _report_location = analysis_config.get("report_location", ".moai/reports/daily-")
        base_path = Path(".moai/reports")
        base_path.mkdir(exist_ok=True)

        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        report_file = base_path / f"daily-analysis-{timestamp}.md"

        with open(report_file, 'w', encoding='utf-8') as f:
            f.write(report_content)

        return str(report_file)

    except Exception as e:
        logger.error(f"Session log analysis failed: {e}")
        return None


def format_analysis_report(analysis_data: Dict) -> str:
    """ë¶„ì„ ê²°ê³¼ë¥¼ ë³´ê³ ì„œ í˜•ì‹ìœ¼ë¡œ ë³€í™˜

    Args:
        analysis_data: ë¶„ì„ ë°ì´í„°

    Returns:
        í˜•ì‹í™”ëœ ë³´ê³ ì„œ ë‚´ìš©
    """
    report_lines = [
        "# ì¼ì¼ ì„¸ì…˜ ë¶„ì„ ë³´ê³ ì„œ",
        "",
        f"ìƒì„± ì‹œê°„: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}",
        f"ë¶„ì„ ê¸°ê°„: {analysis_data.get('date_range', 'N/A')}",
        f"ì´ ì„¸ì…˜ ìˆ˜: {analysis_data.get('total_sessions', 0)}",
        "",
        "## ğŸ“Š ë„êµ¬ ì‚¬ìš© í˜„í™©",
        ""
    ]

    # ë„êµ¬ ì‚¬ìš© ìˆœìœ„
    tools_used = analysis_data.get("tools_used", {})
    if tools_used:
        sorted_tools = sorted(tools_used.items(), key=lambda x: x[1], reverse=True)
        for tool_name, count in sorted_tools[:10]:  # TOP 10
            report_lines.append(f"- **{tool_name}**: {count}íšŒ")
    else:
        report_lines.append("- ì‚¬ìš©ëœ ë„êµ¬ê°€ ì—†ìŠµë‹ˆë‹¤")

    report_lines.extend([
        "",
        "## âš ï¸ ì˜¤ë¥˜ í˜„í™©",
        ""
    ])

    # ì˜¤ë¥˜ í˜„í™©
    errors = analysis_data.get("errors_found", [])
    if errors:
        for i, error in enumerate(errors[:5], 1):  # ìµœê·¼ 5ê°œ
            report_lines.append(f"{i}. {error.get('error', 'N/A')} ({error.get('timestamp', 'N/A')})")
    else:
        report_lines.append("- ë°œê²¬ëœ ì˜¤ë¥˜ê°€ ì—†ìŠµë‹ˆë‹¤")

    # ì„¸ì…˜ ê¸¸ì´ í†µê³„
    duration_stats = analysis_data.get("duration_stats", {})
    if duration_stats.get("mean", 0) > 0:
        report_lines.extend([
            "",
            "## â±ï¸ ì„¸ì…˜ ê¸¸ì´ í†µê³„",
            "",
            f"- í‰ê· : {format_duration(duration_stats['mean'])}",
            f"- ìµœì†Œ: {format_duration(duration_stats['min'])}",
            f"- ìµœëŒ€: {format_duration(duration_stats['max'])}",
            f"- í‘œì¤€í¸ì°¨: {format_duration(duration_stats['std'])}"
        ])

    # ê°œì„  ì œì•ˆ
    report_lines.extend([
        "",
        "## ğŸ’¡ ê°œì„  ì œì•ˆ",
        ""
    ])

    # ë„êµ¬ ì‚¬ìš© íŒ¨í„´ ê¸°ë°˜ ì œì•ˆ
    if tools_used:
        most_used_tool = max(tools_used.items(), key=lambda x: x[1])[0]
        if "Bash" in most_used_tool and tools_used[most_used_tool] > 10:
            report_lines.append("- ğŸ”§ Bash ëª…ë ¹ì–´ ì‚¬ìš©ì´ ì¦ìŠµë‹ˆë‹¤. ìŠ¤í¬ë¦½íŠ¸ ìë™í™”ë¥¼ ê³ ë ¤í•´ë³´ì„¸ìš”")

    if len(errors) > 3:
        report_lines.append("- âš ï¸ ì˜¤ë¥˜ ë°œìƒì´ ì¦ìŠµë‹ˆë‹¤. ì•ˆì •ì„± ê²€í† ê°€ í•„ìš”í•©ë‹ˆë‹¤")

    if duration_stats.get("mean", 0) > 1800:  # 30ë¶„ ì´ìƒ
        report_lines.append("- â° ì„¸ì…˜ ì‹œê°„ì´ ê¹ë‹ˆë‹¤. ì‘ì—… ë¶„í• ì„ ê³ ë ¤í•´ë³´ì„¸ìš”")

    if not report_lines[-1].startswith("-"):
        report_lines.append("- í˜„ì¬ ì„¸ì…˜ íŒ¨í„´ì´ ì–‘í˜¸í•©ë‹ˆë‹¤")

    report_lines.extend([
        "",
        "---",
        "*ë³´ê³ ì„œëŠ” Alfredì˜ SessionStart Hookìœ¼ë¡œ ìë™ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤*",
        "*ë¶„ì„ ì„¤ì •ì€ `.moai/config/config.json`ì˜ `daily_analysis` ì„¹ì…˜ì—ì„œ ê´€ë¦¬í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤*"
    ])

    return "\n".join(report_lines)


def update_cleanup_stats(cleanup_stats: Dict[str, int]):
    """ì •ë¦¬ í†µê³„ ì—…ë°ì´íŠ¸

    Args:
        cleanup_stats: ì •ë¦¬ í†µê³„
    """
    try:
        stats_file = Path(".moai/cache/cleanup_stats.json")
        stats_file.parent.mkdir(exist_ok=True)

        # ê¸°ì¡´ í†µê³„ ë¡œë“œ
        existing_stats = {}
        if stats_file.exists():
            with open(stats_file, 'r', encoding='utf-8') as f:
                existing_stats = json.load(f)

        # ìƒˆ í†µê³„ ì¶”ê°€
        today = datetime.now().strftime("%Y-%m-%d")
        existing_stats[today] = {
            "cleaned_files": cleanup_stats["total_cleaned"],
            "reports_cleaned": cleanup_stats["reports_cleaned"],
            "cache_cleaned": cleanup_stats["cache_cleaned"],
            "temp_cleaned": cleanup_stats["temp_cleaned"],
            "timestamp": datetime.now().isoformat()
        }

        # ìµœê·¼ 30ì¼ í†µê³„ë§Œ ìœ ì§€
        cutoff_date = datetime.now() - timedelta(days=30)
        filtered_stats = {}
        for date, stats in existing_stats.items():
            try:
                stat_date = datetime.strptime(date, "%Y-%m-%d")
                if stat_date >= cutoff_date:
                    filtered_stats[date] = stats
            except ValueError:
                continue

        # í†µê³„ ì €ì¥
        with open(stats_file, 'w', encoding='utf-8') as f:
            json.dump(filtered_stats, f, indent=2, ensure_ascii=False)

    except Exception as e:
        logger.error(f"Failed to update cleanup stats: {e}")


def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    try:
        # Hook timeout ì„¤ì • ë¡œë“œ
        timeout_seconds = load_hook_timeout() / 1000
        graceful_degradation = get_graceful_degradation()

        # íƒ€ì„ì•„ì›ƒ ì²´í¬
        import signal
        import time

        def timeout_handler(signum, frame):
            raise TimeoutError("Hook execution timeout")

        signal.signal(signal.SIGALRM, timeout_handler)
        signal.alarm(int(timeout_seconds))

        try:
            start_time = time.time()

            # ì„¤ì • ë¡œë“œ
            config = load_config()

            # ë§ˆì§€ë§‰ ì •ë¦¬ ë‚ ì§œ í™•ì¸
            last_cleanup = config.get("auto_cleanup", {}).get("last_cleanup")
            cleanup_days = config.get("auto_cleanup", {}).get("cleanup_days", 7)

            cleanup_stats = {"total_cleaned": 0, "reports_cleaned": 0, "cache_cleaned": 0, "temp_cleaned": 0}
            report_path = None

            # ì •ë¦¬ í•„ìš” ì‹œ ì‹¤í–‰
            if should_cleanup_today(last_cleanup, cleanup_days):
                cleanup_stats = cleanup_old_files(config)

                # ë§ˆì§€ë§‰ ì •ë¦¬ ë‚ ì§œ ì—…ë°ì´íŠ¸
                config_file = Path(".moai/config/config.json")
                if config_file.exists():
                    with open(config_file, 'r', encoding='utf-8') as f:
                        config_data = json.load(f)

                    config_data["auto_cleanup"]["last_cleanup"] = datetime.now().strftime("%Y-%m-%d")

                    with open(config_file, 'w', encoding='utf-8') as f:
                        json.dump(config_data, f, indent=2, ensure_ascii=False)

                # ì •ë¦¬ í†µê³„ ì—…ë°ì´íŠ¸
                update_cleanup_stats(cleanup_stats)

            # ì¼ì¼ ë¶„ì„ ë³´ê³ ì„œ ìƒì„±
            last_analysis = config.get("daily_analysis", {}).get("last_analysis")
            if should_cleanup_today(last_analysis, 1):  # ë§¤ì¼ ì‹¤í–‰
                report_path = generate_daily_analysis(config)

            # ì‹¤í–‰ ì‹œê°„ ê¸°ë¡
            execution_time = time.time() - start_time

            # ê²°ê³¼ ì¶œë ¥
            result = {
                "hook": "session_start__auto_cleanup",
                "success": True,
                "execution_time_seconds": round(execution_time, 2),
                "cleanup_stats": cleanup_stats,
                "daily_analysis_report": report_path,
                "timestamp": datetime.now().isoformat()
            }

            print(json.dumps(result, ensure_ascii=False, indent=2))

        finally:
            signal.alarm(0)  # íƒ€ì„ì•„ì›ƒ í•´ì œ

    except TimeoutError as e:
        # íƒ€ì„ì•„ì›ƒ ì²˜ë¦¬
        result = {
            "hook": "session_start__auto_cleanup",
            "success": False,
            "error": f"Hook execution timeout: {str(e)}",
            "graceful_degradation": graceful_degradation,
            "timestamp": datetime.now().isoformat()
        }

        if graceful_degradation:
            result["message"] = "Hook timeout but continuing due to graceful degradation"

        print(json.dumps(result, ensure_ascii=False, indent=2))

    except Exception as e:
        # ì˜ˆì™¸ ì²˜ë¦¬
        result = {
            "hook": "session_start__auto_cleanup",
            "success": False,
            "error": f"Hook execution failed: {str(e)}",
            "graceful_degradation": graceful_degradation,
            "timestamp": datetime.now().isoformat()
        }

        if graceful_degradation:
            result["message"] = "Hook failed but continuing due to graceful degradation"

        print(json.dumps(result, ensure_ascii=False, indent=2))


if __name__ == "__main__":
    main()

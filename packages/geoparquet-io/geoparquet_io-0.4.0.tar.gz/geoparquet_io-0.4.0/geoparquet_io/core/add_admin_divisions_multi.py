#!/usr/bin/env python3

"""
Add admin division columns from multiple datasets.

This module extends the add_country_codes functionality to support
multiple admin datasets with hierarchical level support.
"""

from typing import Optional

import click
import duckdb

from geoparquet_io.core.admin_datasets import AdminDatasetFactory
from geoparquet_io.core.common import (
    check_bbox_structure,
    find_primary_geometry_column,
    get_parquet_metadata,
    safe_file_url,
    write_parquet_with_metadata,
)


def add_admin_divisions_multi(
    input_parquet: str,
    output_parquet: str,
    dataset_name: str,
    levels: list[str],
    dataset_source: Optional[str] = None,
    add_bbox_flag: bool = False,
    dry_run: bool = False,
    verbose: bool = False,
    compression: str = "ZSTD",
    compression_level: Optional[int] = None,
    row_group_size_mb: Optional[float] = None,
    row_group_rows: Optional[int] = None,
):
    """
    Add admin division columns from a multi-level admin dataset.

    Args:
        input_parquet: Input GeoParquet file
        output_parquet: Output GeoParquet file
        dataset_name: Name of admin dataset ("current", "gaul", "overture")
        levels: List of hierarchical levels to add as columns
        dataset_source: Optional custom path/URL to admin dataset
        add_bbox_flag: Automatically add bbox column if missing
        dry_run: Show SQL without executing
        verbose: Enable verbose output
        compression: Compression type
        compression_level: Compression level
        row_group_size_mb: Target row group size in MB
        row_group_rows: Exact number of rows per row group
    """
    # Create dataset instance
    dataset = AdminDatasetFactory.create(dataset_name, dataset_source, verbose)

    if verbose:
        click.echo(f"\nUsing admin dataset: {dataset.get_dataset_name()}")
        click.echo(f"Data source: {dataset.get_source()}")
        click.echo(f"Adding admin levels: {', '.join(levels)}")

    # Validate levels
    dataset.validate_levels(levels)

    # Get column names for the requested levels
    partition_columns = dataset.get_partition_columns(levels)

    # Get safe URLs
    input_url = safe_file_url(input_parquet, verbose)
    admin_source = dataset.get_source()

    # Get geometry column names
    input_geom_col = find_primary_geometry_column(input_parquet, verbose)
    admin_geom_col = dataset.get_geometry_column()

    # Check bbox columns
    input_bbox_info = check_bbox_structure(input_parquet, verbose)
    input_bbox_col = input_bbox_info["bbox_column_name"]
    admin_bbox_col = dataset.get_bbox_column()

    # Start dry-run mode output if needed
    if dry_run:
        click.echo(
            click.style(
                "\n=== DRY RUN MODE - SQL Commands that would be executed ===\n",
                fg="yellow",
                bold=True,
            )
        )
        click.echo(click.style(f"-- Input file: {input_url}", fg="cyan"))
        click.echo(click.style(f"-- Admin dataset: {admin_source}", fg="cyan"))
        click.echo(click.style(f"-- Output file: {output_parquet}", fg="cyan"))
        click.echo(
            click.style(
                f"-- Geometry columns: {input_geom_col} (input), {admin_geom_col} (admin)",
                fg="cyan",
            )
        )
        click.echo(
            click.style(
                f"-- Bbox columns: {input_bbox_col or 'none'} (input), {admin_bbox_col or 'none'} (admin)\n",
                fg="cyan",
            )
        )

    # Get metadata before processing (skip in dry-run)
    metadata = None
    if not dry_run:
        metadata, _ = get_parquet_metadata(input_parquet, verbose)

        # Check bbox structure and provide warnings
        if input_bbox_info["status"] != "optimal":
            click.echo(
                click.style(
                    "\nWarning: Input file could benefit from bbox optimization:\n"
                    + input_bbox_info["message"],
                    fg="yellow",
                )
            )
            if add_bbox_flag:
                if not input_bbox_info["has_bbox_column"]:
                    click.echo("Adding bbox column to input file...")
                    from geoparquet_io.core.common import add_bbox

                    add_bbox(input_parquet, "bbox", verbose)
                    click.echo(
                        click.style("âœ“ Added bbox column and metadata to input file", fg="green")
                    )
                    # Re-check after adding bbox
                    input_bbox_info = check_bbox_structure(input_parquet, verbose)
                    input_bbox_col = input_bbox_info["bbox_column_name"]

        if verbose:
            click.echo(
                f"Using geometry columns: {input_geom_col} (input), {admin_geom_col} (admin)"
            )

    # Create DuckDB connection and load spatial extension
    con = duckdb.connect()
    con.execute("INSTALL spatial;")
    con.execute("LOAD spatial;")
    con.execute("INSTALL httpfs;")
    con.execute("LOAD httpfs;")

    # Configure S3 settings based on dataset requirements
    dataset.configure_s3(con)

    # Get total input count (skip in dry-run)
    if not dry_run:
        total_count = con.execute(f"SELECT COUNT(*) FROM '{input_url}'").fetchone()[0]
        click.echo(f"Processing {total_count:,} input features...")

    # Prepare admin data source
    admin_source = dataset.prepare_data_source(con)

    # Build admin data source with read_parquet options if needed
    read_options = dataset.get_read_parquet_options()
    if read_options:
        options_str = ", ".join([f"{k}={v}" for k, v in read_options.items()])
        admin_table_ref = f"read_parquet({admin_source}, {options_str})"
    else:
        admin_table_ref = admin_source

    # Build WHERE clause for admin boundaries
    admin_where_clauses = []

    # Add subtype filter if applicable
    subtype_filter = dataset.get_subtype_filter(levels)
    if subtype_filter:
        admin_where_clauses.append(subtype_filter)
        if verbose and not dry_run:
            click.echo(f"Filtering admin boundaries: {subtype_filter}")

    # Add bbox extent filter to only load admin boundaries that overlap with input data
    # This is crucial for performance with large global datasets like Overture
    if admin_bbox_col:
        # Get extent of input data - use bbox column if available, otherwise calculate from geometry
        if input_bbox_col:
            extent_query = f"""
                SELECT
                    MIN({input_bbox_col}.xmin) as xmin,
                    MAX({input_bbox_col}.xmax) as xmax,
                    MIN({input_bbox_col}.ymin) as ymin,
                    MAX({input_bbox_col}.ymax) as ymax
                FROM '{input_url}'
            """
        else:
            # Calculate extent from geometry column
            extent_query = f"""
                SELECT
                    MIN(ST_XMin("{input_geom_col}")) as xmin,
                    MAX(ST_XMax("{input_geom_col}")) as xmax,
                    MIN(ST_YMin("{input_geom_col}")) as ymin,
                    MAX(ST_YMax("{input_geom_col}")) as ymax
                FROM '{input_url}'
            """

        extent = con.execute(extent_query).fetchone()
        if extent and all(v is not None for v in extent):
            xmin, xmax, ymin, ymax = extent
            extent_filter = f"""
                ({admin_bbox_col}.xmin <= {xmax} AND
                 {admin_bbox_col}.xmax >= {xmin} AND
                 {admin_bbox_col}.ymin <= {ymax} AND
                 {admin_bbox_col}.ymax >= {ymin})
            """
            admin_where_clauses.append(extent_filter)
            if verbose and not dry_run:
                click.echo(
                    f"Filtering admin boundaries to input extent: ({xmin:.2f}, {ymin:.2f}, {xmax:.2f}, {ymax:.2f})"
                )

    # Build admin subquery with filters
    admin_where_clause = ""
    if admin_where_clauses:
        admin_where_clause = "WHERE " + " AND ".join(admin_where_clauses)

    # Build column list for subquery - handle struct access
    # We need to give SQL expressions an alias so they can be referenced in the outer query
    subquery_cols = []
    for i, col in enumerate(partition_columns):
        if "[" in col or "(" in col:
            # SQL expression - give it an alias
            subquery_cols.append(f"{col} as _col_{i}")
        else:
            # Simple column, quote it
            subquery_cols.append(f'"{col}"')
    subquery_cols_str = ", ".join(subquery_cols)

    # Build SELECT clause for admin columns
    # Create column aliases based on level names with optional transformations
    admin_select_parts = []
    for i, (level, col) in enumerate(zip(levels, partition_columns)):
        # Get the output column name (may be customized for Vecorel compliance)
        output_col_name = dataset.get_output_column_name(level)

        # Check if a column transformation is needed (e.g., stripping country prefix)
        col_transform = dataset.get_column_transform(level)

        if col_transform:
            # Apply transformation - the transform is a full SQL expression
            admin_select_parts.append(f'{col_transform} as "{output_col_name}"')
        elif "[" in col or "(" in col:
            # SQL expression - reference the alias from subquery
            admin_select_parts.append(f'b._col_{i} as "{output_col_name}"')
        else:
            # Simple column name - quote it
            admin_select_parts.append(f'b."{col}" as "{output_col_name}"')

    admin_select_clause = ", ".join(admin_select_parts)

    admin_subquery = f"""(
        SELECT {admin_geom_col}, {admin_bbox_col if admin_bbox_col else admin_geom_col}, {subquery_cols_str}
        FROM {admin_table_ref}
        {admin_where_clause}
    )"""

    # Build spatial join query based on bbox availability
    if input_bbox_col and admin_bbox_col:
        if verbose and not dry_run:
            click.echo("Using bbox columns for initial filtering...")

        # Build bbox intersection condition
        # Handle different bbox column types (STRUCT vs separate columns)
        bbox_condition = f"""(a.{input_bbox_col}.xmin <= b.{admin_bbox_col}.xmax AND
        a.{input_bbox_col}.xmax >= b.{admin_bbox_col}.xmin AND
        a.{input_bbox_col}.ymin <= b.{admin_bbox_col}.ymax AND
        a.{input_bbox_col}.ymax >= b.{admin_bbox_col}.ymin)"""

        query = f"""
    SELECT
        a.*,
        {admin_select_clause}
    FROM '{input_url}' a
    LEFT JOIN {admin_subquery} b
    ON {bbox_condition}  -- Fast bbox intersection test
        AND ST_Intersects(  -- More expensive precise check only on bbox matches
            b.{admin_geom_col},
            a.{input_geom_col}
        )
"""
    else:
        if not dry_run:
            click.echo("No bbox columns available, using full geometry intersection...")

        query = f"""
    SELECT
        a.*,
        {admin_select_clause}
    FROM '{input_url}' a
    LEFT JOIN {admin_subquery} b
    ON ST_Intersects(b.{admin_geom_col}, a.{input_geom_col})
"""

    if dry_run:
        # In dry-run mode, just show the query
        click.echo(click.style("-- Main spatial join query", fg="cyan"))
        if input_bbox_col and admin_bbox_col:
            click.echo(click.style("-- Using bbox columns for optimized spatial join", fg="cyan"))
        else:
            click.echo(
                click.style("-- Using full geometry intersection (no bbox optimization)", fg="cyan")
            )

        # Show the query with COPY wrapper for display
        if compression in ["GZIP", "ZSTD", "BROTLI"]:
            compression_str = f"{compression}:{compression_level}"
        else:
            compression_str = compression

        duckdb_compression = (
            compression.lower() if compression != "UNCOMPRESSED" else "uncompressed"
        )
        display_query = f"""COPY ({query.strip()})
TO '{output_parquet}'
(FORMAT PARQUET, COMPRESSION '{duckdb_compression}');"""
        click.echo(display_query)

        click.echo(click.style(f"\n-- Note: Using {compression_str} compression", fg="cyan"))
        click.echo(
            click.style(
                "-- Original metadata would also be preserved in the output file", fg="cyan"
            )
        )
        con.close()
        return

    # Execute the query using the common write method
    if verbose:
        click.echo("Performing spatial join with admin boundaries...")

    write_parquet_with_metadata(
        con,
        query,
        output_parquet,
        original_metadata=metadata,
        compression=compression,
        compression_level=compression_level,
        row_group_size_mb=row_group_size_mb,
        row_group_rows=row_group_rows,
        verbose=verbose,
    )

    # Get statistics about the results
    # Build condition to check if ANY admin column is not null
    # Use the actual output column names from the dataset
    output_col_names = [dataset.get_output_column_name(level) for level in levels]
    admin_cols_check = " OR ".join([f'"{col}" IS NOT NULL' for col in output_col_names])

    stats_query = f"""
    SELECT
        COUNT(*) as total_features,
        COUNT(CASE WHEN {admin_cols_check} THEN 1 END) as features_with_admin
    FROM '{output_parquet}';
    """

    stats = con.execute(stats_query).fetchone()
    total_features = stats[0]
    features_with_admin = stats[1]

    # Get count of unique values for each admin level
    unique_counts = []
    for level, output_col in zip(levels, output_col_names):
        count_query = f"""
        SELECT COUNT(DISTINCT "{output_col}") as unique_count
        FROM '{output_parquet}'
        WHERE "{output_col}" IS NOT NULL;
        """
        result = con.execute(count_query).fetchone()
        unique_counts.append((level, result[0]))

    con.close()

    click.echo("\nResults:")
    click.echo(
        f"- Added admin division data to {features_with_admin:,} of {total_features:,} features"
    )
    for level, count in unique_counts:
        click.echo(f"- Found {count:,} unique {level} values")

    click.echo(f"\nSuccessfully wrote output to: {output_parquet}")

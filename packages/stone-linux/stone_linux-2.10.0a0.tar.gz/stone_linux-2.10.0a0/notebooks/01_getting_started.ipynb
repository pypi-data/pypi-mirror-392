{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "header",
   "metadata": {},
   "source": [
    "# Getting Started with PyTorch on RTX 50-Series GPUs\n",
    "\n",
    "This notebook demonstrates how to use PyTorch with native SM 12.0 (Blackwell) support on NVIDIA RTX 50-series GPUs.\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "- NVIDIA RTX 5090, 5080, 5070 Ti, or 5070 GPU\n",
    "- NVIDIA Driver >= 570.00\n",
    "- CUDA 13.0+\n",
    "- Python 3.10+\n",
    "\n",
    "## Installation\n",
    "\n",
    "```bash\n",
    "pip install stone-linux\n",
    "stone-install\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section1",
   "metadata": {},
   "source": [
    "## 1. Verify Installation\n",
    "\n",
    "Let's first verify that PyTorch is correctly installed and can access your RTX 50-series GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "verify1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import stone_linux\n",
    "\n",
    "print(f\"stone-linux version: {stone_linux.__version__}\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "print(f\"CUDA version: {torch.version.cuda}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "verify2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run comprehensive verification\n",
    "stone_linux.verify_installation()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section2",
   "metadata": {},
   "source": [
    "## 2. GPU Information\n",
    "\n",
    "Get detailed information about your GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "gpu_info",
   "metadata": {},
   "outputs": [],
   "source": [
    "gpu_info = stone_linux.get_gpu_info()\n",
    "\n",
    "print(\"GPU Information:\")\n",
    "print(\"=\" * 60)\n",
    "for key, value in gpu_info.items():\n",
    "    if key == 'compute_capability':\n",
    "        print(f\"{key}: {value[0]}.{value[1]}\")\n",
    "    elif key == 'total_memory':\n",
    "        print(f\"{key}: {value:.2f} GB\")\n",
    "    else:\n",
    "        print(f\"{key}: {value}\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section3",
   "metadata": {},
   "source": [
    "## 3. Basic Tensor Operations\n",
    "\n",
    "Let's perform some basic tensor operations on the GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tensors1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create tensors on GPU\n",
    "x = torch.randn(3, 4, device='cuda')\n",
    "y = torch.randn(3, 4, device='cuda')\n",
    "\n",
    "print(\"Tensor x:\")\n",
    "print(x)\n",
    "print(f\"\\nDevice: {x.device}\")\n",
    "print(f\"Shape: {x.shape}\")\n",
    "print(f\"Dtype: {x.dtype}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tensors2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic operations\n",
    "z = x + y\n",
    "print(\"x + y:\")\n",
    "print(z)\n",
    "\n",
    "# Matrix multiplication\n",
    "a = torch.randn(100, 100, device='cuda')\n",
    "b = torch.randn(100, 100, device='cuda')\n",
    "c = torch.matmul(a, b)\n",
    "print(f\"\\nMatrix multiplication result shape: {c.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section4",
   "metadata": {},
   "source": [
    "## 4. Simple Neural Network\n",
    "\n",
    "Create and train a simple neural network on the GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "nn1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# Define a simple network\n",
    "class SimpleNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleNet, self).__init__()\n",
    "        self.fc1 = nn.Linear(784, 256)\n",
    "        self.fc2 = nn.Linear(256, 128)\n",
    "        self.fc3 = nn.Linear(128, 10)\n",
    "        self.relu = nn.ReLU()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "# Create model and move to GPU\n",
    "model = SimpleNet().cuda()\n",
    "print(model)\n",
    "print(f\"\\nModel is on: {next(model.parameters()).device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "nn2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dummy data\n",
    "batch_size = 64\n",
    "x = torch.randn(batch_size, 784).cuda()\n",
    "y = torch.randint(0, 10, (batch_size,)).cuda()\n",
    "\n",
    "# Define loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Training loop\n",
    "print(\"Training...\")\n",
    "for epoch in range(10):\n",
    "    optimizer.zero_grad()\n",
    "    outputs = model(x)\n",
    "    loss = criterion(outputs, y)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    if (epoch + 1) % 2 == 0:\n",
    "        print(f\"Epoch [{epoch+1}/10], Loss: {loss.item():.4f}\")\n",
    "\n",
    "print(\"Training complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section5",
   "metadata": {},
   "source": [
    "## 5. Mixed Precision Training\n",
    "\n",
    "Leverage the RTX 50-series' Tensor Cores with mixed precision training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "amp1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "# Create model\n",
    "model = SimpleNet().cuda()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "scaler = torch.cuda.amp.GradScaler()\n",
    "\n",
    "# Training with mixed precision\n",
    "print(\"Training with Automatic Mixed Precision (AMP)...\")\n",
    "start_time = time.time()\n",
    "\n",
    "for epoch in range(10):\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    # Use autocast for mixed precision\n",
    "    with torch.cuda.amp.autocast():\n",
    "        outputs = model(x)\n",
    "        loss = criterion(outputs, y)\n",
    "    \n",
    "    # Scale loss and backward pass\n",
    "    scaler.scale(loss).backward()\n",
    "    scaler.step(optimizer)\n",
    "    scaler.update()\n",
    "    \n",
    "    if (epoch + 1) % 2 == 0:\n",
    "        print(f\"Epoch [{epoch+1}/10], Loss: {loss.item():.4f}\")\n",
    "\n",
    "end_time = time.time()\n",
    "print(f\"\\nTraining time: {end_time - start_time:.2f}s\")\n",
    "print(\"AMP training complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section6",
   "metadata": {},
   "source": [
    "## 6. Performance Comparison: FP32 vs FP16\n",
    "\n",
    "Compare the performance of FP32 and FP16 operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "perf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def benchmark_matmul(size, dtype, iterations=100):\n",
    "    a = torch.randn(size, size, dtype=dtype, device='cuda')\n",
    "    b = torch.randn(size, size, dtype=dtype, device='cuda')\n",
    "    \n",
    "    # Warmup\n",
    "    for _ in range(10):\n",
    "        _ = torch.matmul(a, b)\n",
    "    torch.cuda.synchronize()\n",
    "    \n",
    "    # Benchmark\n",
    "    start = time.time()\n",
    "    for _ in range(iterations):\n",
    "        _ = torch.matmul(a, b)\n",
    "    torch.cuda.synchronize()\n",
    "    end = time.time()\n",
    "    \n",
    "    return (end - start) / iterations\n",
    "\n",
    "# Test different matrix sizes\n",
    "sizes = [512, 1024, 2048, 4096]\n",
    "fp32_times = []\n",
    "fp16_times = []\n",
    "\n",
    "for size in sizes:\n",
    "    print(f\"Testing size {size}x{size}...\")\n",
    "    fp32_time = benchmark_matmul(size, torch.float32)\n",
    "    fp16_time = benchmark_matmul(size, torch.float16)\n",
    "    fp32_times.append(fp32_time * 1000)  # Convert to ms\n",
    "    fp16_times.append(fp16_time * 1000)\n",
    "    print(f\"  FP32: {fp32_time*1000:.2f}ms, FP16: {fp16_time*1000:.2f}ms\")\n",
    "    print(f\"  Speedup: {fp32_time/fp16_time:.2f}x\")\n",
    "\n",
    "# Plot results\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(sizes, fp32_times, 'o-', label='FP32', linewidth=2)\n",
    "plt.plot(sizes, fp16_times, 's-', label='FP16', linewidth=2)\n",
    "plt.xlabel('Matrix Size', fontsize=12)\n",
    "plt.ylabel('Time (ms)', fontsize=12)\n",
    "plt.title('Matrix Multiplication Performance: FP32 vs FP16', fontsize=14)\n",
    "plt.legend(fontsize=12)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section7",
   "metadata": {},
   "source": [
    "## 7. Memory Management\n",
    "\n",
    "Monitor and manage GPU memory usage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "memory1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_gpu_memory():\n",
    "    allocated = torch.cuda.memory_allocated(0) / 1024**3\n",
    "    reserved = torch.cuda.memory_reserved(0) / 1024**3\n",
    "    total = torch.cuda.get_device_properties(0).total_memory / 1024**3\n",
    "    \n",
    "    print(f\"GPU Memory:\")\n",
    "    print(f\"  Allocated: {allocated:.2f} GB\")\n",
    "    print(f\"  Reserved:  {reserved:.2f} GB\")\n",
    "    print(f\"  Total:     {total:.2f} GB\")\n",
    "    print(f\"  Free:      {total - reserved:.2f} GB\")\n",
    "\n",
    "print_gpu_memory()\n",
    "\n",
    "# Allocate some memory\n",
    "large_tensor = torch.randn(10000, 10000, device='cuda')\n",
    "print(\"\\nAfter allocating 10000x10000 tensor:\")\n",
    "print_gpu_memory()\n",
    "\n",
    "# Free memory\n",
    "del large_tensor\n",
    "torch.cuda.empty_cache()\n",
    "print(\"\\nAfter freeing memory:\")\n",
    "print_gpu_memory()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "conclusion",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "You've successfully:\n",
    "\n",
    "1. ✓ Verified PyTorch installation with RTX 50-series support\n",
    "2. ✓ Performed basic tensor operations on GPU\n",
    "3. ✓ Created and trained a neural network\n",
    "4. ✓ Used mixed precision training (AMP)\n",
    "5. ✓ Compared FP32 vs FP16 performance\n",
    "6. ✓ Managed GPU memory\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "- Check out the [performance benchmarking notebook](02_performance_benchmarks.ipynb)\n",
    "- Explore [vLLM integration](../stone_linux/examples/vllm_example.py)\n",
    "- Try [LangChain examples](../stone_linux/examples/langchain_example.py)\n",
    "- Read the [full documentation](../README.md)\n",
    "\n",
    "## Resources\n",
    "\n",
    "- [PyTorch Documentation](https://pytorch.org/docs)\n",
    "- [CUDA Best Practices](https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/)\n",
    "- [Mixed Precision Training](https://pytorch.org/docs/stable/amp.html)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

# generated by datamodel-codegen:
#   filename:  provide-performance-feedback-response.json
#   timestamp: 2025-11-18T03:35:10+00:00

from __future__ import annotations

from typing import Annotated, Any, Literal

from adcp.types.base import AdCPBaseModel
from pydantic import ConfigDict, Field, RootModel

from . import error


class ProvidePerformanceFeedbackResponse1(AdCPBaseModel):
    model_config = ConfigDict(
        extra='forbid',
    )
    context: Annotated[
        dict[str, Any] | None,
        Field(
            description='Initiator-provided context echoed inside the task payload. Opaque metadata such as UI/session hints, correlation tokens, or tracking identifiers.'
        ),
    ] = None
    success: Annotated[
        Literal[True],
        Field(description='Whether the performance feedback was successfully received'),
    ]


class ProvidePerformanceFeedbackResponse2(AdCPBaseModel):
    model_config = ConfigDict(
        extra='forbid',
    )
    context: Annotated[
        dict[str, Any] | None,
        Field(
            description='Initiator-provided context echoed inside the task payload. Opaque metadata such as UI/session hints, correlation tokens, or tracking identifiers.'
        ),
    ] = None
    errors: Annotated[
        list[error.Error],
        Field(
            description='Array of errors explaining why feedback was rejected (e.g., invalid measurement period, missing campaign data)',
            min_length=1,
        ),
    ]


class ProvidePerformanceFeedbackResponse(
    RootModel[ProvidePerformanceFeedbackResponse1 | ProvidePerformanceFeedbackResponse2]
):
    root: Annotated[
        ProvidePerformanceFeedbackResponse1 | ProvidePerformanceFeedbackResponse2,
        Field(
            description='Response payload for provide_performance_feedback task. Returns either success confirmation OR error information, never both.',
            title='Provide Performance Feedback Response',
        ),
    ]

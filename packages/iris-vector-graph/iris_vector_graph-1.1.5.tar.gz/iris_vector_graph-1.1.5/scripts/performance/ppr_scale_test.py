#!/usr/bin/env python3
"""
PPR Scaling Test: Compare Pure Python vs ObjectScript Native at different scales

Tests at 1K, 5K, 10K nodes to find the crossover point.
"""

import sys
import json
import time
import random
import iris

def setup_test_graph(conn, cursor, irispy, num_nodes: int) -> str:
    """Create test graph and return seed node"""
    print(f"\n{'='*70}")
    print(f"Setting up {num_nodes}-node test graph...")
    print(f"{'='*70}")

    # Clean slate
    cursor.execute("DELETE FROM rdf_edges WHERE s LIKE 'SCALE_%'")
    cursor.execute("DELETE FROM nodes WHERE node_id LIKE 'SCALE_%'")
    irispy.kill('^PPR')
    conn.commit()

    # Generate random edges (avg degree ~5)
    random.seed(42)
    edges = []
    for i in range(num_nodes):
        src = f"SCALE_NODE_{i}"
        num_edges = random.randint(3, 7)
        targets = random.sample(range(num_nodes), num_edges)
        for t in targets:
            if t != i:  # No self-loops
                edges.append((src, f"SCALE_NODE_{t}"))

    print(f"  Generated {len(edges)} edges (avg degree: {len(edges)/num_nodes:.1f})")

    # Insert nodes
    node_ids = set()
    for src, dst in edges:
        node_ids.add(src)
        node_ids.add(dst)

    cursor.executemany("INSERT INTO nodes (node_id) VALUES (?)", [(nid,) for nid in node_ids])
    print(f"  Inserted {len(node_ids)} nodes")

    # Insert edges (triggers functional index to build packed lists)
    cursor.executemany("INSERT INTO rdf_edges (s, p, o_id) VALUES (?, 'test', ?)", edges)
    conn.commit()
    print(f"  Inserted {len(edges)} edges")
    print(f"  âœ“ Graph ready")

    return "SCALE_NODE_0"

def benchmark_pure_python(conn, seed: str, runs: int = 5):
    """Benchmark Pure Python PPR using SQL bulk extract"""
    cursor = conn.cursor()
    times = []
    result_size = 0

    for run in range(runs):
        start = time.perf_counter()

        # SQL bulk extract (what Pure Python does)
        cursor.execute("""
            SELECT s, o_id
            FROM rdf_edges
            WHERE s LIKE 'SCALE_%' OR o_id LIKE 'SCALE_%'
        """)
        edges = cursor.fetchall()

        cursor.execute("""
            SELECT node_id
            FROM nodes
            WHERE node_id LIKE 'SCALE_%'
        """)
        nodes = [row[0] for row in cursor.fetchall()]

        # Build graph in memory (simplified PPR would do full computation here)
        graph = {}
        for src, dst in edges:
            if src not in graph:
                graph[src] = []
            graph[src].append(dst)

        elapsed = (time.perf_counter() - start) * 1000
        times.append(elapsed)
        result_size = len(nodes)

    cursor.close()
    avg = sum(times) / len(times)
    return avg, result_size

def benchmark_objectscript_native(irispy, seed: str, runs: int = 5):
    """Benchmark ObjectScript Native with Packed Lists"""
    times = []
    result_size = 0

    for run in range(runs):
        start = time.perf_counter()
        result_json = irispy.classMethodValue(
            'Graph.KG.PPRNative', 'ComputePPR',
            json.dumps([seed]),
            0.85,
            100,
            1e-6
        )
        elapsed = (time.perf_counter() - start) * 1000
        times.append(elapsed)

        result = json.loads(result_json)
        if 'error' not in result:
            result_size = len(result)

    avg = sum(times) / len(times)
    return avg, result_size

def main():
    print("="*70)
    print("PPR SCALING BENCHMARK")
    print("Pure Python vs ObjectScript Native with Packed Lists")
    print("="*70)

    # Connect to IRIS
    conn = iris.connect('localhost', 1972, 'USER', '_SYSTEM', 'SYS')
    cursor = conn.cursor()
    irispy = iris.createIRIS(conn)

    # Test scales
    scales = [
        (1000, "1K"),
        (5000, "5K"),
        (10000, "10K"),
    ]

    results = []

    for num_nodes, label in scales:
        # Setup graph
        seed = setup_test_graph(conn, cursor, irispy, num_nodes)

        # Benchmark Pure Python (SQL bulk extract)
        print(f"\nBenchmarking Pure Python (SQL bulk extract)...")
        py_time, py_size = benchmark_pure_python(conn, seed)
        print(f"  â†’ {py_time:.2f}ms ({py_size} nodes)")

        # Benchmark ObjectScript Native
        print(f"\nBenchmarking ObjectScript Native (Packed Lists + $LISTNEXT)...")
        obj_time, obj_size = benchmark_objectscript_native(irispy, seed)
        print(f"  â†’ {obj_time:.2f}ms ({obj_size} nodes)")

        # Calculate speedup
        speedup = py_time / obj_time if obj_time > 0 else 0

        results.append({
            'scale': label,
            'nodes': num_nodes,
            'pure_python_ms': py_time,
            'objectscript_ms': obj_time,
            'speedup': speedup
        })

        print(f"\n{'='*70}")
        print(f"RESULT: {label} nodes")
        print(f"  Pure Python:    {py_time:.2f}ms")
        print(f"  ObjectScript:   {obj_time:.2f}ms")
        if speedup > 1.0:
            print(f"  Winner:         ObjectScript ({speedup:.2f}x faster)")
        elif speedup < 1.0:
            print(f"  Winner:         Pure Python ({1/speedup:.2f}x faster)")
        else:
            print(f"  Winner:         Tie")
        print(f"{'='*70}")

    # Final summary table
    print(f"\n{'='*70}")
    print("SCALING SUMMARY")
    print(f"{'='*70}\n")
    print(f"{'Scale':<12} {'Pure Python':<15} {'ObjectScript':<15} {'Winner':<20}")
    print(f"{'-'*70}")

    for r in results:
        py_str = f"{r['pure_python_ms']:.2f}ms"
        obj_str = f"{r['objectscript_ms']:.2f}ms"

        if r['speedup'] > 1.05:  # 5% threshold
            winner = f"ObjectScript ({r['speedup']:.2f}x)"
        elif r['speedup'] < 0.95:
            winner = f"Pure Python ({1/r['speedup']:.2f}x)"
        else:
            winner = "Tie (~equal)"

        print(f"{r['scale']:<12} {py_str:<15} {obj_str:<15} {winner:<20}")

    # Analysis
    print(f"\n{'='*70}")
    print("ANALYSIS")
    print(f"{'='*70}\n")

    # Check for crossover point
    obj_wins = [r for r in results if r['speedup'] > 1.05]

    if obj_wins:
        first_win = obj_wins[0]
        print(f"âœ… ObjectScript Native wins starting at {first_win['scale']} nodes")
        print(f"   Speedup: {first_win['speedup']:.2f}x faster than Pure Python")
        print(f"\nðŸ“Š Scaling behavior:")
        print(f"   - At {results[0]['scale']}: {results[0]['speedup']:.2f}x")
        print(f"   - At {results[1]['scale']}: {results[1]['speedup']:.2f}x")
        print(f"   - At {results[2]['scale']}: {results[2]['speedup']:.2f}x")
        print(f"\nðŸŽ¯ Recommendation:")
        print(f"   - Small graphs (<{first_win['nodes']}): Use Pure Python (simpler)")
        print(f"   - Large graphs (â‰¥{first_win['nodes']}): Use ObjectScript Native (faster)")
    else:
        print(f"ðŸ“Š Pure Python competitive at all tested scales")
        print(f"   Pure Python's single SQL bulk extract is very efficient!")
        print(f"\nðŸŽ¯ Recommendation:")
        print(f"   - Use Pure Python for simplicity and portability")
        print(f"   - ObjectScript Native available for ultra-large graphs (100K+ nodes)")

    print(f"\n{'='*70}\n")

    # Cleanup
    cursor.close()
    conn.close()

if __name__ == '__main__':
    try:
        main()
    except KeyboardInterrupt:
        print("\n\nBenchmark interrupted")
        sys.exit(1)
    except Exception as e:
        print(f"\n\nError: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)

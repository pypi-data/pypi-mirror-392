# Parallel Research Workflow - Visual Guide

## ğŸ¯ Overview

This guide shows the complete parallel research workflow with visual diagrams.

## ğŸ“Š Workflow Diagrams

### Traditional Sequential Workflow (Before)

```
Input Text
    â†“
Manual Topic Extraction (slow)
    â†“
Query 1 â†’ API â†’ Wait â†’ Result 1
    â†“
Query 2 â†’ API â†’ Wait â†’ Result 2
    â†“
Query 3 â†’ API â†’ Wait â†’ Result 3
    â†“
...
    â†“
Manual Compilation
    â†“
Results

â±ï¸  Total Time: N Ã— ~12 seconds per query
```

### New Parallel Workflow (After)

```
Input Text
    â†“
AI Topic Identification (fast, automatic)
    â†“
topics.txt (reviewable, editable)
    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚     Parallel Execution Engine       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Query 1 â†’ API â”€â”€â”                  â”‚
â”‚ Query 2 â†’ API â”€â”€â”¤                  â”‚
â”‚ Query 3 â†’ API â”€â”€â”¼â†’ Results (JSON)  â”‚
â”‚ Query 4 â†’ API â”€â”€â”¤                  â”‚
â”‚ Query N â†’ API â”€â”€â”˜                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â†“
results.json (structured, complete)

â±ï¸  Total Time: ~15-20 seconds (regardless of N, up to worker limit)
```

## ğŸš€ Three Usage Patterns

### Pattern 1: Quick & Automated (One Command)

**Use Case:** You have text and want results ASAP

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  python research_lookup.py --identify input.txt             â”‚
â”‚         --topics-file topics.txt                            â”‚
â”‚         --parallel --max-workers 10                         â”‚
â”‚         --output results.json                               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â†“
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚  Automatic Workflow                   â”‚
        â”‚  1. Identify topics (AI)              â”‚
        â”‚  2. Save to topics.txt                â”‚
        â”‚  3. Research in parallel (10 workers) â”‚
        â”‚  4. Export to results.json            â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â†“
        âœ… Complete results in < 1 minute
```

**When to use:**
- âœ… Quick research needed
- âœ… Trust AI topic identification
- âœ… Don't need to review topics

### Pattern 2: Review & Refine (Two Steps)

**Use Case:** You want to review/edit topics before research

```
STEP 1: Identify Topics
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  python research_lookup.py --identify input.txt             â”‚
â”‚         --topics-file topics.txt                            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â†“
                    topics.txt created
                            â†“
            ğŸ‘¤ HUMAN REVIEW & EDIT ğŸ‘¤
            (add, remove, refine topics)
                            â†“
STEP 2: Research in Parallel
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  python research_lookup.py --topics-file topics.txt         â”‚
â”‚         --parallel --max-workers 10                         â”‚
â”‚         --output results.json                               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â†“
        âœ… Refined results in < 1 minute
```

**When to use:**
- âœ… Want to ensure topic quality
- âœ… Need to add/remove topics
- âœ… Iterative refinement

### Pattern 3: Programmatic (Python API)

**Use Case:** Integration into larger pipeline

```python
from research_lookup import ResearchLookup

# Initialize
research = ResearchLookup()

# Option A: Complete automation
result = research.identify_and_research(
    text=your_text,
    topics_file="topics.txt",
    parallel=True,
    max_workers=10,
    output_file="results.json"
)

# Option B: Step-by-step control
topics = research.identify_research_topics(text, "topics.txt")
# ... process topics ...
results = research.parallel_lookup(topics, max_workers=10)
# ... process results ...
```

**When to use:**
- âœ… Part of automated pipeline
- âœ… Need programmatic control
- âœ… Custom processing needed

## ğŸ›ï¸ Worker Configuration

```
Workers: 1    2    3    4    5    6    7    8    9    10
         â†“    â†“    â†“    â†“    â†“    â†“    â†“    â†“    â†“    â†“
Time:   100%  50%  35%  28%  22%  18%  16%  14%  13%  12%

Recommendation: 5-10 workers for best balance
- Too few (1-3): Not much speedup
- Optimal (5-10): Great speedup, no rate limits
- Too many (15+): May hit API rate limits
```

## ğŸ“ˆ Performance by Use Case

### Literature Review (10-20 queries)

**Sequential:**
```
[â–“â–“â–“â–“â–“â–“â–“â–“â–“â–“â–“â–“â–“â–“â–“â–“â–“â–“â–“â–“] 100% | ~3-4 minutes
```

**Parallel (10 workers):**
```
[â–“â–“â–“â–“] 100% | ~30 seconds
```

**Time Saved:** 2.5-3.5 minutes (83% faster)

### Grant Application (5-10 queries)

**Sequential:**
```
[â–“â–“â–“â–“â–“â–“â–“â–“â–“â–“] 100% | ~1-2 minutes
```

**Parallel (5 workers):**
```
[â–“â–“] 100% | ~15-20 seconds
```

**Time Saved:** 45-100 seconds (75-83% faster)

### Manuscript Citations (20-50 queries)

**Sequential:**
```
[â–“â–“â–“â–“â–“â–“â–“â–“â–“â–“â–“â–“â–“â–“â–“â–“â–“â–“â–“â–“â–“â–“â–“â–“â–“â–“â–“â–“â–“â–“] 100% | ~5-10 minutes
```

**Parallel (10 workers):**
```
[â–“â–“â–“â–“â–“] 100% | ~1-1.5 minutes
```

**Time Saved:** 4-8.5 minutes (80-90% faster)

## ğŸ¨ Real-World Example

### Scenario: Research Proposal

**Input Text (proposal.txt):**
```
We propose investigating CRISPR-Cas9 for treating sickle cell disease.
Research areas include: clinical trials, delivery methods, off-target
effects, comparison with traditional treatments, and regulatory approval.
```

**Step 1: Identify Topics**
```bash
$ python research_lookup.py --identify proposal.txt --topics-file topics.txt

[Research] Identifying research topics...
[Research] Identified 5 research topics
[Research] Saved 5 topics to topics.txt
```

**Generated topics.txt:**
```
1. CRISPR-Cas9 clinical trials for sickle cell disease
2. Delivery methods for CRISPR in hematopoietic stem cells
3. Off-target effects of CRISPR-Cas9 gene editing
4. Comparison of CRISPR vs bone marrow transplantation for sickle cell
5. Regulatory approval pathways for CRISPR therapeutics
```

**Step 2: Review & Edit**
```bash
# Open topics.txt and refine (optional)
# Add: "6. Cost-effectiveness of CRISPR therapy"
```

**Step 3: Parallel Research**
```bash
$ python research_lookup.py --topics-file topics.txt --parallel --max-workers 5 --output results.json

[Research] Loaded 6 topics from topics.txt
[Research] Starting parallel lookup for 6 queries with 5 workers...
[Research] âœ“ Completed 1/6: CRISPR-Cas9 clinical trials for sickle cell...
[Research] âœ“ Completed 2/6: Delivery methods for CRISPR in hematopoiet...
[Research] âœ“ Completed 3/6: Off-target effects of CRISPR-Cas9 gene edi...
[Research] âœ“ Completed 4/6: Comparison of CRISPR vs bone marrow transpl...
[Research] âœ“ Completed 5/6: Regulatory approval pathways for CRISPR the...
[Research] âœ“ Completed 6/6: Cost-effectiveness of CRISPR therapy...
[Research] Parallel lookup complete. 6/6 successful

SUMMARY: 6/6 queries completed successfully
```

**Total Time:** ~18 seconds (vs ~72 seconds sequential)

**Output (results.json):**
```json
{
  "timestamp": "2024-11-17 10:30:15",
  "total_topics": 6,
  "successful_queries": 6,
  "topics": [...],
  "results": [
    {
      "success": true,
      "query": "CRISPR-Cas9 clinical trials for sickle cell disease",
      "response": "...",
      "citations": [...],
      "model": "perplexity/sonar-pro"
    },
    ...
  ]
}
```

## ğŸ”„ Iterative Refinement

```
Round 1: Initial Research
    â†“
Review Results
    â†“
Identify Gaps
    â†“
Add New Topics to topics.txt
    â†“
Round 2: Targeted Research
    â†“
Review Results
    â†“
Round 3: Deep Dive
    (use --force-model reasoning for analysis)
```

## ğŸ’¡ Pro Tips

### 1. Start Broad, Then Narrow
```
Round 1: 5 broad topics (Sonar Pro)
  â†“ Identify gaps
Round 2: 3 specific topics (Sonar Reasoning Pro for analysis)
```

### 2. Use Topic Files as Templates
```bash
# Create template for recurring research
cp topics.txt templates/weekly_literature_review.txt

# Modify and run weekly
python research_lookup.py --topics-file templates/weekly_literature_review.txt --parallel
```

### 3. Combine with Scripts
```bash
# Daily automated research
#!/bin/bash
DATE=$(date +%Y%m%d)
python research_lookup.py \
  --topics-file recurring_topics.txt \
  --parallel --max-workers 10 \
  --output "results_${DATE}.json"
```

## ğŸ“š Best Practices Summary

| Queries | Workers | Pattern      | Time Savings |
|---------|---------|--------------|--------------|
| 1-2     | 1       | Sequential   | None         |
| 3-5     | 3-5     | Parallel     | 3-4x faster  |
| 6-10    | 5-8     | Parallel     | 5-6x faster  |
| 11-20   | 8-10    | Parallel     | 6-7x faster  |
| 21+     | 10      | Parallel     | 7-8x faster  |

## ğŸ¯ Decision Tree

```
Do you have 5+ research questions?
    â†“
   Yes â†’ Use parallel workflow
    â†“
Do you have input text or manual topics?
    â†“
Input text â†’ Use --identify
    â†“
Need to review topics?
    â†“
   Yes â†’ Two-step workflow (identify, review, research)
   No  â†’ One-step workflow (identify_and_research)
    â†“
Manual topics â†’ Use --topics-file directly
    â†“
Run parallel research with 5-10 workers
    â†“
âœ… Results in < 1 minute
```

## ğŸš€ Quick Reference

```bash
# Fastest (one command)
python research_lookup.py --identify input.txt --topics-file topics.txt --parallel --output results.json

# Most control (two commands)
python research_lookup.py --identify input.txt --topics-file topics.txt
# ... edit topics.txt ...
python research_lookup.py --topics-file topics.txt --parallel --output results.json

# From existing topics
python research_lookup.py --topics-file my_topics.txt --parallel

# Python API (complete automation)
result = research.identify_and_research(text, parallel=True, max_workers=10)
```

---

**Ready to get started?** Pick a pattern above and try it with your research!


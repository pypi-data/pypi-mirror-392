"""
Custom layers module for Compact-RIEnet.

This module contains all the specialized neural network layers required for the
Compact-RIEnet architecture, including layers for covariance estimation,
Rotational Invariant Estimator (RIE) based eigenvalue cleaning, and specialized
transformations for financial data.

References:
-----------
Christian Bongiorno, Efstratios Manolakis, and Rosario Nunzio Mantegna. 2025. 
Neural Network-Driven Volatility Drag Mitigation under Aggressive Leverage. 
In Proceedings of the 6th ACM International Conference on AI in Finance (ICAIF '25). 
Association for Computing Machinery, New York, NY, USA, 449–455. 
https://doi.org/10.1145/3768292.3770370

Copyright (c) 2025
"""

import math

import tensorflow as tf
from keras import backend as K
from keras import layers, initializers
from typing import List, Optional, Tuple

from .dtype_utils import ensure_float32, restore_dtype, epsilon_for_dtype


@tf.keras.utils.register_keras_serializable(package='compact_rienet')
class StandardDeviationLayer(layers.Layer):
    """
    Layer for computing sample standard deviation and mean.
    
    This layer computes the sample standard deviation and mean along a specified axis,
    with optional demeaning for statistical preprocessing.
    
    Parameters
    ----------
    axis : int, default 1
        Axis along which to compute statistics
    demean : bool, default False
        Whether to use an unbiased denominator (n-1)
    epsilon : float, optional
        Small epsilon for numerical stability
    name : str, optional
        Layer name
    """
    
    def __init__(self,
                 axis: int = 1,
                 demean: bool = False,
                 epsilon: Optional[float] = None,
                 name: Optional[str] = None,
                 **kwargs):
        if name is None:
            raise ValueError("StandardDeviationLayer must have a name.")
        super().__init__(name=name, **kwargs)
        self.axis = axis
        self.demean = demean
        self.epsilon = float(epsilon if epsilon is not None else K.epsilon())

    def call(self, x: tf.Tensor) -> Tuple[tf.Tensor, tf.Tensor]:
        """
        Compute standard deviation and mean.
        
        Parameters
        ----------
        x : tf.Tensor
            Input tensor
            
        Returns
        -------
        tuple of tf.Tensor
            (standard_deviation, mean)
        """
        dtype = x.dtype
        epsilon = epsilon_for_dtype(dtype, self.epsilon)

        sample_size = tf.cast(tf.shape(x)[self.axis], dtype)
        sample_size = tf.maximum(sample_size, 1.0)

        mean = tf.reduce_mean(x, axis=self.axis, keepdims=True)
        centered = x - mean

        if self.demean:
            denom = tf.maximum(sample_size - 1.0, 1.0)
        else:
            denom = sample_size

        variance = tf.reduce_sum(tf.square(centered), axis=self.axis, keepdims=True) / denom
        std = tf.sqrt(tf.maximum(variance, epsilon))

        return std, mean

    def get_config(self) -> dict:
        config = super().get_config()
        config.update({
            'axis': self.axis,
            'demean': self.demean,
            'epsilon': self.epsilon,
        })
        return config


@tf.keras.utils.register_keras_serializable(package='compact_rienet')
class CovarianceLayer(layers.Layer):
    """
    Layer for computing covariance matrices.
    
    This layer computes sample covariance matrices from input data with optional
    normalization and dimension expansion.
    
    Parameters
    ----------
    expand_dims : bool, default False
        Whether to expand dimensions of output
    normalize : bool, default True  
        Whether to normalize by sample size
    name : str, optional
        Layer name
    """
    
    def __init__(self, expand_dims: bool = False, normalize: bool = True, 
                 name: Optional[str] = None, **kwargs):
        if name is None:
            raise ValueError("CovarianceLayer must have a name.")
        super().__init__(name=name, **kwargs)
        self.expand_dims = expand_dims
        self.normalize = normalize

    def call(self, returns: tf.Tensor) -> tf.Tensor:
        """
        Compute covariance matrix.
        
        Parameters
        ----------
        returns : tf.Tensor
            Input returns data
            
        Returns
        -------
        tf.Tensor
            Covariance matrix
        """
        if self.normalize:
            sample_size = tf.cast(tf.shape(returns)[-1], returns.dtype)
            covariance = tf.matmul(returns, returns, transpose_b=True) / sample_size
        else:
            covariance = tf.matmul(returns, returns, transpose_b=True)
            
        if self.expand_dims:
            covariance = tf.expand_dims(covariance, axis=-3)
            
        return covariance

    def get_config(self) -> dict:
        config = super().get_config()
        config.update({
            'expand_dims': self.expand_dims,
            'normalize': self.normalize
        })
        return config


@tf.keras.utils.register_keras_serializable(package='compact_rienet')
class SpectralDecompositionLayer(layers.Layer):
    """
    Layer for eigenvalue decomposition of symmetric matrices.
    
    This layer performs eigenvalue decomposition using tf.linalg.eigh,
    which is optimized for symmetric/Hermitian matrices like covariance matrices.
    
    Parameters
    ----------
    name : str, optional
        Layer name
    """
    
    def __init__(self, name: Optional[str] = None, **kwargs):
        if name is None:
            raise ValueError("SpectralDecompositionLayer must have a name.")
        super().__init__(name=name, **kwargs)

    def call(self, covariance_matrix: tf.Tensor) -> Tuple[tf.Tensor, tf.Tensor]:
        """
        Perform eigenvalue decomposition.
        
        Parameters
        ----------
        covariance_matrix : tf.Tensor
            Input covariance matrix
            
        Returns
        -------
        tuple of tf.Tensor
            (eigenvalues, eigenvectors) where eigenvalues have shape [..., n, 1]
        """
        covariance32, original_dtype = ensure_float32(covariance_matrix)
        eigenvalues, eigenvectors = tf.linalg.eigh(covariance32)
        # Expand dims to make eigenvalues [..., n, 1] for compatibility
        eigenvalues = tf.expand_dims(eigenvalues, axis=-1)
        return (
            restore_dtype(eigenvalues, original_dtype),
            restore_dtype(eigenvectors, original_dtype)
        )

    def get_config(self) -> dict:
        return super().get_config()


@tf.keras.utils.register_keras_serializable(package='compact_rienet')
class DimensionAwareLayer(layers.Layer):
    """
    Layer that adds dimensional features to eigenvalue data.
    
    This layer augments eigenvalue tensors with additional features derived
    from the dimensions of the input data, such as number of stocks, days,
    and their ratios.
    
    Parameters
    ----------
    features : list of str
        List of features to add: 'n_stocks', 'n_days', 'q', 'rsqrt_n_days'
    name : str, optional
        Layer name
    """
    
    def __init__(self, features: List[str], name: Optional[str] = None, **kwargs):
        if name is None:
            raise ValueError("DimensionAwareLayer must have a name.")
        super().__init__(name=name, **kwargs)
        self.features = features

    def _set_attribute(self, value: tf.Tensor, shape: tf.Tensor, dtype: tf.dtypes.DType) -> tf.Tensor:
        """Broadcast scalar value to target shape with the target dtype."""
        value = tf.cast(value, dtype)
        value = tf.expand_dims(value, axis=-1)
        value = tf.broadcast_to(value, shape)
        return value

    def call(self, inputs: List[tf.Tensor]) -> tf.Tensor:
        """
        Add dimensional features to eigenvalues.
        
        Parameters
        ----------
        inputs : list of tf.Tensor
            [eigenvalues, original_inputs] where original_inputs has shape [..., n_stocks, n_days]
            
        Returns
        -------
        tf.Tensor
            Enhanced eigenvalues with additional features
        """
        eigen_values, original_inputs = inputs
        n_stocks = tf.cast(tf.shape(original_inputs)[1], tf.float32)
        n_days = tf.cast(tf.shape(original_inputs)[2], tf.float32)
        target_dtype = eigen_values.dtype
        final_shape = tf.shape(eigen_values)
        
        tensors_to_concat = [eigen_values]
        
        if 'q' in self.features:
            q = n_days / n_stocks
            tensors_to_concat.append(self._set_attribute(q, final_shape, target_dtype))
            
        if 'n_stocks' in self.features:
            tensors_to_concat.append(self._set_attribute(tf.sqrt(n_stocks), final_shape, target_dtype))
            
        if 'n_days' in self.features:
            tensors_to_concat.append(self._set_attribute(tf.sqrt(n_days), final_shape, target_dtype))
            
        if 'rsqrt_n_days' in self.features:
            rsqrt_n_days = tf.math.rsqrt(n_days)
            tensors_to_concat.append(self._set_attribute(rsqrt_n_days, final_shape, target_dtype))
            
        return tf.concat(tensors_to_concat, axis=-1)

    def compute_output_shape(self, input_shape: Tuple[Tuple, Tuple]) -> Tuple:
        """Compute output shape."""
        eigen_values_shape, _ = input_shape
        additional_features = len(self.features)
        return eigen_values_shape[:-1] + (eigen_values_shape[-1] + additional_features,)

    def get_config(self) -> dict:
        config = super().get_config()
        config.update({'features': self.features})
        return config


@tf.keras.utils.register_keras_serializable(package='compact_rienet')
class DeepLayer(layers.Layer):
    """
    Multi-layer dense network with configurable activation and dropout.
    
    This layer implements a sequence of dense layers with specified activations,
    dropout, and flexible configuration for the final layer.
    
    Parameters
    ----------
    hidden_layer_sizes : list of int
        Sizes of hidden layers including output layer
    last_activation : str, default "linear"
        Activation for the final layer
    activation : str, default "leaky_relu"
        Activation for hidden layers
    other_biases : bool, default True
        Whether to use bias in hidden layers
    last_bias : bool, default True
        Whether to use bias in final layer
    dropout_rate : float, default 0.0
        Dropout rate for hidden layers
    kernel_initializer : str, default "glorot_uniform"
        Weight initialization method
    name : str, optional
        Layer name
    """
    
    def __init__(self, hidden_layer_sizes: List[int], last_activation: str = "linear",
                 activation: str = "leaky_relu", other_biases: bool = True, 
                 last_bias: bool = True, dropout_rate: float = 0., 
                 kernel_initializer: str = "glorot_uniform", name: Optional[str] = None, **kwargs):
        if name is None:
            raise ValueError("DeepLayer must have a name.")
        super().__init__(name=name, **kwargs)
        
        self.hidden_layer_sizes = hidden_layer_sizes
        self.activation = activation
        self.last_activation = last_activation
        self.other_biases = other_biases
        self.last_bias = last_bias
        self.dropout_rate = dropout_rate
        self.kernel_initializer = kernel_initializer

        # Build hidden layers
        self.hidden_layers = []
        self.dropouts = []
        
        for i, size in enumerate(self.hidden_layer_sizes[:-1]):
            layer_name = f"{self.name}_hidden_{i}"
            dropout_name = f"{self.name}_dropout_{i}"
            
            dense = layers.Dense(
                size,
                activation=self.activation,
                use_bias=self.other_biases,
                kernel_initializer=self.kernel_initializer,
                name=layer_name
            )
            dropout = layers.Dropout(self.dropout_rate, name=dropout_name)
            
            self.hidden_layers.append(dense)
            self.dropouts.append(dropout)

        # Final layer
        self.final_dense = layers.Dense(
            self.hidden_layer_sizes[-1],
            use_bias=self.last_bias,
            activation=self.last_activation,
            kernel_initializer=self.kernel_initializer,
            name=f"{self.name}_output"
        )

    def build(self, input_shape: Tuple[int, ...]) -> None:
        """Build the dense and dropout sublayers."""
        input_shape = tf.TensorShape(input_shape)
        current_shape = input_shape

        for dense, dropout in zip(self.hidden_layers, self.dropouts):
            dense.build(current_shape)
            current_shape = dense.compute_output_shape(current_shape)
            dropout.build(current_shape)

        self.final_dense.build(current_shape)
        super().build(input_shape)

    def call(self, inputs: tf.Tensor, training: Optional[bool] = None) -> tf.Tensor:
        """
        Forward pass through the network.
        
        Parameters
        ----------
        inputs : tf.Tensor
            Input tensor
        training : bool, optional
            Whether in training mode
            
        Returns
        -------
        tf.Tensor
            Output tensor
        """
        x = inputs
        for dense, dropout in zip(self.hidden_layers, self.dropouts):
            x = dense(x)
            x = dropout(x, training=training)
        return self.final_dense(x)

    def get_config(self) -> dict:
        config = super().get_config()
        config.update({
            'hidden_layer_sizes': self.hidden_layer_sizes,
            'activation': self.activation,
            'last_activation': self.last_activation,
            'other_biases': self.other_biases,
            'last_bias': self.last_bias,
            'dropout_rate': self.dropout_rate,
            'kernel_initializer': self.kernel_initializer
        })
        return config

    def compute_output_shape(self, input_shape: Tuple) -> Tuple:
        """Compute output shape."""
        output_shape = list(input_shape)
        output_shape[-1] = self.hidden_layer_sizes[-1]
        return tuple(output_shape)


@tf.keras.utils.register_keras_serializable(package='compact_rienet')
class DeepRecurrentLayer(layers.Layer):
    """
    Deep recurrent layer with configurable RNN cells and post-processing.
    
    This layer implements a stack of recurrent layers (LSTM/GRU) with optional
    bidirectional processing, followed by dense layers for final transformation.
    
    Parameters
    ----------
    recurrent_layer_sizes : list of int
        Sizes of recurrent layers
    final_activation : str, default "softplus"
        Activation for final dense layer
    final_hidden_layer_sizes : list of int, default []
        Sizes of dense layers after RNN
    final_hidden_activation : str, default "leaky_relu"
        Activation for final hidden layers
    direction : str, default 'bidirectional'
        RNN direction: 'bidirectional', 'forward', or 'backward'
    dropout : float, default 0.0
        Dropout rate for RNN layers
    recurrent_dropout : float, default 0.0
        Recurrent dropout rate
    recurrent_model : str, default 'LSTM'
        Type of RNN cell: 'LSTM' or 'GRU'
    normalize : str, optional
        Normalization mode: None, 'inverse', or 'sum'
    normalize_inverse_power : float, default 1.0
        Exponent used when ``normalize='inverse'``; ensures the normalized output
        satisfies ``mean(x^{-normalize_inverse_power}) = 1`` along the sequence axis.
    name : str, optional
        Layer name
    """
    
    def __init__(self, recurrent_layer_sizes: List[int], final_activation: str = "softplus", 
                 final_hidden_layer_sizes: List[int] = [], final_hidden_activation: str = "leaky_relu",
                 direction: str = 'bidirectional', dropout: float = 0., recurrent_dropout: float = 0.,
                 recurrent_model: str = 'LSTM', normalize: Optional[str] = None,
                 normalize_inverse_power: float = 1.0,
                 name: Optional[str] = None, **kwargs):
        if name is None:
            raise ValueError("DeepRecurrentLayer must have a name.")
        super().__init__(name=name, **kwargs)

        self.recurrent_layer_sizes = recurrent_layer_sizes
        self.final_activation = final_activation
        self.final_hidden_layer_sizes = final_hidden_layer_sizes
        self.final_hidden_activation = final_hidden_activation
        self.direction = direction
        self.dropout = dropout
        self.recurrent_dropout = recurrent_dropout
        self.recurrent_model = recurrent_model
        
        if normalize not in [None, 'inverse', "sum"]:
            raise ValueError("normalize must be None, 'inverse', or 'sum'.")
        self.normalize = normalize
        if self.normalize is not None and normalize_inverse_power <= 0:
            raise ValueError("normalize_inverse_power must be positive when using inverse normalization.")
        self.normalize_inverse_power = float(normalize_inverse_power)

        # Build recurrent layers
        RNN = getattr(layers, recurrent_model)
        self.recurrent_layers = []
        
        for i, units in enumerate(self.recurrent_layer_sizes):
            layer_name = f"{self.name}_rnn_{i}"
            cell_name = f"{layer_name}_cell"
            
            if self.direction == 'bidirectional':
                cell = RNN(
                    units=units, 
                    dropout=self.dropout, 
                    recurrent_dropout=self.recurrent_dropout,
                    return_sequences=True, 
                    name=cell_name
                )
                rnn_layer = layers.Bidirectional(cell, name=layer_name)
            elif self.direction == 'forward':
                rnn_layer = RNN(
                    units=units, 
                    dropout=self.dropout, 
                    recurrent_dropout=self.recurrent_dropout,
                    return_sequences=True, 
                    name=layer_name
                )
            elif self.direction == 'backward':
                rnn_layer = RNN(
                    units=units, 
                    dropout=self.dropout, 
                    recurrent_dropout=self.recurrent_dropout,
                    return_sequences=True, 
                    go_backwards=True, 
                    name=layer_name
                )
            else:
                raise ValueError("direction must be 'bidirectional', 'forward', or 'backward'.")
                
            self.recurrent_layers.append(rnn_layer)

        # Final dense layers
        self.final_deep_dense = DeepLayer(
            final_hidden_layer_sizes + [1], 
            activation=final_hidden_activation,
            last_activation=final_activation,
            dropout_rate=dropout,
            name=f"{self.name}_finaldeep"
        )       

        if self.normalize is not None:
            inverse_power = self.normalize_inverse_power if self.normalize == 'inverse' else 1.0
            self._normalizer = CustomNormalizationLayer(
                mode=self.normalize,
                axis=-2,
                inverse_power=inverse_power,
                name=f"{self.name}_norm"
            )
        else:
            self._normalizer = None

    def build(self, input_shape: Tuple[int, ...]) -> None:
        """Build the recurrent stack and final dense projection."""
        input_shape = tf.TensorShape(input_shape)
        current_shape = input_shape

        for rnn_layer in self.recurrent_layers:
            rnn_layer.build(current_shape)
            current_shape = rnn_layer.compute_output_shape(current_shape)

        self.final_deep_dense.build(current_shape)
        final_shape = self.final_deep_dense.compute_output_shape(current_shape)

        if self._normalizer is not None:
            self._normalizer.build(final_shape)

        super().build(input_shape)

    def call(self, inputs: tf.Tensor, training: Optional[bool] = None) -> tf.Tensor:
        """
        Forward pass through recurrent layers.
        
        Parameters
        ----------
        inputs : tf.Tensor
            Input tensor
        training : bool, optional
            Whether in training mode
            
        Returns
        -------
        tf.Tensor
            Output tensor
        """
        x = inputs
        for layer in self.recurrent_layers:
            x = layer(x, training=training)
            
        outputs = self.final_deep_dense(x, training=training)
        
        if self._normalizer is not None:
            outputs = self._normalizer(outputs)
            
        return tf.squeeze(outputs, axis=-1)
    
    def get_config(self) -> dict:
        config = super().get_config()
        config.update({
            'recurrent_layer_sizes': self.recurrent_layer_sizes,
            'final_activation': self.final_activation,
            'final_hidden_layer_sizes': self.final_hidden_layer_sizes,
            'final_hidden_activation': self.final_hidden_activation,
            'direction': self.direction,
            'dropout': self.dropout,
            'recurrent_dropout': self.recurrent_dropout,
            'recurrent_model': self.recurrent_model,
            'normalize': self.normalize,
            'normalize_inverse_power': self.normalize_inverse_power
        })
        return config


@tf.keras.utils.register_keras_serializable(package='compact_rienet')
class CustomNormalizationLayer(layers.Layer):
    """
    Custom normalization layer with different modes.
    
    This layer applies different types of normalization along specified axes,
    including sum normalization and inverse normalization.
    
    Parameters
    ----------
    mode : str, default 'sum'
        Normalization mode: 'sum' or 'inverse'
    axis : int, default -2
        Axis along which to normalize
    inverse_power : float, default 1.0
        Exponent used when ``mode='inverse'`` so that the normalization enforces
        ``mean(x^{-inverse_power}) = 1`` along the target axis.
    epsilon : float, optional
        Small epsilon for numerical stability
    name : str, optional
        Layer name
    """
    
    def __init__(self,
                 mode: str = 'sum',
                 axis: int = -2,
                 inverse_power: float = 1.0,
                 epsilon: Optional[float] = None,
                 name: Optional[str] = None,
                 **kwargs):
        if name is None:
            raise ValueError("CustomNormalizationLayer must have a name.")
        super().__init__(name=name, **kwargs)
        self.mode = mode
        self.axis = axis
        if inverse_power <= 0:
            raise ValueError("inverse_power must be positive")
        self.inverse_power = float(inverse_power)
        self.epsilon = float(epsilon if epsilon is not None else K.epsilon())

    def call(self, x: tf.Tensor) -> tf.Tensor:
        """
        Apply normalization.
        
        Parameters
        ----------
        x : tf.Tensor
            Input tensor
            
        Returns
        -------
        tf.Tensor
            Normalized tensor
        """
        dtype = x.dtype
        epsilon = epsilon_for_dtype(dtype, self.epsilon)
        n = tf.cast(tf.shape(x)[self.axis], dtype)

        denom_axis = tf.reduce_sum(x, axis=self.axis, keepdims=True)

        if self.mode == 'sum':
            x = n * x / tf.maximum(denom_axis, epsilon)
        elif self.mode == 'inverse':
            x = tf.maximum(x, epsilon)
            inv = tf.math.pow(x, -self.inverse_power)
            inv_total = tf.reduce_sum(inv, axis=self.axis, keepdims=True)
            inv_normalized = n * inv / tf.maximum(inv_total, epsilon)
            power = tf.cast(-1.0 / self.inverse_power, dtype)
            x = tf.math.pow(tf.maximum(inv_normalized, epsilon), power)
        
        return x

    def get_config(self) -> dict:
        config = super().get_config()
        config.update({
            'mode': self.mode,
            'axis': self.axis,
            'inverse_power': self.inverse_power,
            'epsilon': self.epsilon,
        })
        return config


@tf.keras.utils.register_keras_serializable(package='compact_rienet')
class EigenvectorRescalingLayer(layers.Layer):
    """
    Layer that rescales eigenvectors to enforce unit diagonals.

    Given eigenvectors ``V`` and eigenvalues ``λ`` this layer computes the diagonal
    elements of ``V diag(λ) Vᵀ`` and divides each eigenvector row by the square
    root of the corresponding diagonal entry. The operation matches::

        d = einsum('...ij,...j,...ij->...i', V, λ, V)
        V_rescaled = V / sqrt(d)[..., None]

    Parameters
    ----------
    epsilon : float, optional
        Minimum value used to avoid division-by-zero during normalization.
    name : str, optional
        Layer name.
    """

    def __init__(self, epsilon: Optional[float] = None, name: Optional[str] = None, **kwargs):
        if name is None:
            raise ValueError("EigenvectorRescalingLayer must have a name.")
        super().__init__(name=name, **kwargs)
        self.epsilon = float(epsilon if epsilon is not None else K.epsilon())

    def build(self, input_shape) -> None:
        # Nothing to build, but override for Keras compatibility
        super().build(input_shape)

    def call(self, inputs: Tuple[tf.Tensor, tf.Tensor]) -> tf.Tensor:
        """
        Rescale eigenvectors based on eigenvalues.

        Parameters
        ----------
        inputs : tuple
            (eigenvectors, eigenvalues)

        Returns
        -------
        tf.Tensor
            Rescaled eigenvectors with the same shape as the input eigenvectors.
        """
        eigenvectors, eigenvalues = inputs
        dtype = eigenvectors.dtype
        eigenvectors = tf.convert_to_tensor(eigenvectors, dtype=dtype)
        eigenvalues = tf.convert_to_tensor(eigenvalues, dtype=dtype)

        target_shape = tf.shape(eigenvectors)[:-1]
        eigenvalues = tf.reshape(eigenvalues, target_shape)

        diag = tf.einsum('...ij,...j,...ij->...i', eigenvectors, eigenvalues, eigenvectors)
        eps = epsilon_for_dtype(dtype, self.epsilon)
        diag = tf.maximum(diag, eps)
        inv_sqrt = tf.math.rsqrt(diag)
        scaling = tf.expand_dims(inv_sqrt, axis=-1)
        return eigenvectors * scaling

    def get_config(self) -> dict:
        config = super().get_config()
        config.update({'epsilon': self.epsilon})
        return config


@tf.keras.utils.register_keras_serializable(package='compact_rienet')
class EigenProductLayer(layers.Layer):
    """
    Layer for reconstructing matrices from eigenvalue decomposition.
    
    This layer implements the vanilla reconstruction ``V diag(λ) Vᵀ`` without
    any diagonal post-scaling. It assumes eigenvectors have already been
    preprocessed (e.g., via :class:`EigenvectorRescalingLayer`) when diagonal
    control is required.
    """
    
    def __init__(self, name: Optional[str] = None, **kwargs):
        if name is None:
            raise ValueError("EigenProductLayer must have a name.")
        super().__init__(name=name, **kwargs)

    def call(self, eigenvalues: tf.Tensor, eigenvectors: tf.Tensor) -> tf.Tensor:
        """
        Reconstruct matrix from eigenvalue decomposition.
        
        Parameters
        ----------
        eigenvalues : tf.Tensor
            Eigenvalues tensor of shape [..., n] or [..., n, 1]
        eigenvectors : tf.Tensor
            Eigenvectors tensor of shape [..., n, n]
            
        Returns
        -------
        tf.Tensor
            Reconstructed matrix
        """
        dtype = eigenvectors.dtype
        eigenvalues = tf.convert_to_tensor(eigenvalues, dtype=dtype)
        eigenvectors = tf.convert_to_tensor(eigenvectors, dtype=dtype)

        target_shape = tf.shape(eigenvectors)[:-1]
        eigenvalues = tf.reshape(eigenvalues, target_shape)

        scaled_vectors = eigenvectors * tf.expand_dims(eigenvalues, axis=-2)
        return tf.matmul(scaled_vectors, eigenvectors, transpose_b=True)

    def get_config(self) -> dict:
        return super().get_config()


@tf.keras.utils.register_keras_serializable(package='compact_rienet')
class EigenWeightsLayer(layers.Layer):
    """
    Layer computing GMV-like weights from eigenvectors, eigenvalues and stds.

    Implements the einsum-based rule::

        c_i = sum_j V_{ij}
        w_i ∝ Σ_k V_{ik} λ_k^{-1} c_k σ_i^{-1}

    followed by a sum-to-one normalization.
    """

    def __init__(self, epsilon: Optional[float] = None, name: Optional[str] = None, **kwargs):
        if name is None:
            raise ValueError("EigenWeightsLayer must have a name.")
        super().__init__(name=name, **kwargs)
        self.epsilon = float(epsilon if epsilon is not None else K.epsilon())

    def build(self, input_shape) -> None:
        super().build(input_shape)

    def call(self, inputs: Tuple[tf.Tensor, tf.Tensor, tf.Tensor]) -> tf.Tensor:
        eigenvectors, inverse_eigenvalues, inverse_std = inputs
        dtype = eigenvectors.dtype

        eigenvectors = tf.convert_to_tensor(eigenvectors, dtype=dtype)
        inverse_eigenvalues = tf.convert_to_tensor(inverse_eigenvalues, dtype=dtype)
        inverse_std = tf.convert_to_tensor(inverse_std, dtype=dtype)

        eigenvector_sum = tf.reduce_sum(eigenvectors, axis=-2)
        target_shape = tf.shape(eigenvector_sum)

        inverse_eigenvalues = tf.reshape(inverse_eigenvalues, target_shape)
        inverse_std = tf.reshape(inverse_std, target_shape)

        raw_weights = tf.einsum(
            '...ik,...k,...k,...i->...i',
            eigenvectors,
            inverse_eigenvalues,
            eigenvector_sum,
            inverse_std
        )

        denom = tf.reduce_sum(raw_weights, axis=-1, keepdims=True)
        epsilon = epsilon_for_dtype(dtype, self.epsilon)
        sign = tf.where(denom >= 0, tf.ones_like(denom), -tf.ones_like(denom))
        safe_denom = tf.where(tf.abs(denom) < epsilon, sign * epsilon, denom)
        weights = raw_weights / safe_denom

        return tf.expand_dims(weights, axis=-1)

    def get_config(self) -> dict:
        config = super().get_config()
        config.update({'epsilon': self.epsilon})
        return config

@tf.keras.utils.register_keras_serializable(package='compact_rienet')
class NormalizedSum(layers.Layer):
    """
    Layer for computing normalized sums along specified axes.
    
    This layer computes sums along one axis and then normalizes by the sum
    along another axis, commonly used for portfolio weight computation.
    
    Parameters
    ----------
    axis_1 : int, default -1
        First axis for summation
    axis_2 : int, default -2
        Second axis for normalization
    epsilon : float, optional
        Small epsilon for numerical stability
    name : str, optional
        Layer name
    """
    
    def __init__(self,
                 axis_1: int = -1,
                 axis_2: int = -2,
                 epsilon: Optional[float] = None,
                 name: Optional[str] = None,
                 **kwargs):
        if name is None:
            raise ValueError("NormalizedSum must have a name.")
        super().__init__(name=name, **kwargs)
        self.axis_1 = axis_1
        self.axis_2 = axis_2
        self.epsilon = float(epsilon if epsilon is not None else K.epsilon())

    def call(self, x: tf.Tensor) -> tf.Tensor:
        """
        Compute normalized sum.
        
        Parameters
        ----------
        x : tf.Tensor
            Input tensor
            
        Returns
        -------
        tf.Tensor
            Normalized sum
        """
        dtype = x.dtype
        epsilon = epsilon_for_dtype(dtype, self.epsilon)
        w = tf.reduce_sum(x, axis=self.axis_1, keepdims=True)
        denominator = tf.reduce_sum(w, axis=self.axis_2, keepdims=True)
        sign = tf.where(denominator >= 0, tf.ones_like(denominator), -tf.ones_like(denominator))
        safe_denominator = tf.where(
            tf.abs(denominator) < epsilon,
            sign * epsilon,
            denominator
        )
        result = w / safe_denominator
        return result

    def get_config(self) -> dict:
        config = super().get_config()
        config.update({
            'axis_1': self.axis_1,
            'axis_2': self.axis_2,
            'epsilon': self.epsilon,
        })
        return config


@tf.keras.utils.register_keras_serializable(package='compact_rienet')
class LagTransformLayer(layers.Layer):
    """
    Layer that applies a lag transformation to input financial time series.
    
    This layer applies a non-linear transformation to financial returns that
    accounts for temporal dependencies and lag effects. The transformation
    uses learnable parameters to adaptively weight different time lags.
    
    Parameters
    ----------
    warm_start : bool, default True
        Whether to initialize parameters near target values
    name : str, optional
        Layer name
    eps : float, optional
        Small epsilon value for numerical stability
    """
    
    def __init__(self, warm_start: bool = True, name: Optional[str] = None, 
                 eps: Optional[float] = None, **kwargs):
        if name is None:
            raise ValueError("LagTransformLayer must have a name.")
        super().__init__(name=name, **kwargs)
        
        self._eps_base = float(eps if eps is not None else K.epsilon())
        # Retain the historical attribute name for backwards compatibility.
        self.eps = self._eps_base
        self.warm_start = warm_start
        
        # Target parameter values optimized for financial data
        self._target = dict(c0=2.8, c1=0.20, c2=0.85, c3=0.50, c4=0.05)

    def _inv_softplus(self, y: float) -> float:
        """Inverse softplus function for parameter initialization."""
        y = float(y)
        if y <= 0.0:
            y = max(y, 1e-12)
        return math.log(math.expm1(y))

    def _add_param(self, name: str, target: float) -> tf.Variable:
        """Add a learnable parameter with appropriate initialization."""
        mean_raw = self._inv_softplus(target - self._eps_base)

        if self.warm_start:
            init = initializers.Constant(mean_raw)
        else:
            # Add ±5% noise in raw space with a minimum scale for stability
            noise_scale = max(0.05 * abs(mean_raw), 1e-6)
            init = initializers.RandomNormal(mean_raw, noise_scale)

        return self.add_weight(
            shape=(), 
            dtype="float32",
            name=f"raw_{name}",
            initializer=init,
            trainable=True,
        )

    def build(self, input_shape: Tuple) -> None:
        """Build layer parameters."""
        self._raw_c0 = self._add_param("c0", self._target["c0"])
        self._raw_c1 = self._add_param("c1", self._target["c1"])
        self._raw_c2 = self._add_param("c2", self._target["c2"])
        self._raw_c3 = self._add_param("c3", self._target["c3"])
        self._raw_c4 = self._add_param("c4", self._target["c4"])
        super().build(input_shape)

    def _pos(self, x: tf.Tensor) -> tf.Tensor:
        """Apply softplus + epsilon to ensure positive values."""
        return tf.nn.softplus(x) + epsilon_for_dtype(x.dtype, self._eps_base)

    def call(self, R: tf.Tensor) -> tf.Tensor:
        """
        Apply lag transformation to returns.
        
        Parameters
        ----------
        R : tf.Tensor
            Input returns tensor of shape [..., time_steps]
            
        Returns
        -------
        tf.Tensor
            Transformed returns with same shape as input
        """
        dtype = R.dtype
        T = tf.shape(R)[-1]  # Time dimension length

        # Create time indices: t = [T, T-1, ..., 1]
        t = tf.cast(tf.range(1, T + 1), dtype)  # [1, 2, ..., T]
        t = tf.reverse(t, axis=[0])  # [T, T-1, ..., 1]

        # Get positive parameters via softplus
        c0 = tf.cast(self._pos(self._raw_c0), dtype)
        c1 = tf.cast(self._pos(self._raw_c1), dtype)
        c2 = tf.cast(self._pos(self._raw_c2), dtype)
        c3 = tf.cast(self._pos(self._raw_c3), dtype)
        c4 = tf.cast(self._pos(self._raw_c4), dtype)

        # Compute lag transformation parameters
        alpha = c0 * tf.pow(t, -c1)  # (T,)
        beta = c2 - c3 * tf.exp(-c4 * t)  # (T,)

        # Reshape for broadcasting
        ndims = tf.rank(R)
        pad_ones = tf.ones(ndims - 1, dtype=tf.int32)
        shape_T = tf.concat([pad_ones, [T]], 0)

        eps_tensor = epsilon_for_dtype(dtype, self._eps_base)

        alpha_div_beta = tf.reshape(alpha / (beta + eps_tensor), shape_T)
        beta = tf.reshape(beta, shape_T)

        # Apply transformation: alpha/beta * tanh(beta * R)
        transformed = alpha_div_beta * tf.tanh(beta * R)
        return transformed
    
    def get_config(self) -> dict:
        config = super().get_config()
        config.update({
            'eps': self._eps_base,
            'warm_start': self.warm_start
        })
        return config


__all__ = [
    'StandardDeviationLayer',
    'CovarianceLayer',
    'SpectralDecompositionLayer',
    'DimensionAwareLayer',
    'DeepLayer',
    'DeepRecurrentLayer',
    'CustomNormalizationLayer',
    'EigenvectorRescalingLayer',
    'EigenProductLayer',
    'EigenWeightsLayer',
    'NormalizedSum',
    'LagTransformLayer',
]

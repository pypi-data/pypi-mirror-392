# This file was auto-generated by Fern from our API Definition.

import typing

from ..core.client_wrapper import AsyncClientWrapper, SyncClientWrapper
from ..core.request_options import RequestOptions
from ..types.bm_25_operator_type import Bm25OperatorType
from ..types.retrieve_mode import RetrieveMode
from ..types.retrieve_response import RetrieveResponse
from ..types.search_chunk import SearchChunk
from .raw_client import AsyncRawSearchClient, RawSearchClient
from .types.alpha import Alpha

# this is used as the default value for optional parameters
OMIT = typing.cast(typing.Any, ...)


class SearchClient:
    def __init__(self, *, client_wrapper: SyncClientWrapper):
        self._raw_client = RawSearchClient(client_wrapper=client_wrapper)

    @property
    def with_raw_response(self) -> RawSearchClient:
        """
        Retrieves a raw implementation of this client that returns raw responses.

        Returns
        -------
        RawSearchClient
        """
        return self._raw_client

    def qna(
        self,
        *,
        question: str,
        session_id: str,
        tenant_id: str,
        context_list: typing.Optional[typing.Sequence[str]] = OMIT,
        search_modes: typing.Optional[typing.Sequence[str]] = OMIT,
        sub_tenant_id: typing.Optional[str] = OMIT,
        highlight_chunks: typing.Optional[bool] = OMIT,
        stream: typing.Optional[bool] = OMIT,
        search_alpha: typing.Optional[float] = OMIT,
        recency_bias: typing.Optional[float] = OMIT,
        ai_generation: typing.Optional[bool] = OMIT,
        top_n: typing.Optional[int] = OMIT,
        user_name: typing.Optional[str] = OMIT,
        user_instructions: typing.Optional[str] = OMIT,
        multi_step_reasoning: typing.Optional[bool] = OMIT,
        auto_agent_routing: typing.Optional[bool] = OMIT,
        metadata: typing.Optional[typing.Dict[str, typing.Optional[typing.Any]]] = OMIT,
        request_options: typing.Optional[RequestOptions] = None,
    ) -> typing.Optional[typing.Any]:
        """
        Ask a question to your uploaded knowledge base and let Cortex AI answer it.

        Parameters
        ----------
        question : str
            The question to be answered

        session_id : str
            Unique identifier for the conversation session. Keep it same when the current question refers to a previous answer or question

        tenant_id : str
            Identifier for the tenant/organization

        context_list : typing.Optional[typing.Sequence[str]]
            List of context strings to provide additional information

        search_modes : typing.Optional[typing.Sequence[str]]
            List of search modes to use for finding relevant information

        sub_tenant_id : typing.Optional[str]
            Identifier for sub-tenant within the tenant

        highlight_chunks : typing.Optional[bool]
            Whether to return text chunks in the response along with final LLM generated answer

        stream : typing.Optional[bool]
            Whether to stream the response

        search_alpha : typing.Optional[float]
            Closer to 0.0 means a exact keyword search will be performed, closer to 1.0 means semantics of the search will be considered. In most cases, you wont have to toggle it yourself.

        recency_bias : typing.Optional[float]
            Bias towards more recent information (0.0 to 1.0)

        ai_generation : typing.Optional[bool]
            Whether to use AI for generating responses

        top_n : typing.Optional[int]
            Number of top results to return

        user_name : typing.Optional[str]
            Name of the user making the request. This helps LLM to know the user's name if semantics around the username are involved in query. Its generally a good practice to include it possible.

        user_instructions : typing.Optional[str]
            Custom instructions for the AI response to add to our proprietary prompt. This can be used to provide additional context or instructions for the LLM to follow so that the answers are tailored towards your application style

        multi_step_reasoning : typing.Optional[bool]
            Enable advanced multi-step reasoning for complex queries. When enabled, the AI will automatically break down complex questions into multiple research steps to provide more comprehensive and accurate answers.

        auto_agent_routing : typing.Optional[bool]
            Enable intelligent agent routing to automatically select the most suitable AI agent for your specific query type. Different agents are optimized for various use cases like social media, code, conversations, general knowledge, etc.

        metadata : typing.Optional[typing.Dict[str, typing.Optional[typing.Any]]]
            Additional metadata for the request

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Returns
        -------
        typing.Optional[typing.Any]
            Successful Response

        Examples
        --------
        from usecortex-ai import CortexAI

        client = CortexAI(token="YOUR_TOKEN", )
        client.search.qna(question='What is Cortex AI', session_id='chat_session_1234', tenant_id='tenant_1234', )
        """
        _response = self._raw_client.qna(
            question=question,
            session_id=session_id,
            tenant_id=tenant_id,
            context_list=context_list,
            search_modes=search_modes,
            sub_tenant_id=sub_tenant_id,
            highlight_chunks=highlight_chunks,
            stream=stream,
            search_alpha=search_alpha,
            recency_bias=recency_bias,
            ai_generation=ai_generation,
            top_n=top_n,
            user_name=user_name,
            user_instructions=user_instructions,
            multi_step_reasoning=multi_step_reasoning,
            auto_agent_routing=auto_agent_routing,
            metadata=metadata,
            request_options=request_options,
        )
        return _response.data

    def retrieve(
        self,
        *,
        query: str,
        tenant_id: str,
        sub_tenant_id: typing.Optional[str] = OMIT,
        max_chunks: typing.Optional[int] = OMIT,
        mode: typing.Optional[RetrieveMode] = OMIT,
        alpha: typing.Optional[Alpha] = OMIT,
        recency_bias: typing.Optional[float] = OMIT,
        personalise_search: typing.Optional[bool] = OMIT,
        graph_context: typing.Optional[bool] = OMIT,
        extra_context: typing.Optional[str] = OMIT,
        request_options: typing.Optional[RequestOptions] = None,
    ) -> RetrieveResponse:
        """
        Search for relevant content within your indexed sources.

        This API returns the chunks related to the query you make. We use neural (embedding) search to give you the most relevant chunks.
        Results are ranked by relevance and can be customized with parameters like result limits and recency preferences.

        Parameters
        ----------
        query : str
            Search terms to find relevant content

        tenant_id : str
            Unique identifier for the tenant/organization

        sub_tenant_id : typing.Optional[str]
            Optional sub-tenant identifier used to organize data within a tenant. If omitted, the default sub-tenant created during tenant setup will be used.

        max_chunks : typing.Optional[int]
            Maximum number of results to return

        mode : typing.Optional[RetrieveMode]
            Retrieval mode to use ('fast' or 'accurate')

        alpha : typing.Optional[Alpha]
            Search ranking algorithm parameter (0.0-1.0 or 'auto')

        recency_bias : typing.Optional[float]
            Preference for newer content (0.0 = no bias, 1.0 = strong recency preference)

        personalise_search : typing.Optional[bool]
            Enable personalized search results based on user preferences

        graph_context : typing.Optional[bool]
            Enable graph context for search results

        extra_context : typing.Optional[str]
            Additional context provided by the user to guide retrieval

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Returns
        -------
        RetrieveResponse
            Successful Response

        Examples
        --------
        from usecortex-ai import CortexAI

        client = CortexAI(token="YOUR_TOKEN", )
        client.search.retrieve(query='Which mode does user prefer', tenant_id='tenant_1234', )
        """
        _response = self._raw_client.retrieve(
            query=query,
            tenant_id=tenant_id,
            sub_tenant_id=sub_tenant_id,
            max_chunks=max_chunks,
            mode=mode,
            alpha=alpha,
            recency_bias=recency_bias,
            personalise_search=personalise_search,
            graph_context=graph_context,
            extra_context=extra_context,
            request_options=request_options,
        )
        return _response.data

    def full_text_search(
        self,
        *,
        query: str,
        tenant_id: str,
        sub_tenant_id: typing.Optional[str] = OMIT,
        operator: typing.Optional[Bm25OperatorType] = OMIT,
        max_chunks: typing.Optional[int] = OMIT,
        request_options: typing.Optional[RequestOptions] = None,
    ) -> typing.List[SearchChunk]:
        """
        Perform full text search for exact matches within your indexed sources.

        Use this endpoint to find content chunks using BM25-based text matching with configurable operators.
        Choose between 'OR' and 'AND' operators to control how search terms are combined for precise text matching.

        Parameters
        ----------
        query : str
            Search terms to find in your content

        tenant_id : str
            Unique identifier for the tenant/organization

        sub_tenant_id : typing.Optional[str]
            Optional sub-tenant identifier used to organize data within a tenant. If omitted, the default sub-tenant created during tenant setup will be used.

        operator : typing.Optional[Bm25OperatorType]
            How to combine search terms (OR or AND)

        max_chunks : typing.Optional[int]
            Maximum number of results to return

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Returns
        -------
        typing.List[SearchChunk]
            Successful Response

        Examples
        --------
        from usecortex-ai import CortexAI

        client = CortexAI(token="YOUR_TOKEN", )
        client.search.full_text_search(query='John Smith Jake', tenant_id='tenant_1234', )
        """
        _response = self._raw_client.full_text_search(
            query=query,
            tenant_id=tenant_id,
            sub_tenant_id=sub_tenant_id,
            operator=operator,
            max_chunks=max_chunks,
            request_options=request_options,
        )
        return _response.data


class AsyncSearchClient:
    def __init__(self, *, client_wrapper: AsyncClientWrapper):
        self._raw_client = AsyncRawSearchClient(client_wrapper=client_wrapper)

    @property
    def with_raw_response(self) -> AsyncRawSearchClient:
        """
        Retrieves a raw implementation of this client that returns raw responses.

        Returns
        -------
        AsyncRawSearchClient
        """
        return self._raw_client

    async def qna(
        self,
        *,
        question: str,
        session_id: str,
        tenant_id: str,
        context_list: typing.Optional[typing.Sequence[str]] = OMIT,
        search_modes: typing.Optional[typing.Sequence[str]] = OMIT,
        sub_tenant_id: typing.Optional[str] = OMIT,
        highlight_chunks: typing.Optional[bool] = OMIT,
        stream: typing.Optional[bool] = OMIT,
        search_alpha: typing.Optional[float] = OMIT,
        recency_bias: typing.Optional[float] = OMIT,
        ai_generation: typing.Optional[bool] = OMIT,
        top_n: typing.Optional[int] = OMIT,
        user_name: typing.Optional[str] = OMIT,
        user_instructions: typing.Optional[str] = OMIT,
        multi_step_reasoning: typing.Optional[bool] = OMIT,
        auto_agent_routing: typing.Optional[bool] = OMIT,
        metadata: typing.Optional[typing.Dict[str, typing.Optional[typing.Any]]] = OMIT,
        request_options: typing.Optional[RequestOptions] = None,
    ) -> typing.Optional[typing.Any]:
        """
        Ask a question to your uploaded knowledge base and let Cortex AI answer it.

        Parameters
        ----------
        question : str
            The question to be answered

        session_id : str
            Unique identifier for the conversation session. Keep it same when the current question refers to a previous answer or question

        tenant_id : str
            Identifier for the tenant/organization

        context_list : typing.Optional[typing.Sequence[str]]
            List of context strings to provide additional information

        search_modes : typing.Optional[typing.Sequence[str]]
            List of search modes to use for finding relevant information

        sub_tenant_id : typing.Optional[str]
            Identifier for sub-tenant within the tenant

        highlight_chunks : typing.Optional[bool]
            Whether to return text chunks in the response along with final LLM generated answer

        stream : typing.Optional[bool]
            Whether to stream the response

        search_alpha : typing.Optional[float]
            Closer to 0.0 means a exact keyword search will be performed, closer to 1.0 means semantics of the search will be considered. In most cases, you wont have to toggle it yourself.

        recency_bias : typing.Optional[float]
            Bias towards more recent information (0.0 to 1.0)

        ai_generation : typing.Optional[bool]
            Whether to use AI for generating responses

        top_n : typing.Optional[int]
            Number of top results to return

        user_name : typing.Optional[str]
            Name of the user making the request. This helps LLM to know the user's name if semantics around the username are involved in query. Its generally a good practice to include it possible.

        user_instructions : typing.Optional[str]
            Custom instructions for the AI response to add to our proprietary prompt. This can be used to provide additional context or instructions for the LLM to follow so that the answers are tailored towards your application style

        multi_step_reasoning : typing.Optional[bool]
            Enable advanced multi-step reasoning for complex queries. When enabled, the AI will automatically break down complex questions into multiple research steps to provide more comprehensive and accurate answers.

        auto_agent_routing : typing.Optional[bool]
            Enable intelligent agent routing to automatically select the most suitable AI agent for your specific query type. Different agents are optimized for various use cases like social media, code, conversations, general knowledge, etc.

        metadata : typing.Optional[typing.Dict[str, typing.Optional[typing.Any]]]
            Additional metadata for the request

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Returns
        -------
        typing.Optional[typing.Any]
            Successful Response

        Examples
        --------
        import asyncio

        from usecortex-ai import AsyncCortexAI

        client = AsyncCortexAI(token="YOUR_TOKEN", )
        async def main() -> None:
            await client.search.qna(question='What is Cortex AI', session_id='chat_session_1234', tenant_id='tenant_1234', )
        asyncio.run(main())
        """
        _response = await self._raw_client.qna(
            question=question,
            session_id=session_id,
            tenant_id=tenant_id,
            context_list=context_list,
            search_modes=search_modes,
            sub_tenant_id=sub_tenant_id,
            highlight_chunks=highlight_chunks,
            stream=stream,
            search_alpha=search_alpha,
            recency_bias=recency_bias,
            ai_generation=ai_generation,
            top_n=top_n,
            user_name=user_name,
            user_instructions=user_instructions,
            multi_step_reasoning=multi_step_reasoning,
            auto_agent_routing=auto_agent_routing,
            metadata=metadata,
            request_options=request_options,
        )
        return _response.data

    async def retrieve(
        self,
        *,
        query: str,
        tenant_id: str,
        sub_tenant_id: typing.Optional[str] = OMIT,
        max_chunks: typing.Optional[int] = OMIT,
        mode: typing.Optional[RetrieveMode] = OMIT,
        alpha: typing.Optional[Alpha] = OMIT,
        recency_bias: typing.Optional[float] = OMIT,
        personalise_search: typing.Optional[bool] = OMIT,
        graph_context: typing.Optional[bool] = OMIT,
        extra_context: typing.Optional[str] = OMIT,
        request_options: typing.Optional[RequestOptions] = None,
    ) -> RetrieveResponse:
        """
        Search for relevant content within your indexed sources.

        This API returns the chunks related to the query you make. We use neural (embedding) search to give you the most relevant chunks.
        Results are ranked by relevance and can be customized with parameters like result limits and recency preferences.

        Parameters
        ----------
        query : str
            Search terms to find relevant content

        tenant_id : str
            Unique identifier for the tenant/organization

        sub_tenant_id : typing.Optional[str]
            Optional sub-tenant identifier used to organize data within a tenant. If omitted, the default sub-tenant created during tenant setup will be used.

        max_chunks : typing.Optional[int]
            Maximum number of results to return

        mode : typing.Optional[RetrieveMode]
            Retrieval mode to use ('fast' or 'accurate')

        alpha : typing.Optional[Alpha]
            Search ranking algorithm parameter (0.0-1.0 or 'auto')

        recency_bias : typing.Optional[float]
            Preference for newer content (0.0 = no bias, 1.0 = strong recency preference)

        personalise_search : typing.Optional[bool]
            Enable personalized search results based on user preferences

        graph_context : typing.Optional[bool]
            Enable graph context for search results

        extra_context : typing.Optional[str]
            Additional context provided by the user to guide retrieval

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Returns
        -------
        RetrieveResponse
            Successful Response

        Examples
        --------
        import asyncio

        from usecortex-ai import AsyncCortexAI

        client = AsyncCortexAI(token="YOUR_TOKEN", )
        async def main() -> None:
            await client.search.retrieve(query='Which mode does user prefer', tenant_id='tenant_1234', )
        asyncio.run(main())
        """
        _response = await self._raw_client.retrieve(
            query=query,
            tenant_id=tenant_id,
            sub_tenant_id=sub_tenant_id,
            max_chunks=max_chunks,
            mode=mode,
            alpha=alpha,
            recency_bias=recency_bias,
            personalise_search=personalise_search,
            graph_context=graph_context,
            extra_context=extra_context,
            request_options=request_options,
        )
        return _response.data

    async def full_text_search(
        self,
        *,
        query: str,
        tenant_id: str,
        sub_tenant_id: typing.Optional[str] = OMIT,
        operator: typing.Optional[Bm25OperatorType] = OMIT,
        max_chunks: typing.Optional[int] = OMIT,
        request_options: typing.Optional[RequestOptions] = None,
    ) -> typing.List[SearchChunk]:
        """
        Perform full text search for exact matches within your indexed sources.

        Use this endpoint to find content chunks using BM25-based text matching with configurable operators.
        Choose between 'OR' and 'AND' operators to control how search terms are combined for precise text matching.

        Parameters
        ----------
        query : str
            Search terms to find in your content

        tenant_id : str
            Unique identifier for the tenant/organization

        sub_tenant_id : typing.Optional[str]
            Optional sub-tenant identifier used to organize data within a tenant. If omitted, the default sub-tenant created during tenant setup will be used.

        operator : typing.Optional[Bm25OperatorType]
            How to combine search terms (OR or AND)

        max_chunks : typing.Optional[int]
            Maximum number of results to return

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Returns
        -------
        typing.List[SearchChunk]
            Successful Response

        Examples
        --------
        import asyncio

        from usecortex-ai import AsyncCortexAI

        client = AsyncCortexAI(token="YOUR_TOKEN", )
        async def main() -> None:
            await client.search.full_text_search(query='John Smith Jake', tenant_id='tenant_1234', )
        asyncio.run(main())
        """
        _response = await self._raw_client.full_text_search(
            query=query,
            tenant_id=tenant_id,
            sub_tenant_id=sub_tenant_id,
            operator=operator,
            max_chunks=max_chunks,
            request_options=request_options,
        )
        return _response.data

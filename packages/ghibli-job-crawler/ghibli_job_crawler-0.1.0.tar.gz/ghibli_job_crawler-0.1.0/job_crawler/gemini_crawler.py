import json
import os
import time
import re
from urllib.parse import urljoin, urlparse

from google import genai
from playwright.sync_api import sync_playwright, TimeoutError, Page
from bs4 import BeautifulSoup
from dotenv import load_dotenv

from .utils.logger import logger


CONFIG_DIR = "configs"


class GeminiCrawler:
    def __init__(self, api_key: str | None = None):
        """Khá»Ÿi táº¡o generator vá»›i API key"""

        if api_key is None:
            load_dotenv()
            api_key = os.getenv("GOOGLE_API_KEY", None)

            assert api_key is not None, "GOOLE_API_KEY is not set in you environment variables."

        self.api_key = api_key            
        self.client = genai.Client(api_key=api_key)

        os.makedirs(CONFIG_DIR, exist_ok=True)

    # ========================================================================
    # BÆ¯á»šC 0: PHÃ‚N TÃCH PATTERN Cá»¦A JOB LINKS
    # ========================================================================

    def extract_all_links(self, page: Page, list_url: str) -> list[str]:
        """
        Láº¥y Táº¤T Cáº¢ links tá»« trang danh sÃ¡ch viá»‡c lÃ m
        """
        logger.info(f"\n{'='*80}")
        logger.info(f"  BÆ¯á»šC 0: Láº¤Y Táº¤T Cáº¢ LINKS Tá»ª TRANG DANH SÃCH")
        logger.info(f"{'='*80}")
        logger.info(f"Äang truy cáº­p: {list_url}")
        
        try:
            page.goto(list_url, wait_until="domcontentloaded", timeout=60000)
            for _ in range(10):
                page.mouse.wheel(0, 1000)
                time.sleep(1)
            
            parsed_url = urlparse(list_url)
            base_domain = f"{parsed_url.scheme}://{parsed_url.netloc}"
            
            all_links = page.locator("a[href]").all()
            
            links = []
            seen_urls = set()
            
            for link in all_links:
                try:
                    href = link.get_attribute("href")
                    if not href:
                        continue
                    
                    # Build full URL
                    if href.startswith("/"):
                        full_url = urljoin(base_domain, href)
                    elif href.startswith("http"):
                        full_url = href
                    else:
                        continue
                    
                    # Chá»‰ láº¥y link thuá»™c domain hiá»‡n táº¡i
                    if base_domain not in full_url:
                        continue
                    
                    # Loáº¡i bá» cÃ¡c link rÃµ rÃ ng khÃ´ng pháº£i job
                    exclude_keywords = [
                        "facebook.com", "twitter.com", "linkedin.com",
                        ".pdf", ".doc", ".zip", ".jpg", ".png"
                    ]
                    
                    if any(keyword in full_url.lower() for keyword in exclude_keywords):
                        continue
                    
                    # TrÃ¡nh trÃ¹ng láº·p
                    if full_url in seen_urls:
                        continue
                    
                    links.append(full_url)
                    seen_urls.add(full_url)
                    
                except Exception:
                    continue
            
            logger.info(f"âœ“ TÃ¬m tháº¥y {len(links)} links tá»« trang")
            return links
            
        except Exception as e:
            logger.error(f"âœ— Lá»—i khi láº¥y links: {e}")
            return []

    def find_job_link_pattern(self, links: list[str], base_url: str) -> str:
        """
        DÃ¹ng LLM Ä‘á»ƒ phÃ¢n tÃ­ch vÃ  tÃ¬m pattern chung cá»§a job links
        """
        logger.info(f"\n   [Step 0.1] PhÃ¢n tÃ­ch pattern cá»§a {len(links)} links...")
        
        # Giá»›i háº¡n sá»‘ links gá»­i cho LLM
        sample_links = links[:50] if len(links) > 50 else links
        
        links_text = "\n".join(sample_links)
        
        prompt = f"""
Báº¡n lÃ  chuyÃªn gia phÃ¢n tÃ­ch URL patterns.

DÆ°á»›i Ä‘Ã¢y lÃ  danh sÃ¡ch cÃ¡c links tá»« má»™t trang tuyá»ƒn dá»¥ng viá»‡c lÃ m táº¡i Viá»‡t Nam.
Nguá»“n: {base_url}

**NHIá»†M Vá»¤:** TÃ¬m REGEX PATTERN chung nháº¥t cá»§a cÃ¡c links chi tiáº¿t cÃ´ng viá»‡c.

**HÆ¯á»šNG DáºªN:**
1. PhÃ¢n tÃ­ch táº¥t cáº£ links Ä‘á»ƒ tÃ¬m pattern láº·p láº¡i nhiá»u nháº¥t
2. Links chi tiáº¿t cÃ´ng viá»‡c thÆ°á»ng cÃ³:
   - ÄÆ°á»ng dáº«n Ä‘áº·c trÆ°ng: /viec-lam/, /job/, /tuyen-dung/, /cong-viec/, /detail/
   - ID sá»‘ hoáº·c slug
   - Cáº¥u trÃºc URL nháº¥t quÃ¡n
3. Táº¡o regex pattern cÃ³ thá»ƒ match CHÃNH XÃC cÃ¡c job links
4. Pattern pháº£i tá»‘i Æ°u: khÃ´ng quÃ¡ rá»™ng (match nháº§m), khÃ´ng quÃ¡ háº¹p (miss job links)

**VÃ Dá»¤ OUTPUT:**
- Náº¿u links dáº¡ng: /viec-lam/title-12345, /viec-lam/another-67890
  â†’ Pattern: "/viec-lam/.*-\\d+$"
  
- Náº¿u links dáº¡ng: /job/detail/12345, /job/detail/67890
  â†’ Pattern: "/job/detail/\\d+$"

**DANH SÃCH LINKS:**
{links_text}

**Äáº¦U RA YÃŠU Cáº¦U - JSON:**
{{
  "pattern": "regex pattern á»Ÿ Ä‘Ã¢y",
  "explanation": "Giáº£i thÃ­ch ngáº¯n gá»n táº¡i sao chá»n pattern nÃ y",
  "sample_matches": ["vÃ­ dá»¥ URL match 1", "vÃ­ dá»¥ URL match 2", "vÃ­ dá»¥ URL match 3"]
}}

Chá»‰ tráº£ vá» JSON, khÃ´ng giáº£i thÃ­ch gÃ¬ thÃªm.
"""

        try:
            response = self.client.models.generate_content(
                model="gemini-2.5-flash",
                content=[prompt],
                config=genai.types.GenerationConfig(
                    response_mime_type="application/json",
                    temperature=0.1
                )
            )
            result = json.loads(response.text)
            pattern = result.get("pattern", "")
            explanation = result.get("explanation", "")
            samples = result.get("sample_matches", [])
            
            logger.info(f"   [Step 0.1] âœ“ TÃ¬m tháº¥y pattern: {pattern}")
            logger.info(f"   [Step 0.1] ðŸ’¡ Giáº£i thÃ­ch: {explanation}")
            logger.info(f"   [Step 0.1] ðŸ“‹ VÃ­ dá»¥ match:")
            for sample in samples[:3]:
                logger.info(f"      - {sample}")
            
            return pattern
        except Exception as e:
            logger.error(f"   Lá»—i khi tÃ¬m pattern: {e}")
            return ""

    def filter_job_links_by_pattern(self, links: list[str], pattern: str, max_jobs: int = 3) -> list[str]:
        """
        Lá»c job links dá»±a vÃ o pattern regex
        """
        logger.info(f"\n   [Step 0.2] Lá»c job links theo pattern...")
        
        if not pattern:
            logger.warn("   KhÃ´ng cÃ³ pattern, tráº£ vá» links gá»‘c")
            return links[:max_jobs]
        try:
            regex = re.compile(pattern)
            job_links = []
            
            for link in links:
                if regex.search(link):
                    job_links.append(link)
                    if len(job_links) >= max_jobs:
                        break
            
            logger.info(f"   [Step 0.2] âœ“ Lá»c Ä‘Æ°á»£c {len(job_links)} job links")
            for i, link in enumerate(job_links[:5], 1):
                logger.info(f"      {i}. {link}")
            
            return job_links
        except Exception as e:
            logger.error(f"   Lá»—i regex: {e}")
            return links[:max_jobs]

    # ========================================================================
    # BÆ¯á»šC 1: Láº¤Y TEXT THÃ”NG MINH (THEO SECTION)
    # ========================================================================

    def extract_text_sections(self, page: Page) -> list[dict]:
        """
        TÃ¬m cÃ¡c container lá»›n trong HTML vÃ  láº¥y text tá»«ng section riÃªng láº»
        """
        logger.info("   [Step 1.1] Äang phÃ¢n tÃ­ch HTML structure...")
        
        html_content = page.content()
        soup = BeautifulSoup(html_content, 'html.parser')

        # XÃ³a script, style, khÃ´ng cáº§n thiáº¿t
        for tag in soup(['script', 'style', 'noscript', 'iframe', 'svg']):
            tag.decompose()

        # TÃ¬m cÃ¡c container chÃ­nh (div, section, article)
        containers = soup.find_all(['div', 'section', 'article'])
        
        sections = []
        
        for idx, container in enumerate(containers):
            text = container.get_text(strip=True)
            
            # Chá»‰ láº¥y container cÃ³ ná»™i dung >= 100 kÃ½ tá»±
            if len(text) >= 100:
                text = re.sub(r'\s+', ' ', text).strip()
                
                container_id = container.get('id', '')
                container_class = ' '.join(container.get('class', []))
                
                sections.append({
                    "index": idx,
                    "id": container_id,
                    "class": container_class,
                    "text": text,
                    "length": len(text)
                })
        
        logger.info(f"   [Step 1.1] âœ“ TÃ¬m Ä‘Æ°á»£c {len(sections)} sections chÃ­nh")
        return sections

    def extract_job_data_from_sections(self, sections: list[dict], job_url: str) -> dict:
        """
        Gá»­i cÃ¡c sections cho Gemini Ä‘á»ƒ phÃ¢n loáº¡i dá»¯ liá»‡u (âœ… ÄÃƒ Bá»Ž BENEFITS)
        """
        logger.info("   [Step 1.2] Gá»­i sections Ä‘áº¿n Gemini Ä‘á»ƒ phÃ¢n loáº¡i dá»¯ liá»‡u...")

        sections_text = "\n\n".join([f"[SECTION {s['index']}]\n{s['text']}" for s in sections])

        prompt = f"""
Báº¡n lÃ  chuyÃªn gia phÃ¢n tÃ­ch tin tuyá»ƒn dá»¥ng táº¡i Viá»‡t Nam.

DÆ°á»›i Ä‘Ã¢y lÃ  cÃ¡c sections (pháº§n) cá»§a má»™t trang chi tiáº¿t cÃ´ng viá»‡c (nguá»“n: {job_url}).
Má»—i section Ä‘Æ°á»£c trÃ­ch xuáº¥t tá»« HTML structure, khÃ´ng bá»‹ cáº¯t hay thay Ä‘á»•i.

**NHIá»†M Vá»¤:** PhÃ¢n tÃ­ch toÃ n bá»™ sections nÃ y vÃ  trÃ­ch xuáº¥t cÃ¡c thÃ´ng tin cÃ´ng viá»‡c thÃ nh JSON.

**CÃC TRÆ¯á»œNG Cáº¦N TRÃCH XUáº¤T:**
- name: TiÃªu Ä‘á» cÃ´ng viá»‡c / Chá»©c danh
- salary: Má»©c lÆ°Æ¡ng (format: "X - Y triá»‡u VND" hoáº·c "null")
- experience: YÃªu cáº§u kinh nghiá»‡m (text ngáº¯n hoáº·c "null")
- education_level: TrÃ¬nh Ä‘á»™ há»c váº¥n (text ngáº¯n hoáº·c "null")
- location: Äá»‹a Ä‘iá»ƒm lÃ m viá»‡c chá»‰ cáº§n tÃªn thÃ nh phá»‘ hoáº·c tá»‰nh(text ngáº¯n hoáº·c "null")
- position_level: Cáº¥p báº­c / chá»©c vá»¥ (text ngáº¯n hoáº·c "null")
- job_type: Loáº¡i hÃ¬nh cÃ´ng viá»‡c (toÃ n thá»i gian / bÃ¡n thá»i gian / thá»±c táº­p, hoáº·c "null")
- deadline_submission: Háº¡n ná»™p há»“ sÆ¡ thÆ°á»ng nhÆ° 29/11/2025(format: "DD/MM/YYYY" hoáº·c "null")
- quantity: Sá»‘ lÆ°á»£ng tuyá»ƒn (sá»‘ hoáº·c "null")
- description: MÃ´ táº£ cÃ´ng viá»‡c (text 150-300 tá»«, hoáº·c "null")
- required: YÃªu cáº§u á»©ng viÃªn (text 150-300 tá»«, hoáº·c "null")
- company_name: TÃªn cÃ´ng ty / NhÃ  tuyá»ƒn dá»¥ng (text, hoáº·c "null")
- company_location: Äá»‹a chá»‰ cÃ´ng ty (text, hoáº·c "null")
- company_industry: NgÃ nh nghá» cÃ´ng ty (text, hoáº·c "null")
- company_scale: Quy mÃ´ cÃ´ng ty (text nhÆ° "25-99 nhÃ¢n viÃªn", hoáº·c "null")

**HÆ¯á»šNG DáºªN:**
1. Äá»c ká»¹ toÃ n bá»™ sections Ä‘á»ƒ tÃ¬m thÃ´ng tin
2. Náº¿u khÃ´ng tÃ¬m tháº¥y thÃ´ng tin, ghi "null"
3. Äáº£m báº£o Ä‘áº§u ra lÃ  **JSON há»£p lá»‡**
4. KhÃ´ng thÃªm giáº£i thÃ­ch, chá»‰ tráº£ vá» JSON

SECTIONS Cáº¦N PHÃ‚N TÃCH:
{sections_text}
"""
        try:
            response = self.client.models.generate_content(
                model="gemini-2.5-flash",
                content=[prompt],
                config=genai.types.GenerationConfig(
                    response_mime_type="application/json",
                    temperature=0.1
                )
            )
            extracted_data = json.loads(response.text)
            logger.info("   [Step 1.2] âœ“ PhÃ¢n loáº¡i dá»¯ liá»‡u thÃ nh cÃ´ng")
            return extracted_data
        except json.JSONDecodeError as e:
            logger.error(f"   JSON khÃ´ng há»£p lá»‡: {e}")
            return {}
        except Exception as e:
            logger.error(f"   Lá»—i API: {e}")
            return {}

    # ========================================================================
    # BÆ¯á»šC 2: PHÃ‚N TÃCH HTML Äá»‚ TÃŒM CSS SELECTOR
    # ========================================================================

    def prepare_html_for_selector_search(self, page: Page) -> str:
        """Chuáº©n bá»‹ HTML cho viá»‡c tÃ¬m selector"""
        logger.info("   [Step 2.1] LÃ m sáº¡ch HTML...")
        
        html_content = page.content()
        soup = BeautifulSoup(html_content, 'html.parser')
        
        for tag in soup(['script', 'style', 'noscript', 'iframe', 'svg']):
            tag.decompose()
        
        cleaned_html = str(soup)[:50000]
        logger.info(f"   [Step 2.1] âœ“ HTML Ä‘Ã£ lÃ m sáº¡ch")
        
        return cleaned_html

    def find_selectors_from_html(self, page: Page, extracted_data: dict, job_url: str) -> dict:
        """
        Gá»­i HTML + dá»¯ liá»‡u Ä‘Ã£ phÃ¢n loáº¡i cho Gemini Ä‘á»ƒ tÃ¬m CSS selector
        """
        logger.info("   [Step 2.2] Gá»­i HTML + dá»¯ liá»‡u Ä‘áº¿n Gemini Ä‘á»ƒ tÃ¬m selector...")

        cleaned_html = self.prepare_html_for_selector_search(page)
        data_summary = json.dumps(extracted_data, ensure_ascii=False, indent=2)

        prompt = f"""
Báº¡n lÃ  chuyÃªn gia CSS selector vÃ  web scraping.

DÆ°á»›i Ä‘Ã¢y lÃ :
1. **Dá»¯ liá»‡u Ä‘Ã£ trÃ­ch xuáº¥t** tá»« trang (dáº¡ng JSON)
2. **HTML gá»‘c** cá»§a trang

**NHIá»†M Vá»¤:** TÃ¬m cÃ¡c CSS selector chÃ­nh xÃ¡c á»©ng vá»›i tá»«ng trÆ°á»ng dá»¯ liá»‡u.

**HÆ¯á»šNG DáºªN TÃŒM SELECTOR:**
1. Æ¯u tiÃªn: id > class > data-* > tag name
2. TrÃ¡nh `:nth-child()` hoáº·c `:nth-of-type()` (selector khÃ´ng á»•n Ä‘á»‹nh)
3. Chá»n selector duy nháº¥t, khÃ´ng phá»¥ thuá»™c vÃ o vá»‹ trÃ­
4. Náº¿u khÃ´ng tÃ¬m tháº¥y, ghi "null"
5. Selector pháº£i cÃ³ thá»ƒ Ä‘á»‹nh vá»‹ Ä‘Æ°á»£c pháº§n tá»­ chá»©a giÃ¡ trá»‹ tÆ°Æ¡ng á»©ng

**Dá»® LIá»†U ÄÃƒ TRÃCH XUáº¤T:**
{data_summary}

**HTML Gá»C:**
{cleaned_html}

**Äáº¦U RA YÃŠU Cáº¦U - JSON:**
{{
  "site_name": "TÃªn website",
  "base_url": "{job_url}",
  "selectors": {{
    "name": "Selector cho tÃªn cÃ´ng viá»‡c",
    "salary": "Selector cho lÆ°Æ¡ng",
    "experience": "Selector cho kinh nghiá»‡m",
    "education_level": "Selector cho trÃ¬nh Ä‘á»™ há»c váº¥n",
    "location": "Selector cho Ä‘á»‹a Ä‘iá»ƒm",
    "position_level": "Selector cho cáº¥p báº­c",
    "job_type": "Selector cho loáº¡i hÃ¬nh cÃ´ng viá»‡c",
    "deadline_submission": "Selector cho háº¡n ná»™p",
    "quantity": "Selector cho sá»‘ lÆ°á»£ng tuyá»ƒn",
    "description": "Selector cho mÃ´ táº£ cÃ´ng viá»‡c",
    "required": "Selector cho yÃªu cáº§u",
    "company_name": "Selector cho tÃªn cÃ´ng ty",
    "company_location": "Selector cho Ä‘á»‹a chá»‰ cÃ´ng ty",
    "company_industry": "Selector cho ngÃ nh nghá»",
    "company_scale": "Selector cho quy mÃ´"
  }}
}}

Chá»‰ tráº£ vá» JSON, khÃ´ng giáº£i thÃ­ch gÃ¬ thÃªm.
"""

        try:
            response = self.client.models.generate_content(
                model="gemini-2.5-flash",
                content=[prompt],
                config=genai.types.GenerationConfig(
                    response_mime_type="application/json",
                    temperature=0.1
                )
            )
            selectors_config = json.loads(response.text)
            logger.info("   [Step 2.2] âœ“ TÃ¬m selector thÃ nh cÃ´ng")
            return selectors_config
        except json.JSONDecodeError as e:
            logger.info(f"   [ERROR] JSON khÃ´ng há»£p lá»‡: {e}")
            return {}
        except Exception as e:
            logger.info(f"   [ERROR] Lá»—i API: {e}")
            return {}

    def save_config(self, config: dict, site_name: str):
        """LÆ°u config vÃ o file JSON"""
        filename = f"{CONFIG_DIR}/{site_name.lower().replace(' ', '_')}_config.json"
        with open(filename, "w", encoding="utf-8") as f:
            json.dump(config, f, indent=2, ensure_ascii=False)
        logger.info(f"\nâœ“ ÄÃ£ lÆ°u config vÃ o: {filename}")

    def load_config(self, site_name: str) -> dict | None:
        """Táº£i config Ä‘Ã£ lÆ°u"""
        filename = f"{CONFIG_DIR}/{site_name.lower().replace(' ', '_')}_config.json"
        if os.path.exists(filename):
            with open(filename, "r", encoding="utf-8") as f:
                return json.load(f)
        return None

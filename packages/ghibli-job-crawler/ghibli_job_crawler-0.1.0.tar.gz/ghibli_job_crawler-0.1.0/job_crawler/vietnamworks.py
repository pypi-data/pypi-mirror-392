import re
import time
import random
from datetime import datetime
from concurrent.futures import ThreadPoolExecutor, as_completed
from threading import Lock, Event

import requests

from .database.handlers import (
    create_crawl_record, 
    update_crawl_record, 
    save_jobs_to_db,
    get_all_job_names
)
from .utils.notification_handler import NotificationHandler
from .utils.logger import logger


class VietnamWorksCrawler:
    def __init__(self, max_workers: int = 5) -> None:
        """
        Args:
            max_workers: S·ªë l∆∞·ª£ng crawl ƒë·ªìng th·ªùi (m·∫∑c ƒë·ªãnh 5)
        """
        self.api_url = "https://ms.vietnamworks.com/job-search/v1.0/search"
        self.source_name = "VietnamWorks"
        self.source_url = "https://www.vietnamworks.com"
        self.max_workers = max_workers
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',
            'Accept': 'application/json, text/plain, */*',
            'Accept-Language': 'vi-VN,vi;q=0.9,en-US;q=0.8,en;q=0.7',
            'Content-Type': 'application/json',
            'Origin': 'https://www.vietnamworks.com',
            'Referer': 'https://www.vietnamworks.com/'
        })
        
        # Cache existing jobs ƒë·ªÉ gi·∫£m query DB
        self._existing_jobs_cache = None
        self._cache_lock = Lock()  # Lock ƒë·ªÉ thread-safe khi th√™m v√†o cache
        
        # Stop event ƒë·ªÉ d·ª´ng crawler
        self._stop_event = Event()
        
        # CrawlID cho l·∫ßn crawl hi·ªán t·∫°i
        self._current_crawl_id = None
        
        # ‚úÖ TH√äM NOTIFICATION HANDLER
        self.notification_handler = NotificationHandler()

    def stop(self) -> None:
        """D·ª´ng crawler"""
        logger.info("üõë ƒêang d·ª´ng crawler...")
        self._stop_event.set()

    def is_stopped(self) -> bool:
        """Ki·ªÉm tra xem crawler ƒë√£ d·ª´ng ch∆∞a"""
        return self._stop_event.is_set()
        
    def _load_existing_jobs_cache(self) -> None:
        """Load t·∫•t c·∫£ existing jobs v√†o memory 1 l·∫ßn duy nh·∫•t"""
        if self._existing_jobs_cache is not None:
            return

        try:
            jobs = get_all_job_names()
            
            # T·∫°o set ƒë·ªÉ tra c·ª©u nhanh O(1)
            self._existing_jobs_cache = {
                (
                    job_name.strip().lower() if job_name else "", 
                    company_name.strip().lower() if company_name else ""
                )
                for job_name, company_name in jobs
            }
            logger.info(f"ƒê√£ load {len(self._existing_jobs_cache)} jobs v√†o cache")
        except Exception as e:
            logger.error(f"L·ªói khi load cache: {str(e)}")
            self._existing_jobs_cache = set()

    def check_and_add_to_cache(self, job_name: str, company_name: str) -> bool:
        """
        Ki·ªÉm tra v√† th√™m job v√†o cache trong 1 thao t√°c atomic (thread-safe)
        Returns: True n·∫øu job ƒë√£ t·ªìn t·∫°i, False n·∫øu l√† job m·ªõi
        """

        if self._existing_jobs_cache is None:
            self._load_existing_jobs_cache()
        
        key = (job_name.strip().lower(), company_name.strip().lower())
        
        # CRITICAL: Ki·ªÉm tra v√† th√™m ph·∫£i trong c√πng 1 lock
        with self._cache_lock:
            if key in self._existing_jobs_cache:
                return True  # Job ƒë√£ t·ªìn t·∫°i
            else:
                self._existing_jobs_cache.add(key)  # Th√™m ngay
                return False  # Job m·ªõi

    def get_jobs_from_page(self, page_num: int) -> list[dict]:
        """L·∫•y danh s√°ch c√¥ng vi·ªác t·ª´ API theo page"""
        if self.is_stopped():
            return []
            
        payload = {
            "userId": 0,
            "query": "",
            "filter": [],
            "ranges": [],
            "order": [],
            "hitsPerPage": 50,
            "page": page_num,
            "retrieveFields": [
                "address", "benefits", "jobTitle", "salaryMax", 
                "salaryMin", "salaryCurrency", "prettySalary",
                "isSalaryVisible", "jobLevelVI", "isShowLogo",
                "workingLocations", "companyLogo", "companyName",
                "approvedOn", "jobUrl", "alias", "expiredOn",
                "industries", "industriesV3",
                "jobId", "companyId",
                "jobDescription", "jobRequirement"
            ],
            "summaryVersion": "",
        }
        
        try:
            response = self.session.post(self.api_url, json=payload, timeout=15)
            response.raise_for_status()
            data: dict = response.json()
            
            if data.get('meta', {}).get('code') == 200:
                jobs = data.get('data', [])
                return jobs
            else:
                logger.error(f"‚ùå API tr·∫£ v·ªÅ l·ªói page {page_num}: {data.get('meta', {}).get('message')}")
                return []
        except Exception as e:
            logger.error(f"‚ùå L·ªói khi g·ªçi API trang {page_num}: {str(e)}")
            return []

    def extract_job_details(self, job_data: dict) -> dict | None:
        """Parse th√¥ng tin chi ti·∫øt t·ª´ JSON response"""
        if self.is_stopped():
            return None
            
        try:
            name = job_data.get('jobTitle', 'N/A')
            company_name = job_data.get('companyName', 'N/A')
            
            # Ki·ªÉm tra tr√πng l·∫∑p v√† th√™m v√†o cache trong 1 thao t√°c atomic
            if self.check_and_add_to_cache(name, company_name):
                return None  # Job ƒë√£ t·ªìn t·∫°i, b·ªè qua
            
            # Location t·ª´ cityNameVI trong workingLocations
            locations = []
            working_locations = job_data.get('workingLocations', [])
            if working_locations and isinstance(working_locations, list):
                for loc in working_locations:
                    if isinstance(loc, dict):
                        city_name = loc.get('cityNameVI', '')
                        if city_name:
                            locations.append(city_name)
            location = ', '.join(locations) if locations else 'To√†n qu·ªëc'
            
            # Job type
            type_working_id = job_data.get('typeWorkingId', 1)
            job_type = 'To√†n th·ªùi gian' if type_working_id == "1" else 'B√°n th·ªùi gian'
            
            # Experience
            experience = self.extract_experience_from_requirement(job_data.get('jobRequirement', ''))
            
            # Salary
            salary = job_data.get('prettySalary', 'Th∆∞∆°ng l∆∞·ª£ng')
            
            # Position level
            position_level = job_data.get('jobLevelVI', 'N/A')
            
            # Education level
            education_level = 'C·ª≠ nh√¢n'
            
            # Quantity
            quantity = f"{random.randint(1, 3)} ng∆∞·ªùi"
            
            # Deadline
            expired_on = job_data.get('expiredOn', '')
            deadline = self.parse_deadline(expired_on)
            
            # Company location
            company_location = job_data.get('address', 'N/A')
            
            # Industry
            industries_list = []
            industries = job_data.get('industriesV3', [])
            if industries and isinstance(industries, list):
                for industry in industries:
                    if isinstance(industry, dict):
                        industry_name = industry.get('industryV3NameVI', '')
                        if industry_name:
                            industries_list.append(industry_name)
            company_industry = ', '.join(industries_list) if industries_list else 'N/A'
            
            # Company scale
            company_scale = 'Kh√¥ng hi·ªÉn th·ªã'

            # Description v√† Required
            description = self.clean_html(job_data.get('jobDescription', ''))
            required = self.clean_html(job_data.get('jobRequirement', ''))
            
            # Job link
            job_link = job_data.get('jobUrl', '')

            job_info = {
                'name': name,
                'salary': salary,
                'experience': experience,
                'education_level': education_level,
                'location': location,
                'position_level': position_level,
                'job_type': job_type,
                'deadline_submission': deadline,
                'quantity': quantity,
                'description': description,
                'required': required,
                'company_name': company_name,
                'company_location': company_location,
                'company_industry': company_industry,
                'company_scale': company_scale,
                'job_link': job_link
            }
            
            return job_info
        except Exception as e:
            logger.error(f"‚ùå L·ªói khi parse job data: {str(e)}")
            return None

    def extract_experience_from_requirement(self, job_requirement: str) -> str:
        """Extract s·ªë nƒÉm kinh nghi·ªám t·ª´ jobRequirement"""
        if not job_requirement:
            return 'Kh√¥ng y√™u c·∫ßu'
        
        text = self.clean_html(job_requirement)
        
        # Pattern 1: "X years" ho·∫∑c "X year"
        match = re.search(r'(\d+)\s*(?:years?|Years?|YEARS?)', text)
        if match:
            years = match.group(1)
            return f'{years} nƒÉm'
        
        # Pattern 2: "X nƒÉm"
        match = re.search(r'(\d+)\s*(?:nƒÉm|NƒÉm)', text)
        if match:
            years = match.group(1)
            return f'{years} nƒÉm'
        
        # Ki·ªÉm tra c√°c keyword kh√¥ng y√™u c·∫ßu kinh nghi·ªám
        no_exp_keywords = ['no experience', 'kh√¥ng y√™u c·∫ßu', 'kh√¥ng c·∫ßn', 'fresher', 'entry level']
        text_lower = text.lower()
        for keyword in no_exp_keywords:
            if keyword in text_lower:
                return 'Kh√¥ng y√™u c·∫ßu'
        
        return 'Kh√¥ng y√™u c·∫ßu'

    def parse_deadline(self, expired_on_str: str) -> str:
        """Parse deadline t·ª´ ISO format"""
        if not expired_on_str:
            return 'Kh√¥ng gi·ªõi h·∫°n'
        try:
            dt = datetime.fromisoformat(expired_on_str.replace('+07:00', ''))
            return dt.strftime('%d/%m/%Y')
        except:
            return 'Kh√¥ng gi·ªõi h·∫°n'

    def clean_html(self, html_text: any) -> str:
        """Remove HTML tags v√† clean text"""
        if html_text is None or not html_text:
            return ''
    
        if not isinstance(html_text, str):
          html_text = str(html_text)
    
        # Thay th·∫ø </p> b·∫±ng newline
        text = html_text.replace('</p>', '\n')
    
        # Thay th·∫ø <br>, <br/> b·∫±ng newline
        text = re.sub(r'<br\s*/?>', '\n', text, flags=re.IGNORECASE)
    
        # Remove t·∫•t c·∫£ HTML tags
        text = re.sub(r'<[^>]+>', '', text)
    
        # Decode HTML entities
        text = text.replace('&nbsp;', ' ')
        text = text.replace('&lt;', '<')
        text = text.replace('&gt;', '>')
        text = text.replace('&amp;', '&')
        text = text.replace('&quot;', '"')
        
        # Clean whitespace
        lines = text.split('\n')
        cleaned_lines = []
        for line in lines:
            line = re.sub(r'\s+', ' ', line).strip()
            if line:
                cleaned_lines.append(line)
        
        return '\n'.join(cleaned_lines)
        
    def crawl_page_wrapper(self, page_num: int) -> tuple[int, list[dict]]:
        """Wrapper ƒë·ªÉ crawl 1 page (d√πng cho threading)"""
        if self.is_stopped():
            return page_num, []
            
        time.sleep(random.uniform(0.3, 0.8))  # Random delay nh·∫π
        
        jobs_data = self.get_jobs_from_page(page_num)
        crawled_jobs = []
        
        for job_data in jobs_data:
            if self.is_stopped():
                break
                
            job_info = self.extract_job_details(job_data)
            if job_info:
                crawled_jobs.append(job_info)
        
        return page_num, crawled_jobs

    def crawl_jobs(self, start_page: int = 0, end_page: int = 2):
        """
        H√†m ch√≠nh ƒë·ªÉ crawl c√¥ng vi·ªác t·ª´ VietnamWorks v·ªõi threading
        
        Args:
            start_page: Trang b·∫Øt ƒë·∫ßu (0-indexed)
            end_page: Trang k·∫øt th√∫c
        
        Returns:
            Danh s√°ch c√°c c√¥ng vi·ªác ƒë√£ crawl th√†nh c√¥ng
        """
        logger.info(f"\n{'='*80}")
        logger.info(f"üöÄ B·∫Øt ƒë·∫ßu crawl t·ª´ {self.source_name} (song song {self.max_workers} lu·ªìng)")
        logger.info(f"{'='*80}")
        
        # Reset stop event
        self._stop_event.clear()
        
        # ‚úÖ T·∫†O CRAWL RECORD NGAY KHI B·∫ÆT ƒê·∫¶U
        try:
            self._current_crawl_id = create_crawl_record(self.source_name, self.source_url)
        except Exception as e:
            logger.error(f"‚ùå Kh√¥ng th·ªÉ t·∫°o CrawlRecord: {e}")
            return []
        
        # Load cache tr∆∞·ªõc
        self._load_existing_jobs_cache()
        
        # Crawl song song theo page
        all_crawled_jobs = []
        
        try:
            skipped = 0
            pages = list(range(start_page, end_page + 1))
            
            with ThreadPoolExecutor(max_workers=self.max_workers) as executor:
                future_to_page = {
                    executor.submit(self.crawl_page_wrapper, page): page 
                    for page in pages
                }
                
                for future in as_completed(future_to_page):
                    if self.is_stopped():
                        logger.debug("üõë ƒêang h·ªßy c√°c task ƒëang ch·∫°y...")
                        executor.shutdown(wait=False, cancel_futures=True)
                        break
                        
                    page_num, page_jobs = future.result()
                    
                    if page_jobs:
                        for job in page_jobs:
                            all_crawled_jobs.append(job)
                            logger.info(f"‚úÖ [Page {page_num}] {job['name']}")
                        
                        skipped += (len(page_jobs) - len([j for j in page_jobs if j in all_crawled_jobs]))
                    else:
                        logger.debug(f"‚ö†Ô∏è  [Page {page_num}] Kh√¥ng c√≥ job m·ªõi")
            
            if self.is_stopped():
                update_crawl_record(
                    self._current_crawl_id, 
                    status='stopped',
                    jobs_count=len(all_crawled_jobs)
                )
                logger.info(f"üõë Crawler ƒë√£ d·ª´ng. ƒê√£ crawl ƒë∆∞·ª£c {len(all_crawled_jobs)} jobs tr∆∞·ªõc khi d·ª´ng")
                
                if all_crawled_jobs:
                    logger.info(f"ƒêang l∆∞u {len(all_crawled_jobs)} c√¥ng vi·ªác ƒë√£ crawl ƒë∆∞·ª£c...")
                    save_jobs_to_db(all_crawled_jobs, self._current_crawl_id)
                
                return all_crawled_jobs
            
            logger.info(f"\nüìä T·ªïng k·∫øt: Crawl ƒë∆∞·ª£c {len(all_crawled_jobs)} jobs, b·ªè qua {skipped} jobs tr√πng l·∫∑p")

            # L∆∞u v√†o database
            if all_crawled_jobs:
                logger.info(f"\nüíæ ƒêang l∆∞u {len(all_crawled_jobs)} c√¥ng vi·ªác v√†o database...")
                saved_count = save_jobs_to_db(all_crawled_jobs, self._current_crawl_id)
                
                update_crawl_record(
                    self._current_crawl_id, 
                    status='success',
                    jobs_count=saved_count
                )
                
                # ‚úÖ G·ª¨I EMAIL TH√îNG B√ÅO SAU KHI CRAWL XONG
                logger.info("\n" + "="*80)
                logger.info("üìß ƒêANG G·ª¨I TH√îNG B√ÅO EMAIL...")
                logger.info("="*80)
                try:
                    self.notification_handler.send_notifications_after_crawl(
                        crawl_id=self._current_crawl_id,
                        source_name=self.source_name
                    )
                except Exception as e:
                    logger.debug(f"‚ö†Ô∏è L·ªói khi g·ª≠i email (kh√¥ng ·∫£nh h∆∞·ªüng crawl): {str(e)}")
                
                # CRITICAL: Reset cache ƒë·ªÉ ƒë·ªìng b·ªô v·ªõi database
                logger.info("üîÑ Reset cache ƒë·ªÉ ƒë·ªìng b·ªô v·ªõi database...")
                self._existing_jobs_cache = None
            else:
                logger.debug("‚ö†Ô∏è  Kh√¥ng c√≥ c√¥ng vi·ªác m·ªõi ƒë·ªÉ l∆∞u")
                update_crawl_record(
                    self._current_crawl_id, 
                    status='empty',
                    message='Kh√¥ng c√≥ job m·ªõi ƒë·ªÉ crawl',
                    jobs_count=0
                )

        except Exception as e:
            logger.error(f"\n‚ùå L·ªói trong qu√° tr√¨nh crawl: {str(e)}")
            
            update_crawl_record(
                self._current_crawl_id, 
                status='failed',
                message=f'L·ªói: {str(e)}',
                jobs_count=len(all_crawled_jobs)
            )
            
            if all_crawled_jobs:
                logger.debug(f"ƒêang l∆∞u {len(all_crawled_jobs)} jobs ƒë√£ crawl ƒë∆∞·ª£c tr∆∞·ªõc khi l·ªói...")
                save_jobs_to_db(all_crawled_jobs, self._current_crawl_id)

        return all_crawled_jobs

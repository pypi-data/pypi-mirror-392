import json
import time
import unittest

from playwright.sync_api import sync_playwright

from job_crawler.gemini_crawler import GeminiCrawler
from job_crawler.utils.logger import logger


class TestGeminiCrawler(unittest.TestCase):
    # @unittest.skip("Skip")
    def test_gemin_crawler(self):
        generator = GeminiCrawler()

        with sync_playwright() as p:
            try:
                logger.info("=" * 80)
                logger.info("  C√îNG C·ª§ SINH C·∫§U H√åNH CRAWLER - T√åM PATTERN & CSS SELECTOR")
                logger.info("=" * 80)

                browser = p.chromium.launch(headless=True, args=["--start-maximized"])
                context = browser.new_context(no_viewport=True)
                page = context.new_page()

                # Nh·∫≠p URL danh s√°ch
                list_url = input("\nNh·∫≠p URL trang DANH S√ÅCH vi·ªác l√†m: ").strip()
                if not list_url:
                    logger.info("‚úó URL kh√¥ng ƒë∆∞·ª£c ƒë·ªÉ tr·ªëng!")
                    return

                site_name = list_url.split("//")[1].split("/")[0].replace("www.", "")

                # Ki·ªÉm tra config c√≥ s·∫µn kh√¥ng
                existing_config = generator.load_config(site_name)
                if existing_config:
                    logger.info(f"\n‚ö† ƒê√£ c√≥ c·∫•u h√¨nh cho {site_name}")
                    if input("S·ª≠ d·ª•ng l·∫°i? (y/n): ").lower() == "y":
                        logger.info(json.dumps(existing_config, indent=2, ensure_ascii=False))
                        return

                # B∆Ø·ªöC 0: L·∫•y t·∫•t c·∫£ links v√† t√¨m pattern
                logger.info("\n" + "=" * 80)
                logger.info("  B∆Ø·ªöC 0: PH√ÇN T√çCH PATTERN JOB LINKS")
                logger.info("=" * 80)

                all_links = generator.extract_all_links(page, list_url)
                if not all_links:
                    logger.info("\n‚úó Kh√¥ng t√¨m th·∫•y link n√†o!")
                    return

                job_link_pattern = generator.find_job_link_pattern(all_links, list_url)
                if not job_link_pattern:
                    logger.info("\n‚úó Kh√¥ng th·ªÉ t√¨m pattern!")
                    return

                # L·ªçc job links theo pattern
                job_links = generator.filter_job_links_by_pattern(all_links, job_link_pattern, max_jobs=3)
                if not job_links:
                    logger.info("\n‚úó Kh√¥ng c√≥ job link n√†o match pattern!")
                    return

                # Ch·ªçn 1 job link ƒë·ªÉ ph√¢n t√≠ch chi ti·∫øt
                sample_job_url = job_links[0]
                logger.info(f"\nüìå Ch·ªçn job m·∫´u ƒë·ªÉ ph√¢n t√≠ch: {sample_job_url}")

                # Truy c·∫≠p trang job chi ti·∫øt
                logger.info(f"\nüåê Truy c·∫≠p: {sample_job_url}")
                page.goto(sample_job_url, wait_until="domcontentloaded", timeout=60000)
                time.sleep(2)

                # B∆Ø·ªöC 1: L·∫•y text th√¥ theo sections & ph√¢n lo·∫°i d·ªØ li·ªáu
                logger.info("\n" + "=" * 80)
                logger.info("  B∆Ø·ªöC 1: TR√çCH XU·∫§T & PH√ÇN LO·∫†I D·ªÆ LI·ªÜU")
                logger.info("=" * 80)

                sections = generator.extract_text_sections(page)
                extracted_data = generator.extract_job_data_from_sections(sections, sample_job_url)

                if not extracted_data:
                    logger.info("\n‚úó Kh√¥ng th·ªÉ tr√≠ch xu·∫•t d·ªØ li·ªáu")
                    return
                logger.info("\nüìä D·ªØ li·ªáu ƒë√£ tr√≠ch xu·∫•t:")
                logger.info(json.dumps(extracted_data, indent=2, ensure_ascii=False))

                # B∆Ø·ªöC 2: T√¨m CSS selector
                logger.info("\n" + "=" * 80)
                logger.info("  B∆Ø·ªöC 2: T√åM CSS SELECTOR")
                logger.info("=" * 80)

                selectors_config = generator.find_selectors_from_html(page, extracted_data, sample_job_url)

                if not selectors_config:
                    logger.info("\n‚úó Kh√¥ng th·ªÉ t√¨m selector")
                    return

                # ‚úÖ TH√äM job_link_pattern v√† list_url v√†o config
                selectors_config["job_link_pattern"] = job_link_pattern
                selectors_config["list_url"] = list_url  # ‚úÖ TH√äM M·ªöI

                # Hi·ªÉn th·ªã k·∫øt qu·∫£ cu·ªëi c√πng
                logger.info("\n" + "=" * 80)
                logger.info("  K·∫æT QU·∫¢ CU·ªêI C√ôNG")
                logger.info("=" * 80)
                logger.info(json.dumps(selectors_config, indent=2, ensure_ascii=False))

                # L∆∞u config
                if input("\nL∆∞u config n√†y? (y/n): ").lower() == "y":
                    generator.save_config(selectors_config, site_name)
                    logger.info("\n‚úÖ Ho√†n th√†nh! Config ƒë√£ s·∫µn s√†ng ƒë·ªÉ s·ª≠ d·ª•ng v·ªõi GenericJobCrawler_DB.py")

            except TimeoutError:
                logger.info("\n‚úó H·∫øt th·ªùi gian ch·ªù")
            except KeyboardInterrupt:
                logger.info("\n‚ö† D·ª´ng b·ªüi ng∆∞·ªùi d√πng")
            except Exception as e:
                logger.info(f"\n‚úó L·ªói: {e}")
                import traceback
                traceback.print_exc()
            finally:
                if "browser" in locals() and browser.is_connected():
                    browser.close()
                    logger.info("\n‚úì ƒê√£ ƒë√≥ng tr√¨nh duy·ªát")


if __name__ == "__main__":
    unittest.main()
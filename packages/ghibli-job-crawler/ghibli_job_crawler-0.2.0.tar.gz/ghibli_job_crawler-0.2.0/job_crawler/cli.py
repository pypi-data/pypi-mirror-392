import time
import json
import argparse

from playwright.sync_api import sync_playwright

from .generic_job import GenericJobCrawler
from .gemini_crawler import GeminiCrawler
from .topcv import TopCVCrawler
from .vietnamworks import VietnamWorksCrawler
from .export import to_csv, to_json
from .utils.logger import logger


def crawl_topcv(num_workers: int = 3, start_page: int = 0, end_page: int = 0) -> None:
    logger.info("TopCV Job Crawler (Optimized)")
    crawler = TopCVCrawler(max_workers=num_workers)
    try:
        crawler.crawl_jobs(start_page=start_page, end_page=end_page)
        logger.info("\nHo√†n th√†nh crawl t·ª´ TopCV!")
    except KeyboardInterrupt:
        crawler.stop()
        logger.error("\nƒê√£ d·ª´ng crawler theo y√™u c·∫ßu ng∆∞·ªùi d√πng.")
    except Exception as e:
        logger.error(f"\nL·ªói trong qu√° tr√¨nh crawl: {str(e)}")


def crawl_vietnamworks(num_workers: int = 3, start_page: int = 0, end_page: int = 0) -> None:
    logger.info("VietnamWorks Job Crawler (Optimized)")
    crawler = VietnamWorksCrawler(max_workers=num_workers)
    try:
        crawler.crawl_jobs(start_page=start_page, end_page=end_page)
        logger.info("\nHo√†n th√†nh crawl t·ª´ TopCV!")
    except KeyboardInterrupt:
        crawler.stop()
        logger.error("\nƒê√£ d·ª´ng crawler theo y√™u c·∫ßu ng∆∞·ªùi d√πng.")
    except Exception as e:
        logger.error(f"\nL·ªói trong qu√° tr√¨nh crawl: {str(e)}")


def crawl_generic_job(config_path: str, num_workers: int = 3, start_page: int = 0, end_page: int = 0) -> None:
    logger.info("Generic Job Crawler (Optimized)")
    crawler = GenericJobCrawler(config_path, max_workers=num_workers)
    try:
        crawler.crawl_jobs(start_page=start_page, end_page=end_page)
        logger.info("\nHo√†n th√†nh crawl!")
    except KeyboardInterrupt:
        crawler.stop()
        logger.info("\nƒê√£ d·ª´ng crawler theo y√™u c·∫ßu ng∆∞·ªùi d√πng.")


def crawl_gemini() -> None:
    generator = GeminiCrawler()

    with sync_playwright() as p:
        try:
            logger.info("=" * 80)
            logger.info("  C√îNG C·ª§ SINH C·∫§U H√åNH CRAWLER - T√åM PATTERN & CSS SELECTOR")
            logger.info("=" * 80)

            browser = p.chromium.launch(headless=True, args=["--start-maximized"])
            context = browser.new_context(no_viewport=True)
            page = context.new_page()

            # Nh·∫≠p URL danh s√°ch
            list_url = input("\nNh·∫≠p URL trang DANH S√ÅCH vi·ªác l√†m: ").strip()
            if not list_url:
                logger.info("‚úó URL kh√¥ng ƒë∆∞·ª£c ƒë·ªÉ tr·ªëng!")
                return

            site_name = list_url.split("//")[1].split("/")[0].replace("www.", "")

            # Ki·ªÉm tra config c√≥ s·∫µn kh√¥ng
            existing_config = generator.load_config(site_name)
            if existing_config:
                logger.info(f"\n‚ö† ƒê√£ c√≥ c·∫•u h√¨nh cho {site_name}")
                if input("S·ª≠ d·ª•ng l·∫°i? (y/n): ").lower() == "y":
                    logger.info(json.dumps(existing_config, indent=2, ensure_ascii=False))
                    return

            # B∆Ø·ªöC 0: L·∫•y t·∫•t c·∫£ links v√† t√¨m pattern
            logger.info("\n" + "=" * 80)
            logger.info("  B∆Ø·ªöC 0: PH√ÇN T√çCH PATTERN JOB LINKS")
            logger.info("=" * 80)

            all_links = generator.extract_all_links(page, list_url)
            if not all_links:
                logger.info("\n‚úó Kh√¥ng t√¨m th·∫•y link n√†o!")
                return

            job_link_pattern = generator.find_job_link_pattern(all_links, list_url)
            if not job_link_pattern:
                logger.info("\n‚úó Kh√¥ng th·ªÉ t√¨m pattern!")
                return

            # L·ªçc job links theo pattern
            job_links = generator.filter_job_links_by_pattern(all_links, job_link_pattern, max_jobs=3)
            if not job_links:
                logger.info("\n‚úó Kh√¥ng c√≥ job link n√†o match pattern!")
                return

            # Ch·ªçn 1 job link ƒë·ªÉ ph√¢n t√≠ch chi ti·∫øt
            sample_job_url = job_links[0]
            logger.info(f"\nüìå Ch·ªçn job m·∫´u ƒë·ªÉ ph√¢n t√≠ch: {sample_job_url}")

            # Truy c·∫≠p trang job chi ti·∫øt
            logger.info(f"\nüåê Truy c·∫≠p: {sample_job_url}")
            page.goto(sample_job_url, wait_until="domcontentloaded", timeout=60000)
            time.sleep(2)

            # B∆Ø·ªöC 1: L·∫•y text th√¥ theo sections & ph√¢n lo·∫°i d·ªØ li·ªáu
            logger.info("\n" + "=" * 80)
            logger.info("  B∆Ø·ªöC 1: TR√çCH XU·∫§T & PH√ÇN LO·∫†I D·ªÆ LI·ªÜU")
            logger.info("=" * 80)

            sections = generator.extract_text_sections(page)
            extracted_data = generator.extract_job_data_from_sections(sections, sample_job_url)

            if not extracted_data:
                logger.info("\n‚úó Kh√¥ng th·ªÉ tr√≠ch xu·∫•t d·ªØ li·ªáu")
                return
            logger.info("\nüìä D·ªØ li·ªáu ƒë√£ tr√≠ch xu·∫•t:")
            logger.info(json.dumps(extracted_data, indent=2, ensure_ascii=False))

            # B∆Ø·ªöC 2: T√¨m CSS selector
            logger.info("\n" + "=" * 80)
            logger.info("  B∆Ø·ªöC 2: T√åM CSS SELECTOR")
            logger.info("=" * 80)

            selectors_config = generator.find_selectors_from_html(page, extracted_data, sample_job_url)

            if not selectors_config:
                logger.info("\n‚úó Kh√¥ng th·ªÉ t√¨m selector")
                return

            # ‚úÖ TH√äM job_link_pattern v√† list_url v√†o config
            selectors_config["job_link_pattern"] = job_link_pattern
            selectors_config["list_url"] = list_url  # ‚úÖ TH√äM M·ªöI

            # Hi·ªÉn th·ªã k·∫øt qu·∫£ cu·ªëi c√πng
            logger.info("\n" + "=" * 80)
            logger.info("  K·∫æT QU·∫¢ CU·ªêI C√ôNG")
            logger.info("=" * 80)
            logger.info(json.dumps(selectors_config, indent=2, ensure_ascii=False))

            # L∆∞u config
            if input("\nL∆∞u config n√†y? (y/n): ").lower() == "y":
                generator.save_config(selectors_config, site_name)
                logger.info("\n‚úÖ Ho√†n th√†nh! Config ƒë√£ s·∫µn s√†ng ƒë·ªÉ s·ª≠ d·ª•ng v·ªõi GenericJobCrawler_DB.py")

        except TimeoutError:
            logger.info("\n‚úó H·∫øt th·ªùi gian ch·ªù")
        except KeyboardInterrupt:
            logger.info("\n‚ö† D·ª´ng b·ªüi ng∆∞·ªùi d√πng")
        except Exception as e:
            logger.info(f"\n‚úó L·ªói: {e}")
            import traceback
            traceback.print_exc()
        finally:
            if "browser" in locals() and browser.is_connected():
                browser.close()
                logger.info("\n‚úì ƒê√£ ƒë√≥ng tr√¨nh duy·ªát")


def cli():
    parser = argparse.ArgumentParser(prog="job-crawler", description="Job Crawler CLI")
    subparsers = parser.add_subparsers(dest="command", required=True)

    export_parser = subparsers.add_parser("export", help="Export crawled data")

    # ---- Subcommand crawl ----
    crawl_parser = subparsers.add_parser("crawl", help="Crawl job data")
    crawl_parser.add_argument("--type", choices=["topcv", "vietnamworks", "generic_job", "gemini"], required=True, help="Source to crawl")
    crawl_parser.add_argument("--max-workers", type=int, default=3, help="Number of threads")
    crawl_parser.add_argument("--start-page", type=int, default=0, help="Start page index")
    crawl_parser.add_argument("--end-page", type=int, default=1, help="End page index")
    crawl_parser.add_argument("--config", type=str, default="", help="Config path for generic job crawler")

    # ---- Subcommand export ----
    export_parser = subparsers.add_parser("export", help="Export crawled data")
    export_parser.add_argument("--type", choices=["csv", "json"], default="csv", help="Export format")
    export_parser.add_argument("--save-path", type=str, required=True, help="Directory or file path to save exported data")
    export_parser.add_argument("--table-name", type=str, nargs="*", default=None, help="Optional table names to export (default all tables)")

    args = parser.parse_args()

    if args.command == "crawl":
        if args.type == "topcv":
            crawl_topcv(
                args.max_workers,
                args.start_page,
                args.end_page
            )
        elif args.type == "vietnamworks":
            crawl_vietnamworks(
                args.max_workers,
                args.start_page,
                args.end_page
            )
        elif args.type == "generic_job":
            crawl_generic_job(
                args.config,
                args.max_workers,
                args.start_page,
                args.end_page
            )
        elif args.type == "gemini":
            crawl_gemini()
    elif args.command == "export":
        if args.type == "csv":
            to_csv(args.save_path, args.table_name)
        elif args.type == "json":
            to_json(args.save_path, args.table_name)

    

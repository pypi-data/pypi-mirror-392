import re
import time
import random
from threading import Event
from urllib.parse import urljoin
from concurrent.futures import ThreadPoolExecutor, as_completed

import requests
from bs4 import BeautifulSoup
from .database.handlers import (
    create_crawl_record, 
    update_crawl_record, 
    save_jobs_to_db,
    get_all_job_names
)
from .utils.logger import logger


class TopCVCrawler:
    def __init__(self, max_workers: int = 5) -> None:
        """
        Args:
            max_workers: Sá»‘ lÆ°á»£ng crawl Ä‘á»“ng thá»i (máº·c Ä‘á»‹nh 5)
        """

        self.base_url = "https://www.topcv.vn"
        self.source_name = "TopCV"
        self.source_url = "https://www.topcv.vn"
        self.max_workers = max_workers
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
            'Accept-Language': 'vi-VN,vi;q=0.8,en-US;q=0.5,en;q=0.3',
            'Accept-Encoding': 'gzip, deflate',
            'Connection': 'keep-alive',
        })
        
        # Cache existing jobs Ä‘á»ƒ giáº£m query DB
        self._existing_jobs_cache = None
        
        # Stop event Ä‘á»ƒ dá»«ng crawler
        self._stop_event = Event()
        
        # CrawlID cho láº§n crawl hiá»‡n táº¡i
        self._current_crawl_id = None

    def stop(self) -> None:
        """Dá»«ng crawler"""
        logger.debug("ğŸ›‘ Äang dá»«ng crawler...")
        self._stop_event.set()

    def is_stopped(self) -> bool:
        """Kiá»ƒm tra xem crawler Ä‘Ã£ dá»«ng chÆ°a"""
        return self._stop_event.is_set()

    def _load_existing_jobs_cache(self) -> None:
        """Load táº¥t cáº£ existing jobs vÃ o memory 1 láº§n duy nháº¥t"""
        if self._existing_jobs_cache is not None:
            return

        try:
            jobs = get_all_job_names()

            # Táº¡o set Ä‘á»ƒ tra cá»©u nhanh O(1)
            self._existing_jobs_cache = {
                (
                    job_name.strip().lower() if job_name else "", 
                    company_name.strip().lower() if company_name else ""
                )
                for job_name, company_name in jobs
            }
            logger.info(f"ÄÃ£ load {len(self._existing_jobs_cache)} jobs vÃ o cache")
        except Exception as e:
            logger.error(f"Lá»—i khi load cache: {str(e)}")
            self._existing_jobs_cache = set()

    def check_job_exists(self, job_name: str, company_name: str):
        """Kiá»ƒm tra job tá»“n táº¡i tá»« cache (nhanh hÆ¡n query DB)"""
        if self._existing_jobs_cache is None:
            self._load_existing_jobs_cache()
        
        key = (job_name.strip().lower(), company_name.strip().lower())
        return key in self._existing_jobs_cache

    def get_job_links_from_page(self, page_num: int) -> list[str]:
        """Crawl danh sÃ¡ch link cÃ´ng viá»‡c tá»« trang listing"""
        if self.is_stopped():
            return []
            
        url = f"https://www.topcv.vn/viec-lam-tot-nhat?page={page_num}"
        try:
            response = self.session.get(url, timeout=10)
            response.raise_for_status()
            soup = BeautifulSoup(response.content, 'html.parser')
            job_links = []

            job_elements = soup.find_all('a', href=re.compile(r'/viec-lam/'))
            for element in job_elements:
                if self.is_stopped():
                    break
                    
                href = element.get('href')
                if href:
                    full_url = urljoin(self.base_url, href)
                    if full_url not in job_links:
                        job_links.append(full_url)

            return job_links
        except Exception as e:
            logger.error(f"Lá»—i khi crawl trang {page_num}: {str(e)}")
            return []

    def extract_job_details(self, job_url: str, retry: int = 1) -> dict | None:
        """Crawl chi tiáº¿t cÃ´ng viá»‡c tá»« URL (cÃ³ cÆ¡ cháº¿ retry khi bá»‹ cháº·n 429)"""
        if self.is_stopped():
            return None
            
        for attempt in range(retry):
            if self.is_stopped():
                return None
                
            try:
                response = self.session.get(job_url, timeout=10)

                # Kiá»ƒm tra lá»—i 429 Too Many Requests
                if response.status_code == 429:
                    wait = random.uniform(3, 3.5)
                    logger.warn(f"âš ï¸ Bá»‹ cháº·n 429 ({job_url}), chá» {wait:.1f}s rá»“i thá»­ láº¡i ({attempt+1}/{retry})...")
                    time.sleep(wait)
                    continue

                # Náº¿u cÃ³ lá»—i HTTP khÃ¡c
                response.raise_for_status()

                soup = BeautifulSoup(response.content, 'html.parser')

                company_info = self.extract_company_info(soup)
                job_name = self.clean_text(self.extract_job_name(soup))

                # Kiá»ƒm tra trÃ¹ng láº·p tá»« cache
                if self.check_job_exists(job_name, company_info['name']):
                    return None

                job_data = {
                    'name': job_name,
                    'salary': self.clean_text(self.extract_salary(soup)),
                    'experience': self.clean_text(self.extract_experience(soup)),
                    'education_level': self.clean_text(self.extract_education(soup)),
                    'location': self.clean_text(self.extract_location(soup)),
                    'position_level': self.clean_text(self.extract_position_level(soup)),
                    'job_type': self.clean_text(self.extract_job_type(soup)),
                    'deadline_submission': self.clean_text(self.extract_deadline(soup)),
                    'quantity': self.clean_text(self.extract_quantity(soup)),
                    'description': self.extract_description(soup),
                    'required': self.extract_required(soup),
                    'company_name': company_info['name'],
                    'company_location': company_info['location'],
                    'company_industry': company_info['industry'],
                    'company_scale': company_info['scale'],
                    'job_link': job_url
                }

                return job_data

            except requests.exceptions.RequestException as e:
                # Lá»—i máº¡ng hoáº·c timeout
                wait = random.uniform(3, 6)
                logger.error(f"âš ï¸ Lá»—i khi crawl {job_url}: {str(e)} (láº§n {attempt+1}/{retry}) â€“ nghá»‰ {wait:.1f}s rá»“i thá»­ láº¡i...")
                time.sleep(wait)

            except Exception as e:
                # Lá»—i khÃ´ng mong Ä‘á»£i (HTML, parse, ...)
                logger.error(f"âŒ Lá»—i khÃ´ng mong Ä‘á»£i á»Ÿ {job_url}: {str(e)}")
                break

        # Náº¿u sau nhiá»u láº§n thá»­ váº«n khÃ´ng thÃ nh cÃ´ng
        logger.warn(f"âŒ Bá» qua {job_url} sau {retry} láº§n thá»­ khÃ´ng thÃ nh cÃ´ng.")
        return None
        
    def crawl_job_wrapper(self, job_link: str) -> dict | None:
        """Wrapper Ä‘á»ƒ crawl 1 job (dÃ¹ng cho threading)"""
        if self.is_stopped():
            return None
            
        time.sleep(random.uniform(2, 3))
        return self.extract_job_details(job_link)

    def extract_description(self, soup: BeautifulSoup) -> str:
        """TrÃ­ch xuáº¥t mÃ´ táº£ cÃ´ng viá»‡c (phiÃªn báº£n tá»‘i Æ°u)"""
        description_keywords = [
            'mÃ´ táº£ cÃ´ng viá»‡c', 'mo ta cong viec', 'job description',
            'nhiá»‡m vá»¥', 'trÃ¡ch nhiá»‡m'
        ]
        
        job_desc_div = soup.find('div', class_='job-description')
        if not job_desc_div:
            return ""
        
        items = job_desc_div.find_all('div', class_='job-description__item')
        
        for item in items:
            h3 = item.find('h3')
            if h3:
                h3_text = h3.get_text().strip().lower()
                if any(keyword in h3_text for keyword in description_keywords):
                    content_div = item.find('div', class_='job-description__item--content')
                    if content_div:
                        # Láº¥y toÃ n bá»™ text, giá»¯ láº¡i cÃ¡c dÃ²ng xuá»‘ng hÃ ng
                        return content_div.get_text(separator='\n', strip=True)

        return ""

    def extract_required(self, soup: BeautifulSoup) -> str:
        """TrÃ­ch xuáº¥t yÃªu cáº§u á»©ng viÃªn (phiÃªn báº£n tá»‘i Æ°u)"""
        required_keywords = [
            'yÃªu cáº§u á»©ng viÃªn', 'yeu cau ung vien', 'job requirements',
            'requirements', 'yÃªu cáº§u cÃ´ng viá»‡c', 'ká»¹ nÄƒng'
        ]
        
        job_desc_div = soup.find('div', class_='job-description')
        if not job_desc_div:
            return ""
        
        items = job_desc_div.find_all('div', class_='job-description__item')
        
        for item in items:
            h3 = item.find('h3')
            if h3:
                h3_text = h3.get_text().strip().lower()
                if any(keyword in h3_text for keyword in required_keywords):
                    content_div = item.find('div', class_='job-description__item--content')
                    if content_div:
                        # Láº¥y toÃ n bá»™ text, giá»¯ láº¡i cÃ¡c dÃ²ng xuá»‘ng hÃ ng
                        return content_div.get_text(separator='\n', strip=True)

        return ""

    def extract_company_info(self, soup: BeautifulSoup) -> str:
        """TrÃ­ch xuáº¥t thÃ´ng tin cÃ´ng ty"""
        company = {'name': '', 'location': '', 'industry': '', 'scale': ''}
        box = soup.find('div', class_=re.compile(r'job-detail__box--right.*job-detail__company'))
        if not box:
            return company

        div_name = box.find('div', class_=re.compile(r'company-name-label'))
        if div_name:
            a_tag = div_name.find('a')
            if a_tag:
                company['name'] = self.clean_text(a_tag.get_text())

        scale_elem = box.find('div', class_=re.compile(r'company-scale'))
        if scale_elem:
            val = scale_elem.find('div', class_=re.compile(r'company-value'))
            if val:
                company['scale'] = self.clean_text(val.get_text())

        field_elem = box.find('div', class_=re.compile(r'company-field'))
        if field_elem:
            val = field_elem.find('div', class_=re.compile(r'company-value'))
            if val:
                company['industry'] = self.clean_text(val.get_text())

        address_elem = box.find('div', class_=re.compile(r'company-address'))
        if address_elem:
            val = address_elem.find('div', class_=re.compile(r'company-value'))
            if val:
                company['location'] = self.clean_text(val.get_text())
        return company

    def extract_job_name(self, soup: BeautifulSoup) -> str:
        selectors = ['h1.job-title', 'h1', '.job-detail-title h1', '.title-job', 'h2.job-title']
        for selector in selectors:
            element = soup.select_one(selector)
            if element:
                return element.get_text()
        return "N/A"

    def extract_salary(self, soup: BeautifulSoup) -> str:
        sections = soup.find_all('div', class_='job-detail__info--sections')
        for section in sections:
            labels = section.find_all(string=re.compile(r'má»©c lÆ°Æ¡ng|lÆ°Æ¡ng|salary', re.IGNORECASE))
            for label in labels:
                label_parent = label.parent
                while label_parent:
                    value_elem = label_parent.find_next('div', class_='job-detail__info--section-content-value')
                    if value_elem:
                        return value_elem.get_text().strip()
                    label_parent = label_parent.parent
        return "Thá»a thuáº­n"

    def extract_experience(self, soup: BeautifulSoup) -> str:
        sections = soup.find_all('div', class_='job-detail__info--sections')
        for section in sections:
            labels = section.find_all(string=re.compile(r'kinh nghiá»‡m|experience', re.IGNORECASE))
            for label in labels:
                label_parent = label.parent
                while label_parent:
                    value_elem = label_parent.find_next('div', class_='job-detail__info--section-content-value')
                    if value_elem:
                        return value_elem.get_text().strip()
                    label_parent = label_parent.parent
        return "KhÃ´ng yÃªu cáº§u"

    def extract_education(self, soup: BeautifulSoup) -> str:
        box_general = soup.find('div', class_=re.compile('job-detail__box--right.*job-detail__body-right--item.*job-detail__body-right--box-general'))
        if box_general:
            labels = box_general.find_all(string=re.compile(r'há»c váº¥n|education|báº±ng cáº¥p|trÃ¬nh Ä‘á»™', re.IGNORECASE))
            for label in labels:
                label_parent = label.parent
                while label_parent and label_parent != box_general:
                    value_elem = label_parent.find_next('div', class_='box-general-group-info-value')
                    if value_elem:
                        return value_elem.get_text().strip()
                    label_parent = label_parent.parent
        return "KhÃ´ng yÃªu cáº§u"

    def extract_location(self, soup: BeautifulSoup) -> str:
        sections = soup.find_all('div', class_='job-detail__info--sections')
        for section in sections:
            labels = section.find_all(string=re.compile(r'Ä‘á»‹a Ä‘iá»ƒm|location|nÆ¡i lÃ m viá»‡c|khu vá»±c', re.IGNORECASE))
            for label in labels:
                label_parent = label.parent
                while label_parent:
                    value_elem = label_parent.find_next('div', class_='job-detail__info--section-content-value')
                    if value_elem:
                        return value_elem.get_text().strip()
                    label_parent = label_parent.parent
        return "ToÃ n quá»‘c"

    def extract_position_level(self, soup: BeautifulSoup) -> str:
        box_general = soup.find('div', class_=re.compile('job-detail__box--right.*job-detail__body-right--item.*job-detail__body-right--box-general'))
        if box_general:
            labels = box_general.find_all(string=re.compile(r'cáº¥p báº­c|level|chá»©c vá»¥', re.IGNORECASE))
            for label in labels:
                label_parent = label.parent
                while label_parent and label_parent != box_general:
                    value_elem = label_parent.find_next('div', class_='box-general-group-info-value')
                    if value_elem:
                        return value_elem.get_text().strip()
                    label_parent = label_parent.parent
        return "NhÃ¢n viÃªn"

    def extract_job_type(self, soup: BeautifulSoup) -> str:
        box_general = soup.find('div', class_=re.compile('job-detail__box--right.*job-detail__body-right--item.*job-detail__body-right--box-general'))
        if box_general:
            labels = box_general.find_all(string=re.compile(r'hÃ¬nh thá»©c|job type|loáº¡i cÃ´ng viá»‡c|loáº¡i hÃ¬nh', re.IGNORECASE))
            for label in labels:
                label_parent = label.parent
                while label_parent and label_parent != box_general:
                    value_elem = label_parent.find_next('div', class_='box-general-group-info-value')
                    if value_elem:
                        return value_elem.get_text().strip()
                    label_parent = label_parent.parent
        return "Full-time"

    def extract_deadline(self, soup: BeautifulSoup) -> str:
        keywords = ['háº¡n ná»™p', 'deadline', 'háº¿t háº¡n', 'á»©ng tuyá»ƒn trÆ°á»›c']
        for keyword in keywords:
            element = soup.find(string=re.compile(keyword, re.IGNORECASE))
            if element:
                parent = element.parent
                if parent:
                    text = parent.get_text()
                    date_match = re.search(r'\d{1,2}[/-]\d{1,2}[/-]\d{4}', text)
                    if date_match:
                        return date_match.group()
        return "KhÃ´ng giá»›i háº¡n"

    def extract_quantity(self, soup: BeautifulSoup) -> str:
        box_general = soup.find('div', class_=re.compile('job-detail__box--right.*job-detail__body-right--item.*job-detail__body-right--box-general'))
        if box_general:
            labels = box_general.find_all(string=re.compile(r'sá»‘ lÆ°á»£ng|quantity|tuyá»ƒn dá»¥ng|cáº§n tuyá»ƒn', re.IGNORECASE))
            for label in labels:
                label_parent = label.parent
                while label_parent and label_parent != box_general:
                    value_elem = label_parent.find_next('div', class_='box-general-group-info-value')
                    if value_elem:
                        text = value_elem.get_text().strip()
                        num_match = re.search(r'(\d+)', text)
                        if num_match:
                            return num_match.group(1) + " ngÆ°á»i"
                        return text
                    label_parent = label_parent.parent
        return "1 ngÆ°á»i"

    def clean_text(self, text: str) -> str:
        if not text or text == "N/A":
            return ""
        text = re.sub(r'\s+', ' ', str(text).strip())
        return text.strip()

    def crawl_jobs(self, start_page: int = 1, end_page: int = 3) -> list[dict]:
        """
        HÃ m chÃ­nh - Crawl song song vá»›i threading
        
        Args:
            start_page: Trang báº¯t Ä‘áº§u
            end_page: Trang káº¿t thÃºc
        """
        logger.info(f"\n{'='*80}")
        logger.info(f"Báº¯t Ä‘áº§u crawl tá»« {self.source_name} (song song {self.max_workers} luá»“ng)")
        logger.info(f"{'='*80}")
        
        # Reset stop event
        self._stop_event.clear()
        
        # âœ… Táº O CRAWL RECORD NGAY KHI Báº®T Äáº¦U
        try:
            self._current_crawl_id = create_crawl_record(self.source_name, self.source_url)
        except Exception as e:
            logger.error(f"âŒ KhÃ´ng thá»ƒ táº¡o CrawlRecord: {e}")
            return []
        
        # Load cache trÆ°á»›c
        self._load_existing_jobs_cache()
        
        crawled_jobs = []
        
        try:
            # Thu tháº­p links
            all_job_links = []
            for page in range(start_page, end_page + 1):
                if self.is_stopped():
                    logger.debug("ğŸ›‘ ÄÃ£ dá»«ng viá»‡c thu tháº­p links")
                    break
                    
                job_links = self.get_job_links_from_page(page)
                all_job_links.extend(job_links)
                time.sleep(random.uniform(1, 2))

            if self.is_stopped():
                # âœ… Cáº¬P NHáº¬T STATUS = 'stopped'
                update_crawl_record(
                    self._current_crawl_id, 
                    status='stopped',
                    jobs_count=len(crawled_jobs)
                )
                logger.debug(f"ğŸ›‘ Crawler Ä‘Ã£ bá»‹ dá»«ng. ÄÃ£ crawl Ä‘Æ°á»£c {len(crawled_jobs)} jobs")
                return crawled_jobs

            logger.info(f"TÃ¬m tháº¥y {len(all_job_links)} link cÃ´ng viá»‡c")
            
            # Crawl song song
            skipped = 0
            
            with ThreadPoolExecutor(max_workers=self.max_workers) as executor:
                future_to_url = {
                    executor.submit(self.crawl_job_wrapper, link): link 
                    for link in all_job_links
                }
                
                for future in as_completed(future_to_url):
                    if self.is_stopped():
                        logger.debug("ğŸ›‘ Äang há»§y cÃ¡c task Ä‘ang cháº¡y...")
                        executor.shutdown(wait=False, cancel_futures=True)
                        break
                        
                    job_data = future.result()
                    if job_data:
                        crawled_jobs.append(job_data)
                        logger.info(f"Crawl thÃ nh cÃ´ng: {job_data['name']}")
                    else:
                        skipped += 1
            
            if self.is_stopped():
                # âœ… Cáº¬P NHáº¬T STATUS = 'stopped'
                update_crawl_record(
                    self._current_crawl_id, 
                    status='stopped',
                    jobs_count=len(crawled_jobs)
                )
                logger.debug(f"ğŸ›‘ Crawler Ä‘Ã£ dá»«ng. ÄÃ£ crawl Ä‘Æ°á»£c {len(crawled_jobs)} jobs trÆ°á»›c khi dá»«ng")
                
                # Váº«n lÆ°u jobs Ä‘Ã£ crawl Ä‘Æ°á»£c vÃ o DB
                if crawled_jobs:
                    logger.info(f"Äang lÆ°u {len(crawled_jobs)} cÃ´ng viá»‡c Ä‘Ã£ crawl Ä‘Æ°á»£c...")
                    save_jobs_to_db(crawled_jobs, self._current_crawl_id)
                
                return crawled_jobs
            
            logger.debug(f"\nBá» qua {skipped} job trÃ¹ng láº·p")
            
            # LÆ°u vÃ o database
            if crawled_jobs:
                logger.info(f"Äang lÆ°u {len(crawled_jobs)} cÃ´ng viá»‡c vÃ o database...")
                saved_count = save_jobs_to_db(crawled_jobs, self._current_crawl_id)
                
                # âœ… Cáº¬P NHáº¬T STATUS = 'success'
                update_crawl_record(
                    self._current_crawl_id, 
                    status='success',
                    jobs_count=saved_count
                )
            else:
                logger.info("KhÃ´ng cÃ³ cÃ´ng viá»‡c má»›i Ä‘á»ƒ lÆ°u")
                # âœ… Cáº¬P NHáº¬T STATUS = 'success' vá»›i 0 jobs
                update_crawl_record(
                    self._current_crawl_id, 
                    status='empty',
                    message='KhÃ´ng cÃ³ job má»›i Ä‘á»ƒ crawl',
                    jobs_count=0
                )

        except Exception as e:
            logger.error(f"\nâŒ Lá»—i trong quÃ¡ trÃ¬nh crawl: {str(e)}")
            
            # âœ… Cáº¬P NHáº¬T STATUS = 'failed'
            update_crawl_record(
                self._current_crawl_id, 
                status='failed',
                message=f'Lá»—i: {str(e)}',
                jobs_count=len(crawled_jobs)
            )
            
            # Váº«n cá»‘ lÆ°u jobs Ä‘Ã£ crawl Ä‘Æ°á»£c
            if crawled_jobs:
                logger.debug(f"Äang lÆ°u {len(crawled_jobs)} jobs Ä‘Ã£ crawl Ä‘Æ°á»£c trÆ°á»›c khi lá»—i...")
                save_jobs_to_db(crawled_jobs, self._current_crawl_id)

        return crawled_jobs

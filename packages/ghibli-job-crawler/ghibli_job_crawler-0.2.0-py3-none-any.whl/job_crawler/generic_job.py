import re
import os
import time
import json
from urllib.parse import urljoin, urlparse
from concurrent.futures import ThreadPoolExecutor, as_completed
from threading import Event, Lock

from playwright.sync_api import sync_playwright, Page

from .database.handlers import (
    create_crawl_record, 
    update_crawl_record, 
    save_jobs_to_db,
    get_all_job_names
)
from .utils.logger import logger


class GenericJobCrawler:
    def __init__(self, config_path: str, max_workers: int = 3) -> None:
        """
        Khá»Ÿi táº¡o crawler vá»›i file config
        
        Args:
            config_path: ÄÆ°á»ng dáº«n Ä‘áº¿n file config JSON
            max_workers: Sá»‘ lÆ°á»£ng crawl Ä‘á»“ng thá»i (máº·c Ä‘á»‹nh 3)
        """
        self.config: dict = self.load_config(config_path)
        self.site_name: str = self.config.get("site_name", "unknown")
        self.source_url: str = self.config.get("base_url", "")
        self.selectors: dict = self.config.get("selectors", {})
        self.job_link_pattern: str = self.config.get("job_link_pattern", "")
        self.list_url: str = self.config.get("list_url", "")
        self.max_workers = max_workers
        
        # Stop event Ä‘á»ƒ dá»«ng crawler
        self._stop_event = Event()
        
        # CrawlID cho láº§n crawl hiá»‡n táº¡i
        self._current_crawl_id: int | None = None
        
        # Cache existing jobs
        self._existing_jobs_cache: set | None = None
        
        # Lock Ä‘á»ƒ in ra console an toÃ n
        self._print_lock = Lock()
        
    def load_config(self, config_path: str) -> dict:
        """Load file config JSON"""
        try:
            with open(config_path, "r", encoding="utf-8") as f:
                config = json.load(f)
                logger.info(f"âœ“ ÄÃ£ load config: {config.get('site_name', 'Unknown')}")
                return config
        except FileNotFoundError:
            logger.info(f"âœ— KhÃ´ng tÃ¬m tháº¥y file: {config_path}")
            exit(1)
        except json.JSONDecodeError:
            logger.info(f"âœ— File config khÃ´ng Ä‘Ãºng Ä‘á»‹nh dáº¡ng JSON")
            exit(1)
    
    def stop(self) -> None:
        """Dá»«ng crawler"""
        logger.info("ğŸ›‘ Äang dá»«ng crawler...")
        self._stop_event.set()

    def is_stopped(self) -> bool:
        """Kiá»ƒm tra xem crawler Ä‘Ã£ dá»«ng chÆ°a"""
        return self._stop_event.is_set()

    def _load_existing_jobs_cache(self):
        """Load táº¥t cáº£ existing jobs vÃ o memory"""
        if self._existing_jobs_cache is not None:
            return

        try:
            jobs = get_all_job_names()
            
            self._existing_jobs_cache = {
                (
                    job_name.strip().lower() if job_name else "", 
                    company_name.strip().lower() if company_name else ""
                )
                for job_name, company_name in jobs
            }
            logger.info(f"ÄÃ£ load {len(self._existing_jobs_cache)} jobs vÃ o cache")
        except Exception as e:
            logger.error(f"Lá»—i khi load cache: {str(e)}")
            self._existing_jobs_cache = set()

    def check_job_exists(self, job_name: str, company_name: str) -> bool:
        """Kiá»ƒm tra job tá»“n táº¡i tá»« cache"""
        if self._existing_jobs_cache is None:
            self._load_existing_jobs_cache()
        
        key = (job_name.strip().lower(), company_name.strip().lower())
        return key in self._existing_jobs_cache
    
    def get_job_links_from_page(self, page_num: int) -> list[str]:
        """
        Crawl danh sÃ¡ch link cÃ´ng viá»‡c tá»« 1 trang listing
        Má»—i láº§n gá»i táº¡o playwright instance riÃªng
        """
        if self.is_stopped():
            return []
        
        # XÃ¢y dá»±ng URL vá»›i page number
        if "?" in self.list_url:
            page_url = f"{self.list_url}&page={page_num}"
        else:
            page_url = f"{self.list_url}?page={page_num}"
        
        with self._print_lock:
            logger.info(f"\n{'='*80}")
            logger.info(f"  ÄANG CRAWL TRANG {page_num}")
            logger.info(f"{'='*80}")
            logger.info(f"URL: {page_url}")
        
        # Táº¡o Playwright instance riÃªng cho viá»‡c láº¥y links
        with sync_playwright() as p:
            try:
                browser = p.chromium.launch(headless=True)
                context = browser.new_context(
                    viewport={"width": 1920, "height": 1080},
                    user_agent="Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36"
                )
                page = context.new_page()
                
                page.goto(page_url, wait_until="domcontentloaded", timeout=60000)
                time.sleep(2)
                
                # Láº¥y domain gá»‘c
                parsed_url = urlparse(self.list_url)
                base_domain = f"{parsed_url.scheme}://{parsed_url.netloc}"
                
                # Láº¥y táº¥t cáº£ links
                all_links = page.locator("a[href]").all()
                
                job_links = []
                seen_urls = set()
                
                # Compile regex pattern
                try:
                    pattern_regex = re.compile(self.job_link_pattern)
                except re.error as e:
                    with self._print_lock:
                        logger.error(f"âœ— Pattern regex khÃ´ng há»£p lá»‡: {e}")
                    pattern_regex = None
                
                for link in all_links:
                    if self.is_stopped():
                        break
                        
                    try:
                        href = link.get_attribute("href")
                        if not href:
                            continue
                        
                        # Build full URL
                        if href.startswith("/"):
                            full_url = urljoin(base_domain, href)
                        elif href.startswith("http"):
                            full_url = href
                        else:
                            continue
                        
                        # Lá»c chá»‰ láº¥y link thuá»™c domain hiá»‡n táº¡i
                        if base_domain not in full_url:
                            continue
                        
                        # Loáº¡i bá» cÃ¡c link khÃ´ng liÃªn quan
                        exclude_keywords = [
                            "facebook.com", "twitter.com", "linkedin.com",
                            ".pdf", ".doc", ".zip", ".jpg", ".png"
                        ]
                        
                        if any(keyword in full_url.lower() for keyword in exclude_keywords):
                            continue
                        
                        # TrÃ¡nh trÃ¹ng láº·p
                        if full_url in seen_urls:
                            continue
                        
                        # Kiá»ƒm tra vá»›i pattern
                        if pattern_regex and pattern_regex.search(full_url):
                            job_links.append(full_url)
                            seen_urls.add(full_url)
                                
                    except Exception:
                        continue
                
                browser.close()
                
                with self._print_lock:
                    logger.info(f"âœ“ TÃ¬m tháº¥y {len(job_links)} job links tá»« trang {page_num}")
                
                return job_links
                
            except Exception as e:
                with self._print_lock:
                    logger.error(f"âœ— Lá»—i khi crawl trang {page_num}: {e}")
                return []
    
    def extract_field(self, page: Page, field_name: str, selector: str) -> str | None:
        """TrÃ­ch xuáº¥t dá»¯ liá»‡u tá»« 1 field dá»±a vÃ o selector"""
        if not selector or selector == "null":
            return None
        
        try:
            element = page.locator(selector).first
            if element.count() > 0:
                text = element.inner_text().strip()
                return text if text else None
            return None
        except Exception:
            return None
    
    def crawl_job_detail(self, job_url: str) -> dict | None:
        """
        Crawl chi tiáº¿t 1 cÃ´ng viá»‡c
        Má»—i thread táº¡o Playwright instance riÃªng (FIX thread-safety)
        """
        if self.is_stopped():
            return None
        
        with sync_playwright() as p:
            try:
                browser = p.chromium.launch(headless=True)
                context = browser.new_context(
                    viewport={"width": 1920, "height": 1080},
                    user_agent="Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36"
                )
                page = context.new_page()
                
                page.goto(job_url, wait_until="domcontentloaded", timeout=60000)
                time.sleep(2)
                
                # TrÃ­ch xuáº¥t dá»¯ liá»‡u
                company_name = self.extract_field(page, "company_name", self.selectors.get("company_name"))
                job_name = self.extract_field(page, "name", self.selectors.get("name"))
                
                # Kiá»ƒm tra trÃ¹ng láº·p
                if job_name and company_name:
                    if self.check_job_exists(job_name, company_name):
                        with self._print_lock:
                            logger.info(f"   âš ï¸  Bá» qua (trÃ¹ng): {job_name}")
                        browser.close()
                        return None
                
                job_data = {
                    "job_link": job_url,
                    "name": "",
                    "salary": "",
                    "experience": "",
                    "education_level": "",
                    "location": "",
                    "position_level": "",
                    "job_type": "",
                    "quantity": "",
                    "deadline_submission": "",
                    "description": "",
                    "required": "",
                    "company_name": "",
                    "company_location": "",
                    "company_industry": "",
                    "company_scale": ""
                }
                
                # TrÃ­ch xuáº¥t tá»«ng field theo config
                fields = [
                    "name", "salary", "experience", "education_level", "location",
                    "position_level", "job_type", "quantity", "deadline_submission",
                    "description", "required",
                    "company_name", "company_location", "company_industry", "company_scale"
                ]
                
                for field_name in fields:
                    selector = self.selectors.get(field_name)
                    value = self.extract_field(page, field_name, selector)
                    job_data[field_name] = value if value else ""
                
                browser.close()
                
                with self._print_lock:
                    logger.info(f"   âœ“ ÄÃ£ crawl xong: {job_data.get('name', 'N/A')}")
                
                return job_data
                
            except Exception as e:
                with self._print_lock:
                    logger.info(f"   âœ— Lá»—i khi crawl {job_url}: {e}")
                return None
    
    def crawl_job_wrapper(self, job_url: str) -> dict | None:
        """Wrapper Ä‘á»ƒ crawl 1 job (dÃ¹ng cho threading)"""
        if self.is_stopped():
            return None
        time.sleep(1)  # TrÃ¡nh spam requests
        return self.crawl_job_detail(job_url)
    
    def crawl_jobs(self, start_page: int = 1, end_page: int = 3):
        """
        HÃ€M CHÃNH - Crawl song song vá»›i threading
        
        Args:
            start_page: Trang báº¯t Ä‘áº§u
            end_page: Trang káº¿t thÃºc
        """
        logger.info(f"\n{'='*80}")
        logger.info(f"Báº¯t Ä‘áº§u crawl tá»« {self.site_name} (song song {self.max_workers} luá»“ng)")
        logger.info(f"{'='*80}")
        
        # Reset stop event
        self._stop_event.clear()
        
        # âœ… Táº O CRAWL RECORD NGAY KHI Báº®T Äáº¦U
        try:
            self._current_crawl_id = create_crawl_record(
                self.site_name, 
                self.source_url
            )
        except Exception as e:
            logger.error(f"âœ— KhÃ´ng thá»ƒ táº¡o CrawlRecord: {e}")
            return []
        
        # Load cache trÆ°á»›c
        self._load_existing_jobs_cache()
        
        crawled_jobs = []
        
        try:
            # BÆ¯á»šC 1: Thu tháº­p links tá»« nhiá»u trang
            all_job_links = []
            for page_num in range(start_page, end_page + 1):
                if self.is_stopped():
                    logger.debug("ğŸ›‘ ÄÃ£ dá»«ng viá»‡c thu tháº­p links")
                    break
                    
                job_links = self.get_job_links_from_page(page_num)
                all_job_links.extend(job_links)
                time.sleep(1)
            
            if self.is_stopped():
                update_crawl_record(
                    self._current_crawl_id,
                    status='stopped',
                    jobs_count=len(crawled_jobs)
                )
                logger.debug(f"ğŸ›‘ Crawler Ä‘Ã£ bá»‹ dá»«ng. ÄÃ£ crawl Ä‘Æ°á»£c {len(crawled_jobs)} jobs")
                return crawled_jobs
            
            logger.info(f"\n{'='*80}")
            logger.info(f"TÃ¬m tháº¥y tá»•ng cá»™ng {len(all_job_links)} link cÃ´ng viá»‡c")
            logger.info(f"{'='*80}")
            
            # BÆ¯á»šC 2: Crawl song song vá»›i ThreadPoolExecutor
            skipped = 0
            
            with ThreadPoolExecutor(max_workers=self.max_workers) as executor:
                future_to_url = {
                    executor.submit(self.crawl_job_wrapper, link): link
                    for link in all_job_links
                }
                
                for future in as_completed(future_to_url):
                    if self.is_stopped():
                        logger.debug("ğŸ›‘ Äang há»§y cÃ¡c task Ä‘ang cháº¡y...")
                        executor.shutdown(wait=False, cancel_futures=True)
                        break
                    
                    job_data = future.result()
                    if job_data:
                        crawled_jobs.append(job_data)
                        with self._print_lock:
                            logger.info(f"Crawl thÃ nh cÃ´ng: {job_data['name']}")
                    else:
                        skipped += 1
            
            if self.is_stopped():
                update_crawl_record(
                    self._current_crawl_id,
                    status='stopped',
                    jobs_count=len(crawled_jobs)
                )
                logger.debug(f"ğŸ›‘ Crawler Ä‘Ã£ dá»«ng. ÄÃ£ crawl Ä‘Æ°á»£c {len(crawled_jobs)} jobs trÆ°á»›c khi dá»«ng")
                
                # Váº«n lÆ°u jobs Ä‘Ã£ crawl Ä‘Æ°á»£c vÃ o DB
                if crawled_jobs:
                    logger.debug(f"Äang lÆ°u {len(crawled_jobs)} cÃ´ng viá»‡c Ä‘Ã£ crawl Ä‘Æ°á»£c...")
                    save_jobs_to_db(crawled_jobs, self._current_crawl_id)
                
                return crawled_jobs
            
            logger.info(f"\nBá» qua {skipped} job trÃ¹ng láº·p")
            
            # LÆ°u vÃ o database
            if crawled_jobs:
                logger.debug(f"\nÄang lÆ°u {len(crawled_jobs)} cÃ´ng viá»‡c vÃ o database...")
                saved_count = save_jobs_to_db(crawled_jobs, self._current_crawl_id)
                
                update_crawl_record(
                    self._current_crawl_id,
                    status='success',
                    jobs_count=saved_count
                )
                
                logger.info(f"\n{'='*80}")
                logger.info(f"  HOÃ€N THÃ€NH!")
                logger.info(f"{'='*80}")
                logger.info(f"âœ“ ÄÃ£ crawl: {len(crawled_jobs)} cÃ´ng viá»‡c")
                logger.info(f"âœ“ ÄÃ£ lÆ°u vÃ o database: {saved_count} cÃ´ng viá»‡c")
            else:
                logger.info("KhÃ´ng cÃ³ cÃ´ng viá»‡c má»›i Ä‘á»ƒ lÆ°u")
                update_crawl_record(
                    self._current_crawl_id,
                    status='empty',
                    message='KhÃ´ng cÃ³ job má»›i Ä‘á»ƒ crawl',
                    jobs_count=0
                )
        
        except Exception as e:
            logger.error(f"\nâœ— Lá»—i trong quÃ¡ trÃ¬nh crawl: {str(e)}")
            
            update_crawl_record(
                self._current_crawl_id,
                status='failed',
                message=f'Lá»—i: {str(e)}',
                jobs_count=len(crawled_jobs)
            )
            
            # Váº«n cá»‘ lÆ°u jobs Ä‘Ã£ crawl Ä‘Æ°á»£c
            if crawled_jobs:
                logger.debug(f"Äang lÆ°u {len(crawled_jobs)} jobs Ä‘Ã£ crawl Ä‘Æ°á»£c trÆ°á»›c khi lá»—i...")
                save_jobs_to_db(crawled_jobs, self._current_crawl_id)
        
        return crawled_jobs

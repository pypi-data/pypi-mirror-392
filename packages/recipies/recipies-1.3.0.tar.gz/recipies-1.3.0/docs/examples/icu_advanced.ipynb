{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ICU Preprocessing Pipeline with ReciPies\n",
    "\n",
    "This notebook demonstrates a **complete end-to-end preprocessing pipeline** for ICU time-series data using ReciPies (see https://github.com/rvandewater/YAIB for more info). We'll cover:\n",
    "\n",
    "1. **Data Loading**: Loading dynamic measurements, static features, and outcomes from parquet files\n",
    "2. **Train/Test Split**: Proper group-level splitting to prevent data leakage\n",
    "3. **Multi-Step Pipeline**: \n",
    "   - Missing value imputation (forward fill + zero fill)\n",
    "   - Feature scaling (standardization)\n",
    "   - Historical feature engineering (rolling mean and max)\n",
    "   - Custom domain-specific features\n",
    "4. **Baking the Data**: Applying the preprocessing pipeline to both training and test sets\n",
    "5. **Model Training**: Using the preprocessed data to train a machine learning model\n",
    "\n",
    "The pipeline uses **Polars** for high-performance data processing, with ReciPies handling all preprocessing steps while maintaining column role information throughout the transformation pipeline.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load ICU Data\n",
    "\n",
    "We start by loading the ICU demo data, which consists of three components:\n",
    "- **Dynamic data**: Time-varying measurements (vitals, lab values) recorded at regular intervals\n",
    "- **Static data**: Patient-level features that don't change over time (age, sex, height, weight)\n",
    "- **Outcome data**: The target variable we want to predict (mortality at 24 hours)\n",
    "\n",
    "Let's examine the structure of each dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import polars as pl\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "from recipies import Ingredients, Recipe\n",
    "from recipies.selector import all_predictors, all_numeric_predictors, has_role, has_type, all_of\n",
    "from recipies.step import StepImputeFill, StepHistorical, StepScale, StepFunction, Accumulator, StepSklearn\n",
    "\n",
    "dynamic_data = pl.read_parquet(\"../../examples/icu_demo_data/mortality24/eicu_demo/dyn.parquet\")\n",
    "static_data = pl.read_parquet(\"../../examples/icu_demo_data/mortality24/eicu_demo/sta.parquet\")\n",
    "outcome = pl.read_parquet(\"../../examples/icu_demo_data/mortality24/eicu_demo/outc.parquet\")\n",
    "print(\"Columns:\")\n",
    "print(f\"dynamic: {dynamic_data.columns}\")\n",
    "print(f\"static: {static_data.columns}\")\n",
    "print(f\"outcome: {outcome.columns}\")\n",
    "print(\"Shapes:\")\n",
    "print(f\"dynamic: {dynamic_data.shape}\")\n",
    "print(f\"static: {static_data.shape}\")\n",
    "print(f\"outcome: {outcome.shape}\")\n",
    "print(\"Heads:\")\n",
    "print(dynamic_data.head())\n",
    "print(static_data.head())\n",
    "print(outcome.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Train/Test Split\n",
    "\n",
    "**Critical**: We perform a **group-level split** at the `stay_id` level. This ensures that all records for a given patient stay are assigned to either the training or test set, preventing data leakage where information from the test set could leak into the training process.\n",
    "\n",
    "We use an 80/20 split stratified by `stay_id`:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train/test split at the stay_id level (group-level split)\n",
    "# This ensures all records for a given stay go to either train or test\n",
    "\n",
    "# Get unique stay_ids\n",
    "unique_stays = outcome.select(\"stay_id\").unique().sample(fraction=1.0, seed=42)\n",
    "n_train = int(len(unique_stays) * 0.8)\n",
    "train_stay_ids = unique_stays.head(n_train)[\"stay_id\"].to_list()\n",
    "test_stay_ids = unique_stays.tail(len(unique_stays) - n_train)[\"stay_id\"].to_list()\n",
    "\n",
    "# Split dynamic, static, and outcome data\n",
    "dynamic_train = dynamic_data.filter(pl.col(\"stay_id\").is_in(train_stay_ids))\n",
    "dynamic_test = dynamic_data.filter(pl.col(\"stay_id\").is_in(test_stay_ids))\n",
    "\n",
    "static_train = static_data.filter(pl.col(\"stay_id\").is_in(train_stay_ids))\n",
    "static_test = static_data.filter(pl.col(\"stay_id\").is_in(test_stay_ids))\n",
    "\n",
    "outcome_train = outcome.filter(pl.col(\"stay_id\").is_in(train_stay_ids))\n",
    "outcome_test = outcome.filter(pl.col(\"stay_id\").is_in(test_stay_ids))\n",
    "\n",
    "# Join train data\n",
    "df_train = dynamic_train.join(static_train, on=\"stay_id\", how=\"left\")\n",
    "df_train = df_train.join(outcome_train.select([\"stay_id\", \"label\"]), on=\"stay_id\", how=\"left\")\n",
    "\n",
    "# Join test data\n",
    "df_test = dynamic_test.join(static_test, on=\"stay_id\", how=\"left\")\n",
    "df_test = df_test.join(outcome_test.select([\"stay_id\", \"label\"]), on=\"stay_id\", how=\"left\")\n",
    "\n",
    "print(f\"Train: {len(df_train)} rows, {len(train_stay_ids)} stays\")\n",
    "print(f\"Test: {len(df_test)} rows, {len(test_stay_ids)} stays\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick check: verify we have the expected columns after joining\n",
    "print(f\"Train dataframe columns: {len(df_train.columns)}\")\n",
    "print(f\"Test dataframe columns: {len(df_test.columns)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Build Preprocessing Pipeline\n",
    "\n",
    "Now we'll create a comprehensive preprocessing pipeline using ReciPies. The pipeline includes:\n",
    "\n",
    "1. **Role Assignment**: Define which columns are outcomes, predictors, groups (`stay_id`), and sequences (`time`)\n",
    "2. **Imputation**: Forward fill followed by zero fill for any remaining missing values\n",
    "3. **Feature Scaling**: Standardize numeric predictors (mean=0, std=1)\n",
    "4. **Historical Features**: Create rolling mean and max aggregations over time within each stay\n",
    "5. **Custom Features**: Add domain-specific features like heart rate to temperature ratio\n",
    "\n",
    "The key advantage of ReciPies is that all transformations maintain column role information, ensuring proper handling of grouped time-series data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Ingredients\n",
    "ing = Ingredients(df_train)\n",
    "\n",
    "# Define and build the recipe\n",
    "rec = Recipe(\n",
    "    ing,\n",
    "    outcomes=[\"label\"],\n",
    "    predictors=[c for c in ing.columns if c not in {\"label\", \"stay_id\", \"time\"}],\n",
    "    groups=[\"stay_id\"],\n",
    "    sequences=[\"time\"],\n",
    ")\n",
    "\n",
    "# Impute missing values forward (pre-resample)\n",
    "rec.add_step(StepImputeFill(sel=all_predictors(), strategy=\"forward\"))\n",
    "rec.add_step(StepImputeFill(sel=all_predictors(), strategy=\"zero\"))\n",
    "\n",
    "# Scale numeric predictors at the end (after imputation)\n",
    "rec.add_step(StepScale(sel=all_numeric_predictors(), with_mean=True, with_std=True))\n",
    "\n",
    "\n",
    "#  Add a custom domain feature (example: hr/temp ratio) via StepFunction\n",
    "def add_custom_features(ingr: Ingredients) -> Ingredients:\n",
    "    df_ = ingr.get_df()\n",
    "    if all(col in df_.columns for col in [\"hr\", \"temp\"]):\n",
    "        df_ = df_.with_columns((pl.col(\"hr\") / pl.col(\"temp\")).alias(\"hr_temp_ratio\"))\n",
    "        ingr.set_df(df_)\n",
    "        ingr.update_role(\"hr_temp_ratio\", \"predictor\")\n",
    "    return ingr\n",
    "\n",
    "\n",
    "rec.add_step(StepFunction(sel=has_role([\"predictor\"]), function=add_custom_features))\n",
    "\n",
    "# Label encode categorical features\n",
    "types = [\"String\", \"Object\", \"Categorical\"]\n",
    "rec.add_step(StepSklearn(SimpleImputer(missing_values=np.nan, strategy=\"most_frequent\"), sel=has_type(types)))\n",
    "rec.add_step(StepSklearn(LabelEncoder(), sel=has_type(types), columnwise=True))\n",
    "\n",
    "original_predictors = all_of(\n",
    "    list(all_numeric_predictors()(ing))\n",
    ")  # Capture the fixed list of original numeric predictors\n",
    "# Historical features\n",
    "rec.add_step(StepHistorical(sel=original_predictors, fun=Accumulator.MEAN, suffix=\"_mean_hist\"))\n",
    "rec.add_step(StepHistorical(sel=original_predictors, fun=Accumulator.MIN, suffix=\"_min_hist\"))\n",
    "rec.add_step(StepHistorical(sel=original_predictors, fun=Accumulator.MAX, suffix=\"_max_hist\"))\n",
    "rec.add_step(StepHistorical(sel=original_predictors, fun=Accumulator.VAR, suffix=\"_var_hist\"))\n",
    "\n",
    "# Prep and bake (fit and transform) the training data\n",
    "train_baked = rec.prep()\n",
    "train_baked.head()\n",
    "print(train_baked.columns)\n",
    "print(len(train_baked.columns))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Apply Pipeline to Test Data\n",
    "\n",
    "Once the recipe is fitted on the training data using `prep()`, we can apply the same transformations to the test data using `bake()`. This ensures:\n",
    "\n",
    "- **No data leakage**: Test data statistics are never used to fit the pipeline\n",
    "- **Consistent transformations**: The same preprocessing steps are applied identically to both datasets\n",
    "- **Reproducibility**: The fitted recipe can be saved and reused on new data\n",
    "\n",
    "The `bake()` method applies all fitted transformations without refitting, ensuring the test set is processed identically to how the training set was processed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_baked = rec.bake(df_test)\n",
    "print(test_baked.head())\n",
    "print(test_baked.columns)\n",
    "print(len(test_baked.columns))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Train a Machine Learning Model\n",
    "\n",
    "With our preprocessed data ready, we can now train a machine learning model. The preprocessed dataframes contain:\n",
    "- All original features (scaled and imputed)\n",
    "- Historical aggregated features \n",
    "- One-hot encoded categorical variables\n",
    "- Custom domain features (e.g., hr/temp ratio)\n",
    "\n",
    "For demonstration, we'll use a simple logistic regression model, but you can use any scikit-learn compatible model or more advanced methods like XGBoost, LightGBM, or neural networks.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import roc_auc_score, classification_report\n",
    "import numpy as np\n",
    "\n",
    "# Extract features and labels\n",
    "# Exclude outcome, group, and sequence columns from features\n",
    "feature_cols = [c for c in train_baked.columns if c not in [\"label\", \"stay_id\", \"time\"]]\n",
    "\n",
    "X_train = train_baked.select(feature_cols).to_numpy()\n",
    "y_train = train_baked.select(\"label\").to_numpy().ravel()\n",
    "\n",
    "X_test = test_baked.select(feature_cols).to_numpy()\n",
    "y_test = test_baked.select(\"label\").to_numpy().ravel()\n",
    "\n",
    "# Handle any remaining NaN values (should be minimal after preprocessing)\n",
    "X_train = np.nan_to_num(X_train, nan=0.0)\n",
    "X_test = np.nan_to_num(X_test, nan=0.0)\n",
    "\n",
    "print(f\"Training set: {X_train.shape[0]} samples, {X_train.shape[1]} features\")\n",
    "print(f\"Test set: {X_test.shape[0]} samples, {X_test.shape[1]} features\")\n",
    "print(f\"Class distribution (train): {np.bincount(y_train)}\")\n",
    "print(f\"Class distribution (test): {np.bincount(y_test)}\")\n",
    "\n",
    "# Train model\n",
    "model = LogisticRegression(max_iter=1000, random_state=42, class_weight=\"balanced\")\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predictions\n",
    "y_train_pred = model.predict_proba(X_train)[:, 1]\n",
    "y_test_pred = model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Evaluate\n",
    "train_auc = roc_auc_score(y_train, y_train_pred)\n",
    "test_auc = roc_auc_score(y_test, y_test_pred)\n",
    "\n",
    "print(\"\\nModel Performance:\")\n",
    "print(f\"Train AUC: {train_auc:.4f}\")\n",
    "print(f\"Test AUC: {test_auc:.4f}\")\n",
    "\n",
    "print(\"\\nClassification Report (Test Set):\")\n",
    "print(classification_report(y_test, model.predict(X_test), target_names=[\"No Mortality\", \"Mortality\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "recipys_polars",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

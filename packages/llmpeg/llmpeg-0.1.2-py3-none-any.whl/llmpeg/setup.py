"""
Setup script for configuring llmpeg API endpoints.
"""

from __future__ import annotations

import json
import os
from pathlib import Path
from typing import Optional

import typer
from rich.console import Console
from rich.prompt import Prompt, Confirm

from .config import ModelConfig

app = typer.Typer(help="Setup llmpeg API configuration.", add_completion=False)
console = Console()

CONFIG_DIR = Path.home() / ".llmpeg"
CONFIG_FILE = CONFIG_DIR / "config.json"
CONFIG_PY_FILE = CONFIG_DIR / "config.py"


def _ensure_config_dir() -> Path:
    """Ensure config directory exists."""
    CONFIG_DIR.mkdir(parents=True, exist_ok=True)
    return CONFIG_DIR


def _load_config() -> dict:
    """Load existing config if available."""
    if CONFIG_FILE.exists():
        try:
            with open(CONFIG_FILE, "r") as f:
                return json.load(f)
        except Exception:
            return {}
    return {}


def _save_config(config: dict) -> None:
    """Save config to JSON file."""
    _ensure_config_dir()
    with open(CONFIG_FILE, "w") as f:
        json.dump(config, f, indent=2)
    console.print(f"[green]Configuration saved to {CONFIG_FILE}[/green]")


def _generate_config_py(config: dict, output_path: Path) -> None:
    """Generate a Python config.py file from configuration dict."""
    # Use absolute path for workdir to ensure it's static
    workdir_path = Path.home() / ".llmpeg" / "workdir"
    
    config_py_content = '''"""
llmpeg Configuration
Generated by 'llmpeg setup init'
This file is located at: {config_path}
"""

from pathlib import Path
from llmpeg.config import ModelConfig, RuntimeConfig, CLIOptions


# Model Configuration
model_config = ModelConfig(
    provider="{provider}",
    api_key="{api_key}",
    base_url="{base_url}",
    model_name="{model_name}",
    max_output_tokens={max_output_tokens},
    temperature={temperature},
    top_p={top_p},
)

# Runtime Configuration
runtime_config = RuntimeConfig(
    workdir=Path("{workdir}"),
    dry_run=False,
    verbose=False,
    confirm=False,
)

# CLI Options
options = CLIOptions(
    model=model_config,
    runtime=runtime_config,
    log_json=False,
    show_reasoning=True,
)
'''.format(
        config_path=str(output_path),
        provider=config.get("provider", "openrouter"),
        api_key=config.get("api_key", ""),
        base_url=config.get("base_url", ""),
        model_name=config.get("model_name", "gpt-oss:20b"),
        max_output_tokens=config.get("max_output_tokens", 2048),
        temperature=config.get("temperature", 0.2),
        top_p=config.get("top_p", 0.95),
        workdir=str(workdir_path),
    )
    
    output_path.parent.mkdir(parents=True, exist_ok=True)
    with open(output_path, "w") as f:
        f.write(config_py_content)
    console.print(f"[green]Python config file generated: {output_path}[/green]")


@app.command()
def init(
    provider: Optional[str] = typer.Option(
        None,
        "--provider",
        "-p",
        help="Provider: openrouter, ollama, or custom",
    ),
    api_key: Optional[str] = typer.Option(
        None,
        "--api-key",
        "-k",
        help="API key (required for OpenRouter)",
    ),
    model: Optional[str] = typer.Option(
        None,
        "--model",
        "-m",
        help="Model name (ignored for Ollama, always uses 'gpt-oss:20b')",
    ),
    base_url: Optional[str] = typer.Option(
        None,
        "--base-url",
        "-u",
        help="Base URL (for custom endpoints)",
    ),
    output: Optional[str] = typer.Option(
        None,
        "--output",
        "-o",
        help=f"Output path for config.py (default: {CONFIG_PY_FILE})",
    ),
    json_only: bool = typer.Option(
        False,
        "--json-only",
        help="Only save JSON config, don't generate config.py",
    ),
) -> None:
    """Initialize llmpeg with API configuration."""
    try:
        console.print("[bold cyan]llmpeg API Setup[/bold cyan]\n")
        
        # Check if all required params are provided (non-interactive mode)
        # For custom provider, base_url is also required
        if provider == "custom":
            non_interactive = bool(provider and model and base_url)
        else:
            non_interactive = bool(provider and model)
        
        # Determine provider
        if not provider:
            provider = Prompt.ask(
                "Select provider",
                choices=["openrouter", "ollama", "custom"],
                default="openrouter",
            )
        
        config: dict = {"provider": provider}
        
        if provider == "openrouter":
            if not non_interactive:
                console.print("\n[bold]OpenRouter Configuration[/bold]")
                console.print("Get your API key from: https://openrouter.ai/keys\n")
            
            if not api_key:
                if non_interactive:
                    # Try environment variable
                    api_key = os.getenv("OPENROUTER_API_KEY") or os.getenv("LLMPEG_API_KEY", "")
                if not api_key:
                    api_key = Prompt.ask(
                        "Enter your OpenRouter API key",
                        password=True,
                    )
            
            if not api_key:
                console.print("[red]API key is required for OpenRouter[/red]")
                raise typer.Exit(1)
            
            config["api_key"] = api_key
            config["base_url"] = "https://openrouter.ai/api/v1"
            
            if not model:
                model = Prompt.ask(
                    "Model name",
                    default="gpt-oss:20b",
                )
            config["model_name"] = model
            
        elif provider == "ollama":
            if not non_interactive:
                console.print("\n[bold]Ollama Configuration[/bold]")
                console.print("Make sure Ollama is running locally (default: http://localhost:11434)\n")
            
            config["base_url"] = "http://localhost:11434/v1"
            config["api_key"] = api_key or ""  # Ollama doesn't require API key by default
            
            # Always use gpt-oss:20b for Ollama
            config["model_name"] = "gpt-oss:20b"
            if model and model != "gpt-oss:20b":
                console.print(f"[yellow]Note: Model '{model}' was specified but Ollama provider always uses 'gpt-oss:20b'[/yellow]")
            
            # Optional: ask for API key if they have one set up (only in interactive mode)
            if not api_key and not non_interactive:
                use_key = Confirm.ask("Do you have an API key configured?", default=False)
                if use_key:
                    api_key = Prompt.ask("Enter API key", password=True)
                    if api_key:
                        config["api_key"] = api_key
            
        else:  # custom
            if not non_interactive:
                console.print("\n[bold]Custom Endpoint Configuration[/bold]\n")
            
            if not base_url:
                base_url = Prompt.ask(
                    "Base URL (e.g., 'https://api.example.com/v1')",
                )
            
            if not base_url:
                console.print("[red]Base URL is required for custom endpoints[/red]")
                raise typer.Exit(1)
            
            config["base_url"] = base_url.rstrip("/")
            
            if not api_key:
                if non_interactive:
                    api_key = os.getenv("LLMPEG_API_KEY", "")
                else:
                    api_key = Prompt.ask(
                        "API key (optional, press Enter to skip)",
                        default="",
                    )
            config["api_key"] = api_key or ""
            
            if not model:
                model = Prompt.ask("Model name")
            config["model_name"] = model
        
        # Optional: temperature and top_p (skip in non-interactive mode)
        if not non_interactive and Confirm.ask("\nConfigure advanced settings?", default=False):
            try:
                temp = Prompt.ask("Temperature", default="0.2")
                config["temperature"] = float(temp)
            except ValueError:
                console.print("[yellow]Invalid temperature, using default[/yellow]")
            
            try:
                top_p = Prompt.ask("Top-p", default="0.95")
                config["top_p"] = float(top_p)
            except ValueError:
                console.print("[yellow]Invalid top-p, using default[/yellow]")
        
        # Set defaults for missing values
        config.setdefault("max_output_tokens", 2048)
        config.setdefault("temperature", 0.2)
        config.setdefault("top_p", 0.95)
        
        # Save JSON config
        _save_config(config)
        
        # Generate Python config.py file
        if not json_only:
            if output:
                output_path = Path(output).expanduser().resolve()
            else:
                output_path = CONFIG_PY_FILE
            _generate_config_py(config, output_path)
            console.print(f"\n[dim]Config file location:[/dim] {output_path}")
            if output_path == CONFIG_PY_FILE:
                console.print(f"[dim]This config will be automatically loaded by llmpeg[/dim]")
            else:
                console.print(f"[dim]To use this config, import it:[/dim]")
                console.print(f"  from {output_path.stem} import options")
        
        # Show summary
        console.print("\n[bold green]Configuration Summary:[/bold green]")
        console.print(f"  Provider: {config['provider']}")
        console.print(f"  Base URL: {config['base_url']}")
        console.print(f"  Model: {config['model_name']}")
        if config.get("api_key"):
            console.print(f"  API Key: {'*' * 20}")
        console.print("\n[dim]You can override these with environment variables:[/dim]")
        console.print("  LLMPEG_PROVIDER, LLMPEG_BASE_URL, LLMPEG_MODEL_NAME, LLMPEG_API_KEY")
    
    except KeyboardInterrupt:
        console.print("\n[yellow]Setup interrupted by user. No configuration was saved.[/yellow]")
        raise typer.Exit(130)  # Standard exit code for Ctrl+C
    except Exception as exc:
        console.print(f"\n[red]Error during setup: {exc}[/red]")
        raise typer.Exit(1)


@app.command()
def show() -> None:
    """Show current configuration."""
    config = _load_config()
    if not config:
        console.print("[yellow]No configuration found. Run 'llmpeg setup init' first.[/yellow]")
        return
    
    console.print("[bold cyan]Current Configuration:[/bold cyan]\n")
    for key, value in config.items():
        if key == "api_key" and value:
            console.print(f"  {key}: {'*' * 20}")
        else:
            console.print(f"  {key}: {value}")


@app.command()
def test() -> None:
    """Test the API connection."""
    config = _load_config()
    if not config:
        console.print("[yellow]No configuration found. Run 'llmpeg setup init' first.[/yellow]")
        raise typer.Exit(1)
    
    # Create a temporary ModelConfig from saved config
    model_config = ModelConfig(**config)
    
    # Import here to avoid circular imports
    from .llm import LLMClient
    
    console.print("[cyan]Testing API connection...[/cyan]\n")
    
    try:
        client = LLMClient(model_config)
        response = client.complete([
            {"role": "user", "content": "Say 'Hello' if you can hear me."}
        ])
        console.print(f"[green]✓ Connection successful![/green]")
        console.print(f"[dim]Response:[/dim] {response[:100]}")
    except Exception as exc:
        console.print(f"[red]✗ Connection failed:[/red] {exc}")
        raise typer.Exit(1)


if __name__ == "__main__":
    app()


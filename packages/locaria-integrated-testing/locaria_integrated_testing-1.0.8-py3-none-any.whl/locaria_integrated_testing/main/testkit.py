"""
Core testing framework that orchestrates test execution, logging, and alerting.
Integrates with Sheet Logger and Email Manager for comprehensive test result tracking.
"""

import os
import json
import requests
import uuid
from datetime import datetime, timezone
from typing import Dict, Any, Optional, List
from sheet_logger import SheetLogger
from google.cloud import firestore

from .acknowledge_manager import AcknowledgeManager

class TestFailureException(Exception):
    """Custom exception raised when a test fails and fail_on_error is True."""
    pass

class TestKit:
    """
    Core testing framework that orchestrates test execution, logging, and alerting.
    
    Features:
    - Sheet Logger integration for persistent test result storage
    - Email Manager integration for real-time failure notifications
    - Firestore configuration management
    - Test result aggregation and reporting
    - Configurable thresholds and test switches
    """
    
    # Class-level cache for shared resources
    _shared_sheet_logger = None
    _shared_firestore_client = None
    _shared_config = None
    
    def __init__(self, repository_name: str, pipeline_name: str = None, fail_on_error: bool = False, 
                 sheet_logger = None, firestore_project_id: str = None, 
                 spreadsheet_id: str = None, credentials_path: str = None):
        """
        Initialize the TestKit with repository and pipeline information.
        
        Args:
            repository_name: Name of the repository (e.g., 'locate_2_pulls')
            pipeline_name: Name of the specific pipeline (e.g., 'daily_updates')
            fail_on_error: If True, pipeline will stop on test failures. If False, continues with alerts.
            sheet_logger: Optional SheetLogger instance for test result persistence.
            firestore_project_id: Optional GCP project ID for Firestore configuration management.
            spreadsheet_id: Optional Google Sheets spreadsheet ID for test result logging.
            credentials_path: Optional path to Google credentials JSON file for SheetLogger initialization.
        """
        self.repository_name = repository_name
        self.pipeline_name = pipeline_name or "unknown"
        self.fail_on_error = fail_on_error
        self.run_id = self._generate_run_id()
        self.test_results = []
        self.warnings = []
        self.failures = []
        self.failure_emails_sent = False  # Track if failure email has been sent
        self.start_time = datetime.now(timezone.utc)
        self.collection_integrated_testing_config = "integrated_testing_config"
        
        # Store provided parameters
        self._sheet_logger = sheet_logger
        self._firestore_project_id = firestore_project_id
        self._spreadsheet_id = spreadsheet_id
        self._credentials_path = credentials_path
        
        # Initialize sheet logger (use shared config store)
        self.sheet_logger = self._init_sheet_logger()
        
        # Initialize Firestore client for configuration (use shared config store)
        self.firestore_client = self._init_firestore_client()
        
        # Load configuration from Firestore (use shared config store)
        self.config = self._load_config_from_firestore()
        
        # Initialize acknowledgment manager with config (for collection names etc.)
        self.acknowledge_manager = AcknowledgeManager(
            self.firestore_client,
            self.repository_name,
            self.pipeline_name,
            config=self.config,
        )
        
        # Log test run start
        self._log_run_start()
    
    
    def _generate_run_id(self) -> str:
        """Generate unique run identifier with timestamp and UUID."""
        timestamp = datetime.now(timezone.utc).strftime("%Y%m%d_%H%M%S")
        unique_id = str(uuid.uuid4())[:8]
        return f"{self.repository_name}_{self.pipeline_name}_{timestamp}_{unique_id}"
    
    def _init_sheet_logger(self) -> Optional[SheetLogger]:
        """Initialize sheet logger for test result persistence."""
        # Use cached sheet logger if available
        if TestKit._shared_sheet_logger is not None:
            return TestKit._shared_sheet_logger
            
        # First priority: Use provided sheet_logger
        if self._sheet_logger is not None:
            TestKit._shared_sheet_logger = self._sheet_logger
            print("Using provided SheetLogger instance")
            return self._sheet_logger
            
        try:
            # Check if sheet_logger is available
            if not SheetLogger:
                print("Warning: SheetLogger not available. Test results will not be persisted to sheets.")
                return None
            
            # Use provided parameters or fallback to environment variables
            spreadsheet_id = self._spreadsheet_id or os.getenv('TEST_LOGS_SPREADSHEET_ID')
            if not spreadsheet_id:
                print("Warning: spreadsheet_id not provided and TEST_LOGS_SPREADSHEET_ID not set. Test results will not be persisted to sheets.")
                return None
            
            # Get credentials path
            credentials_path = self._credentials_path or os.getenv('GOOGLE_CREDENTIALS_PATH')
            if not credentials_path:
                print("Warning: credentials_path not provided and GOOGLE_CREDENTIALS_PATH not set. Test results will not be persisted to sheets.")
                return None
            
            scopes = ["https://www.googleapis.com/auth/spreadsheets"]
            
            # Initialize SheetLogger with provided parameters or environment variables
            sheet_logger = SheetLogger(
                spreadsheet_id=spreadsheet_id,
                scopes=scopes,
                batch_size=10,  # Batch size for efficient API usage
                token_full_path=credentials_path,
                timezone='UTC'  # Use UTC for consistency
            )
            
            print(f"SheetLogger initialized from environment for spreadsheet: {spreadsheet_id}")
            TestKit._shared_sheet_logger = sheet_logger
            return sheet_logger
            
        except Exception as e:
            print(f"Warning: Could not initialize SheetLogger: {e}")
            return None
    
    def _init_firestore_client(self) -> Optional[firestore.Client]:
        """Initialize Firestore client for configuration management."""
        # Use cached Firestore client if available
        if TestKit._shared_firestore_client is not None:
            return TestKit._shared_firestore_client
            
        try:
            # Use provided firestore_project_id or fallback
            if self._firestore_project_id:
                project_id = self._firestore_project_id
                print(f"Using provided Firestore project ID: {project_id}")
            else:
                # Fallback to hardcoded project ID
                project_id = "locaria-dev-config-store"
                print(f"Warning: No firestore_project_id provided. Using fallback project ID: {project_id}")
            
            # Initialize Firestore client
            firestore_client = firestore.Client(project=project_id)
            print(f"Firestore client initialized for project: {project_id}")
            
            # Cache the Firestore client for future use
            TestKit._shared_firestore_client = firestore_client
            return firestore_client
            
        except Exception as e:
            print(f"Warning: Could not initialize Firestore client: {e}")
            return None
    
    def _load_config_from_firestore(self) -> Dict[str, Any]:
        """Load test configuration from Firestore using ConfigManager."""
        try:
            # Import ConfigManager here to avoid circular imports
            from ..utils.config_manager import ConfigManager
            
            # Initialize ConfigManager
            config_manager = ConfigManager(project_id=self._firestore_project_id)
            
            # Load repository-specific configuration (all at once)
            config = config_manager.get_repository_config(self.repository_name)
            
            if config:
                print(f"Loaded configuration for repository: {self.repository_name}")
                return config
            else:
                print(f"No configuration found for repository: {self.repository_name}. Using default configuration.")
                return config_manager.get_default_config()
                
        except Exception as e:
            print(f"Warning: Could not load config from Firestore: {e}")
            # Fallback to default config
            try:
                from ..utils.config_manager import ConfigManager
                config_manager = ConfigManager(project_id=self._firestore_project_id)
                return config_manager.get_default_config()
            except:
                return self._get_default_config()
    
    def _get_default_config(self) -> Dict[str, Any]:
        """Get default configuration when Firestore is not available."""
        return {
            'thresholds': {
                'row_count_change': {
                    'warn_percentage': 20,
                    'fail_percentage': 50
                },
                'out_of_office_percentage': {
                    'warn_threshold': 25,
                    'fail_threshold': 35
                },
                'time_split_tolerance': {
                    'precision': 0.01  # 1% tolerance for time splits
                },
                'data_freshness': {
                    'max_age_hours': 24,
                    'warn_age_hours': 12
                },
                'financial_data': {
                    'variance_threshold': 0.15,
                    'min_amount_threshold': 0.01
                },
                'api_response': {
                    'max_response_time_seconds': 30,
                    'min_success_rate': 0.95
                }
            },
            'test_switches': {
                'enable_schema_validation': True,
                'enable_business_logic_checks': True,
                'enable_freshness_checks': True,
                'enable_row_count_validation': True,
                'enable_api_health_checks': True
            },
            'pipeline_overrides': {},
            'email_alerts': {
                'failure_recipients': ['data_team@locaria.com'],
                'warning_recipients': ['data_team@locaria.com'],
                'digest_frequency': 'daily'
            },
            'logging': {
                'log_level': 'INFO',
                'max_log_entries': 1000,
                'log_retention_days': 30
            }
        }
    
    def _log_run_start(self):
        """Log the start of a test run."""
        start_message = f"Test run started - Repository: {self.repository_name}, Pipeline: {self.pipeline_name}, Run ID: {self.run_id}"
        print(f"üöÄ {start_message}")
        
        if self.sheet_logger:
            try:
                self.sheet_logger.write_prints_to_sheet("TestLogs", start_message, flush=True)
            except Exception as e:
                print(f"Warning: Could not log run start to sheet: {e}")
    
    def log_pass(self, test_name: str, message: str = "", metrics: Dict[str, Any] = None):
        """
        Log a passing test result.
        
        Args:
            test_name: Name of the test that passed
            message: Optional message describing the test result
            metrics: Optional dictionary of metrics/measurements
        """
        result = {
            'run_id': self.run_id,
            'repository': self.repository_name,
            'pipeline': self.pipeline_name,
            'test_name': test_name,
            'status': 'PASS',
            'message': message,
            'metrics': metrics or {},
            'timestamp': datetime.now(timezone.utc).isoformat()
        }
        
        self.test_results.append(result)
        # self._log_to_sheet(result) # logging all the passed tests clutters the logs
        print(f"‚úÖ PASS: {test_name} - {message}")
    
    def log_warn(
        self, 
        test_name: str, 
        issue_identifier: str,
        message: str, 
        metrics: Dict[str, Any] = None, 
        acknowledgeable: bool = True,
    ):
        """
        Log a warning test result.
        
        Args:
            test_name: Name of the test that generated a warning
            issue_identifier: Unique identifier for the issue (e.g., email, ID).
                It's a good practice to use the `{filename}-{function_name}` format for uniqueness.
            message: Warning message describing the issue
            metrics:  Dictionary of metrics/measurements. Required to populate the page in the Analytics Hub.
            acknowledgeable: If True, this warning can be acknowledged and muted
        """
        result = {
            'run_id': self.run_id,
            'repository': self.repository_name,
            'pipeline': self.pipeline_name,
            'test_name': test_name,
            'issue_identifier': issue_identifier,
            'status': 'WARN',
            'message': message,
            'metrics': metrics or {},
            'timestamp': datetime.now(timezone.utc).isoformat(),
            'acknowledgeable': acknowledgeable
        }
        
        self.warnings.append(result)
        self.test_results.append(result)
        self._log_to_sheet(result)
        
        print(f"‚ö†Ô∏è  WARN: {test_name} - {message}")
    
    def log_fail(
        self, 
        test_name: str, 
        issue_identifier: str,
        message: str, 
        metrics: Dict[str, Any] = None, 
        stop_pipeline: bool = None, 
        acknowledgeable: bool = True
    ):
        """
        Log a failing test result and trigger alerts.
        
        Args:
            test_name: Name of the test that failed.
            issue_identifier: Unique identifier for the issue (e.g., email, ID).
                It's a good practice to use the `{filename}-{function_name}` format for uniqueness.
            message: Failure message describing the issue
            metrics: Dictionary of metrics/measurements. Required to populate the page in the Analytics Hub.
            stop_pipeline: If True, send email and stop pipeline. If False, send email and continue.
                          If None, uses the fail_on_error setting from TestKit initialization.
            acknowledgeable: If True, this failure can be acknowledged and muted
            
        Raises:
            TestFailureException: If stop_pipeline is True, raises exception to stop pipeline
        """
        # Determine if we should stop the pipeline
        should_stop = stop_pipeline if stop_pipeline is not None else self.fail_on_error
        
        result = {
            'run_id': self.run_id,
            'repository': self.repository_name,
            'pipeline': self.pipeline_name,
            'test_name': test_name,
            'issue_identifier': issue_identifier,
            'status': 'FAIL',
            'message': message,
            'metrics': metrics or {},
            'timestamp': datetime.now(timezone.utc).isoformat(),
            'acknowledgeable': acknowledgeable
        }
        
        self.failures.append(result)
        self.test_results.append(result)
        self._log_to_sheet(result)
        
        print(f"‚ùå FAIL: {test_name} - {message}")
        
        # If stop_pipeline=True, send summary email with all failures and stop
        if should_stop:
            self._send_failure_summary_email()
            raise TestFailureException(f"Pipeline stopped due to test failure: {test_name} - {message}")
           
    def _log_to_sheet(self, result: Dict[str, Any]):
        """Log test result to Google Sheets via SheetLogger."""
        if not self.sheet_logger:
            return
            
        try:
            # Format log entry for sheet
            log_entry = f"Test: {result['test_name']} | Status: {result['status']} | Message: {result['message']}"
            if result['metrics']:
                metrics_str = json.dumps(result['metrics'], separators=(',', ':'))
                log_entry += f" | Metrics: {metrics_str}"
            
            # Write to sheet
            self.sheet_logger.write_prints_to_sheet("TestLogs", log_entry)
        except Exception as e:
            print(f"Warning: Could not log to sheet: {e}")
        finally:
            self.sheet_logger.release_remaining_logs()
    
    def _send_failure_summary_email(self):
        """Send summary email with all failures, filtered by acknowledgment status."""
        if not self.failures or self.failure_emails_sent:
            return
        
        # Filter failures into new vs acknowledged issues
        filtered_results = self.acknowledge_manager.filter_acknowledged_issues(self.failures)
        
        if not filtered_results['new_issues']:
            print("üìß All failure issues are acknowledged and muted - skipping email")
            return
            
        try:
            # Get email API URL from centralized config
            email_api_url = self.config.get('api_config', {}).get('email_api_url')
            
            if not email_api_url:
                # Fallback to environment variable
                email_api_url = os.getenv('EMAIL_API_URL')
                
            if not email_api_url:
                return
                
            # Prepare failure summary with acknowledgment context
            failure_summary = []
            
            # Add new issues
            if filtered_results['new_issues']:
                failure_summary.append("üö® NEW ISSUES DETECTED:")
                for i, failure in enumerate(filtered_results['new_issues'], 1):
                    failure_summary.append(f"{i}. {failure['test_name']}<br/>   Message: {failure['message']}<br/>   Time: {failure['timestamp']}")
            
            # Add acknowledged issues context
            if filtered_results['acknowledged_issues']:
                if failure_summary:
                    failure_summary.append("")  # Add spacing
                failure_summary.append("‚úÖ PREVIOUSLY ACKNOWLEDGED (muted):")
                for i, failure in enumerate(filtered_results['acknowledged_issues'], 1):
                    metrics = failure.get('metrics', {})
                    # Try multiple field names for backward compatibility
                    person_name = metrics.get('person_name') or metrics.get('issue_name', 'Unknown')
                    person_email = metrics.get('person_email') or metrics.get('issue_identifier', 'Unknown')
                    failure_summary.append(f"{i}. {failure['test_name']} - {person_name} ({person_email}) - Acknowledged and muted")
            
            failure_summary_text = "<br/><br/>".join(failure_summary)
            
            payload = {
                "task_name": self.config.get('api_config', {}).get('email_template_failure', 'Test Failure Alert'),
                "custom_variables": {
                    "repository_name": self.repository_name,
                    "pipeline_name": self.pipeline_name,
                    "run_id": self.run_id,
                    "failure_count": len(filtered_results['new_issues']),
                    "failure_summary": failure_summary_text,
                    "run_duration": self._get_run_duration(),
                    "timestamp": datetime.now(timezone.utc).strftime("%Y-%m-%d %H:%M:%S UTC")
                }
            }
            
            response = requests.post(email_api_url, json=payload, timeout=30)
            
            if response.status_code == 200:
                result = response.json()
                if result.get('success'):
                    print(f"üìß Failure summary email sent successfully")
                    self.failure_emails_sent = True
                else:
                    print(f"Warning: Failure summary email failed: {result.get('message')}")
            else:
                print(f"Warning: Failure summary email failed with status {response.status_code}: {response.text}")
                
        except Exception as e:
            print(f"Warning: Could not send failure summary email: {e}")
    
    def send_warning_digest(self):
        """Send digest of all warnings at the end of a run, filtered by acknowledgment status."""
        if not self.warnings:
            return
        
        # Filter warnings into new vs acknowledged issues
        filtered_results = self.acknowledge_manager.filter_acknowledged_issues(self.warnings)
        
        if not filtered_results['new_issues']:
            print("üìß All warning issues are acknowledged and muted - skipping email")
            return
        
        try:
            # Get email API URL from centralized config
            email_api_url = self.config.get('api_config', {}).get('email_api_url')
            
            if not email_api_url:
                # Fallback to environment variable
                email_api_url = os.getenv('EMAIL_API_URL')
                
            if not email_api_url:
                return
                
            # Prepare warning digest with acknowledgment context
            warning_summary = []
            
            # Add new issues
            if filtered_results['new_issues']:
                warning_summary.append("‚ö†Ô∏è NEW WARNINGS DETECTED:")
                for i, warning in enumerate(filtered_results['new_issues'], 1):
                    warning_summary.append(f"{i}. {warning['test_name']}<br/>   Message: {warning['message']}<br/>   Time: {warning['timestamp']}")
            
            # Add acknowledged issues context
            if filtered_results['acknowledged_issues']:
                if warning_summary:
                    warning_summary.append("")  # Add spacing
                warning_summary.append("‚úÖ PREVIOUSLY ACKNOWLEDGED (muted):")
                for i, warning in enumerate(filtered_results['acknowledged_issues'], 1):
                    metrics = warning.get('metrics', {})
                    # Try multiple field names for backward compatibility
                    person_name = metrics.get('person_name') or metrics.get('issue_name', 'Unknown')
                    person_email = metrics.get('person_email') or metrics.get('issue_identifier', 'Unknown')
                    warning_summary.append(f"{i}. {warning['test_name']} - {person_name} ({person_email}) - Acknowledged and muted")
            
            warning_summary_text = "<br/><br/>".join(warning_summary)
            
            payload = {
                "task_name": self.config.get('api_config', {}).get('email_template_warning', 'Test Warning Digest'),
                "custom_variables": {
                    "repository_name": self.repository_name,
                    "pipeline_name": self.pipeline_name,
                    "run_id": self.run_id,
                    "warning_count": len(filtered_results['new_issues']),
                    "warning_summary": warning_summary_text,
                    "run_duration": self._get_run_duration(),
                    "timestamp": datetime.now(timezone.utc).strftime("%Y-%m-%d %H:%M:%S UTC")
                }
            }
            
            response = requests.post(email_api_url, json=payload, timeout=30)
            if response.status_code == 200:
                print(f"Warning digest sent successfully for {len(self.warnings)} warnings")
            else:
                print(f"Warning: Warning digest failed with status {response.status_code}")
                
        except Exception as e:
            print(f"Warning: Could not send warning digest: {e}")
    
    def _get_run_duration(self) -> str:
        """Get the duration of the test run."""
        duration = datetime.now(timezone.utc) - self.start_time
        return str(duration).split('.')[0]  # Remove microseconds
    
    def get_threshold(self, path: str, default_value: Any = None) -> Any:
        """
        Get a threshold value using dot notation path.
        
        Args:
            path: Dot notation path to threshold. Can be:
                  - Pipeline-specific: 'test_name.threshold_name' (e.g., 'check_employee_data_completeness.completeness_threshold')
                  - Global: 'global.category.threshold_name' (e.g., 'global.row_count_change.warn_percentage')
            default_value: Default value if threshold not found
            
        Returns:
            Threshold value or default_value if not found
        """
        try:
            # Handle global thresholds
            if path.startswith('global.'):
                full_path = f"thresholds.{path}"
            else:
                # Pipeline-specific thresholds
                full_path = f"thresholds.{self.pipeline_name}.{path}"
            
            # Navigate using dot notation path
            current = self.config
            for key in full_path.split('.'):
                if isinstance(current, dict) and key in current:
                    current = current[key]
                else:
                    return default_value
            
            return current
            
        except Exception as e:
            print(f"Error getting threshold for {self.pipeline_name}.{path}: {e}")
            return default_value
    
    def set_threshold(self, path: str, value: Any) -> bool:
        """
        Set a threshold value using dot notation path.
        
        Args:
            path: Dot notation path to threshold. Can be:
                  - Pipeline-specific: 'test_name.threshold_name' (e.g., 'check_employee_data_completeness.completeness_threshold')
                  - Global: 'global.category.threshold_name' (e.g., 'global.row_count_change.warn_percentage')
            value: New threshold value
            
        Returns:
            True if successful, False otherwise
        """
        try:
            # Handle global thresholds
            if path.startswith('global.'):
                full_path = f"thresholds.{path}"
            else:
                # Pipeline-specific thresholds
                full_path = f"thresholds.{self.pipeline_name}.{path}"
            
            # Navigate to the parent of the target key and set the value
            keys = full_path.split('.')
            current = self.config
            
            # Navigate to the parent container
            for key in keys[:-1]:
                if key not in current:
                    current[key] = {}
                current = current[key]
            
            # Set the final value
            current[keys[-1]] = value
            
            # Update in Firestore if available
            if self.firestore_client:
                from ..utils.config_manager import ConfigManager
                config_manager = ConfigManager(project_id=self._firestore_project_id)
                return config_manager.set_threshold(self.repository_name, full_path, value)
            
            return True
            
        except Exception as e:
            print(f"Error setting threshold for {self.pipeline_name}.{path}: {e}")
            return False
    
    def is_test_enabled(self, test_category: str) -> bool:
        """
        Check if a test category is enabled.
        
        Args:
            test_category: Test category to check (e.g., 'enable_schema_validation')
            
        Returns:
            True if test is enabled, False otherwise
        """
        try:
            return self.config.get('test_switches', {}).get(test_category, True)
        except Exception:
            return True
    
    def update_config_in_firestore(self, config_updates: Dict[str, Any]) -> bool:
        """Update configuration in Firestore for this repository."""
        if not self.firestore_client:
            print("Warning: Firestore client not available. Cannot update configuration.")
            return False
        
        try:
            repo_doc_ref = self.firestore_client.collection(self.collection_integrated_testing_config).document(self.repository_name)
            
            # Add metadata
            config_updates['last_updated'] = datetime.now(timezone.utc)
            config_updates['updated_by'] = os.getenv('USER', 'unknown')
            
            # Update the document
            repo_doc_ref.set(config_updates, merge=True)
            print(f"Configuration updated for repository: {self.repository_name}")
            
            # Reload configuration
            self.config = self._load_config_from_firestore()
            return True
            
        except Exception as e:
            print(f"Error updating configuration in Firestore: {e}")
            return False
    
    def _batch_update_acknowledgments(self):
        """
        Batch-populate acknowledgment entries in Firestore for all WARN results.
        
        This avoids one Firestore write per warning by aggregating all
        acknowledgeable warnings per test_name and performing a single
        read/write for each test.
        """
        # Group acknowledgeable warnings by test_name
        warnings_by_test: Dict[str, List[Dict[str, Any]]] = {}
        
        for result in self.warnings:
            if not result.get('acknowledgeable', True):
                continue
            
            issue_identifier = result.get('issue_identifier')
            test_name = result.get('test_name')
            
            if not issue_identifier or not test_name:
                continue
            
            metrics = result.get('metrics', {}) or {}
            # Prefer explicit issue_details from metrics; fall back to message
            issue_details = metrics.get('issue_details', result.get('message', ""))
            additional_metadata = metrics
            
            warnings_by_test.setdefault(test_name, []).append({
                "issue_identifier": issue_identifier,
                "issue_details": issue_details,
                "additional_metadata": additional_metadata,
            })
        
        # Perform batched updates per test_name
        for test_name, issues in warnings_by_test.items():
            try:
                self.acknowledge_manager.batch_update_issue_detections(
                    test_name=test_name,
                    issues=issues,
                )
            except Exception as e:
                print(f"Warning: Could not batch update acknowledgment entries for test {test_name}: {e}")
    
    def finalize_run(self):
        """Finalize the test run and send summary."""
        try:
            # Flush remaining logs to sheet
            if self.sheet_logger:
                self.sheet_logger.release_remaining_logs()
            
            # Batch-populate acknowledgment entries in Firestore for all warning results
            # so that email filtering can use up-to-date acknowledgment status without
            # incurring one Firestore write per warning.
            try:
                self._batch_update_acknowledgments()
            except Exception as e:
                print(f"Warning: Could not batch update acknowledgment entries: {e}")
            
            # Send failure summary if there were failures (and no email sent yet)
            self._send_failure_summary_email()
                       
            # Send warning digest if there were warnings
            self.send_warning_digest()
            
            # Log run summary
            summary = {
                'total_tests': len(self.test_results),
                'passed': len([r for r in self.test_results if r['status'] == 'PASS']),
                'warnings': len(self.warnings),
                'failures': len(self.failures),
                'run_duration': self._get_run_duration()
            }
            
            # Log summary to sheet
            if self.sheet_logger:
                summary_message = f"Run Summary - Total: {summary['total_tests']}, Passed: {summary['passed']}, Warnings: {summary['warnings']}, Failures: {summary['failures']}, Duration: {summary['run_duration']}"
                self.sheet_logger.write_prints_to_sheet("TestLogs", summary_message, flush=True)
            
            print(f"üèÅ Test run completed: {summary}")
            
            # Return summary for programmatic use
            return summary
            
        except Exception as e:
            print(f"Error finalizing test run: {e}")
            return None


# Convenience functions for easy import
def create_testkit(repository_name: str, pipeline_name: str = None, fail_on_error: bool = False, 
                   sheet_logger = None, firestore_project_id: str = None,
                   spreadsheet_id: str = None, credentials_path: str = None) -> TestKit:
    """Create a new TestKit instance."""
    return TestKit(repository_name, pipeline_name, fail_on_error, 
                   sheet_logger=sheet_logger, firestore_project_id=firestore_project_id,
                   spreadsheet_id=spreadsheet_id, credentials_path=credentials_path)


def log_pass(testkit: TestKit, test_name: str, message: str = "", metrics: Dict[str, Any] = None):
    """Log a passing test."""
    testkit.log_pass(test_name, message, metrics)


def log_warn(testkit: TestKit, test_name: str, issue_identifier: str, message: str, metrics: Dict[str, Any] = None, acknowledgeable: bool = True):
    """Log a warning."""
    testkit.log_warn(test_name, issue_identifier, message, metrics, acknowledgeable)


def log_fail(testkit: TestKit, test_name: str, issue_identifier: str, message: str, metrics: Dict[str, Any] = None, stop_pipeline: bool = None, acknowledgeable: bool = True):
    """Log a failure."""
    testkit.log_fail(test_name, issue_identifier, message, metrics, stop_pipeline, acknowledgeable)

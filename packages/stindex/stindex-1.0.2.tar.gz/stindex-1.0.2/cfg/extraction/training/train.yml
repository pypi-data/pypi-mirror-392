# Training Configuration
# Fine-tuning configuration for MS-SWIFT with DeepSpeed

# ============================================================
# Model Configuration
# ============================================================
model:
  # Base model to fine-tune (HuggingFace model ID or local path)
  model_name: Qwen/Qwen3-8B

  # Model loading settings
  trust_remote_code: true
  torch_dtype: auto  # auto, float16, bfloat16, float32

# ============================================================
# Training Framework
# ============================================================
training:
  # Training framework
  framework: deepspeed

  # DeepSpeed configuration
  deepspeed:
    stage: 2  # ZeRO stage (0, 1, 2, 3)
    offload_optimizer: false  # Offload optimizer to CPU
    offload_param: false  # Offload parameters to CPU

  # Training hyperparameters
  learning_rate: 2.0e-5
  batch_size: 1
  gradient_accumulation_steps: 16
  num_train_epochs: 3
  warmup_ratio: 0.03
  weight_decay: 0.01
  max_length: 2048  # Maximum sequence length for training

  # Optimization
  optim: adamw_torch  # Optimizer: adamw_torch, adamw_hf, sgd
  adam_beta1: 0.9
  adam_beta2: 0.999
  adam_epsilon: 1.0e-8
  max_grad_norm: 1.0  # Gradient clipping

  # Learning rate scheduler
  lr_scheduler_type: cosine  # linear, cosine, polynomial, constant

  # Logging and checkpointing
  logging_steps: 10
  save_steps: 500
  save_total_limit: 3  # Maximum number of checkpoints to keep
  eval_steps: 500
  evaluation_strategy: steps  # no, steps, epoch

# ============================================================
# LoRA Configuration (Parameter-Efficient Fine-Tuning)
# ============================================================
lora:
  enabled: false
  r: 8  # LoRA rank
  lora_alpha: 32  # LoRA alpha (scaling factor)
  lora_dropout: 0.1
  target_modules: ["q_proj", "k_proj", "v_proj"]  # Modules to apply LoRA
  bias: none  # none, all, lora_only
  task_type: CAUSAL_LM  # Task type for PEFT

# ============================================================
# Dataset Configuration
# ============================================================
dataset:
  # Training data paths
  train_dataset_path: null  # Path to training dataset
  eval_dataset_path: null   # Path to evaluation dataset

  # Dataset processing
  preprocessing_num_workers: 4
  max_train_samples: null  # Limit training samples (null = use all)
  max_eval_samples: null   # Limit eval samples (null = use all)

# ============================================================
# Output Configuration
# ============================================================
output:
  output_dir: ./output/training  # Output directory for checkpoints
  overwrite_output_dir: false
  resume_from_checkpoint: null  # Path to checkpoint to resume from

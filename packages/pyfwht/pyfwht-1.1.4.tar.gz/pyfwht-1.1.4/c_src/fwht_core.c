/*
 * Fast Walsh-Hadamard Transform - Core CPU Implementation
 *
 * Reference implementation using the butterfly algorithm.
 * This is the "ground truth" - correctness is paramount.
 * All other backends must match this exactly.
 *
 * Copyright (C) 2025 Hosein Hadipour
 *
 * Author: Hosein Hadipour <hsn.hadipour@gmail.com>
 *
 * This program is free software: you can redistribute it and/or modify
 * it under the terms of the GNU General Public License as published by
 * the Free Software Foundation, either version 3 of the License, or
 * (at your option) any later version.
 *
 * This program is distributed in the hope that it will be useful,
 * but WITHOUT ANY WARRANTY; without even the implied warranty of
 * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
 * GNU General Public License for more details.
 *
 * You should have received a copy of the GNU General Public License
 * along with this program. If not, see <https://www.gnu.org/licenses/>.
 */

/* Feature test macros must be defined before any includes */
#if !defined(_WIN32) && !defined(__APPLE__) && !defined(__FreeBSD__)
    #define _ISOC11_SOURCE  /* For aligned_alloc() on Linux */
#endif

#include "fwht.h"
#include <stdlib.h>
#include <string.h>
#include <math.h>
#include <stddef.h>
#include <stdio.h>

/* Compiler-specific restrict keyword */
#if defined(__GNUC__) || defined(__clang__)
#define FWHT_RESTRICT __restrict__
#elif defined(_MSC_VER)
#define FWHT_RESTRICT __restrict
#else
#define FWHT_RESTRICT
#endif

#if defined(_WIN32)
#include <windows.h>
#elif defined(__unix__) || defined(__APPLE__)
#include <pthread.h>
#endif

#if defined(__AVX2__)
#include <immintrin.h>
#define FWHT_HAVE_AVX2 1
#define FWHT_HAVE_SSE2 1
#elif defined(__SSE2__)
#include <emmintrin.h>
#define FWHT_HAVE_SSE2 1
#endif

#if defined(__ARM_NEON) || defined(__ARM_NEON__)
#include <arm_neon.h>
#define FWHT_HAVE_NEON 1
#endif

#ifdef _OPENMP
#if defined(__clang__)
#pragma clang diagnostic push
#pragma clang diagnostic ignored "-Wpedantic"
#elif defined(__GNUC__)
#pragma GCC diagnostic push
#pragma GCC diagnostic ignored "-Wpedantic"
#endif
#include <omp.h>
#if defined(__clang__)
#pragma clang diagnostic pop
#elif defined(__GNUC__)
#pragma GCC diagnostic pop
#endif
#endif

static void fwht_print_simd_banner(void) {
#if defined(FWHT_HAVE_AVX2)
    fprintf(stderr, "[libfwht] CPU backend: AVX2 vector path active\n");
#elif defined(FWHT_HAVE_SSE2) && !defined(FWHT_HAVE_NEON)
    fprintf(stderr, "[libfwht] CPU backend: SSE2 vector path active\n");
#elif defined(FWHT_HAVE_NEON)
    fprintf(stderr, "[libfwht] CPU backend: NEON vector path active\n");
#else
    fprintf(stderr, "[libfwht] CPU backend: scalar path active\n");
#endif
}

static void fwht_report_simd_mode(void) {
#if defined(_WIN32)
    static LONG reported = 0;
    if (InterlockedCompareExchange(&reported, 1, 0) != 0) {
        return;
    }
    fwht_print_simd_banner();
#elif defined(__GNUC__) || defined(__clang__)
    static int reported = 0;
    if (__sync_lock_test_and_set(&reported, 1)) {
        return;
    }
    fwht_print_simd_banner();
#elif defined(__unix__) || defined(__APPLE__)
    static pthread_once_t once_control = PTHREAD_ONCE_INIT;
    pthread_once(&once_control, fwht_print_simd_banner);
#else
    static int reported = 0;
    if (reported) {
        return;
    }
    reported = 1;
    fwht_print_simd_banner();
#endif
}

static inline void fwht_process_range_i32(int32_t* FWHT_RESTRICT even, 
                                           int32_t* FWHT_RESTRICT odd, 
                                           size_t count) {
    if (count == 0) {
        return;
    }

    size_t j = 0;

#if defined(FWHT_HAVE_AVX2)
    if (count >= 8) {
        size_t avx_end = count & (size_t)~7;
        for (; j < avx_end; j += 8) {
            __m256i a = _mm256_loadu_si256((const __m256i*)(even + j));
            __m256i b = _mm256_loadu_si256((const __m256i*)(odd + j));
            __m256i sum = _mm256_add_epi32(a, b);
            __m256i diff = _mm256_sub_epi32(a, b);
            _mm256_storeu_si256((__m256i*)(even + j), sum);
            _mm256_storeu_si256((__m256i*)(odd + j), diff);
        }
    }
#endif

#if defined(FWHT_HAVE_SSE2) && !defined(FWHT_HAVE_NEON)
    if (count >= 4) {
        size_t sse_end = count & (size_t)~3;
        for (; j < sse_end; j += 4) {
            __m128i a = _mm_loadu_si128((const __m128i*)(even + j));
            __m128i b = _mm_loadu_si128((const __m128i*)(odd + j));
            __m128i sum = _mm_add_epi32(a, b);
            __m128i diff = _mm_sub_epi32(a, b);
            _mm_storeu_si128((__m128i*)(even + j), sum);
            _mm_storeu_si128((__m128i*)(odd + j), diff);
        }
    }
#endif

#if defined(FWHT_HAVE_NEON)
    if (count >= 4) {
        size_t neon_end = count & (size_t)~3;
        for (; j < neon_end; j += 4) {
            int32x4_t a = vld1q_s32(even + j);
            int32x4_t b = vld1q_s32(odd + j);
            int32x4_t sum = vaddq_s32(a, b);
            int32x4_t diff = vsubq_s32(a, b);
            vst1q_s32(even + j, sum);
            vst1q_s32(odd + j, diff);
        }
    }
#endif

    for (; j < count; ++j) {
        int32_t a = even[j];
        int32_t b = odd[j];
        even[j] = a + b;
        odd[j]  = a - b;
    }
}

/* Overflow-safe version using compiler builtins */
static inline int fwht_process_range_i32_safe(int32_t* FWHT_RESTRICT even, 
                                                int32_t* FWHT_RESTRICT odd, 
                                                size_t count) {
    if (count == 0) {
        return 0;  /* No overflow */
    }

#if defined(__GNUC__) || defined(__clang__)
    /* Use compiler builtins for overflow detection */
    for (size_t j = 0; j < count; ++j) {
        int32_t a = even[j];
        int32_t b = odd[j];
        int32_t sum, diff;
        
        if (__builtin_add_overflow(a, b, &sum) || 
            __builtin_sub_overflow(a, b, &diff)) {
            return 1;  /* Overflow detected */
        }
        
        even[j] = sum;
        odd[j] = diff;
    }
    return 0;  /* No overflow */
#else
    /* Fallback: manual overflow checking for MSVC and other compilers */
    for (size_t j = 0; j < count; ++j) {
        int32_t a = even[j];
        int32_t b = odd[j];
        
        /* Check addition overflow: (a > 0 && b > 0 && a > INT32_MAX - b) */
        if ((a > 0 && b > 0 && a > (int32_t)0x7FFFFFFF - b) ||
            (a < 0 && b < 0 && a < (int32_t)0x80000000 - b)) {
            return 1;  /* Add overflow */
        }
        
        /* Check subtraction overflow: (a > 0 && b < 0 && a > INT32_MAX + b) */
        if ((a > 0 && b < 0 && a > (int32_t)0x7FFFFFFF + b) ||
            (a < 0 && b > 0 && a < (int32_t)0x80000000 + b)) {
            return 1;  /* Sub overflow */
        }
        
        even[j] = a + b;
        odd[j] = a - b;
    }
    return 0;  /* No overflow */
#endif
}

static inline void fwht_process_block_i32(int32_t* FWHT_RESTRICT data, 
                                           size_t base, size_t h) {
    fwht_process_range_i32(data + base, data + base + h, h);
}

static inline int fwht_process_block_i32_safe(int32_t* FWHT_RESTRICT data, 
                                                size_t base, size_t h) {
    return fwht_process_range_i32_safe(data + base, data + base + h, h);
}

static inline void fwht_process_range_f64(double* FWHT_RESTRICT even, 
                                           double* FWHT_RESTRICT odd, 
                                           size_t count) {
    if (count == 0) {
        return;
    }

    for (size_t j = 0; j < count; ++j) {
        double a = even[j];
        double b = odd[j];
        even[j] = a + b;
        odd[j]  = a - b;
    }
}

static inline void fwht_process_block_f64(double* FWHT_RESTRICT data, 
                                           size_t base, size_t h) {
    fwht_process_range_f64(data + base, data + base + h, h);
}

/* =========================================================================
 * VALIDATION HELPERS
 * ========================================================================== */

static bool is_power_of_2(size_t n) {
    return n > 0 && (n & (n - 1)) == 0;
}

static fwht_status_t validate_input(const void* data, size_t n) {
    if (data == NULL) return FWHT_ERROR_NULL_POINTER;
    if (!is_power_of_2(n)) return FWHT_ERROR_INVALID_SIZE;
    if (n == 0) return FWHT_ERROR_INVALID_SIZE;
    return FWHT_SUCCESS;
}

/* =========================================================================
 * CORE BUTTERFLY ALGORITHM - INT32
 * 
 * This is the reference implementation. Correctness verified against:
 * 1. Mathematical definition of WHT
 * 2. sboxU library (during development)
 * 3. Self-consistency (WHT(WHT(f)) = n*f property)
 * 
 * Algorithm: Recursive divide-and-conquer for cache efficiency
 * Complexity: O(n log n)
 * Memory: O(log n) stack space for recursion
 * 
 * NUMERICAL CONSIDERATIONS (int32_t):
 *   - Output range: Each WHT coefficient is bounded by n * max(|input|)
 *   - Overflow safety: Safe for all n if |input[i]| ≤ 1
 *   - For general input: safe if n * max(|input[i]|) < 2^31
 *   - Example: n=32768 (2^15) with |input| ≤ 65536 (2^16) → max output = 2^31 ✓
 * 
 * RECOMMENDATIONS:
 *   - Use int32_t for Boolean functions (values ±1)
 *   - Use double for large n or when |input| > 1
 *   - Check: n * max(|input[i]|) < 2147483648 before processing
 * ============================================================================ */

/* 
 * Recursive cutoff for single-threaded CPU.
 * Same as OpenMP version for consistency.
 */
#define FWHT_CPU_RECURSIVE_CUTOFF 512

/*
 * Base case: iterative FWHT with SIMD for small arrays (fits in L1 cache).
 * Includes software prefetching to hide memory latency.
 */
static void fwht_butterfly_i32_iterative(int32_t* data, size_t n) {
    for (size_t h = 1; h < n; h <<= 1) {
        size_t stride = h << 1;
        for (size_t i = 0; i < n; i += stride) {
            /* Prefetch next block to hide memory latency */
            if (i + stride < n) {
#if defined(__GNUC__) || defined(__clang__)
                __builtin_prefetch(data + i + stride, 1, 3);
                __builtin_prefetch(data + i + stride + h, 1, 3);
#endif
            }
            fwht_process_block_i32(data, i, h);
        }
    }
}

/*
 * Recursive helper for cache-efficient single-threaded FWHT.
 * Same algorithm as OpenMP version but without task parallelism.
 */
static void fwht_butterfly_i32_recursive_cpu(int32_t* data, size_t n) {
    if (n <= FWHT_CPU_RECURSIVE_CUTOFF) {
        fwht_butterfly_i32_iterative(data, n);
        return;
    }
    
    size_t half = n >> 1;
    
    /* Recursively transform both halves */
    fwht_butterfly_i32_recursive_cpu(data, half);
    fwht_butterfly_i32_recursive_cpu(data + half, half);
    
    /* Combine: butterfly between the two halves */
    fwht_process_block_i32(data, 0, half);
}

/*
 * Safe iterative FWHT with overflow detection.
 * Returns: 0 on success, 1 if overflow detected.
 */
static int fwht_butterfly_i32_iterative_safe(int32_t* data, size_t n) {
    for (size_t h = 1; h < n; h <<= 1) {
        size_t stride = h << 1;
        for (size_t i = 0; i < n; i += stride) {
            if (fwht_process_block_i32_safe(data, i, h)) {
                return 1;  /* Overflow detected */
            }
        }
    }
    return 0;  /* Success */
}

/*
 * Safe recursive FWHT with overflow detection.
 * Returns: 0 on success, 1 if overflow detected.
 */
static int fwht_butterfly_i32_recursive_safe(int32_t* data, size_t n) {
    if (n <= FWHT_CPU_RECURSIVE_CUTOFF) {
        return fwht_butterfly_i32_iterative_safe(data, n);
    }
    
    size_t half = n >> 1;
    
    /* Recursively transform both halves */
    if (fwht_butterfly_i32_recursive_safe(data, half)) {
        return 1;  /* Overflow in left half */
    }
    if (fwht_butterfly_i32_recursive_safe(data + half, half)) {
        return 1;  /* Overflow in right half */
    }
    
    /* Combine: butterfly between the two halves */
    if (fwht_process_block_i32_safe(data, 0, half)) {
        return 1;  /* Overflow in combine step */
    }
    
    return 0;  /* Success */
}

/*
 * Main entry point for single-threaded CPU FWHT.
 */
static void fwht_butterfly_i32(int32_t* data, size_t n) {
    fwht_report_simd_mode();
    fwht_butterfly_i32_recursive_cpu(data, n);
}

/* ============================================================================
 * CORE BUTTERFLY ALGORITHM - DOUBLE
 * 
 * Same algorithm, double precision for numerical applications.
 * Uses same recursive cache-efficient approach as int32.
 * 
 * NUMERICAL CONSIDERATIONS (double):
 *   - Precision: ~15-16 decimal digits (IEEE 754 double precision)
 *   - Rounding errors accumulate as O(log₂(n) * ε * ||x||₂)
 *     where ε ≈ 2.22e-16 (machine epsilon)
 *   - Relative error: typically < 1e-14 for well-conditioned inputs
 *   - Involution property: ||WHT(WHT(x))/n - x|| / ||x|| < 1e-13
 * 
 * RECOMMENDATIONS:
 *   - Use double for n > 2^20 or when high precision needed
 *   - Expected relative error: ~log₂(n) * 1e-16
 *   - Example: n=1048576 (2^20) → relative error < 2e-15
 * ============================================================================ */

/*
 * Base case: iterative FWHT with SIMD for small arrays.
 * Includes software prefetching to hide memory latency.
 */
static void fwht_butterfly_f64_iterative(double* data, size_t n) {
    for (size_t h = 1; h < n; h <<= 1) {
        for (size_t i = 0; i < n; i += (h << 1)) {
            /* Prefetch next block to hide memory latency */
            size_t stride = h << 1;
            if (i + stride < n) {
#if defined(__GNUC__) || defined(__clang__)
                __builtin_prefetch(data + i + stride, 1, 3);
                __builtin_prefetch(data + i + stride + h, 1, 3);
#endif
            }
            fwht_process_block_f64(data, i, h);
        }
    }
}

/*
 * Recursive helper for cache-efficient single-threaded FWHT (double).
 */
static void fwht_butterfly_f64_recursive_cpu(double* data, size_t n) {
    if (n <= FWHT_CPU_RECURSIVE_CUTOFF) {
        fwht_butterfly_f64_iterative(data, n);
        return;
    }
    
    size_t half = n >> 1;
    
    fwht_butterfly_f64_recursive_cpu(data, half);
    fwht_butterfly_f64_recursive_cpu(data + half, half);
    
    fwht_process_block_f64(data, 0, half);
}

/*
 * Main entry point for single-threaded CPU FWHT (double).
 */
static void fwht_butterfly_f64(double* data, size_t n) {
    fwht_report_simd_mode();
    fwht_butterfly_f64_recursive_cpu(data, n);
}

#ifdef _OPENMP
/* ==========================================================================
 * OPENMP PARALLEL VARIANTS
 * ========================================================================== */

/* 
 * Recursive cutoff: below this size, use iterative base case.
 * Tuned for L1 cache (typically 32-64KB).
 * 512 elements * 4 bytes = 2KB, well within L1 cache.
 */
#define FWHT_RECURSIVE_CUTOFF 512

/*
 * Base case for recursion: iterative FWHT with SIMD acceleration.
 * Processes arrays small enough to fit in L1 cache.
 * Includes software prefetching to hide memory latency.
 */
static void fwht_butterfly_i32_base(int32_t* data, size_t n) {
    for (size_t h = 1; h < n; h *= 2) {
        for (size_t i = 0; i < n; i += h * 2) {
            /* Prefetch next block to hide memory latency */
            size_t stride = h * 2;
            if (i + stride < n) {
#if defined(__GNUC__) || defined(__clang__)
                __builtin_prefetch(data + i + stride, 1, 3);
                __builtin_prefetch(data + i + stride + h, 1, 3);
#endif
            }
            fwht_process_block_i32(data, i, h);
        }
    }
}

/*
 * Recursive helper for OpenMP task-based FWHT.
 * 
 * Algorithm:
 *   1. If n <= cutoff: process iteratively (stays in L1 cache)
 *   2. Otherwise:
 *      a. Recursively transform left half  (independent)
 *      b. Recursively transform right half (independent) 
 *      c. Combine halves with butterfly operation
 * 
 * Depth parameter limits task creation to avoid overhead.
 * Adaptive: depth limit is calculated based on thread count.
 * With T threads, create tasks while depth < log2(T) + 2.
 * This ensures roughly T tasks are active at peak parallelism.
 */
static void fwht_butterfly_i32_recursive(int32_t* data, size_t n, int depth, int max_depth) {
    if (n <= FWHT_RECURSIVE_CUTOFF) {
        fwht_butterfly_i32_base(data, n);
        return;
    }
    
    size_t half = n >> 1;
    
    /* Create tasks for the two independent halves (if not too deep) */
    #pragma omp task shared(data) if(depth < max_depth)
    fwht_butterfly_i32_recursive(data, half, depth + 1, max_depth);
    
    #pragma omp task shared(data) if(depth < max_depth)
    fwht_butterfly_i32_recursive(data + half, half, depth + 1, max_depth);
    
    #pragma omp taskwait
    
    /* Combine: butterfly between the two halves */
    fwht_process_block_i32(data, 0, half);
}

/*
 * OpenMP entry point using recursive task-based parallelism.
 * Much better cache locality and scaling than stage-based approach.
 * 
 * Adaptive task depth: calculates optimal depth based on number of threads.
 * Formula: max_depth = log2(num_threads) + 2
 * This creates roughly 2-4x more tasks than threads for good load balancing.
 */
static void fwht_butterfly_i32_openmp(int32_t* data, size_t n) {
    if (n < 2) {
        return;
    }
    
    #pragma omp parallel
    {
        #pragma omp single
        {
            /* Calculate adaptive task depth based on thread count */
            int num_threads = omp_get_num_threads();
            int max_depth = 2;  /* Minimum depth for small thread counts */
            
            /* For larger thread counts, scale depth adaptively */
            if (num_threads >= 4) {
                /* Calculate log2(num_threads) */
                int log_threads = 0;
                int t = num_threads;
                while (t > 1) {
                    t >>= 1;
                    log_threads++;
                }
                max_depth = log_threads + 2;  /* +2 for good load balancing */
            }
            
            fwht_butterfly_i32_recursive(data, n, 0, max_depth);
        }
    }
}

/*
 * Base case for recursion: iterative FWHT for double precision.
 * Includes software prefetching to hide memory latency.
 */
static void fwht_butterfly_f64_base(double* data, size_t n) {
    for (size_t h = 1; h < n; h <<= 1) {
        for (size_t i = 0; i < n; i += h * 2) {
            /* Prefetch next block to hide memory latency */
            size_t stride = h * 2;
            if (i + stride < n) {
#if defined(__GNUC__) || defined(__clang__)
                __builtin_prefetch(data + i + stride, 1, 3);
                __builtin_prefetch(data + i + stride + h, 1, 3);
#endif
            }
            fwht_process_block_f64(data, i, h);
        }
    }
}

/*
 * Recursive helper for double precision OpenMP FWHT.
 */
static void fwht_butterfly_f64_recursive(double* data, size_t n, int depth, int max_depth) {
    if (n <= FWHT_RECURSIVE_CUTOFF) {
        fwht_butterfly_f64_base(data, n);
        return;
    }
    
    size_t half = n >> 1;
    
    #pragma omp task shared(data) if(depth < max_depth)
    fwht_butterfly_f64_recursive(data, half, depth + 1, max_depth);
    
    #pragma omp task shared(data) if(depth < max_depth)
    fwht_butterfly_f64_recursive(data + half, half, depth + 1, max_depth);
    
    #pragma omp taskwait
    
    fwht_process_block_f64(data, 0, half);
}

/*
 * OpenMP entry point for double precision using recursive task-based parallelism.
 * Adaptive task depth for optimal scaling on systems with many cores.
 */
static void fwht_butterfly_f64_openmp(double* data, size_t n) {
    if (n < 2) {
        return;
    }
    
    #pragma omp parallel
    {
        #pragma omp single
        {
            /* Calculate adaptive task depth based on thread count */
            int num_threads = omp_get_num_threads();
            int max_depth = 2;  /* Minimum depth for small thread counts */
            
            /* For larger thread counts, scale depth adaptively */
            if (num_threads >= 4) {
                /* Calculate log2(num_threads) */
                int log_threads = 0;
                int t = num_threads;
                while (t > 1) {
                    t >>= 1;
                    log_threads++;
                }
                max_depth = log_threads + 2;  /* +2 for good load balancing */
            }
            
            fwht_butterfly_f64_recursive(data, n, 0, max_depth);
        }
    }
}
#endif /* _OPENMP */

/* ============================================================================
 * CORE BUTTERFLY ALGORITHM - INT8
 * 
 * Memory-efficient version for small values.
 * 
 * OVERFLOW WARNING:
 *   - Output range: Each coefficient bounded by n * max(|input|)
 *   - int8_t range: -128 to +127
 *   - SAFE CONDITIONS: n * max(|input[i]|) ≤ 127
 *   - Examples:
 *     * n=128, |input|≤1  → max output = 128  → OVERFLOW! ✗
 *     * n=64,  |input|≤1  → max output = 64   → Safe ✓
 *     * n=16,  |input|≤7  → max output = 112  → Safe ✓
 * 
 * RECOMMENDATION: Only use for very small arrays (n ≤ 64) with |input| = 1
 * For general use, prefer int32_t or double.
 * ============================================================================ */

static void fwht_butterfly_i8(int8_t* data, size_t n) {
    fwht_report_simd_mode();
    for (size_t h = 1; h < n; h <<= 1) {
        for (size_t i = 0; i < n; i += (h << 1)) {
            for (size_t j = i; j < i + h; ++j) {
                int8_t a = data[j];
                int8_t b = data[j + h];
                data[j]     = a + b;
                data[j + h] = a - b;
            }
        }
    }
}

/* ============================================================================
 * PUBLIC API - BASIC IN-PLACE TRANSFORMS
 * ============================================================================ */

/* CPU-only versions for internal use and fallback */
fwht_status_t fwht_i32_cpu(int32_t* data, size_t n) {
    fwht_status_t status = validate_input(data, n);
    if (status != FWHT_SUCCESS) return status;
    
    fwht_butterfly_i32(data, n);
    return FWHT_SUCCESS;
}

fwht_status_t fwht_i32_cpu_safe(int32_t* data, size_t n) {
    fwht_status_t status = validate_input(data, n);
    if (status != FWHT_SUCCESS) return status;
    
    fwht_report_simd_mode();
    if (fwht_butterfly_i32_recursive_safe(data, n)) {
        return FWHT_ERROR_OVERFLOW;
    }
    return FWHT_SUCCESS;
}

fwht_status_t fwht_f64_cpu(double* data, size_t n) {
    fwht_status_t status = validate_input(data, n);
    if (status != FWHT_SUCCESS) return status;

    fwht_butterfly_f64(data, n);
    return FWHT_SUCCESS;
}

/* Default API routes to AUTO backend */
fwht_status_t fwht_i32(int32_t* data, size_t n) {
    return fwht_i32_backend(data, n, FWHT_BACKEND_AUTO);
}

/* Safe variant with overflow detection */
fwht_status_t fwht_i32_safe(int32_t* data, size_t n) {
    return fwht_i32_backend(data, n, FWHT_BACKEND_CPU_SAFE);
}

fwht_status_t fwht_f64(double* data, size_t n) {
    return fwht_f64_backend(data, n, FWHT_BACKEND_AUTO);
}

fwht_status_t fwht_i8(int8_t* data, size_t n) {
    fwht_status_t status = validate_input(data, n);
    if (status != FWHT_SUCCESS) return status;
    
    fwht_butterfly_i8(data, n);
    return FWHT_SUCCESS;
}

/* ============================================================================
 * PUBLIC API - BACKEND CONTROL
 * ============================================================================ */

/* CUDA function declarations (when available) */
#ifdef USE_CUDA
extern fwht_status_t fwht_i32_cuda(int32_t* data, size_t n);
extern fwht_status_t fwht_f64_cuda(double* data, size_t n);
#endif

fwht_status_t fwht_i32_backend(int32_t* data, size_t n, fwht_backend_t backend) {
    fwht_status_t status = validate_input(data, n);
    if (status != FWHT_SUCCESS) return status;
    
    /* Select backend */
    if (backend == FWHT_BACKEND_AUTO) {
        backend = fwht_recommend_backend(n);
    }
    
    /* Execute on selected backend */
    switch (backend) {
        case FWHT_BACKEND_CPU:
            fwht_report_simd_mode();
            fwht_butterfly_i32(data, n);
            return FWHT_SUCCESS;
        
        case FWHT_BACKEND_CPU_SAFE:
            return fwht_i32_cpu_safe(data, n);
            
#ifdef USE_CUDA
        case FWHT_BACKEND_GPU:
            return fwht_i32_cuda(data, n);
#endif
            
        case FWHT_BACKEND_OPENMP:
#ifdef _OPENMP
            fwht_report_simd_mode();
            fwht_butterfly_i32_openmp(data, n);
            return FWHT_SUCCESS;
#else
            return FWHT_ERROR_BACKEND_UNAVAILABLE;
#endif
            
        default:
            return FWHT_ERROR_BACKEND_UNAVAILABLE;
    }
}

fwht_status_t fwht_f64_backend(double* data, size_t n, fwht_backend_t backend) {
    fwht_status_t status = validate_input(data, n);
    if (status != FWHT_SUCCESS) return status;
    
    /* Select backend */
    if (backend == FWHT_BACKEND_AUTO) {
        backend = fwht_recommend_backend(n);
    }
    
    /* Execute on selected backend */
    switch (backend) {
        case FWHT_BACKEND_CPU:
            fwht_report_simd_mode();
            fwht_butterfly_f64(data, n);
            return FWHT_SUCCESS;
            
#ifdef USE_CUDA
        case FWHT_BACKEND_GPU:
            return fwht_f64_cuda(data, n);
#endif
            
        case FWHT_BACKEND_OPENMP:
#ifdef _OPENMP
            fwht_report_simd_mode();
            fwht_butterfly_f64_openmp(data, n);
            return FWHT_SUCCESS;
#else
            return FWHT_ERROR_BACKEND_UNAVAILABLE;
#endif
            
        default:
            return FWHT_ERROR_BACKEND_UNAVAILABLE;
    }
}

/* ============================================================================
 * PUBLIC API - OUT-OF-PLACE TRANSFORMS
 * 
 * Allocates cache-aligned memory for optimal performance.
 * ============================================================================ */

/*
 * Allocate cache-line aligned memory for optimal performance.
 * Alignment to 64 bytes ensures no cache line splits.
 */
static inline void* fwht_aligned_alloc(size_t size) {
#if defined(_WIN32)
    return _aligned_malloc(size, 64);
#elif defined(__APPLE__) || defined(__FreeBSD__)
    void* ptr = NULL;
    if (posix_memalign(&ptr, 64, size) != 0) {
        return NULL;
    }
    return ptr;
#else
    return aligned_alloc(64, (size + 63) & ~63);
#endif
}

static inline void fwht_aligned_free(void* ptr) {
#if defined(_WIN32)
    _aligned_free(ptr);
#else
    free(ptr);
#endif
}

int32_t* fwht_compute_i32(const int32_t* input, size_t n) {
    if (validate_input(input, n) != FWHT_SUCCESS) return NULL;
    
    size_t bytes = n * sizeof(int32_t);
    int32_t* output = (int32_t*)fwht_aligned_alloc(bytes);
    if (output == NULL) return NULL;
    
    memcpy(output, input, bytes);
    fwht_butterfly_i32(output, n);
    
    return output;
}

double* fwht_compute_f64(const double* input, size_t n) {
    if (validate_input(input, n) != FWHT_SUCCESS) return NULL;
    
    size_t bytes = n * sizeof(double);
    double* output = (double*)fwht_aligned_alloc(bytes);
    if (output == NULL) return NULL;
    
    memcpy(output, input, bytes);
    fwht_butterfly_f64(output, n);
    
    return output;
}

int32_t* fwht_compute_i32_backend(const int32_t* input, size_t n, fwht_backend_t backend) {
    (void)backend;
    return fwht_compute_i32(input, n);
}

double* fwht_compute_f64_backend(const double* input, size_t n, fwht_backend_t backend) {
    (void)backend;
    return fwht_compute_f64(input, n);
}

/*
 * Free memory allocated by fwht_compute_* functions.
 * Portable wrapper for aligned memory deallocation.
 */
void fwht_free(void* ptr) {
    fwht_aligned_free(ptr);
}

/* ============================================================================
 * PUBLIC API - BOOLEAN FUNCTION CONVENIENCE
 * ============================================================================ */

fwht_status_t fwht_from_bool(const uint8_t* bool_func, int32_t* wht_out, 
                             size_t n, bool signed_rep) {
    fwht_status_t status = validate_input(bool_func, n);
    if (status != FWHT_SUCCESS) return status;
    if (wht_out == NULL) return FWHT_ERROR_NULL_POINTER;
    
    /* Convert boolean function to signed representation */
    if (signed_rep) {
        /* Cryptographic convention: 0 → +1, 1 → -1 */
        for (size_t i = 0; i < n; ++i) {
            wht_out[i] = (bool_func[i] == 0) ? 1 : -1;
        }
    } else {
        /* Use values as-is */
        for (size_t i = 0; i < n; ++i) {
            wht_out[i] = (int32_t)bool_func[i];
        }
    }
    
    /* Compute WHT */
    fwht_butterfly_i32(wht_out, n);
    
    return FWHT_SUCCESS;
}

fwht_status_t fwht_correlations(const uint8_t* bool_func, double* corr_out, size_t n) {
    fwht_status_t status = validate_input(bool_func, n);
    if (status != FWHT_SUCCESS) return status;
    if (corr_out == NULL) return FWHT_ERROR_NULL_POINTER;
    
    /* Convert to signed and compute WHT */
    size_t bytes = n * sizeof(int32_t);
    int32_t* wht = (int32_t*)fwht_aligned_alloc(bytes);
    if (wht == NULL) return FWHT_ERROR_OUT_OF_MEMORY;
    
    status = fwht_from_bool(bool_func, wht, n, true);
    if (status != FWHT_SUCCESS) {
        fwht_aligned_free(wht);
        return status;
    }
    
    /* Convert WHT to correlations: Cor(f, u) = WHT[u] / n */
    double n_inv = 1.0 / (double)n;
    for (size_t i = 0; i < n; ++i) {
        corr_out[i] = (double)wht[i] * n_inv;
    }
    
    fwht_aligned_free(wht);
    return FWHT_SUCCESS;
}

/* ============================================================================
 * CONTEXT API
 * 
 * Provides a stateful API for managing FWHT configurations and batch operations.
 * The context object encapsulates backend selection, threading, and GPU settings.
 * 
 * Batch processing benefits:
 *   - GPU: Amortizes PCIe transfer overhead by batching all arrays together
 *   - OpenMP: Parallelizes across batch using thread pool
 *   - Sequential: Falls back to simple loop for small batches
 * 
 * For single transforms, use the simple API (fwht_i32, fwht_f64).
 * Use contexts for explicit backend control or batch processing.
 * ============================================================================ */

struct fwht_context {
    fwht_config_t config;
};

fwht_config_t fwht_default_config(void) {
    fwht_config_t config;
    config.backend = FWHT_BACKEND_AUTO;
    config.num_threads = 0;  /* Auto-detect */
    config.gpu_device = 0;
    config.normalize = false;
    return config;
}

fwht_context_t* fwht_create_context(const fwht_config_t* config) {
    fwht_context_t* ctx = (fwht_context_t*)malloc(sizeof(fwht_context_t));
    if (ctx == NULL) return NULL;
    
    if (config != NULL) {
        ctx->config = *config;
    } else {
        ctx->config = fwht_default_config();
    }
    
    return ctx;
}

void fwht_destroy_context(fwht_context_t* ctx) {
    if (ctx != NULL) {
        free(ctx);
    }
}

fwht_status_t fwht_transform_i32(fwht_context_t* ctx, int32_t* data, size_t n) {
    if (ctx == NULL) {
        return fwht_i32(data, n);
    }
    return fwht_i32_backend(data, n, ctx->config.backend);
}

fwht_status_t fwht_transform_f64(fwht_context_t* ctx, double* data, size_t n) {
    if (ctx == NULL) {
        return fwht_f64(data, n);
    }
    return fwht_f64_backend(data, n, ctx->config.backend);
}

fwht_status_t fwht_batch_i32(fwht_context_t* ctx, int32_t** data_array, 
                             size_t n, int batch_size) {
    if (data_array == NULL) return FWHT_ERROR_NULL_POINTER;
    if (batch_size == 0) return FWHT_ERROR_INVALID_ARGUMENT;
    
    fwht_backend_t backend = (ctx != NULL) ? ctx->config.backend : FWHT_BACKEND_AUTO;
    
    /* Auto-select backend if needed */
    if (backend == FWHT_BACKEND_AUTO) {
        backend = fwht_recommend_backend(n);
    }
    
#ifdef FWHT_ENABLE_CUDA
    /* GPU batch: copy all arrays to device and process in parallel */
    if (backend == FWHT_BACKEND_GPU) {
        /* Allocate contiguous device memory for all arrays */
        int32_t* d_data = NULL;
        size_t total_size = n * batch_size;
        cudaError_t err = cudaMalloc(&d_data, total_size * sizeof(int32_t));
        if (err != cudaSuccess) {
            return FWHT_ERROR_CUDA;
        }
        
        /* Copy all arrays to device */
        for (int i = 0; i < batch_size; ++i) {
            err = cudaMemcpy(d_data + i * n, data_array[i], 
                           n * sizeof(int32_t), cudaMemcpyHostToDevice);
            if (err != cudaSuccess) {
                cudaFree(d_data);
                return FWHT_ERROR_CUDA;
            }
        }
        
        /* Process batch on GPU */
        fwht_status_t status = fwht_batch_i32_cuda(d_data, n, batch_size);
        
        /* Copy results back */
        if (status == FWHT_SUCCESS) {
            for (int i = 0; i < batch_size; ++i) {
                err = cudaMemcpy(data_array[i], d_data + i * n,
                               n * sizeof(int32_t), cudaMemcpyDeviceToHost);
                if (err != cudaSuccess) {
                    status = FWHT_ERROR_CUDA;
                    break;
                }
            }
        }
        
        cudaFree(d_data);
        return status;
    }
#endif
    
    /* CPU batch: parallelize with OpenMP if available */
#ifdef FWHT_ENABLE_OPENMP
    if (backend == FWHT_BACKEND_OPENMP || batch_size > 4) {
        int success = 1;
        fwht_status_t first_error = FWHT_SUCCESS;
        
        #pragma omp parallel for if(batch_size > 1)
        for (int i = 0; i < batch_size; ++i) {
            if (success) {  /* Skip if another thread failed */
                fwht_status_t status = fwht_i32_backend(data_array[i], n, backend);
                if (status != FWHT_SUCCESS) {
                    #pragma omp critical
                    {
                        if (success) {
                            success = 0;
                            first_error = status;
                        }
                    }
                }
            }
        }
        
        return first_error;
    }
#endif
    
    /* Sequential fallback */
    for (int i = 0; i < batch_size; ++i) {
        fwht_status_t status = fwht_i32_backend(data_array[i], n, backend);
        if (status != FWHT_SUCCESS) return status;
    }
    
    return FWHT_SUCCESS;
}

fwht_status_t fwht_batch_f64(fwht_context_t* ctx, double** data_array,
                             size_t n, int batch_size) {
    if (data_array == NULL) return FWHT_ERROR_NULL_POINTER;
    if (batch_size == 0) return FWHT_ERROR_INVALID_ARGUMENT;
    
    fwht_backend_t backend = (ctx != NULL) ? ctx->config.backend : FWHT_BACKEND_AUTO;
    
    /* Auto-select backend if needed */
    if (backend == FWHT_BACKEND_AUTO) {
        backend = fwht_recommend_backend(n);
    }
    
#ifdef FWHT_ENABLE_CUDA
    /* GPU batch: copy all arrays to device and process in parallel */
    if (backend == FWHT_BACKEND_GPU) {
        /* Allocate contiguous device memory for all arrays */
        double* d_data = NULL;
        size_t total_size = n * batch_size;
        cudaError_t err = cudaMalloc(&d_data, total_size * sizeof(double));
        if (err != cudaSuccess) {
            return FWHT_ERROR_CUDA;
        }
        
        /* Copy all arrays to device */
        for (int i = 0; i < batch_size; ++i) {
            err = cudaMemcpy(d_data + i * n, data_array[i], 
                           n * sizeof(double), cudaMemcpyHostToDevice);
            if (err != cudaSuccess) {
                cudaFree(d_data);
                return FWHT_ERROR_CUDA;
            }
        }
        
        /* Process batch on GPU */
        fwht_status_t status = fwht_batch_f64_cuda(d_data, n, batch_size);
        
        /* Copy results back */
        if (status == FWHT_SUCCESS) {
            for (int i = 0; i < batch_size; ++i) {
                err = cudaMemcpy(data_array[i], d_data + i * n,
                               n * sizeof(double), cudaMemcpyDeviceToHost);
                if (err != cudaSuccess) {
                    status = FWHT_ERROR_CUDA;
                    break;
                }
            }
        }
        
        cudaFree(d_data);
        return status;
    }
#endif
    
    /* CPU batch: parallelize with OpenMP if available */
#ifdef FWHT_ENABLE_OPENMP
    if (backend == FWHT_BACKEND_OPENMP || batch_size > 4) {
        int success = 1;
        fwht_status_t first_error = FWHT_SUCCESS;
        
        #pragma omp parallel for if(batch_size > 1)
        for (int i = 0; i < batch_size; ++i) {
            if (success) {  /* Skip if another thread failed */
                fwht_status_t status = fwht_f64_backend(data_array[i], n, backend);
                if (status != FWHT_SUCCESS) {
                    #pragma omp critical
                    {
                        if (success) {
                            success = 0;
                            first_error = status;
                        }
                    }
                }
            }
        }
        
        return first_error;
    }
#endif
    
    /* Sequential fallback */
    for (int i = 0; i < batch_size; ++i) {
        fwht_status_t status = fwht_f64_backend(data_array[i], n, backend);
        if (status != FWHT_SUCCESS) return status;
    }
    
    return FWHT_SUCCESS;
}

from vero.metrics import RecallScore, PrecisionScore, SufficiencyScore, CitationScore, MeanRR, MeanAP, RerankerNDCG, CumulativeNDCG
from vero.metrics import OverlapScore, BertScore, RougeScore, SemScore, BartScore, BleurtScore, AlignScore, GEvalScore, NumericalHallucinationScore


ch_r=[1,2,3,5,6]
ch_t=[2,3,4]
rs = RecallScore(ch_r,ch_t)
ps = PrecisionScore(ch_r,ch_t)
ss = SufficiencyScore(ch_r,ch_t)
cs = CitationScore(ch_r,ch_t)
print(rs.evaluate())
print(ps.evaluate())
print(ss.evaluate())
print(cs.evaluate())
#
rr = [[1,2,3,5,6],[1,2,3,5,6]]
gt = [[2,3,6],[2,3,6]]
ranks = [{2: 2, 3: 2},{2: 2, 3: 2, 6: 1}]
mrr = MeanRR(rr,gt)
map = MeanAP(rr,gt)
rndcg = RerankerNDCG(rr,ranks)
cndcg = CumulativeNDCG(rr,ranks)
print(mrr.evaluate())
print(map.evaluate())
print(rndcg.evaluate())
print(cndcg.evaluate())

kt = ['vitamins','neurotoxin']
ans = 'You are advised to increase the intake of vitamins to counter the neurotoxin that you have ingested.'
os = OverlapScore(ans,kt)
print(os.evaluate())

import re
import pandas as pd
from tqdm import tqdm
import torch

def extract_page_content(text_blob):
    pattern = r"page_content='(.*?)'"
    matches = re.findall(pattern, text_blob)
    return matches
df_new = pd.read_csv(r'testing.csv')
chunks_list = df_new['Context'].apply(extract_page_content).tolist()
answers_list = df_new['Answer'].tolist()

print("Processing SemScore...")
with SemScore() as sem_score:
    sem_results = [sem_score.evaluate(chunk, ans) for chunk, ans in tqdm(zip(chunks_list, answers_list), total=len(df_new))]
# print(sem_results)


print("\nProcessing BERT Score...")
bs = BertScore()
with BertScore() as bs:
    bert_results = [bs.evaluate(chunk, ans) for chunk, ans in tqdm(zip(chunks_list, answers_list), total=len(df_new))]
bert_dicts = [{'Precision': p, 'Recall': r, 'F1score': f} for p, r, f in bert_results]
print(bert_dicts)


print("\nProcessing RougeL Score...")
with RougeScore() as rouge:
    rouge_results = [rouge.evaluate(chunk, ans) for chunk, ans in tqdm(zip(chunks_list, answers_list), total=len(df_new))]
rouge_dicts = [{'Precision': p, 'Recall': r, 'F1score': f} for p, r, f in rouge_results]
print(rouge_dicts)



print('BartScore')
with BartScore() as bart_score:
    score = [bart_score.evaluate(chunk, ans) for chunk, ans in tqdm(zip(chunks_list, answers_list), total=len(df_new))]
print(score)

print("Processing BLUERTScore...")
with BleurtScore() as bleurt:
    bl_results = [bleurt.evaluate(chunk, ans) for chunk, ans in tqdm(zip(chunks_list, answers_list), total=len(df_new))]
print(bl_results)

# TODO: Figure this out

allocated = torch.cuda.memory_allocated(device=0)
print(allocated)
print("Processing AlignScore...")
with AlignScore() as align:
    al_results = [align.evaluate(chunk, ans) for chunk, ans in tqdm(zip(chunks_list, answers_list), total=len(df_new))]
print(al_results)
allocated = torch.cuda.memory_allocated(device=0)
print(allocated)


print("\nProcessing G-Eval...")
with GEvalScore(api_key) as g_eval:
    g_eval_results = [g_eval.evaluate(chunk,ans, metric='Faithfulness') for chunk, ans in tqdm(zip(chunks_list, answers_list), total=len(df_new))]
print(g_eval_results)

from src.vero.evaluator import Evaluator
import pandas as pd

evaluator = Evaluator()

evaluator.evaluate_generation('', r'testing.csv')

evaluator.parse_retriever_data(r'test_dataset_generator.csv', 'testing.csv')

evaluator.evaluate_reranker('test_dataset_generator.csv', 'ranked_chunks_data.csv')

evaluator.evaluate_retrieval('testing.csv', 'ranked_chunks_data.csv')

from vero.report_generation_workflow import ReportGenerator

report_generator = ReportGenerator()

report_generator.generate_report('pipe_config_data.json','Generation_Scores.csv','Retrieval_Scores.csv','Reranked_Scores.csv')
# Multi-Formatter Comparison for Lab Equipment Dataset
# This config generates the SAME raw data but outputs it in 3 different formats
# Goal: Identify which format yields the best parameter accuracy after fine-tuning

dataset_system_prompt: |
  You are a laboratory automation assistant with access to analytical instruments and sample preparation equipment.
  You help scientists configure and operate lab equipment by calling the appropriate instrument control functions.

topic_tree:
  topic_prompt: |
    Laboratory workflows and analytical procedures across different scientific disciplines:
    - Molecular biology: PCR, DNA/RNA quantification, gel electrophoresis, cloning
    - Analytical chemistry: HPLC analysis, spectroscopy, chromatography, compound identification
    - Biochemistry: protein assays, enzyme kinetics, bradford assays, western blots
    - Cell biology: cell culture, flow cytometry, microscopy, cell counting
    - Quality control: sample testing, calibration curves, method validation
    - High-throughput screening: 96-well assays, drug discovery, compound libraries

  model: "gpt-4o-mini"
  provider: "openai"
  degree: 4
  depth: 5
  temperature: 0.9

  # Rate limiting for topic tree generation
  rate_limit:
    max_retries: 7
    base_delay: 3.0
    max_delay: 180.0
    backoff_strategy: "exponential_jitter"
    exponential_base: 2.0
    jitter: true
    respect_retry_after: true

  save_as: "lab_topics_comparison.jsonl"

data_engine:
  generation_system_prompt: |
    You are a laboratory automation assistant with access to analytical instruments and sample preparation equipment.
    You help scientists configure and operate lab equipment by calling the appropriate instrument control functions.

  dataset_system_prompt: |
    You are a laboratory automation assistant with access to analytical instruments and sample preparation equipment.
    You help scientists configure and operate lab equipment by calling the appropriate instrument control functions.

  provider: "openai"
  model:  "gpt-4o-mini"
  temperature: 0.8
  max_tokens: 8000  # Increased for long lab equipment tool responses

  # Rate limiting configuration to handle OpenAI TPM limits
  rate_limit:
    max_retries: 7
    base_delay: 3.0
    max_delay: 180.0
    backoff_strategy: "exponential_jitter"
    exponential_base: 2.0
    jitter: true
    respect_retry_after: true

  # Agent configuration
  conversation_type: "chain_of_thought"
  agent_mode: "single_turn"
  reasoning_style: "structured"

  # Tool configuration
  tool_registry_path: "examples/mcp/lab_equipment_tools.yaml"
  available_tools: []
  max_tools_per_query: 2
  max_tools_strict: false  # Keep samples with extra tools, just truncate to limit

  default_batch_size: 5
  default_num_examples: 0
  sys_msg: true

  instructions: |
    CRITICAL: Generate training examples with REALISTIC, SPECIFIC parameter values.
    NEVER use placeholders like [...], <value>, or generic examples.

    **TOOL USAGE:** Use exactly 1 or 2 tools per example. DO NOT use more than 2 tools.

    **TOOL RESPONSES:** Keep tool responses CONCISE (2-4 sentences max). Focus on key results,
    not detailed tables or lengthy reports. Example: "Scan completed. Peak absorbance: 0.78 AU
    at 525nm. Baseline correction applied successfully."

    **Numbers:** Use realistic values (e.g., 450, 12000, 2.5, not <value> or 0)
    **Strings:** Use exact enum values and realistic identifiers
    **Lists:** Provide actual arrays with specific items (e.g., ["A1", "A2", "A3"])
    **Dicts:** Complete gradient programs with all time points

    See lab_equipment_baseline.yaml for detailed parameter value requirements.

dataset:
  creation:
    num_steps: 510  # 200 total samples (40 steps Ã— 5 batch size)
    batch_size: 2 # Reduced to avoid rate limits

  save_as: "lab_comparison_raw.jsonl"

  # Generate the SAME data in 3 different formats for comparison
  formatters:
    # Format 1: OpenAI Schema (OpenAI function calling style)
    - name: "format_openai"
      template: "builtin://openai"
      output: "lab_comparison_trl.jsonl"
      config:
        include_system_prompt: true
        validate_tool_schemas: true
        remove_available_tools_field: false

    # Format 2: Standard Tool Calling
    - name: "format_standard"
      template: "builtin://tool_calling"
      output: "lab_comparison_standard.jsonl"
      config:
        include_tools_in_system: true

    # Format 3: Single Tool Call (stricter format)
    - name: "format_single"
      template: "builtin://single_tool_call"
      output: "lab_comparison_single.jsonl"
      config:
        strict_validation: true

# ===================================================================
# COMPARISON EXPERIMENT PLAN
# ===================================================================
#
# 1. GENERATE DATA:
#    deepfabric generate examples/mcp/lab_equipment_comparison.yaml
#
#    This creates:
#    - lab_comparison_raw.jsonl (200 samples, raw format)
#    - lab_comparison_trl.jsonl (200 samples, TRL format)
#    - lab_comparison_standard.jsonl (200 samples, standard format)
#    - lab_comparison_single.jsonl (200 samples, single-tool format)
#
# 2. SPLIT EACH DATASET:
#    from deepfabric.evaluation import split_dataset, SplitConfig
#
#    config = SplitConfig(test_size=0.2, seed=42)
#
#    # Split TRL format
#    split_dataset("lab_comparison_trl.jsonl", "trl_train.jsonl", "trl_eval.jsonl", config)
#
#    # Split Standard format
#    split_dataset("lab_comparison_standard.jsonl", "standard_train.jsonl", "standard_eval.jsonl", config)
#
#    # Split Single format
#    split_dataset("lab_comparison_single.jsonl", "single_train.jsonl", "single_eval.jsonl", config)
#
# 3. TRAIN 3 MODELS (one per format):
#    - Model A: Train on trl_train.jsonl
#    - Model B: Train on standard_train.jsonl
#    - Model C: Train on single_train.jsonl
#
#    (Use same hyperparameters for fair comparison)
#
# 4. EVALUATE ALL 3 MODELS:
#    - Evaluate each on the SAME eval set (use trl_eval.jsonl)
#    - Compare parameter accuracy scores
#
# 5. RESULTS:
#    The format with the highest parameter accuracy is the winner!
#
# ===================================================================

Metadata-Version: 2.4
Name: promptguard-pro
Version: 0.1.2
Summary: Production-ready framework for reliable LLM orchestration
License: MIT
License-File: LICENSE
Keywords: llm,ai,prompt,reliability,orchestration
Author: PromptGuard Team
Author-email: team@promptguard.dev
Requires-Python: >=3.9,<4.0
Classifier: Development Status :: 4 - Beta
Classifier: Intended Audience :: Developers
Classifier: License :: OSI Approved :: MIT License
Classifier: Programming Language :: Python :: 3
Classifier: Programming Language :: Python :: 3.9
Classifier: Programming Language :: Python :: 3.10
Classifier: Programming Language :: Python :: 3.11
Classifier: Programming Language :: Python :: 3.12
Classifier: Programming Language :: Python :: 3.13
Classifier: Programming Language :: Python :: 3.14
Provides-Extra: all
Provides-Extra: cache
Provides-Extra: metrics
Provides-Extra: providers
Provides-Extra: validation
Requires-Dist: aiohttp (>=3.9.0,<4.0.0)
Requires-Dist: anthropic (>=0.21.0,<0.22.0)
Requires-Dist: cohere (>=5.0.0,<6.0.0) ; extra == "providers" or extra == "all"
Requires-Dist: google-generativeai (>=0.3.0,<0.4.0) ; extra == "providers" or extra == "all"
Requires-Dist: groq (>=0.9.0,<0.10.0) ; extra == "providers" or extra == "all"
Requires-Dist: openai (>=1.12.0,<2.0.0)
Requires-Dist: prometheus-client (>=0.19.0,<0.20.0) ; extra == "metrics" or extra == "all"
Requires-Dist: pydantic (>=2.5.0,<3.0.0)
Requires-Dist: pydantic-settings (>=2.0.0,<3.0.0)
Requires-Dist: redis (>=5.0.0,<6.0.0) ; extra == "cache" or extra == "all"
Requires-Dist: sentence-transformers (>=2.3.0,<3.0.0) ; extra == "validation" or extra == "all"
Requires-Dist: structlog (>=24.1.0,<25.0.0)
Requires-Dist: tiktoken (>=0.6.0,<0.7.0)
Project-URL: Documentation, https://promptguard.dev/docs
Project-URL: Homepage, https://promptguard.dev
Project-URL: Repository, https://github.com/promptguard/promptguard
Description-Content-Type: text/markdown

# PromptGuard ðŸ›¡ï¸

**The Framework for Reliable LLM Orchestration**

PromptGuard is a Python library that brings production-grade reliability, type safety, and observability to LLM applications. Think of it as "Pydantic meets Circuit Breaker for AI" - reducing boilerplate by 80% while making your AI apps bulletproof.

[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)
[![Python 3.9+](https://img.shields.io/badge/python-3.9+-blue.svg)](https://www.python.org/downloads/)

---

## ðŸŽ¯ The Problem We Solve

Every AI engineer writes the same boilerplate:
- Manual retry logic with exponential backoff
- Model fallback chains when primary fails
- Response parsing with regex/string manipulation
- Token counting and cost tracking
- Response validation and error handling

**PromptGuard eliminates all of this.**

---

## âœ¨ Core Features

### 1. Smart Execution with Auto-Retry & Fallbacks
```python
from promptguard import PromptChain

chain = PromptChain(
    models=["anthropic/claude-3-5-sonnet", "openai/gpt-4o", "groq/llama-70b"],
    strategy="cascade",
    max_retries=3,
    retry_delay="exponential"
)

result = await chain.execute("Analyze this document...")
print(result.response)  # Guaranteed to succeed or raise clear error
```

### 2. Type-Safe Response Validation
```python
from pydantic import BaseModel, Field
from promptguard import PromptChain

class EvaluationResponse(BaseModel):
    evaluation: str = Field(description="Overall evaluation")
    score: int = Field(ge=0, le=100)
    reason: str

chain = PromptChain(
    models=["anthropic/claude-3-5-sonnet"],
    response_schema=EvaluationResponse,
    validation_mode="strict"
)

result = await chain.execute(prompt)
print(result.response.score)  # Type-safe!
```

### 3. Multi-Provider Support
```python
chain = PromptChain(
    models=[
        "anthropic/claude-3-5-sonnet",
        "openai/gpt-4o",
        "groq/llama-3-70b",
        "cohere/command-r-plus",
        "google/gemini-1.5-pro"
    ]
)
```

### 4. Automatic Token Tracking & Cost Estimation
```python
result = await chain.execute(prompt)

print(result.metadata.tokens_used)
print(result.metadata.estimated_cost)
print(result.metadata.model_used)
print(result.metadata.execution_time_ms)
```

### 5. Response Caching
```python
from promptguard import CacheBackend

chain = PromptChain(
    models=["anthropic/claude-3-5-sonnet"],
    cache=CacheBackend.memory(),  # or .redis() or .disk()
    cache_ttl=3600
)

result1 = await chain.execute("What is AI?", cache_key="ai_def_v1")
result2 = await chain.execute("What is AI?", cache_key="ai_def_v1")
assert result2.metadata.cached == True
```

### 6. Semantic Response Validation
```python
from promptguard import validators

chain = PromptChain(
    models=["anthropic/claude-3-5-sonnet"],
    validators=[
        validators.length_range(min_chars=100, max_chars=5000),
        validators.contains_keywords(["risk", "evaluation"]),
        validators.has_citations(required=True),
        validators.sentiment_check(allowed=["neutral", "positive"])
    ]
)
```

### 7. Streaming Support
```python
chain = PromptChain(models=["anthropic/claude-3-5-sonnet"])

async for chunk in chain.stream("Write a long essay..."):
    print(chunk.delta, end="", flush=True)
```

### 8. Batch Processing
```python
prompts = ["Evaluate doc 1...", "Evaluate doc 2...", ...]

results = await chain.batch_execute(
    prompts,
    max_concurrent=5,
    show_progress=True
)
```

---

## ðŸ“¦ Installation

```bash
# Basic installation
pip install promptguard

# With all features
pip install promptguard[all]

# With specific features
pip install promptguard[cache]      # Redis caching
pip install promptguard[validation]  # Semantic validators
pip install promptguard[metrics]     # Prometheus metrics
```

---

## ðŸš€ Quick Start

```python
import asyncio
from promptguard import PromptChain, validators, CacheBackend
from pydantic import BaseModel

class Analysis(BaseModel):
    summary: str
    score: int
    recommendations: list[str]

async def main():
    chain = PromptChain(
        models=[
            "anthropic/claude-3-5-sonnet",
            "openai/gpt-4o",
            "groq/llama-70b"
        ],
        strategy="cascade",
        max_retries=3,
        response_schema=Analysis,
        validators=[
            validators.length_range(min_chars=100),
            validators.has_citations()
        ],
        cache=CacheBackend.memory(),
        cache_ttl=3600
    )
    
    result = await chain.execute(
        prompt="Analyze this document: ...",
        cache_key="analysis_v1"
    )
    
    print(f"Score: {result.response.score}")
    print(f"Cost: ${result.metadata.estimated_cost:.4f}")
    print(f"Time: {result.metadata.execution_time_ms:.0f}ms")

asyncio.run(main())
```
---

## ðŸ“š Documentation

- [Getting Started](./docs/getting_started.md)
- [API Reference](./docs/api_reference.md)
- [Examples](./examples/)
- [Advanced Usage](./docs/advanced.md)
- [Contributing](./CONTRIBUTING.md)

---

## ðŸ§ª Testing

```bash
# Run all tests
pytest tests/ -v

# Run with coverage
pytest tests/ --cov=promptguard --cov-report=html

# Run specific test file
pytest tests/unit/test_core.py -v
```

---

## ðŸ”§ Configuration

### Environment Variables

```bash
# API Keys
export ANTHROPIC_API_KEY=sk-ant-...
export OPENAI_API_KEY=sk-...
export GROQ_API_KEY=gsk-...
export COHERE_API_KEY=...
export GOOGLE_API_KEY=...

# Redis
export REDIS_URL=redis://localhost:6379
```

---

## ðŸ¤ Contributing

Contributions are welcome! Please see [CONTRIBUTING.md](./CONTRIBUTING.md) for guidelines.

---

## ðŸ“„ License

MIT License - See [LICENSE](./LICENSE) file for details

---


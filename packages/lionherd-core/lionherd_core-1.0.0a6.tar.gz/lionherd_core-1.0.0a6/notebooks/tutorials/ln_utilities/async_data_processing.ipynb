{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial: Async Data Processing with alcall/bcall\n",
    "\n",
    "**Category**: ln Utilities\n",
    "**Difficulty**: Intermediate\n",
    "**Time**: 20-25 minutes\n",
    "\n",
    "## Problem Statement\n",
    "\n",
    "Processing large datasets with I/O-bound operations (API calls, LLM inference, database queries) sequentially wastes time. Naive parallelization without rate limiting overwhelms services.\n",
    "\n",
    "**Why This Matters**:\n",
    "- **Performance**: Concurrent processing reduces wall time by 10-100×\n",
    "- **Resource Management**: Rate limiting prevents service disruption\n",
    "- **Reliability**: Retry with backoff handles transient failures\n",
    "\n",
    "**What You'll Build**:\n",
    "Production-ready async data processing pipelines using `alcall()` (async map) and `bcall()` (batch processor)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "\n",
    "**Prior Knowledge**:\n",
    "- Python async/await fundamentals\n",
    "- Basic understanding of concurrency concepts\n",
    "\n",
    "**Required Packages**:\n",
    "```bash\n",
    "pip install lionherd-core  # >=0.1.0\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard library\n",
    "import asyncio\n",
    "from dataclasses import dataclass\n",
    "from time import time\n",
    "\n",
    "# lionherd-core\n",
    "from lionherd_core.ln import alcall, bcall"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solution Overview\n",
    "\n",
    "We'll implement async data processing patterns:\n",
    "\n",
    "1. **alcall Basics**: Concurrent mapping with concurrency control\n",
    "2. **Rate Limiting**: Control concurrent requests to protect services\n",
    "3. **bcall Batching**: Process large datasets in memory-efficient batches\n",
    "4. **Production Patterns**: Retry strategies, error handling, fallbacks\n",
    "\n",
    "**Key Components**:\n",
    "- `alcall`: Async map with retry, timeout, and concurrency control\n",
    "- `bcall`: Batch processor (async generator yielding results)\n",
    "\n",
    "**Pattern**: Process data concurrently while respecting service limits and handling failures gracefully."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: alcall Basics\n",
    "\n",
    "Understand concurrent async mapping with timeout and concurrency control.\n",
    "\n",
    "**Key Point**: `alcall()` processes items concurrently while preserving input order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 10 items in 0.10s\n",
      "Results: [{'id': 0, 'data': 'result_0'}, {'id': 1, 'data': 'result_1'}, {'id': 2, 'data': 'result_2'}]...\n",
      "Speedup: ~9.6x vs sequential\n"
     ]
    }
   ],
   "source": [
    "# Simulated async API call\n",
    "async def fetch_data(item_id: int) -> dict:\n",
    "    \"\"\"Simulate API call with variable latency.\"\"\"\n",
    "    await asyncio.sleep(0.1)  # Simulate network delay\n",
    "    return {\"id\": item_id, \"data\": f\"result_{item_id}\"}\n",
    "\n",
    "\n",
    "async def demo_basic():\n",
    "    # Sequential: ~1 second for 10 items\n",
    "    items = list(range(10))\n",
    "\n",
    "    start = time()\n",
    "    # Concurrent processing (no limit)\n",
    "    results = await alcall(items, fetch_data)\n",
    "    elapsed = time() - start\n",
    "\n",
    "    print(f\"Processed {len(results)} items in {elapsed:.2f}s\")\n",
    "    print(f\"Results: {results[:3]}...\")  # First 3 results\n",
    "    print(f\"Speedup: ~{10 * 0.1 / elapsed:.1f}x vs sequential\")\n",
    "\n",
    "\n",
    "await demo_basic()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Concurrency Control with max_concurrent\n",
    "\n",
    "Limit concurrent operations to prevent overwhelming services.\n",
    "\n",
    "**Key Point**: `max_concurrent` uses semaphore to cap active tasks (critical for rate limiting)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unlimited: 0.20s (all 20 concurrent)\n",
      "Limited (5): 0.80s (batches of 5)\n",
      "Success: unlimited=20, limited=20\n"
     ]
    }
   ],
   "source": [
    "async def simulate_api_call(item_id: int) -> dict:\n",
    "    \"\"\"Simulate API with rate limits (max 5 concurrent).\"\"\"\n",
    "    await asyncio.sleep(0.2)\n",
    "    return {\"id\": item_id, \"status\": \"success\"}\n",
    "\n",
    "\n",
    "async def demo_rate_limiting():\n",
    "    items = list(range(20))\n",
    "\n",
    "    # Without limit: might overwhelm API\n",
    "    start = time()\n",
    "    results_unlimited = await alcall(items, simulate_api_call)\n",
    "    elapsed_unlimited = time() - start\n",
    "\n",
    "    # With limit: max 5 concurrent (respects API limits)\n",
    "    start = time()\n",
    "    results_limited = await alcall(\n",
    "        items,\n",
    "        simulate_api_call,\n",
    "        max_concurrent=5,  # Semaphore: only 5 tasks active at once\n",
    "    )\n",
    "    elapsed_limited = time() - start\n",
    "\n",
    "    print(f\"Unlimited: {elapsed_unlimited:.2f}s (all 20 concurrent)\")\n",
    "    print(f\"Limited (5): {elapsed_limited:.2f}s (batches of 5)\")\n",
    "    print(f\"Success: unlimited={len(results_unlimited)}, limited={len(results_limited)}\")\n",
    "\n",
    "\n",
    "await demo_rate_limiting()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Retry Strategies\n",
    "\n",
    "Handle transient failures with exponential backoff.\n",
    "\n",
    "**Key Point**: `retry_attempts` + `retry_backoff` implements resilient error recovery."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results: [{'id': 1, 'attempts': 3}, {'id': 2, 'attempts': 3}, {'id': 3, 'attempts': 3}]\n",
      "All succeeded after retries\n"
     ]
    }
   ],
   "source": [
    "# Simulated flaky API\n",
    "_call_counts = {}\n",
    "\n",
    "\n",
    "async def flaky_api(item_id: int) -> dict:\n",
    "    \"\"\"Fails first 2 attempts, succeeds on 3rd.\"\"\"\n",
    "    _call_counts[item_id] = _call_counts.get(item_id, 0) + 1\n",
    "\n",
    "    if _call_counts[item_id] < 3:\n",
    "        raise ConnectionError(f\"Transient failure for {item_id}\")\n",
    "\n",
    "    return {\"id\": item_id, \"attempts\": _call_counts[item_id]}\n",
    "\n",
    "\n",
    "async def demo_retry():\n",
    "    _call_counts.clear()  # Reset counts\n",
    "    items = [1, 2, 3]\n",
    "\n",
    "    # With retry: succeeds after 3 attempts\n",
    "    results = await alcall(\n",
    "        items,\n",
    "        flaky_api,\n",
    "        retry_attempts=3,  # Max 3 retries\n",
    "        retry_initial_delay=0.1,  # Start with 100ms\n",
    "        retry_backoff=2.0,  # Double delay each retry (100ms → 200ms → 400ms)\n",
    "        return_exceptions=False,  # Raise if all retries fail\n",
    "    )\n",
    "\n",
    "    print(f\"Results: {results}\")\n",
    "    print(\"All succeeded after retries\")\n",
    "\n",
    "\n",
    "await demo_retry()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Timeout Protection\n",
    "\n",
    "Prevent indefinite hangs with per-call timeouts.\n",
    "\n",
    "**Key Point**: `retry_timeout` bounds execution time per item (uses `move_on_after`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Item 1: ✓ Success\n",
      "Item 2: ✓ Success\n",
      "Item 5: ✗ Timeout\n",
      "Item 7: ✓ Success\n"
     ]
    }
   ],
   "source": [
    "async def slow_operation(item_id: int) -> dict:\n",
    "    \"\"\"Simulates operation that might hang.\"\"\"\n",
    "    if item_id == 5:\n",
    "        await asyncio.sleep(10)  # Simulates hang\n",
    "    else:\n",
    "        await asyncio.sleep(0.1)\n",
    "    return {\"id\": item_id}\n",
    "\n",
    "\n",
    "async def demo_timeout():\n",
    "    items = [1, 2, 5, 7]  # Item 5 will timeout\n",
    "\n",
    "    # With timeout: prevents hanging on item 5\n",
    "    results = await alcall(\n",
    "        items,\n",
    "        slow_operation,\n",
    "        retry_timeout=1.0,  # Max 1 second per call\n",
    "        return_exceptions=True,  # Return TimeoutError instead of raising\n",
    "    )\n",
    "\n",
    "    for i, result in zip(items, results, strict=True):\n",
    "        if isinstance(result, TimeoutError):\n",
    "            print(f\"Item {i}: ✗ Timeout\")\n",
    "        else:\n",
    "            print(f\"Item {i}: ✓ Success\")\n",
    "\n",
    "\n",
    "await demo_timeout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5: bcall for Batch Processing\n",
    "\n",
    "Process large datasets in memory-efficient batches.\n",
    "\n",
    "**Key Point**: `bcall()` is an async generator yielding batch results (prevents loading all results in memory)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 1: 10 items\n",
      "  First result: {'id': 0, 'processed': True}\n",
      "\n",
      "Processed 100 items in 10 batches\n"
     ]
    }
   ],
   "source": [
    "async def process_item(item: int) -> dict:\n",
    "    \"\"\"Simulate processing.\"\"\"\n",
    "    await asyncio.sleep(0.05)\n",
    "    return {\"id\": item, \"processed\": True}\n",
    "\n",
    "\n",
    "async def demo_batching():\n",
    "    # Large dataset (1000 items)\n",
    "    items = list(range(100))\n",
    "\n",
    "    # Process in batches of 10\n",
    "    batch_num = 0\n",
    "    total_processed = 0\n",
    "\n",
    "    async for batch_results in bcall(\n",
    "        items,\n",
    "        process_item,\n",
    "        batch_size=10,  # 10 items per batch\n",
    "        max_concurrent=5,  # 5 concurrent within each batch\n",
    "    ):\n",
    "        batch_num += 1\n",
    "        total_processed += len(batch_results)\n",
    "\n",
    "        # Process batch results immediately (streaming pattern)\n",
    "        if batch_num == 1:\n",
    "            print(f\"Batch {batch_num}: {len(batch_results)} items\")\n",
    "            print(f\"  First result: {batch_results[0]}\")\n",
    "\n",
    "    print(f\"\\nProcessed {total_processed} items in {batch_num} batches\")\n",
    "\n",
    "\n",
    "await demo_batching()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 6: Throttling Requests\n",
    "\n",
    "Add delay between starting tasks to smooth out request rate.\n",
    "\n",
    "**Key Point**: `throttle_period` staggers task starts (useful for strict rate limits)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 10 items in 0.96s\n",
      "Rate: ~10.4 req/s (respects 10 req/s limit)\n"
     ]
    }
   ],
   "source": [
    "async def rate_limited_api(item_id: int) -> dict:\n",
    "    \"\"\"API with strict rate limit: 10 requests/second.\"\"\"\n",
    "    await asyncio.sleep(0.05)\n",
    "    return {\"id\": item_id}\n",
    "\n",
    "\n",
    "async def demo_throttling():\n",
    "    items = list(range(10))\n",
    "\n",
    "    start = time()\n",
    "    # Throttle: 0.1s delay between starting each task\n",
    "    results = await alcall(\n",
    "        items,\n",
    "        rate_limited_api,\n",
    "        throttle_period=0.1,  # 100ms between task starts (10 req/s)\n",
    "        max_concurrent=3,  # Still limit concurrent\n",
    "    )\n",
    "    elapsed = time() - start\n",
    "\n",
    "    print(f\"Processed {len(results)} items in {elapsed:.2f}s\")\n",
    "    print(f\"Rate: ~{len(results) / elapsed:.1f} req/s (respects 10 req/s limit)\")\n",
    "\n",
    "\n",
    "await demo_throttling()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Complete Working Example\n",
    "\n",
    "Production-ready LLM batch inference with comprehensive error handling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Error: ValueError: Invalid prompt: error: invalid\n",
      "  Error: ValueError: Invalid prompt: error: invalid\n",
      "  Error: ValueError: Invalid prompt: error: invalid\n",
      "  Error: ValueError: Invalid prompt: error: invalid\n",
      "  Error: ValueError: Invalid prompt: error: invalid\n",
      "\n",
      "Completed in 9.43s\n",
      "Success: 15\n",
      "Errors: 5\n",
      "Throughput: 2.1 prompts/s\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Production async batch processor.\"\"\"\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class ProcessingConfig:\n",
    "    max_concurrent: int = 5\n",
    "    batch_size: int = 10\n",
    "    retry_attempts: int = 3\n",
    "    retry_initial_delay: float = 0.5\n",
    "    retry_backoff: float = 2.0\n",
    "    timeout_per_item: float = 30.0\n",
    "    throttle_period: float = 0.1\n",
    "\n",
    "\n",
    "class BatchProcessor:\n",
    "    \"\"\"Production batch processor with retry and rate limiting.\"\"\"\n",
    "\n",
    "    def __init__(self, config: ProcessingConfig = None):\n",
    "        self.config = config or ProcessingConfig()\n",
    "\n",
    "    async def process_batch(self, items: list, process_func, handle_error=None):\n",
    "        \"\"\"Process items in batches with error handling.\"\"\"\n",
    "        results = []\n",
    "        errors = []\n",
    "\n",
    "        async for batch_results in bcall(\n",
    "            items,\n",
    "            process_func,\n",
    "            batch_size=self.config.batch_size,\n",
    "            max_concurrent=self.config.max_concurrent,\n",
    "            retry_attempts=self.config.retry_attempts,\n",
    "            retry_initial_delay=self.config.retry_initial_delay,\n",
    "            retry_backoff=self.config.retry_backoff,\n",
    "            retry_timeout=self.config.timeout_per_item,\n",
    "            throttle_period=self.config.throttle_period,\n",
    "            return_exceptions=True,  # Don't fail entire batch on error\n",
    "        ):\n",
    "            # Separate successes from failures\n",
    "            for result in batch_results:\n",
    "                if isinstance(result, BaseException):\n",
    "                    errors.append(result)\n",
    "                    if handle_error:\n",
    "                        await handle_error(result)\n",
    "                else:\n",
    "                    results.append(result)\n",
    "\n",
    "        return {\"results\": results, \"errors\": errors}\n",
    "\n",
    "\n",
    "# Simulated LLM inference\n",
    "async def llm_inference(prompt: str) -> dict:\n",
    "    \"\"\"Simulate LLM API call.\"\"\"\n",
    "    await asyncio.sleep(0.2)  # Simulate inference time\n",
    "    if \"error\" in prompt:\n",
    "        raise ValueError(f\"Invalid prompt: {prompt}\")\n",
    "    return {\"prompt\": prompt, \"response\": f\"Generated from: {prompt}\"}\n",
    "\n",
    "\n",
    "# Usage\n",
    "async def demo_production():\n",
    "    prompts = [\n",
    "        \"Explain quantum computing\",\n",
    "        \"Write a poem about AI\",\n",
    "        \"error: invalid\",  # Will fail\n",
    "        \"Summarize this article\",\n",
    "    ] * 5  # 20 prompts total\n",
    "\n",
    "    config = ProcessingConfig(\n",
    "        max_concurrent=3,\n",
    "        batch_size=5,\n",
    "        retry_attempts=2,\n",
    "        timeout_per_item=10.0,\n",
    "    )\n",
    "\n",
    "    processor = BatchProcessor(config)\n",
    "\n",
    "    async def log_error(error):\n",
    "        print(f\"  Error: {type(error).__name__}: {error}\")\n",
    "\n",
    "    start = time()\n",
    "    result = await processor.process_batch(prompts, llm_inference, log_error)\n",
    "    elapsed = time() - start\n",
    "\n",
    "    print(f\"\\nCompleted in {elapsed:.2f}s\")\n",
    "    print(f\"Success: {len(result['results'])}\")\n",
    "    print(f\"Errors: {len(result['errors'])}\")\n",
    "    print(f\"Throughput: {len(prompts) / elapsed:.1f} prompts/s\")\n",
    "\n",
    "\n",
    "await demo_production()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Production Considerations\n",
    "\n",
    "### Error Handling Patterns\n",
    "\n",
    "```python\n",
    "# Pattern 1: Return exceptions for partial success\n",
    "results = await alcall(\n",
    "    items,\n",
    "    func,\n",
    "    return_exceptions=True,  # Get partial results\n",
    ")\n",
    "\n",
    "successes = [r for r in results if not isinstance(r, BaseException)]\n",
    "failures = [r for r in results if isinstance(r, BaseException)]\n",
    "\n",
    "# Pattern 2: Default value on failure\n",
    "results = await alcall(\n",
    "    items,\n",
    "    func,\n",
    "    retry_attempts=3,\n",
    "    retry_default=None,  # Return None after retry exhaustion\n",
    ")\n",
    "```\n",
    "\n",
    "### Performance Tuning\n",
    "\n",
    "**Concurrency Limits by Use Case**:\n",
    "- Public APIs: 5-10 concurrent (respect rate limits)\n",
    "- LLM inference: 3-5 concurrent (cost/throughput balance)\n",
    "- Database queries: 10-50 concurrent (connection pool size)\n",
    "- Internal services: 50-200 concurrent (based on capacity)\n",
    "\n",
    "**Batch Size Guidelines**:\n",
    "- Memory-bound: Smaller batches (10-50 items)\n",
    "- CPU-bound: Larger batches (100-1000 items)\n",
    "- Progress tracking: Smaller batches for frequent updates\n",
    "\n",
    "### Testing Strategies\n",
    "\n",
    "```python\n",
    "async def test_retry_exhaustion():\n",
    "    \"\"\"Verify retry attempts are exhausted correctly.\"\"\"\n",
    "    \n",
    "    async def always_fail(x):\n",
    "        raise ValueError(\"Always fails\")\n",
    "    \n",
    "    results = await alcall(\n",
    "        [1, 2],\n",
    "        always_fail,\n",
    "        retry_attempts=2,\n",
    "        retry_default=\"FAILED\",\n",
    "    )\n",
    "    \n",
    "    assert results == [\"FAILED\", \"FAILED\"]\n",
    "\n",
    "\n",
    "async def test_timeout_triggers():\n",
    "    \"\"\"Verify timeout protection works.\"\"\"\n",
    "    \n",
    "    async def slow(x):\n",
    "        await asyncio.sleep(5)\n",
    "        return x\n",
    "    \n",
    "    results = await alcall(\n",
    "        [1],\n",
    "        slow,\n",
    "        retry_timeout=0.1,\n",
    "        return_exceptions=True,\n",
    "    )\n",
    "    \n",
    "    assert isinstance(results[0], TimeoutError)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Real-World Use Cases\n",
    "\n",
    "### Use Case 1: Concurrent API Fetching\n",
    "\n",
    "```python\n",
    "async def fetch_user_data(user_ids: list[int]) -> list[dict]:\n",
    "    \"\"\"Fetch user data from external API.\"\"\"\n",
    "    return await alcall(\n",
    "        user_ids,\n",
    "        lambda uid: fetch_api(f\"/users/{uid}\"),\n",
    "        max_concurrent=10,  # API rate limit\n",
    "        retry_attempts=3,\n",
    "        retry_timeout=5.0,\n",
    "        retry_backoff=2.0,\n",
    "    )\n",
    "```\n",
    "\n",
    "### Use Case 2: Batch LLM Inference\n",
    "\n",
    "```python\n",
    "async def batch_llm_inference(prompts: list[str]):\n",
    "    \"\"\"Process prompts in batches with rate limiting.\"\"\"\n",
    "    results = []\n",
    "    \n",
    "    async for batch in bcall(\n",
    "        prompts,\n",
    "        llm_api_call,\n",
    "        batch_size=10,\n",
    "        max_concurrent=3,  # Cost control\n",
    "        retry_attempts=2,\n",
    "        retry_timeout=30.0,\n",
    "    ):\n",
    "        results.extend(batch)\n",
    "        # Save intermediate results\n",
    "        await save_checkpoint(results)\n",
    "    \n",
    "    return results\n",
    "```\n",
    "\n",
    "### Use Case 3: Database Bulk Operations\n",
    "\n",
    "```python\n",
    "async def bulk_insert(records: list[dict]):\n",
    "    \"\"\"Insert records in batches.\"\"\"\n",
    "    async for batch_results in bcall(\n",
    "        records,\n",
    "        db.insert,\n",
    "        batch_size=100,  # Database batch size\n",
    "        max_concurrent=10,  # Connection pool size\n",
    "        retry_attempts=3,\n",
    "    ):\n",
    "        # Commit per batch\n",
    "        await db.commit()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "**What You Accomplished**:\n",
    "- ✅ Implemented concurrent async processing with `alcall()`\n",
    "- ✅ Added rate limiting and concurrency control\n",
    "- ✅ Built retry strategies with exponential backoff\n",
    "- ✅ Processed large datasets with memory-efficient batching via `bcall()`\n",
    "- ✅ Created production-ready error handling patterns\n",
    "\n",
    "**Key Takeaways**:\n",
    "1. **Concurrency != Parallelism**: `max_concurrent` controls active tasks (I/O-bound speedup)\n",
    "2. **Always use timeouts**: Unbounded operations hang production systems\n",
    "3. **Retry with backoff**: Exponential backoff handles transient failures gracefully\n",
    "4. **Batch for memory efficiency**: `bcall()` streams results instead of loading all in memory\n",
    "5. **Order preservation**: `alcall()`/`bcall()` maintain input order despite concurrent execution\n",
    "\n",
    "**When to Use**:\n",
    "- ✅ I/O-bound operations (API calls, database queries, file I/O)\n",
    "- ✅ LLM inference with rate limits\n",
    "- ✅ Large datasets requiring batch processing\n",
    "- ❌ CPU-bound tasks (use multiprocessing instead)\n",
    "- ❌ Operations requiring strict sequential ordering\n",
    "\n",
    "## Related Resources\n",
    "\n",
    "- [alcall API](../../../docs/api/ln/async_call.md)\n",
    "- [bcall API](../../../docs/api/ln/async_call.md)\n",
    "- [Async Path Creation](./async_path_creation.ipynb) - Another async utility tutorial\n",
    "- [Multi-Stage Pipeline](./multistage_pipeline.ipynb) - Composable data pipelines with `lcall()`"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lionherd-core",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

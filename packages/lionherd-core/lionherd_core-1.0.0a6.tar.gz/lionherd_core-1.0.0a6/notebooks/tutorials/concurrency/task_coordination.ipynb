{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial: Task Coordination Patterns\n",
    "\n",
    "**Category**: Concurrency  \n",
    "**Difficulty**: Advanced  \n",
    "**Time**: 20-30 minutes\n",
    "\n",
    "## Overview\n",
    "\n",
    "Production concurrent systems need two critical capabilities:\n",
    "\n",
    "1. **Fan-out/fan-in**: Distribute work across multiple workers (fan-out), collect results efficiently (fan-in)\n",
    "2. **Graceful shutdown**: Stop accepting new work, complete in-flight operations, run cleanup despite cancellation\n",
    "\n",
    "This tutorial teaches both patterns using lionherd-core's concurrency primitives:\n",
    "\n",
    "- `TaskGroup` - Structured concurrency for coordinating multiple concurrent operations\n",
    "- `Queue` - Work distribution with backpressure control\n",
    "- `shield()` - Protect cleanup operations from cancellation\n",
    "- `non_cancel_subgroup()` - Distinguish expected cancellations from real errors\n",
    "\n",
    "**Why These Patterns Matter**:\n",
    "- **Throughput**: Worker pools maximize CPU/I/O utilization (10-100x speedup for parallelizable tasks)\n",
    "- **Reliability**: Graceful shutdown prevents data loss (incomplete transactions, orphaned resources, cascading failures)\n",
    "- **Resource Management**: Bounded queues prevent memory exhaustion from fast producers overwhelming slow consumers\n",
    "- **Observability**: Proper error separation (cancellations vs failures) enables accurate alerting\n",
    "\n",
    "**What You'll Build**: Production-ready patterns for coordinating concurrent workers with proper lifecycle management, error handling, and graceful shutdown that you can immediately deploy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prerequisites and Setup\n",
    "\n",
    "**Prior Knowledge**:\n",
    "- Python async/await fundamentals (asyncio basics, coroutines, await syntax)\n",
    "- Basic understanding of producer-consumer patterns\n",
    "- Task groups and structured concurrency concepts (or willingness to learn)\n",
    "\n",
    "**Required Packages**:\n",
    "```bash\n",
    "pip install lionherd-core  # >=0.1.0\n",
    "```\n",
    "\n",
    "**Imports**:\n",
    "```python\n",
    "# lionherd-core concurrency primitives\n",
    "from lionherd_core.libs.concurrency import (\n",
    "    Queue,                    # Bounded async queue with backpressure\n",
    "    TaskGroup,                # Structured concurrency primitive\n",
    "    create_task_group,        # TaskGroup factory\n",
    "    shield,                   # Cancellation protection\n",
    "    non_cancel_subgroup,      # Exception group filtering\n",
    "    sleep,                    # Async sleep (checkpoint-aware)\n",
    "    fail_after,               # Timeout context manager\n",
    ")\n",
    "\n",
    "# Standard library\n",
    "import asyncio\n",
    "from typing import Any, Callable\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## API Overview\n",
    "\n",
    "### TaskGroup - Structured Concurrency\n",
    "\n",
    "Ensures all concurrent tasks complete or are cancelled together.\n",
    "\n",
    "```python\n",
    "async with create_task_group() as tg:\n",
    "    tg.start_soon(worker_1())\n",
    "    tg.start_soon(worker_2())\n",
    "```\n",
    "\n",
    "**Properties**: Structured lifetime (no orphaned tasks), error propagation (one fails → all cancel), resource safety.\n",
    "\n",
    "### Queue - Work Distribution with Backpressure\n",
    "\n",
    "Async FIFO queue with size limits.\n",
    "\n",
    "```python\n",
    "queue = Queue.with_maxsize(10)\n",
    "await queue.put(item)      # Blocks if full\n",
    "item = await queue.get()   # Blocks if empty\n",
    "```\n",
    "\n",
    "**Properties**: Bounded size (prevents memory exhaustion), backpressure (throttles fast producers).\n",
    "\n",
    "### shield() - Cancellation Protection\n",
    "\n",
    "Protects operations from outer cancellation.\n",
    "\n",
    "```python\n",
    "try:\n",
    "    with fail_after(1.0):\n",
    "        await long_operation()\n",
    "except TimeoutError:\n",
    "    await shield(cleanup())  # Completes despite timeout\n",
    "```\n",
    "\n",
    "**Use cases**: Closing connections, flushing buffers, persisting state.\n",
    "\n",
    "### non_cancel_subgroup() - Error Filtering\n",
    "\n",
    "Extracts non-cancellation exceptions from ExceptionGroup.\n",
    "\n",
    "```python\n",
    "except ExceptionGroup as eg:\n",
    "    real_errors = non_cancel_subgroup(eg)\n",
    "    if real_errors:\n",
    "        log_errors(real_errors)  # Only real failures\n",
    "```\n",
    "\n",
    "**Pattern**: Distinguish \"service failed\" from \"service stopped\" during shutdown."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pattern 1: Fan-Out/Fan-In with Queue Workers\n",
    "\n",
    "Distribute work across N workers, collect results. The fundamental pattern for parallel processing.\n",
    "\n",
    "**Key Concepts**:\n",
    "- Queue for work distribution (bounded, with backpressure)\n",
    "- TaskGroup to manage worker lifecycle (structured concurrency)\n",
    "- Poison pill pattern: sentinel value (object()) signals shutdown\n",
    "- Result collection: Workers append to shared list\n",
    "\n",
    "**Pattern Flow**:\n",
    "1. Create bounded queue and result list\n",
    "2. Start N workers in TaskGroup\n",
    "3. Feed tasks to queue (blocks if full - backpressure)\n",
    "4. Send poison pill to signal completion\n",
    "5. Workers process until seeing poison pill, then propagate it and exit\n",
    "6. TaskGroup blocks until all workers complete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Worker 0 exiting\n",
      "Worker 1 exiting\n",
      "Worker 2 exiting\n",
      "\n",
      "Processed 15 tasks with 3 workers\n",
      "Results (first 5): ['result-0', 'result-1', 'result-2', 'result-3', 'result-4']\n",
      "Results (last 5): ['result-10', 'result-11', 'result-12', 'result-13', 'result-14']\n"
     ]
    }
   ],
   "source": [
    "from typing import Any\n",
    "\n",
    "from lionherd_core.libs.concurrency import Queue, create_task_group, sleep\n",
    "\n",
    "\n",
    "async def fan_out_fan_in(tasks: list, num_workers: int = 3) -> list[Any]:\n",
    "    \"\"\"Distribute work to N workers, collect results.\n",
    "\n",
    "    Args:\n",
    "        tasks: List of async callables to execute\n",
    "        num_workers: Number of concurrent workers\n",
    "\n",
    "    Returns:\n",
    "        List of results from all tasks\n",
    "\n",
    "    Example:\n",
    "        tasks = [lambda: fetch(url) for url in urls]\n",
    "        results = await fan_out_fan_in(tasks, num_workers=5)\n",
    "    \"\"\"\n",
    "    work_queue = Queue.with_maxsize(10)  # Bounded queue\n",
    "    results = []  # Shared result list (append is thread-safe)\n",
    "    DONE = object()  # Poison pill sentinel\n",
    "\n",
    "    async def worker(worker_id: int):\n",
    "        \"\"\"Pull tasks from queue and process.\"\"\"\n",
    "        while True:\n",
    "            task = await work_queue.get()\n",
    "\n",
    "            # Check for poison pill\n",
    "            if task is DONE:\n",
    "                # Propagate to other workers\n",
    "                await work_queue.put(DONE)\n",
    "                print(f\"Worker {worker_id} exiting\")\n",
    "                break\n",
    "\n",
    "            # Process task\n",
    "            result = await task()\n",
    "            results.append(result)\n",
    "\n",
    "    async with create_task_group() as tg:\n",
    "        # Start workers\n",
    "        for i in range(num_workers):\n",
    "            tg.start_soon(worker, i)\n",
    "\n",
    "        # Feed tasks (blocks if queue full - backpressure)\n",
    "        for task in tasks:\n",
    "            await work_queue.put(task)\n",
    "\n",
    "        # Signal completion\n",
    "        await work_queue.put(DONE)\n",
    "\n",
    "    # TaskGroup exit means all workers completed\n",
    "    return results\n",
    "\n",
    "\n",
    "# Demo: Process 15 tasks with 3 workers\n",
    "async def work(i: int) -> str:\n",
    "    \"\"\"Simulate CPU/IO work.\"\"\"\n",
    "    await sleep(0.05)\n",
    "    return f\"result-{i}\"\n",
    "\n",
    "\n",
    "tasks = [lambda i=i: work(i) for i in range(15)]\n",
    "results = await fan_out_fan_in(tasks, num_workers=3)\n",
    "\n",
    "print(f\"\\nProcessed {len(results)} tasks with 3 workers\")\n",
    "print(f\"Results (first 5): {results[:5]}\")\n",
    "print(f\"Results (last 5): {results[-5:]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pattern Explanation: Fan-Out/Fan-In\n",
    "\n",
    "**How It Works**:\n",
    "\n",
    "1. **Fan-out**: Producer adds tasks to shared queue. Workers pull concurrently. Queue.maxsize provides backpressure.\n",
    "\n",
    "2. **Processing**: Workers process independently (embarrassingly parallel). Fast workers naturally process more tasks.\n",
    "\n",
    "3. **Fan-in**: Workers append to shared list. List.append() is atomic (no lock needed). Results in completion order.\n",
    "\n",
    "4. **Shutdown**: Producer sends DONE sentinel. First worker propagates it for others. All workers exit. TaskGroup guarantees cleanup.\n",
    "\n",
    "**Critical Details**:\n",
    "- **object() sentinel**: Unique, can't be confused with None or results\n",
    "- **Poison pill propagation**: N workers need N pills, propagation ensures all see it\n",
    "- **Result ordering**: Completion order, not submission order (use (index, result) tuples if order matters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pattern 2: Graceful Shutdown with shield()\n",
    "\n",
    "Services must complete cleanup operations even when cancelled. Without shield(), cleanup gets cancelled mid-execution, leaving inconsistent state.\n",
    "\n",
    "**Key Concepts**:\n",
    "- shield() makes cleanup immune to outer cancellation\n",
    "- Context manager pattern (__aexit__) guarantees cleanup runs\n",
    "- CancelledError must be re-raised after cleanup (don't suppress)\n",
    "\n",
    "**Pattern Flow**:\n",
    "1. Service starts (acquire resources: connections, files, locks)\n",
    "2. Service runs (process requests, handle events)\n",
    "3. Cancellation arrives (SIGTERM, timeout, exception in other tasks)\n",
    "4. __aexit__ called with exception info\n",
    "5. shield(cleanup) ensures cleanup completes despite cancellation\n",
    "6. Resources released, exceptions propagated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Demo 1: Normal operation ===\n",
      "[api-server] Starting\n",
      "[api-server] Started (resource: api-server-resource)\n",
      "Service running: True\n",
      "[api-server] Stopping (cleanup started)\n",
      "[api-server] Stopped (cleanup complete)\n",
      "\n",
      "Final state: running=False, resource=None\n",
      "\n",
      "=== Demo 2: Cancelled during operation ===\n",
      "[worker-pool] Starting\n",
      "[worker-pool] Started (resource: worker-pool-resource)\n",
      "Service running: True\n",
      "[worker-pool] Stopping (cleanup started)\n",
      "[worker-pool] Stopped (cleanup complete)\n",
      "Caught: TimeoutError\n",
      "Final state: running=False, resource=None\n",
      "\n",
      "=== Demo 3: Exception during operation ===\n",
      "[metrics] Starting\n",
      "[metrics] Started (resource: metrics-resource)\n",
      "[metrics] Stopping (cleanup started)\n",
      "[metrics] Stopped (cleanup complete)\n",
      "Caught: Simulated failure\n",
      "Final state: running=False, resource=None\n"
     ]
    }
   ],
   "source": [
    "from lionherd_core.libs.concurrency import fail_after, shield\n",
    "\n",
    "\n",
    "class Service:\n",
    "    \"\"\"Service with guaranteed cleanup via shield().\"\"\"\n",
    "\n",
    "    def __init__(self, name: str):\n",
    "        self.name = name\n",
    "        self.running = False\n",
    "        self.resource = None  # Simulate resource (connection, file, etc)\n",
    "\n",
    "    async def start(self):\n",
    "        \"\"\"Initialize service - acquire resources.\"\"\"\n",
    "        print(f\"[{self.name}] Starting\")\n",
    "        await sleep(0.05)  # Simulate startup time\n",
    "        self.running = True\n",
    "        self.resource = f\"{self.name}-resource\"  # Acquire resource\n",
    "        print(f\"[{self.name}] Started (resource: {self.resource})\")\n",
    "\n",
    "    async def stop(self):\n",
    "        \"\"\"Cleanup - this MUST complete to avoid resource leaks.\"\"\"\n",
    "        print(f\"[{self.name}] Stopping (cleanup started)\")\n",
    "\n",
    "        # Simulate cleanup work: close connections, flush buffers, etc\n",
    "        await sleep(0.15)  # Cleanup takes longer than startup\n",
    "\n",
    "        # Release resource\n",
    "        self.resource = None\n",
    "        self.running = False\n",
    "\n",
    "        print(f\"[{self.name}] Stopped (cleanup complete)\")\n",
    "\n",
    "    async def __aenter__(self):\n",
    "        \"\"\"Context manager entry.\"\"\"\n",
    "        await self.start()\n",
    "        return self\n",
    "\n",
    "    async def __aexit__(self, exc_type, exc_val, exc_tb):\n",
    "        \"\"\"Context manager exit - guaranteed cleanup.\n",
    "\n",
    "        shield() ensures cleanup completes even if:\n",
    "        - Outer context is cancelled\n",
    "        - Timeout expires\n",
    "        - Exception raised in service\n",
    "        \"\"\"\n",
    "        # Shield cleanup from outer cancellation\n",
    "        await shield(self.stop)\n",
    "\n",
    "        # Don't suppress exceptions (return False)\n",
    "        return False\n",
    "\n",
    "\n",
    "# Demo 1: Normal operation (no cancellation)\n",
    "print(\"=== Demo 1: Normal operation ===\")\n",
    "async with Service(\"api-server\") as svc:\n",
    "    print(f\"Service running: {svc.running}\")\n",
    "    await sleep(0.1)\n",
    "# Output: Start → Stop (cleanup completes)\n",
    "\n",
    "print(f\"\\nFinal state: running={svc.running}, resource={svc.resource}\\n\")\n",
    "\n",
    "\n",
    "# Demo 2: Cancellation during operation\n",
    "print(\"=== Demo 2: Cancelled during operation ===\")\n",
    "try:\n",
    "    async with Service(\"worker-pool\") as svc:\n",
    "        print(f\"Service running: {svc.running}\")\n",
    "        # Cancel before cleanup can complete\n",
    "        with fail_after(0.12):  # Timeout before cleanup finishes (0.15s)\n",
    "            await sleep(1.0)  # Would block forever\n",
    "except Exception as e:\n",
    "    print(f\"Caught: {type(e).__name__}\")\n",
    "\n",
    "print(f\"Final state: running={svc.running}, resource={svc.resource}\")\n",
    "# Output: Cleanup completes DESPITE timeout\n",
    "\n",
    "print(\"\\n=== Demo 3: Exception during operation ===\")\n",
    "try:\n",
    "    async with Service(\"metrics\") as svc:\n",
    "        raise ValueError(\"Simulated failure\")\n",
    "except ValueError as e:\n",
    "    print(f\"Caught: {e}\")\n",
    "\n",
    "print(f\"Final state: running={svc.running}, resource={svc.resource}\")\n",
    "# Output: Cleanup completes DESPITE exception"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pattern Explanation: Graceful Shutdown\n",
    "\n",
    "**Lifecycle States**: INITIAL → STARTING → RUNNING → STOPPING → STOPPED\n",
    "\n",
    "**Cancellation Sources**:\n",
    "- Outer timeout (fail_after, wait_for)\n",
    "- TaskGroup (one task failed, all cancelled)\n",
    "- OS signal (SIGTERM, SIGINT)\n",
    "\n",
    "**shield() Protection**:\n",
    "- `await shield(self.stop)` creates cancellation-immune scope\n",
    "- Cleanup completes even if outer context is cancelled\n",
    "- If cleanup itself times out, cancellation propagates\n",
    "\n",
    "**Resource Guarantees**: Connections closed, files flushed, locks released, state persisted.\n",
    "\n",
    "**Critical Details**:\n",
    "- **shield in __aexit__**: Ensures cleanup runs regardless of how context exits\n",
    "- **return False**: Don't suppress exceptions, propagate after cleanup\n",
    "- **Idempotency**: Calling stop() twice should be safe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combined Pattern: Worker Pool with Graceful Shutdown\n",
    "\n",
    "Real systems need both: distribute work across workers AND shutdown gracefully when cancelled.\n",
    "\n",
    "**Key Concepts**:\n",
    "- TaskGroup manages worker lifecycle\n",
    "- Queue distributes work\n",
    "- shutdown_event enables cooperative shutdown (\"please stop when convenient\")\n",
    "- Cancellation triggers forceful shutdown (\"stop now\")\n",
    "- shield() ensures cleanup completes\n",
    "\n",
    "**Two-Phase Shutdown**:\n",
    "1. **Cooperative** (Phase 1): Set shutdown_event, workers stop accepting new work\n",
    "2. **Forceful** (Phase 2): Cancel workers if they don't exit gracefully\n",
    "\n",
    "---\n",
    "\n",
    "## Error Handling: non_cancel_subgroup()\n",
    "\n",
    "ExceptionGroup may contain both expected cancellations and real errors. `non_cancel_subgroup()` filters out cancellations so you can handle real errors separately.\n",
    "\n",
    "**Why This Matters**:\n",
    "- During shutdown, cancellations are expected (not errors)\n",
    "- Real failures need investigation (alerts, logs, error tracking)\n",
    "- Mixing them creates alert fatigue (\"service stopped\" != \"service failed\")\n",
    "\n",
    "**Patterns to demonstrate**:\n",
    "1. Combined worker pool with graceful shutdown (next cell)\n",
    "2. Error filtering with non_cancel_subgroup() (following cell)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Feeding 20 tasks to 3 workers...\n",
      "Worker 0 started\n",
      "Worker 1 started\n",
      "Worker 2 started\n",
      "\n",
      "\n",
      ">>> Cancelling pool after 9 results...\n",
      "\n",
      "Worker 0 cancelled (forceful shutdown)\n",
      "Worker 0 exiting\n",
      "Worker 2 cancelled (forceful shutdown)\n",
      "Worker 2 exiting\n",
      "Worker 1 cancelled (forceful shutdown)\n",
      "Worker 1 exiting\n",
      "\n",
      "WorkerPool cancelled, initiating two-phase shutdown...\n",
      "Phase 1: Setting shutdown_event (cooperative)\n",
      "Phase 2: Cancelling workers (forceful)\n",
      "\n",
      "WorkerPool cleanup: draining remaining work...\n",
      "WorkerPool cleanup complete: processed 8 remaining items\n",
      "\n",
      ">>> Pool shutdown complete\n",
      "\n",
      "Total processed: 17 results, 0 errors\n",
      "Sample results: ['w0: task-0', 'w1: task-1', 'w2: task-2', 'w0: task-3', 'w1: task-4']\n",
      "More results: ['cleanup: task-17', 'cleanup: task-18', 'cleanup: task-19']\n"
     ]
    }
   ],
   "source": [
    "import asyncio\n",
    "\n",
    "from lionherd_core.libs.concurrency import Event, sleep\n",
    "\n",
    "\n",
    "class WorkerPool:\n",
    "    \"\"\"Worker pool with fan-out/fan-in and graceful shutdown.\n",
    "\n",
    "    Features:\n",
    "    - Distribute work across N workers (fan-out/fan-in)\n",
    "    - Cooperative shutdown (shutdown_event)\n",
    "    - Forceful shutdown (cancellation)\n",
    "    - Guaranteed cleanup (shield)\n",
    "    - Error collection (don't crash on individual failures)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, num_workers: int = 3, queue_size: int = 10):\n",
    "        self.num_workers = num_workers\n",
    "        self.work_queue = Queue.with_maxsize(queue_size)\n",
    "        self.results = []\n",
    "        self.errors = []\n",
    "        self.shutdown_event = Event()  # Cooperative shutdown signal\n",
    "\n",
    "    async def worker(self, worker_id: int):\n",
    "        \"\"\"Worker loop - processes until shutdown.\"\"\"\n",
    "        print(f\"Worker {worker_id} started\")\n",
    "\n",
    "        try:\n",
    "            while not self.shutdown_event.is_set():\n",
    "                try:\n",
    "                    # Non-blocking get with timeout (check shutdown_event periodically)\n",
    "                    task = await asyncio.wait_for(\n",
    "                        self.work_queue.get(),\n",
    "                        timeout=0.1,  # Check shutdown every 100ms\n",
    "                    )\n",
    "\n",
    "                    # Process task\n",
    "                    result = await task()\n",
    "                    self.results.append(f\"w{worker_id}: {result}\")\n",
    "\n",
    "                except TimeoutError:\n",
    "                    # No work available, check shutdown_event again\n",
    "                    continue\n",
    "\n",
    "                except Exception as e:\n",
    "                    # Task failed, collect error but continue processing\n",
    "                    self.errors.append(e)\n",
    "\n",
    "        except asyncio.CancelledError:\n",
    "            # Forceful shutdown (Phase 2)\n",
    "            print(f\"Worker {worker_id} cancelled (forceful shutdown)\")\n",
    "            raise\n",
    "\n",
    "        finally:\n",
    "            print(f\"Worker {worker_id} exiting\")\n",
    "\n",
    "    async def cleanup(self):\n",
    "        \"\"\"Drain remaining work, close resources.\n",
    "\n",
    "        This is shielded from cancellation, so it completes\n",
    "        even if outer context is cancelled.\n",
    "        \"\"\"\n",
    "        print(\"\\nWorkerPool cleanup: draining remaining work...\")\n",
    "\n",
    "        # Process remaining items in queue (non-blocking)\n",
    "        drained = 0\n",
    "        try:\n",
    "            while True:\n",
    "                task = self.work_queue.get_nowait()\n",
    "                result = await task()\n",
    "                self.results.append(f\"cleanup: {result}\")\n",
    "                drained += 1\n",
    "        except Exception:  # Queue empty\n",
    "            pass\n",
    "\n",
    "        print(f\"WorkerPool cleanup complete: processed {drained} remaining items\")\n",
    "\n",
    "    async def run(self, tasks: list):\n",
    "        \"\"\"Run worker pool until cancelled.\n",
    "\n",
    "        Two-phase shutdown:\n",
    "        1. Cooperative: Set shutdown_event (workers exit loops)\n",
    "        2. Forceful: Cancel workers (if they don't exit gracefully)\n",
    "        \"\"\"\n",
    "        try:\n",
    "            async with create_task_group() as tg:\n",
    "                # Start workers\n",
    "                for i in range(self.num_workers):\n",
    "                    tg.start_soon(self.worker, i)\n",
    "\n",
    "                # Feed work\n",
    "                print(f\"\\nFeeding {len(tasks)} tasks to {self.num_workers} workers...\")\n",
    "                for task in tasks:\n",
    "                    await self.work_queue.put(task)\n",
    "\n",
    "                # Wait for external cancellation or completion\n",
    "                # In production, this would be signal handling or service lifecycle\n",
    "                await asyncio.sleep(3600)  # Blocks until cancelled\n",
    "\n",
    "        except asyncio.CancelledError:\n",
    "            print(\"\\nWorkerPool cancelled, initiating two-phase shutdown...\")\n",
    "\n",
    "            # Phase 1: Cooperative shutdown\n",
    "            print(\"Phase 1: Setting shutdown_event (cooperative)\")\n",
    "            self.shutdown_event.set()\n",
    "\n",
    "            # Give workers brief time to exit gracefully\n",
    "            await sleep(0.05)\n",
    "\n",
    "            # Phase 2: Forceful cancellation (implicit via raise)\n",
    "            print(\"Phase 2: Cancelling workers (forceful)\")\n",
    "            raise\n",
    "\n",
    "        finally:\n",
    "            # Cleanup completes despite cancellation (shield protects it)\n",
    "            await shield(self.cleanup)\n",
    "\n",
    "\n",
    "# Demo: Cancel worker pool mid-execution\n",
    "async def demo_worker_pool():\n",
    "    pool = WorkerPool(num_workers=3, queue_size=10)\n",
    "\n",
    "    async def task(i: int) -> str:\n",
    "        await sleep(0.1)\n",
    "        return f\"task-{i}\"\n",
    "\n",
    "    tasks = [lambda i=i: task(i) for i in range(20)]\n",
    "\n",
    "    # Run pool in background\n",
    "    pool_task = asyncio.create_task(pool.run(tasks))\n",
    "\n",
    "    # Let it process some work, then cancel\n",
    "    await sleep(0.35)\n",
    "    print(f\"\\n\\n>>> Cancelling pool after {len(pool.results)} results...\\n\")\n",
    "    pool_task.cancel()\n",
    "\n",
    "    try:\n",
    "        await pool_task\n",
    "    except asyncio.CancelledError:\n",
    "        print(\"\\n>>> Pool shutdown complete\\n\")\n",
    "\n",
    "    print(f\"Total processed: {len(pool.results)} results, {len(pool.errors)} errors\")\n",
    "    print(f\"Sample results: {pool.results[:5]}\")\n",
    "    if len(pool.results) > 5:\n",
    "        print(f\"More results: {pool.results[-3:]}\")\n",
    "\n",
    "\n",
    "await demo_worker_pool()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Worker 0: Running normally...\n",
      "Worker 3: Running normally...\n",
      "Worker 4: Running normally...\n",
      "\n",
      "=== ExceptionGroup Analysis ===\n",
      "Total exceptions: 2\n",
      "\n",
      "All exceptions:\n",
      "  1. ValueError: Worker 1: Database connection failed\n",
      "  2. RuntimeError: Worker 2: Out of memory\n",
      "\n",
      "=== Real Errors (non-cancellations) ===\n",
      "Count: 2\n",
      "  1. ValueError: Worker 1: Database connection failed\n",
      "  2. RuntimeError: Worker 2: Out of memory\n",
      "\n",
      "=== Production Actions ===\n",
      "- Log to error tracking (Sentry, DataDog)\n",
      "- Alert on-call engineer\n",
      "- Increment error metrics\n",
      "- Execute error recovery procedures\n"
     ]
    }
   ],
   "source": [
    "from lionherd_core.libs.concurrency import ExceptionGroup, non_cancel_subgroup, sleep\n",
    "\n",
    "\n",
    "async def coordinated_workers_with_errors(num_workers: int = 5):\n",
    "    \"\"\"Worker pool demonstrating error vs cancellation distinction.\n",
    "\n",
    "    Scenario:\n",
    "    - Worker 1: Fails with ValueError (real error)\n",
    "    - Worker 2: Fails with RuntimeError (real error)\n",
    "    - Workers 3-5: Running normally, get cancelled due to worker 1/2 failures\n",
    "\n",
    "    Expected:\n",
    "    - TaskGroup filters CancelledErrors automatically (you won't see them in ExceptionGroup)\n",
    "    - ExceptionGroup contains only the 2 real errors\n",
    "    - non_cancel_subgroup() is still useful when cancellations ARE included (e.g., manual task spawning)\n",
    "    - Output shows: \"Total exceptions: 2\" (both are real errors)\n",
    "\n",
    "    Note: In other contexts (not TaskGroup), you may see both errors and cancellations.\n",
    "    This demonstrates the filtering pattern for those cases.\n",
    "    \"\"\"\n",
    "\n",
    "    async def worker(worker_id: int):\n",
    "        \"\"\"Worker that may fail or be cancelled.\"\"\"\n",
    "        await sleep(0.1)\n",
    "\n",
    "        # Workers 1-2 fail with real errors\n",
    "        if worker_id == 1:\n",
    "            raise ValueError(f\"Worker {worker_id}: Database connection failed\")\n",
    "        elif worker_id == 2:\n",
    "            raise RuntimeError(f\"Worker {worker_id}: Out of memory\")\n",
    "\n",
    "        # Other workers would run forever (will be cancelled)\n",
    "        print(f\"Worker {worker_id}: Running normally...\")\n",
    "        await sleep(3600)\n",
    "\n",
    "    try:\n",
    "        async with create_task_group() as tg:\n",
    "            for i in range(num_workers):\n",
    "                tg.start_soon(worker, i)\n",
    "\n",
    "            # TaskGroup automatically cancels all tasks when one raises\n",
    "\n",
    "    except ExceptionGroup as eg:\n",
    "        print(\"\\n=== ExceptionGroup Analysis ===\")\n",
    "        print(f\"Total exceptions: {len(eg.exceptions)}\")\n",
    "\n",
    "        # Show all exceptions\n",
    "        print(\"\\nAll exceptions:\")\n",
    "        for i, exc in enumerate(eg.exceptions, 1):\n",
    "            print(f\"  {i}. {type(exc).__name__}: {exc}\")\n",
    "\n",
    "        # Filter to real errors only\n",
    "        real_errors = non_cancel_subgroup(eg)\n",
    "\n",
    "        if real_errors:\n",
    "            print(\"\\n=== Real Errors (non-cancellations) ===\")\n",
    "            print(f\"Count: {len(real_errors.exceptions)}\")\n",
    "            for i, exc in enumerate(real_errors.exceptions, 1):\n",
    "                print(f\"  {i}. {type(exc).__name__}: {exc}\")\n",
    "\n",
    "            print(\"\\n=== Production Actions ===\")\n",
    "            print(\"- Log to error tracking (Sentry, DataDog)\")\n",
    "            print(\"- Alert on-call engineer\")\n",
    "            print(\"- Increment error metrics\")\n",
    "            print(\"- Execute error recovery procedures\")\n",
    "\n",
    "            # In production: raise to propagate\n",
    "            # raise real_errors\n",
    "        else:\n",
    "            print(\"\\n=== All Cancellations ===\")\n",
    "            print(\"All exceptions were cancellations (expected during shutdown)\")\n",
    "            print(\"No alerts needed - this is normal operation\")\n",
    "\n",
    "\n",
    "await coordinated_workers_with_errors()\n",
    "\n",
    "# Output shows:\n",
    "# - 5 total exceptions (2 errors + 3 cancellations)\n",
    "# - non_cancel_subgroup() extracts only the 2 errors\n",
    "# - Production can handle errors without false alarms from cancellations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Production-Ready Code\n",
    "\n",
    "Complete implementation combining all patterns. Copy-paste this into your project.\n",
    "\n",
    "**Features**:\n",
    "- Fan-out/fan-in worker pool with configurable workers\n",
    "- Graceful shutdown with two-phase termination\n",
    "- Error vs cancellation distinction\n",
    "- Shielded cleanup operations\n",
    "- Configurable timeouts\n",
    "- Production error handling and logging points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Processing Complete ===\n",
      "Total tasks: 50\n",
      "Successful: 38\n",
      "Failed: 5\n",
      "\n",
      "Sample successful results:\n",
      "  {'item': 1, 'processed': 2, 'status': 'success'}\n",
      "  {'item': 2, 'processed': 4, 'status': 'success'}\n",
      "  {'item': 3, 'processed': 6, 'status': 'success'}\n",
      "\n",
      "Sample errors:\n",
      "  ValueError: Processing failed for item 0\n",
      "  ValueError: Processing failed for item 10\n",
      "  ValueError: Processing failed for item 20\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Production-ready task coordination with graceful shutdown.\n",
    "Copy this cell into your project and customize.\n",
    "\"\"\"\n",
    "\n",
    "from collections.abc import Callable\n",
    "from typing import Any\n",
    "\n",
    "from lionherd_core.libs.concurrency import (\n",
    "    sleep,\n",
    ")\n",
    "\n",
    "\n",
    "class ProductionWorkerPool:\n",
    "    \"\"\"Production-ready worker pool with graceful shutdown.\n",
    "\n",
    "    Features:\n",
    "    - Fan-out/fan-in pattern for parallel processing\n",
    "    - Two-phase shutdown (cooperative + forceful)\n",
    "    - Shielded cleanup (guaranteed resource release)\n",
    "    - Error collection (per-task failures don't crash pool)\n",
    "    - Cancellation filtering (distinguish errors from expected cancellations)\n",
    "\n",
    "    Usage:\n",
    "        pool = ProductionWorkerPool(num_workers=10)\n",
    "        results, errors = await pool.process(tasks, process_func)\n",
    "\n",
    "    Args:\n",
    "        num_workers: Number of concurrent workers\n",
    "        queue_size: Max items in queue (backpressure threshold)\n",
    "        shutdown_timeout: Seconds to wait for cooperative shutdown\n",
    "        cleanup_timeout: Max seconds for cleanup operations\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_workers: int = 4,\n",
    "        queue_size: int = 20,\n",
    "        shutdown_timeout: float = 5.0,\n",
    "        cleanup_timeout: float = 10.0,\n",
    "    ):\n",
    "        self.num_workers = num_workers\n",
    "        self.queue_size = queue_size\n",
    "        self.shutdown_timeout = shutdown_timeout\n",
    "        self.cleanup_timeout = cleanup_timeout\n",
    "\n",
    "        self.work_queue: Queue = Queue.with_maxsize(queue_size)\n",
    "        self.results: list[Any] = []\n",
    "        self.errors: list[Exception] = []\n",
    "        self.shutdown_event = Event()\n",
    "        self.tasks_processed = 0\n",
    "\n",
    "    async def _worker(self, worker_id: int, process_func: Callable):\n",
    "        \"\"\"Worker loop - pulls from queue and processes.\n",
    "\n",
    "        Error handling:\n",
    "        - Task failures collected but don't crash worker\n",
    "        - CancelledError propagated (structured concurrency)\n",
    "        \"\"\"\n",
    "        processed = 0\n",
    "\n",
    "        try:\n",
    "            while not self.shutdown_event.is_set():\n",
    "                try:\n",
    "                    # Timeout enables periodic shutdown check\n",
    "                    task = await asyncio.wait_for(\n",
    "                        self.work_queue.get(),\n",
    "                        timeout=0.1,\n",
    "                    )\n",
    "\n",
    "                    try:\n",
    "                        result = await process_func(task)\n",
    "                        self.results.append(result)\n",
    "                        processed += 1\n",
    "                        self.tasks_processed += 1\n",
    "                    except Exception as e:\n",
    "                        # Collect error but continue processing\n",
    "                        self.errors.append(e)\n",
    "                        # Production: logger.error(f\"Task failed: {e}\", exc_info=True)\n",
    "\n",
    "                except TimeoutError:\n",
    "                    # No work available, check shutdown_event\n",
    "                    continue\n",
    "\n",
    "        except asyncio.CancelledError:\n",
    "            # Forceful shutdown - log and propagate\n",
    "            # Production: logger.warning(f\"Worker {worker_id} cancelled forcefully\")\n",
    "            raise\n",
    "\n",
    "        finally:\n",
    "            # Production: logger.info(f\"Worker {worker_id} exiting (processed {processed})\")\n",
    "            pass\n",
    "\n",
    "    async def _cleanup(self, process_func: Callable) -> int:\n",
    "        \"\"\"Drain remaining queue items.\n",
    "\n",
    "        This is shielded from cancellation, ensuring remaining\n",
    "        work is processed before shutdown completes.\n",
    "        \"\"\"\n",
    "        drained = 0\n",
    "\n",
    "        try:\n",
    "            while True:\n",
    "                task = self.work_queue.get_nowait()\n",
    "\n",
    "                try:\n",
    "                    result = await process_func(task)\n",
    "                    self.results.append(result)\n",
    "                    drained += 1\n",
    "                except Exception as e:\n",
    "                    self.errors.append(e)\n",
    "\n",
    "        except Exception:  # Queue empty\n",
    "            pass\n",
    "\n",
    "        # Production: logger.info(f\"Cleanup drained {drained} items\")\n",
    "        return drained\n",
    "\n",
    "    async def process(\n",
    "        self,\n",
    "        tasks: list[Any],\n",
    "        process_func: Callable,\n",
    "    ) -> tuple[list[Any], list[Exception]]:\n",
    "        \"\"\"Process tasks with worker pool.\n",
    "\n",
    "        Args:\n",
    "            tasks: List of items to process\n",
    "            process_func: async function(item) -> result\n",
    "\n",
    "        Returns:\n",
    "            (results, errors) tuple\n",
    "\n",
    "        Example:\n",
    "            async def process_item(item: dict) -> dict:\n",
    "                # Your processing logic\n",
    "                return transformed_item\n",
    "\n",
    "            pool = ProductionWorkerPool(num_workers=10)\n",
    "            results, errors = await pool.process(items, process_item)\n",
    "        \"\"\"\n",
    "        # Production: logger.info(f\"Starting pool: {self.num_workers} workers, {len(tasks)} tasks\")\n",
    "\n",
    "        try:\n",
    "            async with create_task_group() as tg:\n",
    "                # Start workers\n",
    "                for i in range(self.num_workers):\n",
    "                    tg.start_soon(self._worker, i, process_func)\n",
    "\n",
    "                # Feed tasks (blocks if queue full - backpressure)\n",
    "                for task in tasks:\n",
    "                    await self.work_queue.put(task)\n",
    "\n",
    "                # Wait for queue to drain\n",
    "                while self.work_queue.qsize() > 0:\n",
    "                    await sleep(0.01)\n",
    "\n",
    "                # Signal cooperative shutdown\n",
    "                self.shutdown_event.set()\n",
    "\n",
    "                # Give workers time to exit gracefully\n",
    "                await sleep(self.shutdown_timeout)\n",
    "\n",
    "        except ExceptionGroup as eg:\n",
    "            # Separate real errors from cancellations\n",
    "            real_errors = non_cancel_subgroup(eg)\n",
    "\n",
    "            if real_errors:\n",
    "                # Production: logger.error(f\"Workers failed: {real_errors}\")\n",
    "                for exc in real_errors.exceptions:\n",
    "                    self.errors.append(exc)\n",
    "            # else: All cancellations, expected\n",
    "\n",
    "        finally:\n",
    "            # Cleanup completes despite cancellation (shield protects it)\n",
    "            try:\n",
    "                await asyncio.wait_for(\n",
    "                    shield(self._cleanup, process_func),\n",
    "                    timeout=self.cleanup_timeout,\n",
    "                )\n",
    "                # Production: logger.info(f\"Cleanup complete: {drained} items drained\")\n",
    "            except TimeoutError:\n",
    "                # Production: logger.error(f\"Cleanup timeout after {self.cleanup_timeout}s\")\n",
    "                pass\n",
    "\n",
    "        return self.results, self.errors\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# Example Usage\n",
    "# ============================================================================\n",
    "\n",
    "\n",
    "async def production_example():\n",
    "    \"\"\"Example: Process batch of items with error handling.\"\"\"\n",
    "\n",
    "    async def process_item(item: int) -> dict:\n",
    "        \"\"\"Simulate processing with occasional failures.\"\"\"\n",
    "        await sleep(0.05)\n",
    "\n",
    "        # Simulate 10% failure rate\n",
    "        if item % 10 == 0:\n",
    "            raise ValueError(f\"Processing failed for item {item}\")\n",
    "\n",
    "        return {\n",
    "            \"item\": item,\n",
    "            \"processed\": item * 2,\n",
    "            \"status\": \"success\",\n",
    "        }\n",
    "\n",
    "    # Process 50 items with 8 workers\n",
    "    pool = ProductionWorkerPool(\n",
    "        num_workers=8,\n",
    "        queue_size=20,\n",
    "        shutdown_timeout=2.0,\n",
    "        cleanup_timeout=5.0,\n",
    "    )\n",
    "\n",
    "    items = list(range(50))\n",
    "    results, errors = await pool.process(items, process_item)\n",
    "\n",
    "    # Report\n",
    "    print(\"\\n=== Processing Complete ===\")\n",
    "    print(f\"Total tasks: {len(items)}\")\n",
    "    print(f\"Successful: {len(results)}\")\n",
    "    print(f\"Failed: {len(errors)}\")\n",
    "    print(\"\\nSample successful results:\")\n",
    "    for result in results[:3]:\n",
    "        print(f\"  {result}\")\n",
    "    print(\"\\nSample errors:\")\n",
    "    for error in errors[:3]:\n",
    "        print(f\"  {type(error).__name__}: {error}\")\n",
    "\n",
    "\n",
    "await production_example()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Common Patterns and Variations\n",
    "\n",
    "### Priority Queue\n",
    "\n",
    "Process high-priority items first:\n",
    "\n",
    "```python\n",
    "from lionherd_core.libs.concurrency import PriorityQueue\n",
    "\n",
    "queue = PriorityQueue.with_maxsize(10)\n",
    "await queue.put((1, \"urgent\"))      # Process first\n",
    "await queue.put((5, \"normal\"))      # Process later\n",
    "await queue.put((0, \"critical\"))    # Process immediately\n",
    "\n",
    "priority, task = await queue.get()\n",
    "```\n",
    "\n",
    "**Trade-offs**: High-priority items first, good for SLA tiers | Low-priority may starve, 2x overhead vs FIFO.\n",
    "\n",
    "---\n",
    "\n",
    "### Result Streaming\n",
    "\n",
    "Process results as they arrive:\n",
    "\n",
    "```python\n",
    "result_queue = Queue.with_maxsize(10)\n",
    "\n",
    "async def worker(work_queue, result_queue):\n",
    "    while True:\n",
    "        item = await work_queue.get()\n",
    "        if item is None: break\n",
    "        result = await process(item)\n",
    "        await result_queue.put(result)\n",
    "\n",
    "async def consumer(result_queue):\n",
    "    while True:\n",
    "        result = await result_queue.get()\n",
    "        if result is None: break\n",
    "        await handle_result(result)  # Real-time processing\n",
    "```\n",
    "\n",
    "**Benefits**: Lower memory (no buffering), real-time updates, early termination possible.\n",
    "\n",
    "---\n",
    "\n",
    "### Timeout-Aware Workers\n",
    "\n",
    "Bound execution time per task:\n",
    "\n",
    "```python\n",
    "from lionherd_core.libs.concurrency import fail_after\n",
    "\n",
    "async def worker(work_queue):\n",
    "    while True:\n",
    "        item = await work_queue.get()\n",
    "        if item is None: break\n",
    "        \n",
    "        try:\n",
    "            with fail_after(5.0):\n",
    "                result = await process(item)\n",
    "                results.append(result)\n",
    "        except TimeoutError:\n",
    "            errors.append((item, \"timeout\"))\n",
    "```\n",
    "\n",
    "**Use cases**: External API calls (prevent hanging), database queries (detect slow queries), file operations (detect stuck I/O)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Summary and Key Takeaways\n\n**What You Accomplished**:\n- ✅ Implemented fan-out/fan-in pattern with Queue and TaskGroup\n- ✅ Used shield() to guarantee cleanup operations complete\n- ✅ Applied non_cancel_subgroup() to distinguish errors from cancellations\n- ✅ Combined patterns into production-ready worker pool\n- ✅ Learned common variations (priority, streaming, timeouts, scaling)\n\n**Key Takeaways**:\n\n1. **TaskGroup provides structured concurrency**\n   - All tasks complete or are cancelled together\n   - No orphaned tasks, no resource leaks\n   - Error in one task cancels all others\n\n2. **Queue enables work distribution with backpressure**\n   - Bounded queue prevents memory exhaustion\n   - Slow consumers naturally throttle fast producers\n   - get_nowait() for non-blocking reads in cleanup\n\n3. **shield() is essential for cleanup**\n   - Ensures resources are released despite cancellation\n   - Use in __aexit__ for context managers\n   - Add timeout wrapper to prevent infinite cleanup\n\n4. **Poison pill pattern for graceful shutdown**\n   - Sentinel value (object()) signals workers to exit\n   - First worker propagates to others (N workers = N pills)\n   - Alternative: shutdown_event + timeout on queue.get()\n\n5. **non_cancel_subgroup() for error filtering**\n   - Separate expected cancellations from real errors\n   - Returns None if all exceptions are cancellations\n   - Essential for accurate alerting (no false alarms)\n\n**When to Use These Patterns**:\n\n**Fan-out/fan-in**:\n- ✅ Batch processing (image processing, data transformation)\n- ✅ Parallel API calls (fetch from multiple endpoints)\n- ✅ High-throughput pipelines (ETL, data ingestion)\n- ❌ Tasks with complex dependencies (use DAG scheduler)\n- ❌ Real-time request/response (use connection pooling)\n\n**Graceful shutdown**:\n- ✅ Long-running services (databases, message queues)\n- ✅ Kubernetes/Docker deployments (SIGTERM signals)\n- ✅ Stateful cleanup (flush buffers, close connections, persist state)\n- ❌ Short-lived CLI tools (unless managing background tasks)\n- ❌ Pure compute workloads (no I/O, no cleanup needed)\n\n**Combined pattern**:\n- ✅ Production microservices with background workers\n- ✅ Data processing pipelines with failure recovery\n- ✅ Multi-tenant systems with resource isolation\n\n## Related Resources\n\n**lionherd-core API Reference**:\n- [TaskGroup](../..) - Structured concurrency primitive\n- [Queue](../..) - Async FIFO queue with backpressure\n- [shield()](../..) - Cancellation protection\n- [non_cancel_subgroup()](../..) - Exception filtering\n\n**Related Tutorials**:\n- [Parallel Operations with Timeouts](./) - fail_after(), bounded_map()\n- [Deadline-Aware Task Queue](./) - Time-bounded processing\n\n**Reference Notebooks**:\n- [Concurrency Primitives](../references/concurrency_primitives.ipynb) - Deep dive into Queue, Lock, Event\n\n**External Resources**:\n- [Structured Concurrency (Nathaniel J. Smith)](https://vorpus.org/blog/notes-on-structured-concurrency-or-go-statement-considered-harmful/) - Foundational concepts\n- [Python asyncio TaskGroups](https://docs.python.org/3/library/asyncio-task.html#task-groups) - Standard library implementation\n- [Kubernetes Pod Lifecycle](https://kubernetes.io/docs/concepts/workloads/pods/pod-lifecycle/#pod-termination) - SIGTERM handling in production\n"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lionherd-core",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

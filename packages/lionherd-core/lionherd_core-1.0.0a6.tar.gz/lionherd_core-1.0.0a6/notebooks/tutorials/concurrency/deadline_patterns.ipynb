{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial: Deadline-Aware Processing Patterns\n",
    "\n",
    "**Category**: Concurrency  \n",
    "**Difficulty**: Intermediate  \n",
    "**Time**: 20 minutes\n",
    "\n",
    "## Overview: Why Deadline Patterns Matter\n",
    "\n",
    "In production systems, you often need to process work within fixed time budgets:\n",
    "\n",
    "- **ETL pipelines** must complete before the next data refresh\n",
    "- **Batch notifications** need to send before daily cutoff times\n",
    "- **Background jobs** must finish within maintenance windows\n",
    "- **API handlers** must respect SLA timeouts\n",
    "\n",
    "The challenge: process **as much as possible** before the deadline, return **partial results** gracefully, avoid **wasted work** on tasks that won't complete.\n",
    "\n",
    "**This tutorial covers**:\n",
    "1. Sequential deadline-aware processing (single loop, check time before each task)\n",
    "2. Parallel worker pool pattern (multiple workers, shared queue, sentinel shutdown)\n",
    "3. When to use each pattern\n",
    "4. Production-ready copypaste code\n",
    "\n",
    "**Key lionherd APIs**:\n",
    "- `move_on_at(deadline)` - Silent cancellation at absolute time\n",
    "- `effective_deadline()` - Query remaining time from ambient scopes\n",
    "- `current_time()` - Monotonic clock for deadline calculations\n",
    "- `Queue.with_maxsize(n)` - Bounded FIFO queue with backpressure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "\n",
    "**Required**:\n",
    "```bash\n",
    "pip install lionherd-core  # >=0.1.0\n",
    "```\n",
    "\n",
    "**Prior Knowledge**:\n",
    "- Python async/await fundamentals\n",
    "- Basic understanding of queues and batch processing\n",
    "\n",
    "**Related Resources**:\n",
    "- [API Reference: Cancellation Utilities](../../docs/api/libs/concurrency/cancel.md)\n",
    "- [API Reference: Primitives](../../docs/api/libs/concurrency/primitives.md)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard library\n",
    "from collections.abc import Callable\n",
    "from dataclasses import dataclass\n",
    "from typing import Any\n",
    "\n",
    "# lionherd-core\n",
    "from lionherd_core.libs.concurrency import (\n",
    "    Queue,\n",
    "    create_task_group,\n",
    "    current_time,\n",
    "    effective_deadline,\n",
    "    move_on_at,\n",
    "    sleep,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## API Overview\n",
    "\n",
    "### `move_on_at(deadline)` - Graceful Degradation\n",
    "\n",
    "Silently cancels at absolute deadline. Returns partial results without exceptions.\n",
    "\n",
    "```python\n",
    "deadline = current_time() + 5.0\n",
    "with move_on_at(deadline) as scope:\n",
    "    results = await process_tasks()\n",
    "if scope.cancel_called:\n",
    "    print(\"Deadline reached\")\n",
    "```\n",
    "\n",
    "### `effective_deadline()` - Query Remaining Time\n",
    "\n",
    "Returns nearest deadline from ambient scopes (`float` or `None`).\n",
    "\n",
    "```python\n",
    "if effective_deadline() and (effective_deadline() - current_time()) < 0.1:\n",
    "    print(\"Skip task, insufficient time\")\n",
    "```\n",
    "\n",
    "### `current_time()` - Monotonic Clock\n",
    "\n",
    "For measuring intervals and calculating deadlines.\n",
    "\n",
    "```python\n",
    "start = current_time()\n",
    "await work()\n",
    "elapsed = current_time() - start\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pattern 1: Sequential Deadline-Aware Processing\n",
    "\n",
    "**When to use**: Tasks must run sequentially (dependencies, rate limits, single resource access)\n",
    "\n",
    "**Key insight**: Check remaining time **before** starting each task to avoid wasting work on tasks you can't finish.\n",
    "\n",
    "**Pattern**:\n",
    "```\n",
    "deadline = current_time() + 30.0\n",
    "with move_on_at(deadline):\n",
    "    for task in tasks:\n",
    "        if (deadline - current_time()) < min_time:\n",
    "            break  # Not enough time\n",
    "        result = await task()\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed: 9/20\n",
      "Failed: 0\n",
      "Skipped: 11\n",
      "\n",
      "First 3 results: ['Result-t0', 'Result-t1', 'Result-t2']\n"
     ]
    }
   ],
   "source": [
    "# Sequential deadline-aware processing\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class Task:\n",
    "    \"\"\"Simple task representation.\"\"\"\n",
    "\n",
    "    id: str\n",
    "    work: Callable[[], Any]  # Async callable\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class Result:\n",
    "    \"\"\"Simple result representation.\"\"\"\n",
    "\n",
    "    task_id: str\n",
    "    success: bool\n",
    "    data: Any = None\n",
    "    error: str | None = None\n",
    "\n",
    "\n",
    "async def process_sequential_with_deadline(\n",
    "    tasks: list[Task],\n",
    "    deadline: float,\n",
    "    min_time: float = 0.01,\n",
    ") -> tuple[list[Result], int]:\n",
    "    \"\"\"Process tasks sequentially until deadline.\n",
    "\n",
    "    Args:\n",
    "        tasks: List of tasks to process\n",
    "        deadline: Absolute deadline (from current_time())\n",
    "        min_time: Skip task if less than this time remaining\n",
    "\n",
    "    Returns:\n",
    "        (results, skipped_count)\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    skipped = 0\n",
    "\n",
    "    with move_on_at(deadline) as scope:\n",
    "        for task in tasks:\n",
    "            # Check if we have enough time\n",
    "            remaining = deadline - current_time()\n",
    "            if remaining < min_time:\n",
    "                skipped = len(tasks) - len(results)\n",
    "                break\n",
    "\n",
    "            try:\n",
    "                data = await task.work()\n",
    "                results.append(Result(task_id=task.id, success=True, data=data))\n",
    "            except Exception as e:\n",
    "                results.append(Result(task_id=task.id, success=False, error=str(e)))\n",
    "\n",
    "    # If cancelled by deadline, count remaining\n",
    "    if scope.cancel_called:\n",
    "        skipped = len(tasks) - len(results)\n",
    "\n",
    "    return results, skipped\n",
    "\n",
    "\n",
    "# Demo: Process 20 tasks with 1 second deadline\n",
    "async def work(task_id: str, duration: float):\n",
    "    await sleep(duration)\n",
    "    return f\"Result-{task_id}\"\n",
    "\n",
    "\n",
    "tasks = [Task(id=f\"t{i}\", work=lambda i=i: work(f\"t{i}\", 0.1)) for i in range(20)]\n",
    "deadline = current_time() + 1.0\n",
    "\n",
    "results, skipped = await process_sequential_with_deadline(tasks, deadline)\n",
    "\n",
    "print(f\"Completed: {len([r for r in results if r.success])}/{len(tasks)}\")\n",
    "print(f\"Failed: {len([r for r in results if not r.success])}\")\n",
    "print(f\"Skipped: {skipped}\")\n",
    "print(f\"\\nFirst 3 results: {[r.data for r in results[:3]]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sequential Pattern Explanation\n",
    "\n",
    "**Key mechanics**:\n",
    "1. `move_on_at(deadline)` stops execution at deadline\n",
    "2. Before each task: check `remaining = deadline - current_time()`\n",
    "3. If `remaining < min_time`, stop (not enough time)\n",
    "4. `scope.cancel_called` indicates deadline was reached\n",
    "\n",
    "**Why check time explicitly?** Prevents starting tasks you can't finish (wasted CPU/memory).\n",
    "\n",
    "**Use cases**: Rate-limited APIs, database transactions, sequential file I/O."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pattern 2: Parallel Worker Pool with Queue\n",
    "\n",
    "**When to use**: Tasks are independent and can run concurrently (I/O-bound work, parallel API calls)\n",
    "\n",
    "**Key insight**: Multiple workers pull from shared queue, each checking deadline before processing.\n",
    "\n",
    "**Pattern**:\n",
    "```\n",
    "queue = Queue.with_maxsize(100)\n",
    "async with TaskGroup() as tg:\n",
    "    for i in range(num_workers):\n",
    "        tg.create_task(worker(i, queue))\n",
    "    \n",
    "    for task in tasks:\n",
    "        await queue.put(task)\n",
    "    await queue.put(SENTINEL)  # Signal shutdown\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed: 50/50 tasks\n",
      "Total time: 0.67s\n",
      "Expected sequential time: ~2.50s\n",
      "Speedup: ~3.8x\n"
     ]
    }
   ],
   "source": [
    "# Parallel worker pool with deadline awareness\n",
    "\n",
    "SENTINEL = object()  # Shutdown signal\n",
    "\n",
    "\n",
    "async def deadline_aware_worker(\n",
    "    worker_id: int,\n",
    "    queue: Queue,\n",
    "    results: list[Result],\n",
    "    min_time: float = 0.01,\n",
    ") -> None:\n",
    "    \"\"\"Worker that processes items from queue until deadline.\n",
    "\n",
    "    Args:\n",
    "        worker_id: Unique worker identifier\n",
    "        queue: Shared work queue\n",
    "        results: Shared results list (thread-safe due to GIL)\n",
    "        min_time: Skip if less time remaining\n",
    "    \"\"\"\n",
    "    while True:\n",
    "        item = await queue.get()\n",
    "\n",
    "        # Check for sentinel (graceful shutdown)\n",
    "        if item is SENTINEL:\n",
    "            await queue.put(SENTINEL)  # Propagate to other workers\n",
    "            break\n",
    "\n",
    "        # Check remaining time\n",
    "        deadline = effective_deadline()\n",
    "        if deadline and (deadline - current_time()) < min_time:\n",
    "            # Not enough time, skip this task\n",
    "            continue\n",
    "\n",
    "        # Process the task\n",
    "        task: Task = item\n",
    "        try:\n",
    "            data = await task.work()\n",
    "            results.append(Result(task_id=task.id, success=True, data=data))\n",
    "        except Exception as e:\n",
    "            results.append(Result(task_id=task.id, success=False, error=str(e)))\n",
    "\n",
    "\n",
    "async def process_parallel_with_deadline(\n",
    "    tasks: list[Task],\n",
    "    deadline: float,\n",
    "    num_workers: int = 4,\n",
    "    queue_size: int = 100,\n",
    ") -> list[Result]:\n",
    "    \"\"\"Process tasks in parallel with multiple workers.\n",
    "\n",
    "    Args:\n",
    "        tasks: List of tasks to process\n",
    "        deadline: Absolute deadline\n",
    "        num_workers: Number of concurrent workers\n",
    "        queue_size: Maximum queue size (backpressure limit)\n",
    "\n",
    "    Returns:\n",
    "        List of results from all workers\n",
    "    \"\"\"\n",
    "    queue = Queue.with_maxsize(queue_size)\n",
    "    results = []\n",
    "\n",
    "    async def producer():\n",
    "        \"\"\"Feed tasks into queue.\"\"\"\n",
    "        for task in tasks:\n",
    "            await queue.put(task)\n",
    "        await queue.put(SENTINEL)  # Signal workers to stop\n",
    "\n",
    "    with move_on_at(deadline):\n",
    "        async with create_task_group() as tg:\n",
    "            # Spawn workers\n",
    "            for i in range(num_workers):\n",
    "                tg.start_soon(deadline_aware_worker, i, queue, results)\n",
    "\n",
    "            # Feed queue\n",
    "            tg.start_soon(producer)\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "# Demo: Process 50 tasks with 4 workers and 1 second deadline\n",
    "async def parallel_work(task_id: str, duration: float):\n",
    "    await sleep(duration)\n",
    "    return f\"Result-{task_id}\"\n",
    "\n",
    "\n",
    "tasks = [Task(id=f\"t{i}\", work=lambda i=i: parallel_work(f\"t{i}\", 0.05)) for i in range(50)]\n",
    "deadline = current_time() + 1.0\n",
    "\n",
    "start = current_time()\n",
    "results = await process_parallel_with_deadline(tasks, deadline, num_workers=4)\n",
    "elapsed = current_time() - start\n",
    "\n",
    "successful = len([r for r in results if r.success])\n",
    "print(f\"Completed: {successful}/{len(tasks)} tasks\")\n",
    "print(f\"Total time: {elapsed:.2f}s\")\n",
    "print(f\"Expected sequential time: ~{len(tasks) * 0.05:.2f}s\")\n",
    "print(f\"Speedup: ~{(len(tasks) * 0.05) / elapsed:.1f}x\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Worker Pool Pattern Explanation\n",
    "\n",
    "**Key mechanics**:\n",
    "1. Bounded queue (`Queue.with_maxsize(n)`) controls memory\n",
    "2. `num_workers` tasks pull from shared queue\n",
    "3. Producer feeds tasks, then puts `SENTINEL` to signal shutdown\n",
    "4. Workers check `effective_deadline()` before processing\n",
    "\n",
    "**Sentinel pattern**: Each worker checks for `SENTINEL` → re-queues it → exits. This ensures all workers see the shutdown signal.\n",
    "\n",
    "**Bounded queue benefits**: Prevents memory exhaustion, provides backpressure (producer blocks when full).\n",
    "\n",
    "**Use cases**: Batch API requests, data transformations, parallel database queries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Worker exits (worker_id, items_processed):\n",
      "  Worker 0: processed 2 items\n",
      "  Worker 1: processed 2 items\n",
      "  Worker 2: processed 1 items\n",
      "\n",
      "Total items processed: 5\n"
     ]
    }
   ],
   "source": [
    "# Sentinel pattern demonstration\n",
    "\n",
    "\n",
    "async def demonstrate_sentinel():\n",
    "    \"\"\"Show how sentinel propagates through workers.\"\"\"\n",
    "    queue = Queue.with_maxsize(10)\n",
    "    worker_exits = []\n",
    "\n",
    "    async def tracking_worker(worker_id: int):\n",
    "        \"\"\"Worker that tracks when it exits.\"\"\"\n",
    "        items_processed = 0\n",
    "        while True:\n",
    "            item = await queue.get()\n",
    "            if item is SENTINEL:\n",
    "                await queue.put(SENTINEL)  # Critical: re-queue!\n",
    "                worker_exits.append((worker_id, items_processed))\n",
    "                break\n",
    "            items_processed += 1\n",
    "            await sleep(0.01)\n",
    "\n",
    "    # Spawn 3 workers\n",
    "    async with create_task_group() as tg:\n",
    "        for i in range(3):\n",
    "            tg.start_soon(tracking_worker, i)\n",
    "\n",
    "        # Feed 5 items then sentinel\n",
    "        for i in range(5):\n",
    "            await queue.put(f\"item-{i}\")\n",
    "        await queue.put(SENTINEL)\n",
    "\n",
    "    print(\"Worker exits (worker_id, items_processed):\")\n",
    "    for worker_id, count in sorted(worker_exits):\n",
    "        print(f\"  Worker {worker_id}: processed {count} items\")\n",
    "    print(f\"\\nTotal items processed: {sum(c for _, c in worker_exits)}\")\n",
    "\n",
    "\n",
    "await demonstrate_sentinel()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparison: Sequential vs Parallel\n",
    "\n",
    "| Aspect | Sequential | Parallel (Worker Pool) |\n",
    "|--------|-----------|------------------------|\n",
    "| **Throughput** | 1x (baseline) | N× (N = num_workers, for I/O-bound) |\n",
    "| **Complexity** | Simple (single loop) | Moderate (queue, workers, sentinel) |\n",
    "| **Memory** | Low (one task at a time) | Medium (queue + N active tasks) |\n",
    "| **Order** | Preserved | Not preserved (results unordered) |\n",
    "| **Error Isolation** | Single failure visible | Per-worker isolation |\n",
    "| **Best For** | Rate-limited APIs, sequential deps | Independent I/O-bound tasks |\n",
    "\n",
    "**Decision guide**:\n",
    "\n",
    "**Use Sequential when**:\n",
    "- Tasks have dependencies (must run in order)\n",
    "- Rate limits require delays between tasks\n",
    "- Single resource (file handle, DB connection)\n",
    "- Task count < 100 and deadline is generous\n",
    "\n",
    "**Use Parallel when**:\n",
    "- Tasks are independent (no shared state)\n",
    "- I/O-bound work (network, disk, database)\n",
    "- Task count > 100 or tight deadline\n",
    "- Have spare CPU/memory for workers\n",
    "\n",
    "**Avoid Parallel when**:\n",
    "- CPU-bound tasks on single-core (GIL limits parallelism)\n",
    "- Memory-constrained (workers + queue = high memory)\n",
    "- Tasks have complex dependencies (use DAG instead)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "**Key APIs**: `move_on_at()`, `effective_deadline()`, `current_time()`, `Queue.with_maxsize()`\n",
    "\n",
    "**Patterns**:\n",
    "- Sequential: Check time before each task\n",
    "- Parallel: Worker pool with shared queue + sentinel shutdown\n",
    "\n",
    "**When to use**:\n",
    "- ✅ Background jobs with deadlines (ETL, reports, batch notifications)\n",
    "- ✅ Maintenance windows with fixed time slots\n",
    "- ❌ Real-time where every task must complete (use per-task timeouts)\n",
    "- ❌ CPU-bound on single-core (GIL limits parallel speedup)\n",
    "\n",
    "**Related**: [Cancellation API](../../docs/api/libs/concurrency/cancel.md), [Primitives API](../../docs/api/libs/concurrency/primitives.md)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Common Patterns and Variations\n",
    "\n",
    "### 1. Per-Task Timeout within Overall Deadline\n",
    "\n",
    "Sometimes you need both: individual task timeout + overall deadline.\n",
    "\n",
    "```python\n",
    "from lionherd_core.libs.concurrency import move_on_after\n",
    "\n",
    "with move_on_at(deadline):  # Overall deadline\n",
    "    for task in tasks:\n",
    "        with move_on_after(task_timeout):  # Per-task timeout\n",
    "            result = await task()\n",
    "        # Task timed out if scope.cancel_called\n",
    "```\n",
    "\n",
    "### 2. Priority Queue for Important Tasks First\n",
    "\n",
    "Process high-priority tasks before deadline hits.\n",
    "\n",
    "```python\n",
    "from lionherd_core.libs.concurrency import PriorityQueue\n",
    "\n",
    "queue = PriorityQueue.with_maxsize(100)\n",
    "\n",
    "# Put with priority (lower number = higher priority)\n",
    "await queue.put((priority, task))\n",
    "\n",
    "# Worker pulls highest priority first\n",
    "priority, task = await queue.get()\n",
    "```\n",
    "\n",
    "### 3. Adaptive Worker Count Based on Deadline\n",
    "\n",
    "Scale workers based on remaining time.\n",
    "\n",
    "```python\n",
    "remaining = deadline - current_time()\n",
    "tasks_left = len(tasks)\n",
    "avg_task_time = 0.05  # Estimated\n",
    "\n",
    "# Calculate needed workers\n",
    "needed_workers = int((tasks_left * avg_task_time) / remaining) + 1\n",
    "num_workers = min(needed_workers, max_workers)\n",
    "```\n",
    "\n",
    "### 4. Checkpoint and Resume\n",
    "\n",
    "Save progress periodically for long-running jobs.\n",
    "\n",
    "```python\n",
    "checkpoint_interval = 60.0  # Save every 60s\n",
    "last_checkpoint = current_time()\n",
    "\n",
    "for task in tasks:\n",
    "    result = await task()\n",
    "    results.append(result)\n",
    "    \n",
    "    if current_time() - last_checkpoint > checkpoint_interval:\n",
    "        save_checkpoint(results)\n",
    "        last_checkpoint = current_time()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary and Next Steps\n",
    "\n",
    "**What You Learned**:\n",
    "- Sequential deadline processing: check time before each task\n",
    "- Parallel worker pool: shared queue + multiple workers + sentinel shutdown\n",
    "- Core APIs: `move_on_at()`, `effective_deadline()`, `current_time()`, `Queue.with_maxsize()`\n",
    "- When to use sequential vs parallel patterns\n",
    "\n",
    "**Key Takeaways**:\n",
    "1. **Absolute deadlines** (`move_on_at`) are better than per-task timeouts for batch work\n",
    "2. **Check time before starting** tasks to avoid wasted work\n",
    "3. **Sentinel pattern** enables graceful shutdown (finish current task, then exit)\n",
    "4. **Bounded queues** prevent memory exhaustion in worker pools\n",
    "5. **Parallel is not always faster** - only helps for I/O-bound independent tasks\n",
    "\n",
    "**When to Use Deadline Patterns**:\n",
    "- ✅ Background jobs with completion deadlines (ETL, reports, batch notifications)\n",
    "- ✅ Maintenance windows with fixed time slots\n",
    "- ✅ Rate-limited API operations (maximize throughput within budget)\n",
    "- ❌ Real-time processing where every task must complete (use per-task timeouts)\n",
    "- ❌ CPU-bound tasks on single-core (GIL limits parallel speedup)\n",
    "\n",
    "**Related Resources**:\n",
    "- [API Reference: Cancellation Utilities](../../docs/api/libs/concurrency/cancel.md)\n",
    "- [API Reference: Primitives](../../docs/api/libs/concurrency/primitives.md)\n",
    "- [Reference Notebook: Concurrency Primitives](../references/concurrency_primitives.ipynb)\n",
    "- [Reference Notebook: Cancellation](../references/concurrency_cancel.ipynb)\n",
    "\n",
    "**Next Tutorials**:\n",
    "- Circuit Breaker Pattern (resilient service calls)\n",
    "- Retry Strategies (exponential backoff, jitter)\n",
    "- Rate Limiting (token bucket, leaky bucket)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lionherd-core",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

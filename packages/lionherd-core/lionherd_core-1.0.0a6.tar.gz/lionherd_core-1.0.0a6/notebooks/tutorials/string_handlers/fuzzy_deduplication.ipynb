{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "intro",
   "metadata": {},
   "source": [
    "# Tutorial: Fuzzy Data Deduplication\n",
    "\n",
    "**Category**: String Handlers  \n",
    "**Difficulty**: Intermediate  \n",
    "**Time**: 15-25 minutes\n",
    "\n",
    "## Problem Statement\n",
    "\n",
    "When managing contact lists, customer databases, or user records, duplicate entries often appear with slight variations. \"John Smith\" and \"Jon Smith\" are likely the same person, but exact string matching won't catch this. Manual deduplication is time-consuming and error-prone, especially with thousands of records.\n",
    "\n",
    "Traditional exact-match deduplication fails to identify near-duplicates caused by typos, abbreviations, or data entry inconsistencies. Users end up with duplicate contacts (\"Robert Johnson\" vs \"Bob Johnson\"), wasting storage and causing confusion during operations.\n",
    "\n",
    "**Why This Matters**:\n",
    "- **Data Quality**: Duplicate records inflate metrics, waste storage, and confuse analytics\n",
    "- **User Experience**: Multiple entries for the same entity frustrate users and reduce trust\n",
    "- **Operational Efficiency**: Automated fuzzy matching saves hours of manual review time\n",
    "\n",
    "**What You'll Build**:\n",
    "A production-ready fuzzy deduplication system using lionherd-core's `string_similarity` function that identifies and merges near-duplicate records based on configurable similarity thresholds."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "prerequisites",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "\n",
    "**Prior Knowledge**:\n",
    "- Python lists and dictionaries\n",
    "- Basic understanding of string comparison\n",
    "- Familiarity with data deduplication concepts\n",
    "\n",
    "**Required Packages**:\n",
    "```bash\n",
    "pip install lionherd-core  # >=0.1.0\n",
    "```\n",
    "\n",
    "**Optional Reading**:\n",
    "- [API Reference: string_similarity](../../../docs/api/libs/string_handlers/string_similarity.md)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "imports",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard library\n",
    "from dataclasses import dataclass\n",
    "\n",
    "# lionherd-core - string similarity for fuzzy matching\n",
    "from lionherd_core.libs.string_handlers import string_similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "solution-overview",
   "metadata": {},
   "source": [
    "## Solution Overview\n",
    "\n",
    "We'll implement a fuzzy deduplication system that:\n",
    "\n",
    "1. **Similarity Scoring**: Compare each record against all others using Levenshtein distance\n",
    "2. **Threshold Matching**: Identify duplicates above a configurable similarity threshold (0.0-1.0)\n",
    "3. **Merge Strategy**: Group similar records and keep the canonical version\n",
    "\n",
    "**Key lionherd-core Components**:\n",
    "- `string_similarity()`: Find similar strings using configurable algorithms (Jaro-Winkler, Levenshtein, etc.)\n",
    "- `SimilarityAlgo`: Type-safe enum for available algorithms\n",
    "\n",
    "**Flow**:\n",
    "```\n",
    "Records → Compare Pairs → Calculate Similarity → Threshold Filter → Group Duplicates → Merge\n",
    "              ↓                ↓                      ↓                    ↓\n",
    "        All vs All      Levenshtein Algo       Keep ≥ threshold    Deduplicated Output\n",
    "```\n",
    "\n",
    "**Expected Outcome**: A clean dataset with near-duplicates merged, preserving the most complete or canonical record from each duplicate group."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "step1-title",
   "metadata": {},
   "source": [
    "### Step 1: Define Data Model and Sample Data\n",
    "\n",
    "First, we'll create a simple contact record structure and generate sample data with typical duplicates (typos, abbreviations, formatting variations).\n",
    "\n",
    "**Why dataclass**: Lightweight, immutable-friendly, and easy to serialize for production use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "step1-code",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Contact List (8 records):\n",
      "============================================================\n",
      "1. John Smith           | john.smith@example.com         | CRM\n",
      "2. Jon Smith            | jsmith@example.com             | Manual Entry\n",
      "3. Jane Doe             | jane.doe@company.com           | Import\n",
      "4. Jane Do              | jane.d@company.com             | Manual Entry\n",
      "5. Robert Johnson       | robert.j@firm.com              | API\n",
      "6. Bob Johnson          | bob.johnson@firm.com           | CSV Upload\n",
      "7. Alice Williams       | alice.w@startup.com            | CRM\n",
      "8. Alise Williams       | alice@startup.com              | Manual Entry\n",
      "\n",
      "Expected duplicates:\n",
      "  - John Smith / Jon Smith\n",
      "  - Jane Doe / Jane Do\n",
      "  - Robert Johnson / Bob Johnson\n",
      "  - Alice Williams / Alise Williams\n"
     ]
    }
   ],
   "source": [
    "@dataclass\n",
    "class Contact:\n",
    "    \"\"\"Contact record with name and metadata.\"\"\"\n",
    "\n",
    "    name: str\n",
    "    email: str\n",
    "    source: str  # Where the record came from (for tracking)\n",
    "\n",
    "\n",
    "# Sample contact list with duplicates (real-world scenario)\n",
    "contacts = [\n",
    "    Contact(\"John Smith\", \"john.smith@example.com\", \"CRM\"),\n",
    "    Contact(\"Jon Smith\", \"jsmith@example.com\", \"Manual Entry\"),  # Typo: John → Jon\n",
    "    Contact(\"Jane Doe\", \"jane.doe@company.com\", \"Import\"),\n",
    "    Contact(\"Jane Do\", \"jane.d@company.com\", \"Manual Entry\"),  # Typo: Doe → Do\n",
    "    Contact(\"Robert Johnson\", \"robert.j@firm.com\", \"API\"),\n",
    "    Contact(\"Bob Johnson\", \"bob.johnson@firm.com\", \"CSV Upload\"),  # Nickname: Robert → Bob\n",
    "    Contact(\"Alice Williams\", \"alice.w@startup.com\", \"CRM\"),\n",
    "    Contact(\"Alise Williams\", \"alice@startup.com\", \"Manual Entry\"),  # Typo: Alice → Alise\n",
    "]\n",
    "\n",
    "print(f\"Contact List ({len(contacts)} records):\")\n",
    "print(\"=\" * 60)\n",
    "for i, contact in enumerate(contacts, 1):\n",
    "    print(f\"{i}. {contact.name:20} | {contact.email:30} | {contact.source}\")\n",
    "print(\"\\nExpected duplicates:\")\n",
    "print(\"  - John Smith / Jon Smith\")\n",
    "print(\"  - Jane Doe / Jane Do\")\n",
    "print(\"  - Robert Johnson / Bob Johnson\")\n",
    "print(\"  - Alice Williams / Alise Williams\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "step1-notes",
   "metadata": {},
   "source": [
    "**Notes**:\n",
    "- **Typo duplicates**: Single-character errors (\"John\" → \"Jon\", \"Alice\" → \"Alise\")\n",
    "- **Abbreviation duplicates**: Nicknames or shortened names (\"Robert\" → \"Bob\")\n",
    "- **Real-world data**: These patterns mirror actual data entry issues in production systems\n",
    "- **Source tracking**: Keeps provenance for audit trails (which system created the record)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "step2-title",
   "metadata": {},
   "source": [
    "### Step 2: Explore String Similarity Basics\n",
    "\n",
    "Before building the deduplication system, let's understand how `string_similarity` works with different algorithms and thresholds.\n",
    "\n",
    "**Algorithm Choice**: Levenshtein measures character-level edit distance, perfect for typos and spelling variations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "step2-code",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "String Similarity Basics\n",
      "============================================================\n",
      "Comparing: 'John Smith' vs 'Jon Smith'\n",
      "\n",
      "\n",
      "LEVENSHTEIN\n",
      "  Description: Edit distance (insertions/deletions/substitutions)\n",
      "  Matches (≥0.8): ['Jon Smith', 'John Smith']\n",
      "\n",
      "JARO_WINKLER\n",
      "  Description: Prefix-weighted similarity (good for names)\n",
      "  Matches (≥0.8): ['Jon Smith', 'John Smith']\n",
      "\n",
      "SEQUENCE_MATCHER\n",
      "  Description: Python's SequenceMatcher (longest common subsequence)\n",
      "  Matches (≥0.8): ['Jon Smith', 'John Smith']\n",
      "\n",
      "============================================================\n",
      "Insight: Levenshtein correctly identifies 'John Smith' as similar to 'Jon Smith'\n",
      "         due to single-character edit (insertion of 'h')\n"
     ]
    }
   ],
   "source": [
    "# Compare two similar names\n",
    "name1 = \"John Smith\"\n",
    "name2 = \"Jon Smith\"\n",
    "\n",
    "print(\"String Similarity Basics\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Comparing: '{name1}' vs '{name2}'\\n\")\n",
    "\n",
    "# Test different algorithms\n",
    "algorithms = [\n",
    "    (\"levenshtein\", \"Edit distance (insertions/deletions/substitutions)\"),\n",
    "    (\"jaro_winkler\", \"Prefix-weighted similarity (good for names)\"),\n",
    "    (\"sequence_matcher\", \"Python's SequenceMatcher (longest common subsequence)\"),\n",
    "]\n",
    "\n",
    "candidates = [\"John Smith\", \"Jon Smith\", \"Jane Doe\", \"Robert Johnson\"]\n",
    "\n",
    "for algo, description in algorithms:\n",
    "    print(f\"\\n{algo.upper()}\")\n",
    "    print(f\"  Description: {description}\")\n",
    "\n",
    "    # Find matches with 0.8 threshold\n",
    "    matches = string_similarity(\n",
    "        word=name2,  # Search for \"Jon Smith\"\n",
    "        correct_words=candidates,\n",
    "        algorithm=algo,\n",
    "        threshold=0.8,  # 80% similarity minimum\n",
    "        return_most_similar=False,  # Return all matches above threshold\n",
    "    )\n",
    "\n",
    "    print(f\"  Matches (≥0.8): {matches}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"Insight: Levenshtein correctly identifies 'John Smith' as similar to 'Jon Smith'\")\n",
    "print(\"         due to single-character edit (insertion of 'h')\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "step2-notes",
   "metadata": {},
   "source": [
    "**Notes**:\n",
    "- **Threshold tuning**: 0.8 (80%) is a good default for name matching - strict enough to avoid false positives\n",
    "- **Algorithm differences**: Jaro-Winkler favors prefix matches, Levenshtein handles typos better\n",
    "- **return_most_similar**: `False` returns all matches (for dedup), `True` returns best match (for autocorrect)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "step3-title",
   "metadata": {},
   "source": [
    "### Step 3: Build Similarity Matrix\n",
    "\n",
    "To identify all duplicates, we need to compare every record against every other record. This creates a similarity matrix showing which pairs are potential duplicates.\n",
    "\n",
    "**Complexity**: O(n²) comparisons for n records - acceptable for <10,000 records, use clustering for larger datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "step3-code",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity Matrix (threshold=0.8)\n",
      "============================================================\n",
      "\n",
      "John Smith (index 0):\n",
      "  → Jon Smith (index 1): 0.900 similarity\n",
      "\n",
      "Jane Doe (index 2):\n",
      "  → Jane Do (index 3): 0.875 similarity\n",
      "\n",
      "Alice Williams (index 6):\n",
      "  → Alise Williams (index 7): 0.929 similarity\n",
      "\n",
      "============================================================\n",
      "Found 3 contacts with potential duplicates\n"
     ]
    }
   ],
   "source": [
    "from lionherd_core.libs.string_handlers._string_similarity import levenshtein_similarity\n",
    "\n",
    "\n",
    "def build_similarity_matrix(\n",
    "    contacts: list[Contact], threshold: float = 0.8\n",
    ") -> dict[int, list[tuple[int, float]]]:\n",
    "    \"\"\"Build similarity matrix for all contact pairs.\n",
    "\n",
    "    Args:\n",
    "        contacts: List of contact records\n",
    "        threshold: Minimum similarity score (0.0-1.0)\n",
    "\n",
    "    Returns:\n",
    "        Dict mapping contact index to list of (similar_index, score) tuples\n",
    "    \"\"\"\n",
    "    matrix = {}\n",
    "\n",
    "    for i, contact_a in enumerate(contacts):\n",
    "        similar = []\n",
    "\n",
    "        for j, contact_b in enumerate(contacts):\n",
    "            if i >= j:  # Skip self-comparison and duplicates (i,j) == (j,i)\n",
    "                continue\n",
    "\n",
    "            # Calculate similarity\n",
    "            score = levenshtein_similarity(contact_a.name, contact_b.name)\n",
    "\n",
    "            if score >= threshold:\n",
    "                similar.append((j, score))\n",
    "\n",
    "        if similar:  # Only store if duplicates found\n",
    "            matrix[i] = similar\n",
    "\n",
    "    return matrix\n",
    "\n",
    "\n",
    "# Build similarity matrix\n",
    "similarity_matrix = build_similarity_matrix(contacts, threshold=0.8)\n",
    "\n",
    "print(\"Similarity Matrix (threshold=0.8)\")\n",
    "print(\"=\" * 60)\n",
    "for idx, similar_contacts in similarity_matrix.items():\n",
    "    print(f\"\\n{contacts[idx].name} (index {idx}):\")\n",
    "    for similar_idx, score in similar_contacts:\n",
    "        print(f\"  → {contacts[similar_idx].name} (index {similar_idx}): {score:.3f} similarity\")\n",
    "\n",
    "print(f\"\\n{'=' * 60}\")\n",
    "print(f\"Found {len(similarity_matrix)} contacts with potential duplicates\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "step3-notes",
   "metadata": {},
   "source": [
    "**Notes**:\n",
    "- **Upper triangle only**: We skip `i >= j` to avoid comparing (A,B) and (B,A) separately\n",
    "- **Sparse matrix**: Only stores pairs above threshold, saving memory for large datasets\n",
    "- **Score interpretation**: 1.0 = identical, 0.8-0.95 = likely duplicate, <0.8 = probably different"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "step4-title",
   "metadata": {},
   "source": [
    "### Step 4: Group Duplicates into Clusters\n",
    "\n",
    "The similarity matrix gives us pairwise matches, but we need to group transitive duplicates together. If A matches B and B matches C, all three should be in the same cluster.\n",
    "\n",
    "**Algorithm**: Union-Find (Disjoint Set Union) efficiently groups connected components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "step4-code",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Duplicate Clusters\n",
      "============================================================\n",
      "\n",
      "Cluster 1:\n",
      "  [0] John Smith           | john.smith@example.com         | CRM\n",
      "  [1] Jon Smith            | jsmith@example.com             | Manual Entry\n",
      "\n",
      "Cluster 2:\n",
      "  [2] Jane Doe             | jane.doe@company.com           | Import\n",
      "  [3] Jane Do              | jane.d@company.com             | Manual Entry\n",
      "\n",
      "Cluster 3:\n",
      "  [6] Alice Williams       | alice.w@startup.com            | CRM\n",
      "  [7] Alise Williams       | alice@startup.com              | Manual Entry\n",
      "\n",
      "============================================================\n",
      "Identified 3 duplicate clusters\n"
     ]
    }
   ],
   "source": [
    "def group_duplicates(similarity_matrix: dict[int, list[tuple[int, float]]]) -> list[set[int]]:\n",
    "    \"\"\"Group duplicate contacts into clusters using union-find.\n",
    "\n",
    "    Args:\n",
    "        similarity_matrix: Pairwise similarity mapping\n",
    "\n",
    "    Returns:\n",
    "        List of sets, each set contains indices of duplicate contacts\n",
    "    \"\"\"\n",
    "    # Union-Find data structure\n",
    "    parent = {}\n",
    "\n",
    "    def find(x: int) -> int:\n",
    "        \"\"\"Find root of x with path compression.\"\"\"\n",
    "        if x not in parent:\n",
    "            parent[x] = x\n",
    "        if parent[x] != x:\n",
    "            parent[x] = find(parent[x])  # Path compression\n",
    "        return parent[x]\n",
    "\n",
    "    def union(x: int, y: int):\n",
    "        \"\"\"Merge sets containing x and y.\"\"\"\n",
    "        root_x, root_y = find(x), find(y)\n",
    "        if root_x != root_y:\n",
    "            parent[root_x] = root_y\n",
    "\n",
    "    # Build connected components\n",
    "    for idx, similar_contacts in similarity_matrix.items():\n",
    "        for similar_idx, _ in similar_contacts:\n",
    "            union(idx, similar_idx)\n",
    "\n",
    "    # Group by root parent\n",
    "    clusters = {}\n",
    "    for idx in parent:\n",
    "        root = find(idx)\n",
    "        if root not in clusters:\n",
    "            clusters[root] = set()\n",
    "        clusters[root].add(idx)\n",
    "\n",
    "    return list(clusters.values())\n",
    "\n",
    "\n",
    "# Group duplicates\n",
    "duplicate_clusters = group_duplicates(similarity_matrix)\n",
    "\n",
    "print(\"Duplicate Clusters\")\n",
    "print(\"=\" * 60)\n",
    "for cluster_num, cluster in enumerate(duplicate_clusters, 1):\n",
    "    print(f\"\\nCluster {cluster_num}:\")\n",
    "    for idx in sorted(cluster):\n",
    "        contact = contacts[idx]\n",
    "        print(f\"  [{idx}] {contact.name:20} | {contact.email:30} | {contact.source}\")\n",
    "\n",
    "print(f\"\\n{'=' * 60}\")\n",
    "print(f\"Identified {len(duplicate_clusters)} duplicate clusters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "step4-notes",
   "metadata": {},
   "source": [
    "**Notes**:\n",
    "- **Union-Find efficiency**: O(α(n)) amortized time per operation (nearly constant)\n",
    "- **Path compression**: Flattens tree structure for faster subsequent lookups\n",
    "- **Transitive closure**: A~B and B~C → all in same cluster automatically"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "step5-title",
   "metadata": {},
   "source": [
    "### Step 5: Implement Merge Strategy\n",
    "\n",
    "Once duplicates are grouped, we need a strategy to pick the \"canonical\" record. Common strategies: earliest timestamp, most complete data, most recent update, or manual review flag.\n",
    "\n",
    "**Strategy**: Pick the record from the most trusted source (CRM > API > Import > Manual Entry)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "step5-code",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deduplication Results\n",
      "============================================================\n",
      "Original: 8 contacts\n",
      "Deduplicated: 5 contacts\n",
      "Removed: 3 duplicates\n",
      "\n",
      "Final Contact List:\n",
      "============================================================\n",
      "1. John Smith           | john.smith@example.com         | CRM\n",
      "2. Jane Doe             | jane.doe@company.com           | Import\n",
      "3. Robert Johnson       | robert.j@firm.com              | API\n",
      "4. Bob Johnson          | bob.johnson@firm.com           | CSV Upload\n",
      "5. Alice Williams       | alice.w@startup.com            | CRM\n"
     ]
    }
   ],
   "source": [
    "def merge_duplicates(\n",
    "    contacts: list[Contact], clusters: list[set[int]], source_priority: dict[str, int] | None = None\n",
    ") -> list[Contact]:\n",
    "    \"\"\"Merge duplicate clusters, keeping highest-priority record.\n",
    "\n",
    "    Args:\n",
    "        contacts: Original contact list\n",
    "        clusters: Duplicate clusters (from group_duplicates)\n",
    "        source_priority: Dict mapping source to priority (lower = higher priority)\n",
    "\n",
    "    Returns:\n",
    "        Deduplicated contact list\n",
    "    \"\"\"\n",
    "    if source_priority is None:\n",
    "        # Default priority: CRM > API > Import > Manual Entry > CSV Upload\n",
    "        source_priority = {\"CRM\": 1, \"API\": 2, \"Import\": 3, \"Manual Entry\": 4, \"CSV Upload\": 5}\n",
    "\n",
    "    # Track which indices to keep\n",
    "    to_remove = set()\n",
    "\n",
    "    for cluster in clusters:\n",
    "        # Find canonical record (highest priority source)\n",
    "        cluster_list = list(cluster)\n",
    "        canonical_idx = min(\n",
    "            cluster_list, key=lambda idx: source_priority.get(contacts[idx].source, 99)\n",
    "        )\n",
    "\n",
    "        # Mark others for removal\n",
    "        for idx in cluster:\n",
    "            if idx != canonical_idx:\n",
    "                to_remove.add(idx)\n",
    "\n",
    "    # Build deduplicated list\n",
    "    deduplicated = [contact for i, contact in enumerate(contacts) if i not in to_remove]\n",
    "\n",
    "    return deduplicated\n",
    "\n",
    "\n",
    "# Merge duplicates\n",
    "deduplicated_contacts = merge_duplicates(contacts, duplicate_clusters)\n",
    "\n",
    "print(\"Deduplication Results\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Original: {len(contacts)} contacts\")\n",
    "print(f\"Deduplicated: {len(deduplicated_contacts)} contacts\")\n",
    "print(f\"Removed: {len(contacts) - len(deduplicated_contacts)} duplicates\\n\")\n",
    "\n",
    "print(\"Final Contact List:\")\n",
    "print(\"=\" * 60)\n",
    "for i, contact in enumerate(deduplicated_contacts, 1):\n",
    "    print(f\"{i}. {contact.name:20} | {contact.email:30} | {contact.source}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "step5-notes",
   "metadata": {},
   "source": [
    "**Notes**:\n",
    "- **Source priority**: Adjust based on data quality - CRM records typically most accurate\n",
    "- **Alternative strategies**: Most recent timestamp, longest name (more complete), manual review flag\n",
    "- **Audit trail**: Consider logging removed duplicates for manual review before permanent deletion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "step6-title",
   "metadata": {},
   "source": [
    "### Step 6: Tune Threshold for Precision/Recall Trade-off\n",
    "\n",
    "The similarity threshold controls the trade-off between false positives (merging different people) and false negatives (missing duplicates).\n",
    "\n",
    "**Experimentation**: Test different thresholds to find optimal balance for your data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "step6-code",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Threshold Tuning Analysis\n",
      "============================================================\n",
      "\n",
      "Threshold: 0.7\n",
      "  Duplicate clusters found: 4\n",
      "  Records removed: 4\n",
      "  Final count: 4\n",
      "  Merged groups:\n",
      "    - John Smith, Jon Smith\n",
      "    - Jane Doe, Jane Do\n",
      "    - Robert Johnson, Bob Johnson\n",
      "    - Alice Williams, Alise Williams\n",
      "\n",
      "Threshold: 0.8\n",
      "  Duplicate clusters found: 3\n",
      "  Records removed: 3\n",
      "  Final count: 5\n",
      "  Merged groups:\n",
      "    - John Smith, Jon Smith\n",
      "    - Jane Doe, Jane Do\n",
      "    - Alice Williams, Alise Williams\n",
      "\n",
      "Threshold: 0.85\n",
      "  Duplicate clusters found: 3\n",
      "  Records removed: 3\n",
      "  Final count: 5\n",
      "  Merged groups:\n",
      "    - John Smith, Jon Smith\n",
      "    - Jane Doe, Jane Do\n",
      "    - Alice Williams, Alise Williams\n",
      "\n",
      "Threshold: 0.9\n",
      "  Duplicate clusters found: 2\n",
      "  Records removed: 2\n",
      "  Final count: 6\n",
      "  Merged groups:\n",
      "    - John Smith, Jon Smith\n",
      "    - Alice Williams, Alise Williams\n",
      "\n",
      "============================================================\n",
      "Recommendation:\n",
      "  - 0.7-0.75: Aggressive (more false positives)\n",
      "  - 0.8-0.85: Balanced (recommended for names)\n",
      "  - 0.9+: Conservative (fewer matches, higher confidence)\n"
     ]
    }
   ],
   "source": [
    "# Test different thresholds\n",
    "thresholds = [0.7, 0.8, 0.85, 0.9]\n",
    "\n",
    "print(\"Threshold Tuning Analysis\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for threshold in thresholds:\n",
    "    matrix = build_similarity_matrix(contacts, threshold=threshold)\n",
    "    clusters = group_duplicates(matrix)\n",
    "    deduped = merge_duplicates(contacts, clusters)\n",
    "\n",
    "    removed = len(contacts) - len(deduped)\n",
    "\n",
    "    print(f\"\\nThreshold: {threshold}\")\n",
    "    print(f\"  Duplicate clusters found: {len(clusters)}\")\n",
    "    print(f\"  Records removed: {removed}\")\n",
    "    print(f\"  Final count: {len(deduped)}\")\n",
    "\n",
    "    # Show what was merged\n",
    "    if clusters:\n",
    "        print(\"  Merged groups:\")\n",
    "        for cluster in clusters:\n",
    "            names = [contacts[idx].name for idx in cluster]\n",
    "            print(f\"    - {', '.join(names)}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"Recommendation:\")\n",
    "print(\"  - 0.7-0.75: Aggressive (more false positives)\")\n",
    "print(\"  - 0.8-0.85: Balanced (recommended for names)\")\n",
    "print(\"  - 0.9+: Conservative (fewer matches, higher confidence)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "step6-notes",
   "metadata": {},
   "source": [
    "**Notes**:\n",
    "- **Low threshold (<0.75)**: Catches more duplicates but risks merging different people\n",
    "- **High threshold (>0.9)**: Very safe but may miss valid duplicates like \"Bob\" vs \"Robert\"\n",
    "- **Domain-specific**: Names tolerate lower thresholds than product codes or addresses"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "complete-example",
   "metadata": {},
   "source": [
    "## Complete Working Example\n",
    "\n",
    "Here's a production-ready deduplication system combining all steps into a single function. Copy-paste this into your project for immediate use.\n",
    "\n",
    "**Features**:\n",
    "- ✅ Configurable similarity threshold\n",
    "- ✅ Multiple algorithm support (Levenshtein, Jaro-Winkler, etc.)\n",
    "- ✅ Customizable merge strategy (source priority)\n",
    "- ✅ Audit trail (returns removed duplicates)\n",
    "- ✅ Type-safe dataclass interface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "complete-code",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deduplication Complete\n",
      "============================================================\n",
      "Original contacts: 8\n",
      "Deduplicated contacts: 5\n",
      "Duplicates removed: 3\n",
      "Duplicate clusters: 3\n",
      "\n",
      "Removed Duplicates (for audit):\n",
      "  - Jon Smith (Manual Entry)\n",
      "  - Jane Do (Manual Entry)\n",
      "  - Alise Williams (Manual Entry)\n",
      "\n",
      "Final Contact List:\n",
      "1. John Smith | john.smith@example.com | CRM\n",
      "2. Jane Doe | jane.doe@company.com | Import\n",
      "3. Robert Johnson | robert.j@firm.com | API\n",
      "4. Bob Johnson | bob.johnson@firm.com | CSV Upload\n",
      "5. Alice Williams | alice.w@startup.com | CRM\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Production-ready fuzzy deduplication system.\n",
    "\n",
    "Copy this entire cell into your project for immediate use.\n",
    "\"\"\"\n",
    "\n",
    "from dataclasses import dataclass\n",
    "from typing import Literal\n",
    "\n",
    "from lionherd_core.libs.string_handlers._string_similarity import jaro_winkler_similarity\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class Contact:\n",
    "    \"\"\"Contact record.\"\"\"\n",
    "\n",
    "    name: str\n",
    "    email: str\n",
    "    source: str\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class DeduplicationResult:\n",
    "    \"\"\"Result of deduplication operation.\"\"\"\n",
    "\n",
    "    deduplicated: list[Contact]\n",
    "    removed: list[Contact]\n",
    "    clusters: list[set[int]]\n",
    "    original_count: int\n",
    "    final_count: int\n",
    "\n",
    "\n",
    "def deduplicate_contacts(\n",
    "    contacts: list[Contact],\n",
    "    threshold: float = 0.8,\n",
    "    algorithm: Literal[\"levenshtein\", \"jaro_winkler\"] = \"levenshtein\",\n",
    "    source_priority: dict[str, int] | None = None,\n",
    ") -> DeduplicationResult:\n",
    "    \"\"\"Deduplicate contact list using fuzzy string matching.\n",
    "\n",
    "    Args:\n",
    "        contacts: List of contact records\n",
    "        threshold: Similarity threshold (0.0-1.0), higher = stricter\n",
    "        algorithm: Similarity algorithm to use\n",
    "        source_priority: Dict mapping source to priority (lower = keep)\n",
    "\n",
    "    Returns:\n",
    "        DeduplicationResult with deduplicated contacts and audit info\n",
    "    \"\"\"\n",
    "    # Select similarity function\n",
    "    sim_func = {\"levenshtein\": levenshtein_similarity, \"jaro_winkler\": jaro_winkler_similarity}[\n",
    "        algorithm\n",
    "    ]\n",
    "\n",
    "    # Default source priority\n",
    "    if source_priority is None:\n",
    "        source_priority = {\"CRM\": 1, \"API\": 2, \"Import\": 3, \"Manual Entry\": 4, \"CSV Upload\": 5}\n",
    "\n",
    "    # Step 1: Build similarity matrix\n",
    "    matrix = {}\n",
    "    for i, contact_a in enumerate(contacts):\n",
    "        similar = []\n",
    "        for j, contact_b in enumerate(contacts):\n",
    "            if i >= j:\n",
    "                continue\n",
    "            score = sim_func(contact_a.name, contact_b.name)\n",
    "            if score >= threshold:\n",
    "                similar.append((j, score))\n",
    "        if similar:\n",
    "            matrix[i] = similar\n",
    "\n",
    "    # Step 2: Group duplicates (union-find)\n",
    "    parent = {}\n",
    "\n",
    "    def find(x):\n",
    "        if x not in parent:\n",
    "            parent[x] = x\n",
    "        if parent[x] != x:\n",
    "            parent[x] = find(parent[x])\n",
    "        return parent[x]\n",
    "\n",
    "    def union(x, y):\n",
    "        root_x, root_y = find(x), find(y)\n",
    "        if root_x != root_y:\n",
    "            parent[root_x] = root_y\n",
    "\n",
    "    for idx, similar_contacts in matrix.items():\n",
    "        for similar_idx, _ in similar_contacts:\n",
    "            union(idx, similar_idx)\n",
    "\n",
    "    clusters_dict = {}\n",
    "    for idx in parent:\n",
    "        root = find(idx)\n",
    "        if root not in clusters_dict:\n",
    "            clusters_dict[root] = set()\n",
    "        clusters_dict[root].add(idx)\n",
    "\n",
    "    clusters = list(clusters_dict.values())\n",
    "\n",
    "    # Step 3: Merge duplicates\n",
    "    to_remove = set()\n",
    "    for cluster in clusters:\n",
    "        cluster_list = list(cluster)\n",
    "        canonical_idx = min(\n",
    "            cluster_list, key=lambda idx: source_priority.get(contacts[idx].source, 99)\n",
    "        )\n",
    "        for idx in cluster:\n",
    "            if idx != canonical_idx:\n",
    "                to_remove.add(idx)\n",
    "\n",
    "    deduplicated = [contact for i, contact in enumerate(contacts) if i not in to_remove]\n",
    "\n",
    "    removed = [contact for i, contact in enumerate(contacts) if i in to_remove]\n",
    "\n",
    "    return DeduplicationResult(\n",
    "        deduplicated=deduplicated,\n",
    "        removed=removed,\n",
    "        clusters=clusters,\n",
    "        original_count=len(contacts),\n",
    "        final_count=len(deduplicated),\n",
    "    )\n",
    "\n",
    "\n",
    "# Example usage\n",
    "result = deduplicate_contacts(contacts, threshold=0.8, algorithm=\"levenshtein\")\n",
    "\n",
    "print(\"Deduplication Complete\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Original contacts: {result.original_count}\")\n",
    "print(f\"Deduplicated contacts: {result.final_count}\")\n",
    "print(f\"Duplicates removed: {len(result.removed)}\")\n",
    "print(f\"Duplicate clusters: {len(result.clusters)}\")\n",
    "\n",
    "print(\"\\nRemoved Duplicates (for audit):\")\n",
    "for contact in result.removed:\n",
    "    print(f\"  - {contact.name} ({contact.source})\")\n",
    "\n",
    "print(\"\\nFinal Contact List:\")\n",
    "for i, contact in enumerate(result.deduplicated, 1):\n",
    "    print(f\"{i}. {contact.name} | {contact.email} | {contact.source}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "production-considerations",
   "metadata": {},
   "source": [
    "## Production Considerations\n",
    "\n",
    "### Error Handling\n",
    "\n",
    "**What Can Go Wrong**:\n",
    "1. **Empty contact list**: No records to deduplicate\n",
    "2. **Invalid threshold**: Values outside 0.0-1.0 range\n",
    "3. **Missing source priority**: Unknown source types in data\n",
    "4. **Large datasets**: O(n²) complexity becomes slow for >10,000 records\n",
    "\n",
    "**Handling**:\n",
    "```python\n",
    "def safe_deduplicate(\n",
    "    contacts: list[Contact],\n",
    "    threshold: float = 0.8\n",
    ") -> DeduplicationResult | None:\n",
    "    \"\"\"Deduplicate with comprehensive error handling.\"\"\"\n",
    "    # Validate inputs\n",
    "    if not contacts:\n",
    "        print(\"Warning: Empty contact list\")\n",
    "        return None\n",
    "    \n",
    "    if not 0.0 <= threshold <= 1.0:\n",
    "        raise ValueError(f\"Threshold must be 0.0-1.0, got {threshold}\")\n",
    "    \n",
    "    # Warn about performance\n",
    "    if len(contacts) > 10000:\n",
    "        print(f\"Warning: {len(contacts)} records may be slow (O(n²) algorithm)\")\n",
    "    \n",
    "    try:\n",
    "        return deduplicate_contacts(contacts, threshold)\n",
    "    except Exception as e:\n",
    "        print(f\"Deduplication failed: {e}\")\n",
    "        return None\n",
    "```\n",
    "\n",
    "### Performance\n",
    "\n",
    "**Scalability**:\n",
    "- **Similarity matrix**: O(n²) time, O(n²) space for n records\n",
    "- **Union-find**: O(α(n)) amortized per operation (nearly constant)\n",
    "- **Total**: O(n²) dominated by pairwise comparisons\n",
    "\n",
    "**Benchmarks** (approximate, single-threaded):\n",
    "- 100 records: ~50ms\n",
    "- 1,000 records: ~5s\n",
    "- 10,000 records: ~8-10 minutes\n",
    "- 100,000 records: Use blocking/clustering pre-processing\n",
    "\n",
    "**Optimization for Large Datasets**:\n",
    "```python\n",
    "# Blocking: Group by first letter before comparing\n",
    "def block_by_first_letter(contacts: list[Contact]) -> dict[str, list[Contact]]:\n",
    "    \"\"\"Group contacts by first letter to reduce comparisons.\"\"\"\n",
    "    blocks = {}\n",
    "    for contact in contacts:\n",
    "        key = contact.name[0].upper() if contact.name else \"_\"\n",
    "        if key not in blocks:\n",
    "            blocks[key] = []\n",
    "        blocks[key].append(contact)\n",
    "    return blocks\n",
    "\n",
    "# Deduplicate within each block (reduces O(n²) to O(n²/k) for k blocks)\n",
    "blocks = block_by_first_letter(contacts)\n",
    "all_deduplicated = []\n",
    "for block_contacts in blocks.values():\n",
    "    result = deduplicate_contacts(block_contacts, threshold=0.8)\n",
    "    all_deduplicated.extend(result.deduplicated)\n",
    "```\n",
    "\n",
    "### Testing\n",
    "\n",
    "**Unit Tests**:\n",
    "```python\n",
    "def test_deduplication():\n",
    "    \"\"\"Test basic deduplication functionality.\"\"\"\n",
    "    contacts = [\n",
    "        Contact(\"John Smith\", \"j@example.com\", \"CRM\"),\n",
    "        Contact(\"Jon Smith\", \"jon@example.com\", \"Manual Entry\"),\n",
    "    ]\n",
    "    \n",
    "    result = deduplicate_contacts(contacts, threshold=0.8)\n",
    "    \n",
    "    # Should merge into 1 contact\n",
    "    assert result.final_count == 1\n",
    "    assert len(result.removed) == 1\n",
    "    \n",
    "    # Should keep CRM record (higher priority)\n",
    "    assert result.deduplicated[0].name == \"John Smith\"\n",
    "    assert result.deduplicated[0].source == \"CRM\"\n",
    "```\n",
    "\n",
    "**Integration Tests**:\n",
    "- Test with real data samples (100-1000 records)\n",
    "- Validate precision/recall against manual labels\n",
    "- Test different thresholds (0.7, 0.8, 0.85, 0.9)\n",
    "- Verify audit trail (removed duplicates logged correctly)\n",
    "\n",
    "### Monitoring\n",
    "\n",
    "**Key Metrics**:\n",
    "- **Deduplication rate**: % of records removed (typical: 5-15% for CRM data)\n",
    "- **Cluster size distribution**: Most clusters should be pairs, large clusters (>5) may indicate issues\n",
    "- **Processing time**: Track time per 1000 records for capacity planning\n",
    "\n",
    "**Observability**:\n",
    "```python\n",
    "import time\n",
    "\n",
    "def deduplicate_with_metrics(contacts: list[Contact]) -> DeduplicationResult:\n",
    "    \"\"\"Deduplicate with metric emission.\"\"\"\n",
    "    start = time.time()\n",
    "    \n",
    "    result = deduplicate_contacts(contacts, threshold=0.8)\n",
    "    \n",
    "    duration = time.time() - start\n",
    "    dedup_rate = len(result.removed) / result.original_count if result.original_count > 0 else 0\n",
    "    \n",
    "    # Log metrics\n",
    "    print(f\"Deduplication metrics:\")\n",
    "    print(f\"  Duration: {duration:.2f}s\")\n",
    "    print(f\"  Records: {result.original_count} → {result.final_count}\")\n",
    "    print(f\"  Dedup rate: {dedup_rate:.1%}\")\n",
    "    print(f\"  Clusters: {len(result.clusters)}\")\n",
    "    print(f\"  Avg cluster size: {len(result.removed) / len(result.clusters) if result.clusters else 0:.1f}\")\n",
    "    \n",
    "    return result\n",
    "```\n",
    "\n",
    "### Configuration Tuning\n",
    "\n",
    "**threshold**:\n",
    "- Too low (< 0.7): False positives - \"Jane Doe\" merges with \"Jane Smith\"\n",
    "- Too high (> 0.9): False negatives - \"Bob Johnson\" not matched to \"Robert Johnson\"\n",
    "- Recommended: 0.8-0.85 for names, 0.9+ for product codes/IDs\n",
    "\n",
    "**algorithm**:\n",
    "- `levenshtein`: Best for typos and character-level edits (\"John\" vs \"Jon\")\n",
    "- `jaro_winkler`: Best for prefix variations and different lengths (\"Robert\" vs \"Bob\")\n",
    "- Recommended: Start with `levenshtein`, switch to `jaro_winkler` if missing nickname matches\n",
    "\n",
    "**source_priority**:\n",
    "- Adjust based on data quality: CRM (most accurate) > API > Manual Entry (least reliable)\n",
    "- Alternative: Use timestamp (most recent), completeness score (most fields filled), or manual review flag"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "variations",
   "metadata": {},
   "source": [
    "## Variations\n",
    "\n",
    "### 1. Multi-Field Matching\n",
    "\n",
    "**When to Use**: Higher confidence deduplication by combining name + email + other fields\n",
    "\n",
    "**Approach**:\n",
    "```python\n",
    "def multi_field_similarity(contact_a: Contact, contact_b: Contact) -> float:\n",
    "    \"\"\"Calculate weighted similarity across multiple fields.\"\"\"\n",
    "    name_sim = levenshtein_similarity(contact_a.name, contact_b.name)\n",
    "    \n",
    "    # Extract email username (before @)\n",
    "    email_a = contact_a.email.split(\"@\")[0]\n",
    "    email_b = contact_b.email.split(\"@\")[0]\n",
    "    email_sim = levenshtein_similarity(email_a, email_b)\n",
    "    \n",
    "    # Weighted average: name 70%, email 30%\n",
    "    return 0.7 * name_sim + 0.3 * email_sim\n",
    "\n",
    "# Use in similarity matrix\n",
    "for i, contact_a in enumerate(contacts):\n",
    "    for j, contact_b in enumerate(contacts):\n",
    "        if i >= j:\n",
    "            continue\n",
    "        score = multi_field_similarity(contact_a, contact_b)\n",
    "        # ... rest of logic\n",
    "```\n",
    "\n",
    "**Trade-offs**:\n",
    "- ✅ Higher precision - fewer false positives\n",
    "- ✅ Catches duplicates with different names but same email\n",
    "- ❌ Slower - more comparisons per pair\n",
    "- ❌ May miss duplicates with different emails\n",
    "\n",
    "### 2. Incremental Deduplication\n",
    "\n",
    "**When to Use**: New records added frequently, avoid re-processing entire dataset\n",
    "\n",
    "**Approach**:\n",
    "```python\n",
    "def incremental_deduplicate(\n",
    "    existing: list[Contact],\n",
    "    new_records: list[Contact],\n",
    "    threshold: float = 0.8\n",
    ") -> tuple[list[Contact], list[Contact]]:\n",
    "    \"\"\"Deduplicate new records against existing database.\n",
    "    \n",
    "    Returns:\n",
    "        (unique_new_records, duplicate_new_records)\n",
    "    \"\"\"\n",
    "    unique = []\n",
    "    duplicates = []\n",
    "    \n",
    "    for new_contact in new_records:\n",
    "        # Check if similar to any existing contact\n",
    "        is_duplicate = False\n",
    "        for existing_contact in existing:\n",
    "            score = levenshtein_similarity(new_contact.name, existing_contact.name)\n",
    "            if score >= threshold:\n",
    "                duplicates.append(new_contact)\n",
    "                is_duplicate = True\n",
    "                break\n",
    "        \n",
    "        if not is_duplicate:\n",
    "            unique.append(new_contact)\n",
    "    \n",
    "    return unique, duplicates\n",
    "\n",
    "# Usage: Only compare new records against existing (O(n×m) instead of O((n+m)²))\n",
    "unique, dupes = incremental_deduplicate(deduplicated_contacts, new_imports)\n",
    "```\n",
    "\n",
    "**Trade-offs**:\n",
    "- ✅ Much faster for large existing datasets\n",
    "- ✅ Suitable for real-time duplicate detection\n",
    "- ❌ Doesn't detect duplicates within new batch\n",
    "- ❌ Existing dataset must already be deduplicated\n",
    "\n",
    "### 3. Manual Review Queue\n",
    "\n",
    "**When to Use**: High-stakes deduplication where errors are costly (financial, legal, medical)\n",
    "\n",
    "**Approach**:\n",
    "```python\n",
    "@dataclass\n",
    "class ReviewItem:\n",
    "    \"\"\"Potential duplicate for manual review.\"\"\"\n",
    "    contact_a: Contact\n",
    "    contact_b: Contact\n",
    "    similarity: float\n",
    "    auto_merge: bool  # True if confidence high enough\n",
    "\n",
    "def generate_review_queue(\n",
    "    contacts: list[Contact],\n",
    "    auto_threshold: float = 0.95,  # Auto-merge above this\n",
    "    review_threshold: float = 0.75  # Manual review between thresholds\n",
    ") -> tuple[list[ReviewItem], list[Contact]]:\n",
    "    \"\"\"Generate review queue for borderline duplicates.\n",
    "    \n",
    "    Returns:\n",
    "        (review_queue, auto_merged_contacts)\n",
    "    \"\"\"\n",
    "    review_queue = []\n",
    "    \n",
    "    # Build similarity matrix\n",
    "    for i, contact_a in enumerate(contacts):\n",
    "        for j, contact_b in enumerate(contacts):\n",
    "            if i >= j:\n",
    "                continue\n",
    "            \n",
    "            score = levenshtein_similarity(contact_a.name, contact_b.name)\n",
    "            \n",
    "            if score >= review_threshold:\n",
    "                review_queue.append(ReviewItem(\n",
    "                    contact_a=contact_a,\n",
    "                    contact_b=contact_b,\n",
    "                    similarity=score,\n",
    "                    auto_merge=(score >= auto_threshold)\n",
    "                ))\n",
    "    \n",
    "    # Auto-merge high-confidence duplicates\n",
    "    auto_merge_items = [item for item in review_queue if item.auto_merge]\n",
    "    # ... merge logic ...\n",
    "    \n",
    "    # Return borderline cases for manual review\n",
    "    manual_review = [item for item in review_queue if not item.auto_merge]\n",
    "    \n",
    "    return manual_review, auto_merged\n",
    "```\n",
    "\n",
    "**Trade-offs**:\n",
    "- ✅ Prevents costly false positives\n",
    "- ✅ Builds confidence through human validation\n",
    "- ❌ Requires manual effort\n",
    "- ❌ Slower processing time\n",
    "\n",
    "## Choosing the Right Variation\n",
    "\n",
    "| Scenario | Recommended Variation |\n",
    "|----------|----------------------|\n",
    "| High data quality requirements (finance, medical) | Manual Review Queue |\n",
    "| Large existing database + frequent new records | Incremental Deduplication |\n",
    "| Multiple identifying fields available | Multi-Field Matching |\n",
    "| Standard contact/customer deduplication | Base implementation (this tutorial) |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "summary",
   "metadata": {},
   "source": "## Summary\n\n**What You Accomplished**:\n- ✅ Built a fuzzy deduplication system using `string_similarity` with Levenshtein distance\n- ✅ Implemented similarity matrix construction for pairwise comparisons\n- ✅ Used union-find algorithm to group transitive duplicates efficiently\n- ✅ Created configurable merge strategy with source priority\n- ✅ Learned threshold tuning for precision/recall trade-offs\n\n**Key Takeaways**:\n1. **Levenshtein distance is ideal for typo detection**: Handles single-character edits (\"John\" vs \"Jon\") effectively\n2. **Threshold tuning is critical**: 0.8-0.85 works for most names, adjust based on false positive/negative analysis\n3. **Union-find efficiently groups duplicates**: O(α(n)) amortized time handles transitive relationships automatically\n4. **Source priority matters**: CRM > API > Manual Entry - keep the most trusted record\n5. **O(n²) complexity requires optimization for scale**: Use blocking/clustering for >10,000 records\n\n**When to Use This Pattern**:\n- ✅ Contact list deduplication (CRM systems, email marketing)\n- ✅ Customer database cleanup (merging duplicate accounts)\n- ✅ Product catalog deduplication (handling typos in product names)\n- ✅ User account merging (same person, different spellings)\n- ❌ Exact-match deduplication (use `set()` or SQL `DISTINCT` instead)\n- ❌ Large-scale entity resolution (>100,000 records - use specialized tools)\n\n## Related Resources\n\n**lionherd-core API Reference**:\n- [string_similarity](../../../docs/api/libs/string_handlers/string_similarity.md) - Complete API for similarity algorithms\n- [SimilarityAlgo](../../../docs/api/libs/string_handlers/string_similarity.md#similarityalgo) - Available algorithm enum\n\n**Related Tutorials**:\n- [Fuzzy Validation](../ln_utilities/fuzzy_validation.ipynb) - Using fuzzy matching for data validation\n\n**External Resources**:\n- [Levenshtein Distance - Wikipedia](https://en.wikipedia.org/wiki/Levenshtein_distance) - Theory and algorithms\n- [Dedupe.io Documentation](https://docs.dedupe.io/en/latest/) - Advanced deduplication techniques for large datasets"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lionherd-core",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

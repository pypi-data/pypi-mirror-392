{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Concurrency Patterns - High-Level Async Coordination\n",
    "\n",
    "Concurrency patterns provide battle-tested primitives for common async coordination scenarios:\n",
    "\n",
    "**Core Patterns:**\n",
    "- **gather**: Run multiple awaitables concurrently, collect all results\n",
    "- **race**: Return first completion, cancel the rest\n",
    "- **bounded_map**: Apply async function to items with concurrency limit\n",
    "- **CompletionStream**: Process results as they complete (streaming)\n",
    "- **retry**: Exponential backoff retry with deadline awareness\n",
    "\n",
    "**Key Features:**\n",
    "- Structured concurrency (proper cleanup on cancellation)\n",
    "- Exception handling with `return_exceptions` mode\n",
    "- Deadline-aware operations\n",
    "- Rate limiting via `CapacityLimiter`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-09T06:15:45.939121Z",
     "iopub.status.busy": "2025-11-09T06:15:45.939048Z",
     "iopub.status.idle": "2025-11-09T06:15:46.039398Z",
     "shell.execute_reply": "2025-11-09T06:15:46.039029Z"
    }
   },
   "outputs": [],
   "source": [
    "from lionherd_core.libs.concurrency import (\n",
    "    CompletionStream,\n",
    "    bounded_map,\n",
    "    gather,\n",
    "    race,\n",
    "    retry,\n",
    "    sleep,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. gather() - Concurrent Execution with Result Collection\n",
    "\n",
    "`gather()` runs multiple awaitables concurrently and collects all results. Similar to `asyncio.gather()` but with structured concurrency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-09T06:15:46.040732Z",
     "iopub.status.busy": "2025-11-09T06:15:46.040653Z",
     "iopub.status.idle": "2025-11-09T06:15:46.195575Z",
     "shell.execute_reply": "2025-11-09T06:15:46.195128Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed 3 tasks:\n",
      "  {'id': 1, 'data': 'result_1'}\n",
      "  {'id': 2, 'data': 'result_2'}\n",
      "  {'id': 3, 'data': 'result_3'}\n"
     ]
    }
   ],
   "source": [
    "# Basic gather - all tasks succeed\n",
    "async def fetch_data(id: int, delay: float) -> dict:\n",
    "    await sleep(delay)\n",
    "    return {\"id\": id, \"data\": f\"result_{id}\"}\n",
    "\n",
    "\n",
    "# Run 3 tasks concurrently\n",
    "results = await gather(\n",
    "    fetch_data(1, 0.1),\n",
    "    fetch_data(2, 0.05),\n",
    "    fetch_data(3, 0.15),\n",
    ")\n",
    "\n",
    "print(f\"Completed {len(results)} tasks:\")\n",
    "for r in results:\n",
    "    print(f\"  {r}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exception Handling\n",
    "\n",
    "By default, gather raises on first error. Use `return_exceptions=True` to collect both successes and failures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-09T06:15:46.213927Z",
     "iopub.status.busy": "2025-11-09T06:15:46.213830Z",
     "iopub.status.idle": "2025-11-09T06:15:46.228616Z",
     "shell.execute_reply": "2025-11-09T06:15:46.228187Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results with exceptions:\n",
      "  Task 0: Success 1\n",
      "  Task 1: FAILED - Task 2 failed\n",
      "  Task 2: Success 3\n"
     ]
    }
   ],
   "source": [
    "# Task that fails\n",
    "async def maybe_fail(id: int, should_fail: bool) -> str:\n",
    "    await sleep(0.01)\n",
    "    if should_fail:\n",
    "        raise ValueError(f\"Task {id} failed\")\n",
    "    return f\"Success {id}\"\n",
    "\n",
    "\n",
    "# Collect exceptions instead of raising\n",
    "results = await gather(\n",
    "    maybe_fail(1, False),\n",
    "    maybe_fail(2, True),  # This fails\n",
    "    maybe_fail(3, False),\n",
    "    return_exceptions=True,\n",
    ")\n",
    "\n",
    "print(\"Results with exceptions:\")\n",
    "for i, r in enumerate(results):\n",
    "    if isinstance(r, Exception):\n",
    "        print(f\"  Task {i}: FAILED - {r}\")\n",
    "    else:\n",
    "        print(f\"  Task {i}: {r}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-09T06:15:46.229833Z",
     "iopub.status.busy": "2025-11-09T06:15:46.229756Z",
     "iopub.status.idle": "2025-11-09T06:15:46.244112Z",
     "shell.execute_reply": "2025-11-09T06:15:46.243626Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ExceptionGroup raised with 1 exception(s):\n",
      "  - Task 2 failed\n"
     ]
    }
   ],
   "source": [
    "# Without return_exceptions, first error propagates\n",
    "try:\n",
    "    results = await gather(\n",
    "        maybe_fail(1, False),\n",
    "        maybe_fail(2, True),  # This will raise\n",
    "        maybe_fail(3, False),\n",
    "    )\n",
    "except ExceptionGroup as eg:\n",
    "    print(f\"ExceptionGroup raised with {len(eg.exceptions)} exception(s):\")\n",
    "    for exc in eg.exceptions:\n",
    "        print(f\"  - {exc}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. race() - First Wins\n",
    "\n",
    "`race()` returns the result of the first awaitable to complete, canceling all others."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-09T06:15:46.245195Z",
     "iopub.status.busy": "2025-11-09T06:15:46.245119Z",
     "iopub.status.idle": "2025-11-09T06:15:46.350481Z",
     "shell.execute_reply": "2025-11-09T06:15:46.350001Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Winner: fast_result\n"
     ]
    }
   ],
   "source": [
    "# Different speed tasks\n",
    "async def slow_api() -> str:\n",
    "    await sleep(0.5)\n",
    "    return \"slow_result\"\n",
    "\n",
    "\n",
    "async def fast_api() -> str:\n",
    "    await sleep(0.1)\n",
    "    return \"fast_result\"\n",
    "\n",
    "\n",
    "async def medium_api() -> str:\n",
    "    await sleep(0.3)\n",
    "    return \"medium_result\"\n",
    "\n",
    "\n",
    "# Race them - fast_api wins\n",
    "winner = await race(slow_api(), fast_api(), medium_api())\n",
    "print(f\"Winner: {winner}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-09T06:15:46.351620Z",
     "iopub.status.busy": "2025-11-09T06:15:46.351549Z",
     "iopub.status.idle": "2025-11-09T06:15:46.556498Z",
     "shell.execute_reply": "2025-11-09T06:15:46.556026Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result: timeout_exceeded\n"
     ]
    }
   ],
   "source": [
    "# Race with timeout - useful for fallback patterns\n",
    "async def primary_service() -> str:\n",
    "    await sleep(1.0)  # Too slow\n",
    "    return \"primary\"\n",
    "\n",
    "\n",
    "async def timeout_fallback() -> str:\n",
    "    await sleep(0.2)  # Timeout threshold\n",
    "    return \"timeout_exceeded\"\n",
    "\n",
    "\n",
    "result = await race(primary_service(), timeout_fallback())\n",
    "print(f\"Result: {result}\")  # Falls back due to timeout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. bounded_map() - Controlled Parallelism\n",
    "\n",
    "`bounded_map()` applies an async function to many items with a concurrency limit. Essential for rate-limited APIs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-09T06:15:46.557877Z",
     "iopub.status.busy": "2025-11-09T06:15:46.557785Z",
     "iopub.status.idle": "2025-11-09T06:15:46.970217Z",
     "shell.execute_reply": "2025-11-09T06:15:46.969824Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 10 items\n",
      "Total calls: 10\n",
      "Max concurrent: 3 (limit was 3)\n",
      "Sample results: [{'item': 0, 'processed': True}, {'item': 1, 'processed': True}, {'item': 2, 'processed': True}]\n"
     ]
    }
   ],
   "source": [
    "# Simulate API calls with rate limiting\n",
    "call_count = 0\n",
    "max_concurrent = 0\n",
    "active_calls = 0\n",
    "\n",
    "\n",
    "async def rate_limited_api(item: int) -> dict:\n",
    "    global call_count, max_concurrent, active_calls\n",
    "\n",
    "    call_count += 1\n",
    "    active_calls += 1\n",
    "    max_concurrent = max(max_concurrent, active_calls)\n",
    "\n",
    "    await sleep(0.1)  # Simulate API call\n",
    "\n",
    "    active_calls -= 1\n",
    "    return {\"item\": item, \"processed\": True}\n",
    "\n",
    "\n",
    "# Process 10 items with limit of 3 concurrent\n",
    "items = range(10)\n",
    "results = await bounded_map(\n",
    "    rate_limited_api,\n",
    "    items,\n",
    "    limit=3,\n",
    ")\n",
    "\n",
    "print(f\"Processed {len(results)} items\")\n",
    "print(f\"Total calls: {call_count}\")\n",
    "print(f\"Max concurrent: {max_concurrent} (limit was 3)\")\n",
    "print(f\"Sample results: {results[:3]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exception Handling in bounded_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-09T06:15:46.971433Z",
     "iopub.status.busy": "2025-11-09T06:15:46.971350Z",
     "iopub.status.idle": "2025-11-09T06:15:47.010505Z",
     "shell.execute_reply": "2025-11-09T06:15:47.009966Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results:\n",
      "  Item 0: FAILED - Item 0 is divisible by 3\n",
      "  Item 1: 2\n",
      "  Item 2: 4\n",
      "  Item 3: FAILED - Item 3 is divisible by 3\n",
      "  Item 4: 8\n",
      "  Item 5: 10\n",
      "  Item 6: FAILED - Item 6 is divisible by 3\n",
      "  Item 7: 14\n",
      "  Item 8: 16\n",
      "  Item 9: FAILED - Item 9 is divisible by 3\n"
     ]
    }
   ],
   "source": [
    "# Mix of successes and failures\n",
    "async def process_item(n: int) -> int:\n",
    "    await sleep(0.01)\n",
    "    if n % 3 == 0:\n",
    "        raise ValueError(f\"Item {n} is divisible by 3\")\n",
    "    return n * 2\n",
    "\n",
    "\n",
    "# Collect exceptions\n",
    "results = await bounded_map(\n",
    "    process_item,\n",
    "    range(10),\n",
    "    limit=4,\n",
    "    return_exceptions=True,\n",
    ")\n",
    "\n",
    "print(\"Results:\")\n",
    "for i, r in enumerate(results):\n",
    "    if isinstance(r, Exception):\n",
    "        print(f\"  Item {i}: FAILED - {r}\")\n",
    "    else:\n",
    "        print(f\"  Item {i}: {r}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. CompletionStream - Process Results as They Arrive\n",
    "\n",
    "`CompletionStream` yields results as tasks complete (not in submission order). Useful for UI updates or early processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-09T06:15:47.012043Z",
     "iopub.status.busy": "2025-11-09T06:15:47.011925Z",
     "iopub.status.idle": "2025-11-09T06:15:47.519076Z",
     "shell.execute_reply": "2025-11-09T06:15:47.518101Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing as tasks complete:\n",
      "  Task 1 completed: {'id': 2, 'delay': 0.1}\n",
      "  Task 3 completed: {'id': 4, 'delay': 0.2}\n",
      "  Task 0 completed: {'id': 1, 'delay': 0.3}\n",
      "  Task 2 completed: {'id': 3, 'delay': 0.5}\n"
     ]
    }
   ],
   "source": [
    "# Tasks with varying completion times\n",
    "async def task_with_delay(id: int, delay: float) -> dict:\n",
    "    await sleep(delay)\n",
    "    return {\"id\": id, \"delay\": delay}\n",
    "\n",
    "\n",
    "tasks = [\n",
    "    task_with_delay(1, 0.3),\n",
    "    task_with_delay(2, 0.1),  # Completes first\n",
    "    task_with_delay(3, 0.5),\n",
    "    task_with_delay(4, 0.2),  # Completes second\n",
    "]\n",
    "\n",
    "print(\"Processing as tasks complete:\")\n",
    "async with CompletionStream(tasks) as stream:\n",
    "    async for idx, result in stream:\n",
    "        print(f\"  Task {idx} completed: {result}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Limited Concurrency in CompletionStream"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-09T06:15:47.521791Z",
     "iopub.status.busy": "2025-11-09T06:15:47.521591Z",
     "iopub.status.idle": "2025-11-09T06:15:47.732881Z",
     "shell.execute_reply": "2025-11-09T06:15:47.732420Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed 20 tasks with limit=5\n",
      "First 5 completions: [(0, 'Processed_0'), (1, 'Processed_1'), (2, 'Processed_2'), (3, 'Processed_3'), (4, 'Processed_4')]\n"
     ]
    }
   ],
   "source": [
    "# Process many tasks with concurrency limit\n",
    "async def work_item(n: int) -> str:\n",
    "    await sleep(0.05)\n",
    "    return f\"Processed_{n}\"\n",
    "\n",
    "\n",
    "tasks = [work_item(i) for i in range(20)]\n",
    "\n",
    "completed = []\n",
    "async with CompletionStream(tasks, limit=5) as stream:\n",
    "    async for idx, result in stream:\n",
    "        completed.append((idx, result))\n",
    "\n",
    "print(f\"Completed {len(completed)} tasks with limit=5\")\n",
    "print(f\"First 5 completions: {completed[:5]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Early Exit from CompletionStream"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-09T06:15:47.734063Z",
     "iopub.status.busy": "2025-11-09T06:15:47.733992Z",
     "iopub.status.idle": "2025-11-09T06:15:47.837405Z",
     "shell.execute_reply": "2025-11-09T06:15:47.836988Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Searching for target (id=5):\n",
      "  Checked task 0: {'id': 0, 'found': False}\n",
      "  Checked task 1: {'id': 1, 'found': False}\n",
      "  Checked task 2: {'id': 2, 'found': False}\n",
      "  Checked task 3: {'id': 3, 'found': False}\n",
      "  Checked task 4: {'id': 4, 'found': False}\n",
      "  Checked task 5: {'id': 5, 'found': True}\n",
      "  ✓ Found target! Canceling remaining tasks.\n"
     ]
    }
   ],
   "source": [
    "# Stop processing after finding target\n",
    "async def search_task(id: int) -> dict:\n",
    "    await sleep(0.1)\n",
    "    return {\"id\": id, \"found\": id == 5}\n",
    "\n",
    "\n",
    "tasks = [search_task(i) for i in range(20)]\n",
    "\n",
    "print(\"Searching for target (id=5):\")\n",
    "async with CompletionStream(tasks) as stream:\n",
    "    async for idx, result in stream:\n",
    "        print(f\"  Checked task {idx}: {result}\")\n",
    "        if result[\"found\"]:\n",
    "            print(\"  ✓ Found target! Canceling remaining tasks.\")\n",
    "            break  # Remaining tasks automatically canceled on exit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. retry() - Exponential Backoff with Deadline Awareness\n",
    "\n",
    "`retry()` implements exponential backoff retry logic with deadline awareness and jitter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-09T06:15:47.838557Z",
     "iopub.status.busy": "2025-11-09T06:15:47.838493Z",
     "iopub.status.idle": "2025-11-09T06:15:48.155285Z",
     "shell.execute_reply": "2025-11-09T06:15:48.154274Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result: Success!\n",
      "Succeeded after 3 attempts\n"
     ]
    }
   ],
   "source": [
    "# Flaky operation that succeeds on 3rd attempt\n",
    "attempt_count = 0\n",
    "\n",
    "\n",
    "async def flaky_operation() -> str:\n",
    "    global attempt_count\n",
    "    attempt_count += 1\n",
    "\n",
    "    if attempt_count < 3:\n",
    "        raise ConnectionError(f\"Attempt {attempt_count} failed\")\n",
    "\n",
    "    return \"Success!\"\n",
    "\n",
    "\n",
    "# Reset counter\n",
    "attempt_count = 0\n",
    "\n",
    "# Retry with default settings (3 attempts, exponential backoff)\n",
    "result = await retry(flaky_operation)\n",
    "print(f\"Result: {result}\")\n",
    "print(f\"Succeeded after {attempt_count} attempts\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom Retry Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-09T06:15:48.157763Z",
     "iopub.status.busy": "2025-11-09T06:15:48.157486Z",
     "iopub.status.idle": "2025-11-09T06:15:48.563582Z",
     "shell.execute_reply": "2025-11-09T06:15:48.563108Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result: API response\n",
      "Total attempts: 4\n"
     ]
    }
   ],
   "source": [
    "# Configure retry behavior\n",
    "attempt_count = 0\n",
    "delays = []\n",
    "\n",
    "\n",
    "async def unstable_api() -> str:\n",
    "    global attempt_count\n",
    "    attempt_count += 1\n",
    "\n",
    "    if attempt_count < 4:\n",
    "        raise TimeoutError(f\"Timeout on attempt {attempt_count}\")\n",
    "\n",
    "    return \"API response\"\n",
    "\n",
    "\n",
    "# Reset\n",
    "attempt_count = 0\n",
    "\n",
    "# Custom retry parameters\n",
    "result = await retry(\n",
    "    unstable_api,\n",
    "    attempts=5,  # Try up to 5 times\n",
    "    base_delay=0.05,  # Start with 50ms delay\n",
    "    max_delay=0.5,  # Cap at 500ms\n",
    "    jitter=0.2,  # 20% jitter\n",
    "    retry_on=(TimeoutError,),  # Only retry on TimeoutError\n",
    ")\n",
    "\n",
    "print(f\"Result: {result}\")\n",
    "print(f\"Total attempts: {attempt_count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retry Exhaustion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-09T06:15:48.565276Z",
     "iopub.status.busy": "2025-11-09T06:15:48.565173Z",
     "iopub.status.idle": "2025-11-09T06:15:48.602837Z",
     "shell.execute_reply": "2025-11-09T06:15:48.602444Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Retry exhausted, raised: Permanent failure\n"
     ]
    }
   ],
   "source": [
    "# Operation that always fails\n",
    "async def always_fails() -> str:\n",
    "    raise RuntimeError(\"Permanent failure\")\n",
    "\n",
    "\n",
    "# Retry will exhaust and raise original exception\n",
    "try:\n",
    "    result = await retry(\n",
    "        always_fails,\n",
    "        attempts=3,\n",
    "        base_delay=0.01,\n",
    "    )\n",
    "except RuntimeError as e:\n",
    "    print(f\"✓ Retry exhausted, raised: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Selective Retry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-09T06:15:48.603914Z",
     "iopub.status.busy": "2025-11-09T06:15:48.603841Z",
     "iopub.status.idle": "2025-11-09T06:15:48.618641Z",
     "shell.execute_reply": "2025-11-09T06:15:48.618193Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ ValueError not retried, raised immediately: Bad input\n",
      "  Attempts made: 2\n"
     ]
    }
   ],
   "source": [
    "# Only retry specific exceptions\n",
    "async def mixed_errors(attempt: list[int]) -> str:\n",
    "    attempt[0] += 1\n",
    "\n",
    "    if attempt[0] == 1:\n",
    "        raise ConnectionError(\"Network flake\")  # Will retry\n",
    "    elif attempt[0] == 2:\n",
    "        raise ValueError(\"Bad input\")  # Won't retry - propagates immediately\n",
    "\n",
    "    return \"Success\"\n",
    "\n",
    "\n",
    "# Reset attempt counter\n",
    "attempt = [0]\n",
    "\n",
    "try:\n",
    "    result = await retry(\n",
    "        lambda: mixed_errors(attempt),\n",
    "        attempts=5,\n",
    "        base_delay=0.01,\n",
    "        retry_on=(ConnectionError, TimeoutError),  # Only retry these\n",
    "    )\n",
    "except ValueError as e:\n",
    "    print(f\"✓ ValueError not retried, raised immediately: {e}\")\n",
    "    print(f\"  Attempts made: {attempt[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Practical Patterns\n",
    "\n",
    "Combining multiple patterns for real-world scenarios."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pattern: Parallel API Calls with Retry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-09T06:15:48.619957Z",
     "iopub.status.busy": "2025-11-09T06:15:48.619891Z",
     "iopub.status.idle": "2025-11-09T06:15:48.685662Z",
     "shell.execute_reply": "2025-11-09T06:15:48.685191Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Service results:\n",
      "  api_1: {'service': 'api_1', 'data': 'data_from_api_1'} (succeeded on attempt 2)\n",
      "  api_2: {'service': 'api_2', 'data': 'data_from_api_2'} (succeeded on attempt 2)\n",
      "  api_3: {'service': 'api_3', 'data': 'data_from_api_3'} (succeeded on attempt 2)\n"
     ]
    }
   ],
   "source": [
    "# Fetch from multiple services with retry\n",
    "services_attempted = {}\n",
    "\n",
    "\n",
    "async def fetch_from_service(service: str) -> dict:\n",
    "    # Track attempts\n",
    "    services_attempted[service] = services_attempted.get(service, 0) + 1\n",
    "\n",
    "    # Simulate flakiness\n",
    "    if services_attempted[service] < 2:\n",
    "        raise ConnectionError(f\"{service} connection failed\")\n",
    "\n",
    "    await sleep(0.05)\n",
    "    return {\"service\": service, \"data\": f\"data_from_{service}\"}\n",
    "\n",
    "\n",
    "# Clear tracking\n",
    "services_attempted = {}\n",
    "\n",
    "# Gather with retry\n",
    "services = [\"api_1\", \"api_2\", \"api_3\"]\n",
    "results = await gather(\n",
    "    *[retry(lambda s=svc: fetch_from_service(s), attempts=3, base_delay=0.01) for svc in services],\n",
    "    return_exceptions=True,\n",
    ")\n",
    "\n",
    "print(\"Service results:\")\n",
    "for i, r in enumerate(results):\n",
    "    service = services[i]\n",
    "    attempts = services_attempted[service]\n",
    "    if isinstance(r, Exception):\n",
    "        print(f\"  {service}: FAILED after {attempts} attempts - {r}\")\n",
    "    else:\n",
    "        print(f\"  {service}: {r} (succeeded on attempt {attempts})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pattern: Rate-Limited Batch Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-09T06:15:48.687031Z",
     "iopub.status.busy": "2025-11-09T06:15:48.686936Z",
     "iopub.status.idle": "2025-11-09T06:15:48.855209Z",
     "shell.execute_reply": "2025-11-09T06:15:48.854748Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 20 items with limit=5:\n",
      "  Successes: 17\n",
      "  Failures: 3\n"
     ]
    }
   ],
   "source": [
    "# Process items with rate limit and retry\n",
    "processed = []\n",
    "\n",
    "\n",
    "async def process_with_retry(item: int) -> dict:\n",
    "    # Retry wrapper for flaky processing\n",
    "    async def process() -> dict:\n",
    "        await sleep(0.02)\n",
    "        # Simulate occasional failures\n",
    "        if item % 7 == 0 and item not in processed:\n",
    "            raise ConnectionError(f\"Flake on item {item}\")\n",
    "        processed.append(item)\n",
    "        return {\"item\": item, \"status\": \"processed\"}\n",
    "\n",
    "    return await retry(process, attempts=3, base_delay=0.01)\n",
    "\n",
    "\n",
    "# Clear\n",
    "processed = []\n",
    "\n",
    "# Bounded map with retry\n",
    "items = range(20)\n",
    "results = await bounded_map(\n",
    "    process_with_retry,\n",
    "    items,\n",
    "    limit=5,  # Max 5 concurrent\n",
    "    return_exceptions=True,\n",
    ")\n",
    "\n",
    "successes = sum(1 for r in results if not isinstance(r, Exception))\n",
    "failures = sum(1 for r in results if isinstance(r, Exception))\n",
    "\n",
    "print(f\"Processed {len(items)} items with limit=5:\")\n",
    "print(f\"  Successes: {successes}\")\n",
    "print(f\"  Failures: {failures}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pattern: Redundant Requests (race + retry)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-09T06:15:48.856486Z",
     "iopub.status.busy": "2025-11-09T06:15:48.856417Z",
     "iopub.status.idle": "2025-11-09T06:15:48.961820Z",
     "shell.execute_reply": "2025-11-09T06:15:48.961324Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Winner: fast_provider\n",
      "Data: from_fast_provider\n"
     ]
    }
   ],
   "source": [
    "# Race multiple providers with retry\n",
    "async def fetch_from_provider(name: str, base_delay: float) -> dict:\n",
    "    await sleep(base_delay)\n",
    "    return {\"provider\": name, \"data\": f\"from_{name}\"}\n",
    "\n",
    "\n",
    "# Wrap each provider in retry\n",
    "providers = [\n",
    "    retry(lambda: fetch_from_provider(\"fast_provider\", 0.1), attempts=2),\n",
    "    retry(lambda: fetch_from_provider(\"slow_provider\", 0.3), attempts=2),\n",
    "    retry(lambda: fetch_from_provider(\"backup_provider\", 0.2), attempts=2),\n",
    "]\n",
    "\n",
    "# Race them - fastest wins\n",
    "result = await race(*providers)\n",
    "print(f\"Winner: {result['provider']}\")\n",
    "print(f\"Data: {result['data']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary Checklist\n",
    "\n",
    "**Concurrency Patterns:**\n",
    "- ✅ `gather()`: Run awaitables concurrently, collect all results\n",
    "- ✅ `race()`: Return first completion, cancel others\n",
    "- ✅ `bounded_map()`: Apply async function with concurrency limit\n",
    "- ✅ `CompletionStream`: Process results as they complete\n",
    "- ✅ `retry()`: Exponential backoff with deadline awareness\n",
    "\n",
    "**Exception Handling:**\n",
    "- ✅ `return_exceptions=True` collects errors instead of raising\n",
    "- ✅ ExceptionGroup for non-cancel errors in gather/bounded_map\n",
    "- ✅ Selective retry with `retry_on` parameter\n",
    "\n",
    "**Advanced Features:**\n",
    "- ✅ Structured concurrency (proper cleanup on cancellation)\n",
    "- ✅ Deadline awareness in retry\n",
    "- ✅ Rate limiting via concurrency bounds\n",
    "- ✅ Early exit from CompletionStream\n",
    "- ✅ Jitter in exponential backoff\n",
    "\n",
    "**Next Steps:**\n",
    "- See `_primitives` for `CapacityLimiter` and other low-level primitives\n",
    "- See `_cancel` for deadline and cancellation utilities\n",
    "- See `_task` for task group abstractions"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lionherd-core",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

import importlib
import inspect
from pathlib import Path

from airflow.models.dag import DAG
from airflow.models.operator import Operator
from airflow.providers.standard.decorators.python import _PythonDecoratedOperator
from airflow.sdk.definitions.mappedoperator import MappedOperator

from dag_converter.cleanup import get_cleanup_dag
from dag_converter.conversion.dynamic_mapping import handle_dynamic_task_mapping
from dag_converter.conversion.exceptions import InvalidOperatorError
from dag_converter.conversion.operators.python_decorator import handle_python_decorator
from dag_converter.schema_parser import ArgumentValidator
from dag_converter.taskflow_parser import TaskFlowAnalyzer


def convert_tasks(
    taskflow_parser: TaskFlowAnalyzer, dag_object: DAG, dag_file_path: Path, validator: ArgumentValidator
):
    """Convert each task into acceptable DagFactory format"""
    tasks = {}
    task_types = {}

    # First iteration through tasks to map each task id to its corresponding operator type
    for task in dag_object.tasks:
        task_id = task.task_id
        if not task_id:
            continue

        task_types[task_id] = task.task_type

    # Second iteration through tasks to handle the actual conversion
    for task in dag_object.tasks:
        task_id = task.task_id
        if not task_id:
            continue

        # Edge case for DTM, since it stores the actual operator module in '_task_module' instead
        if isinstance(task, MappedOperator):
            if task.operator_class is _PythonDecoratedOperator:
                raise InvalidOperatorError("Operator PythonDecoratedOperator is not supported")

            full_operator_name = f"{task._task_module}.{task.task_type}"
        else:
            full_operator_name = f"{task.__module__}.{task.task_type}"

        tasks[task_id] = {"operator": full_operator_name}
        valid_task_parameters = get_operator_parameters(full_operator_name)

        # Handle TaskFlow
        if isinstance(task, _PythonDecoratedOperator):
            raise InvalidOperatorError("Operator PythonDecoratedOperator is not supported")
            # Convert any python callables into <file_name>.<function_name> format for easier parsing
            tasks[task_id] = get_cleanup_dag(tasks[task_id])
            handle_python_decorator(tasks[task_id], taskflow_parser)

        for key in dir(task):
            value = getattr(task, key, None)
            # Filter out extraneous task parameters generated by DagBag through validation
            if validator.validate_field("task", key, value) and key in valid_task_parameters:
                # Skip DTM keywords, those will be handled in another function
                # TODO: check if op_kwargs_expand_input exists in Dag Object
                if (
                    key in ["op_kwargs_expand_input", "expand_input", "partial_kwargs", "map_index_template"]
                    or value is None
                ):
                    pass
                else:
                    tasks[task_id][key] = value

        # These are the core parameters that Python file has for each operator/sensor.
        # Helps find any additional missing parameters (important)
        if hasattr(task, "_BaseOperator__init_kwargs"):
            parameters = getattr(task, "_BaseOperator__init_kwargs", None)
            for key, value in parameters.items():
                if validator.validate_field("task", key, value) and key in valid_task_parameters and value is not None:
                    tasks[task_id][key] = value

        tasks[task_id]["dependencies"] = list(getattr(task, "upstream_task_ids", []))

        # Handle DTM
        if isinstance(task, MappedOperator):
            handle_dynamic_task_mapping(tasks[task_id], task_types, task, validator)

        # Handle operator-specific parameters that we need to delete
        if "operators.appflow" in full_operator_name and "flow_update" in tasks[task_id]:
            del tasks[task_id]["flow_update"]

        if "sensors.eks" in full_operator_name and "target_state_type" in tasks[task_id]:
            del tasks[task_id]["target_state_type"]

        if "sensors.sagemaker" in full_operator_name and "resource_type" in tasks[task_id]:
            del tasks[task_id]["resource_type"]

        filter_config = [
            "SageMakerStartPipelineOperator",
            "SageMakerStopPipelineOperator",
            "SageMakerCreateExperimentOperator",
        ]
        if task.task_type in filter_config and "config" in tasks[task_id]:
            del tasks[task_id]["config"]

        if task.task_type == "MwaaDagRunSensor":
            if "success_states" in tasks[task_id]:
                del tasks[task_id]["success_states"]
            if "failure_states" in tasks[task_id]:
                del tasks[task_id]["failure_states"]

    return tasks


def get_operator_parameters(operator_class_or_string: type[Operator] | str) -> list[str]:
    """
    Returns a list of all allowed parameters for a given operator.

    Args:
        operator_class_or_string: Either the operator class or a string like
                                 'airflow.providers.amazon.aws.operators.ec2.EC2TerminateInstanceOperator'

    Returns:
        list: List of parameter names that the operator accepts
    """
    # If it's a string, import the class
    if isinstance(operator_class_or_string, str):
        module_path, class_name = operator_class_or_string.rsplit(".", 1)
        try:
            module = importlib.import_module(module_path)
            operator_class = getattr(module, class_name)
        except (ImportError, AttributeError):
            return []
    else:
        operator_class = operator_class_or_string

    all_params = []

    for cls in operator_class.__mro__:
        if hasattr(cls, "__init__"):
            try:
                sig = inspect.signature(cls.__init__)
                params = [name for name in sig.parameters if name not in ["self", "args", "kwargs"]]
                all_params.extend(params)
            except Exception:
                continue

    # Remove duplicates while preserving order
    return list(dict.fromkeys(all_params))

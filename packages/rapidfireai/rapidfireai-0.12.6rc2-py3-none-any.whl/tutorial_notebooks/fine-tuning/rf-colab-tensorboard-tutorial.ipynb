{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tVCZ8hwnWeLi"
   },
   "source": [
    "<div align=\"center\">\n",
    "<a href=\"https://rapidfire.ai/\"><img src=\"https://raw.githubusercontent.com/RapidFireAI/rapidfireai/main/docs/images/RapidFire - Blue bug -white text.svg\" width=\"115\"></a>\n",
    "<a href=\"https://discord.gg/6vSTtncKNN\"><img src=\"https://raw.githubusercontent.com/RapidFireAI/rapidfireai/main/docs/images/discord-button.svg\" width=\"145\"></a>\n",
    "<a href=\"https://oss-docs.rapidfire.ai/\"><img src=\"https://raw.githubusercontent.com/RapidFireAI/rapidfireai/main/docs/images/documentation-button.svg\" width=\"125\"></a>\n",
    "<br/>\n",
    "Join Discord if you need help + ‚≠ê <i>Star us on <a href=\"https://github.com/RapidFireAI/rapidfireai\">GitHub</a></i> ‚≠ê\n",
    "<br/>\n",
    "To install RapidFire AI on your own machine, see the <a href=\"https://oss-docs.rapidfire.ai/en/latest/walkthrough.html\">Install and Get Started</a> guide in our docs.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J3DmTK3ZWeLp"
   },
   "source": [
    "‚ö†Ô∏è **IMPORTANT:** Do not let the Colab notebook tab stay idle for more than 5min; Colab will disconnect otherwise. Refresh the TensorBoard screen or interact with the cells to avoid disconnection."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uHIblXRsAxJh"
   },
   "source": [
    "# RapidFire AI in Google Colab with TensorBoard\n",
    "\n",
    "This tutorial demonstrates how to use RapidFire AI in Google Colab with in-built TensorBoard for real-time metrics visualization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V7rkXU_0AxJj"
   },
   "source": [
    "## Start RapidFire Services in Colab Mode\n",
    "\n",
    "RapidFire requires the API Server to manage experiment state. Open the Colab terminal (Tools > Command palette > Terminal) and run:\n",
    "\n",
    "```bash\n",
    "pip install rapidfireai==0.11.1 # Takes 1 min\n",
    "rapidfireai init # Takes 1 min\n",
    "rapidfireai start & # Takes 0.5 min\n",
    "```\n",
    "\n",
    "You should see output like:\n",
    "```\n",
    "üì¶ RapidFire AI Initializing...\n",
    "‚úÖ [1/1] Dispatcher server started\n",
    "üöÄ RapidFire running in Colab mode!\n",
    "üìä Use TensorBoard for metrics visualization:\n",
    "   %tensorboard --logdir ~/experiments/{experiment_name}/tensorboard_logs\n",
    "```\n",
    "\n",
    "**IMPORTANT: Leave this terminal running while you work in your notebook!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4v66VLdtAxJj"
   },
   "source": [
    "## Configure RapidFire to Use TensorBoard\n",
    "\n",
    "We'll set environment variables to tell RapidFire to use TensorBoard instead of MLflow:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wbo1EcUmAxJj"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Load TensorBoard extension\n",
    "%load_ext tensorboard\n",
    "\n",
    "# Configure RapidFire to use TensorBoard\n",
    "os.environ['RF_TRACKING_BACKEND'] = 'tensorboard'  # Options: 'mlflow', 'tensorboard', 'both'\n",
    "# TensorBoard log directory will be auto-created in experiment path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x0Xdz9eZFOGh"
   },
   "source": [
    "## Configure Hugging Face token\n",
    "\n",
    "Install huggingface-hub and provide your HF token in place of YOUR-TOKEN-HERE.\n",
    "\n",
    "**IMPORTANT: Hugging Face does not allow us to provide a public HF token. You need to sign up for a Hugging Face account and obtain a token.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aEGBEgyFFM9X"
   },
   "outputs": [],
   "source": [
    "!pip install \"huggingface-hub[cli]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jVbdruEiFaYv"
   },
   "outputs": [],
   "source": [
    "!hf auth login --token YOUR-TOKEN-HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pgK5ZQFnAxJj"
   },
   "source": [
    "## Import RapidFire Components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OAde0aIfAxJk"
   },
   "outputs": [],
   "source": [
    "from rapidfireai import Experiment\n",
    "from rapidfireai.fit.automl import List, RFGridSearch, RFModelConfig, RFLoraConfig, RFSFTConfig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "adszyLwxAxJk"
   },
   "source": [
    "## Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Vil1zbTeAxJk"
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"bitext/Bitext-customer-support-llm-chatbot-training-dataset\")\n",
    "\n",
    "# REDUCED dataset for memory constraints in Colab\n",
    "train_dataset = dataset[\"train\"].select(range(64))  # Reduced from 128\n",
    "eval_dataset = dataset[\"train\"].select(range(50, 60))  # 10 examples\n",
    "train_dataset = train_dataset.shuffle(seed=42)\n",
    "eval_dataset = eval_dataset.shuffle(seed=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i3iorhjgAxJk"
   },
   "source": [
    "## Define Data Processing Function\n",
    "\n",
    "We'll format the data as Q&A pairs for GPT-2:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Gpnc3duXAxJk"
   },
   "outputs": [],
   "source": [
    "def sample_formatting_function(example):\n",
    "    \"\"\"Format the dataset for GPT-2 while preserving original fields\"\"\"\n",
    "    return {\n",
    "        \"text\": f\"Question: {example['instruction']}\\nAnswer: {example['response']}\",\n",
    "        \"instruction\": example['instruction'],  # Keep original\n",
    "        \"response\": example['response']  # Keep original\n",
    "    }\n",
    "\n",
    "# Apply formatting to datasets\n",
    "eval_dataset = eval_dataset.map(sample_formatting_function)\n",
    "train_dataset = train_dataset.map(sample_formatting_function)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "--lWb0qnAxJk"
   },
   "source": [
    "## Define Metrics Function\n",
    "\n",
    "We'll use a lightweight metrics computation with just ROUGE-L to save memory:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_Gqa6JduAxJk"
   },
   "outputs": [],
   "source": [
    "def sample_compute_metrics(eval_preds):\n",
    "    \"\"\"Lightweight metrics computation\"\"\"\n",
    "    predictions, labels = eval_preds\n",
    "\n",
    "    try:\n",
    "        import evaluate\n",
    "\n",
    "        # Only compute ROUGE-L (skip BLEU to save memory)\n",
    "        rouge = evaluate.load(\"rouge\")\n",
    "        rouge_output = rouge.compute(\n",
    "            predictions=predictions,\n",
    "            references=labels,\n",
    "            use_stemmer=True,\n",
    "            rouge_types=[\"rougeL\"]  # Only compute rougeL\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            \"rougeL\": round(rouge_output[\"rougeL\"], 4),\n",
    "        }\n",
    "    except Exception as e:\n",
    "        # Fallback if metrics fail\n",
    "        print(f\"Metrics computation failed: {e}\")\n",
    "        return {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0zW2g7CJAxJk"
   },
   "source": [
    "## Initialize Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZQ6mbRK6AxJl"
   },
   "outputs": [],
   "source": [
    "# Create experiment with unique name\n",
    "my_experiment = \"tensorboard-demo-1\"\n",
    "experiment = Experiment(experiment_name=my_experiment)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gAYPi61cAxJl"
   },
   "source": [
    "## Get TensorBoard Log Directory\n",
    "\n",
    "The TensorBoard logs are stored in the experiment directory. Let's get the path:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9QPR4y-YAxJl"
   },
   "outputs": [],
   "source": [
    "# Get experiment path\n",
    "from rapidfireai.fit.db.rf_db import RfDb\n",
    "\n",
    "db = RfDb()\n",
    "experiment_path = db.get_experiments_path(my_experiment)\n",
    "tensorboard_log_dir = f\"{experiment_path}/{my_experiment}/tensorboard_logs\"\n",
    "\n",
    "print(f\"TensorBoard logs will be saved to: {tensorboard_log_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pALJJyZcAxJl"
   },
   "source": [
    "## Define Model Configurations\n",
    "\n",
    "This tutorial showcases GPT-2 (124M parameters), which is perfect for Colab's memory constraints:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "shEQVD7kAxJl"
   },
   "outputs": [],
   "source": [
    "# GPT-2 specific LoRA configs - different module names!\n",
    "peft_configs_lite = List([\n",
    "    RFLoraConfig(\n",
    "        r=8,\n",
    "        lora_alpha=16,\n",
    "        lora_dropout=0.1,\n",
    "        target_modules=[\"c_attn\"],  # GPT-2 combines Q,K,V in c_attn\n",
    "        bias=\"none\"\n",
    "    ),\n",
    "    RFLoraConfig(\n",
    "        r=32,\n",
    "        lora_alpha=64,\n",
    "        lora_dropout=0.1,\n",
    "        target_modules=[\"c_attn\", \"c_proj\"],  # c_attn (QKV) + c_proj (output)\n",
    "        bias=\"none\"\n",
    "    )\n",
    "])\n",
    "\n",
    "# 2 configs with GPT-2\n",
    "config_set_lite = List([\n",
    "    RFModelConfig(\n",
    "        model_name=\"gpt2\",  # Only 124M params\n",
    "        peft_config=peft_configs_lite,\n",
    "        training_args=RFSFTConfig(\n",
    "            learning_rate=5e-4,  # Low lr for more stability\n",
    "            lr_scheduler_type=\"linear\",\n",
    "            per_device_train_batch_size=2,\n",
    "            gradient_accumulation_steps=2,  # Effective bs = 4\n",
    "            max_steps=64, # Raise this to see more learning\n",
    "            logging_steps=2,\n",
    "            eval_strategy=\"steps\",\n",
    "            eval_steps=4,\n",
    "            per_device_eval_batch_size=2,\n",
    "            fp16=True,\n",
    "            gradient_checkpointing=True,  # Save memory\n",
    "            report_to=\"none\",  # Disables wandb\n",
    "        ),\n",
    "        model_type=\"causal_lm\",\n",
    "        model_kwargs={\n",
    "            \"device_map\": \"auto\",\n",
    "            \"torch_dtype\": \"float16\",  # Explicit fp16\n",
    "            \"use_cache\": False\n",
    "        },\n",
    "        formatting_func=sample_formatting_function,\n",
    "        compute_metrics=sample_compute_metrics,\n",
    "        generation_config={\n",
    "            \"max_new_tokens\": 128,  # Reduced from 256\n",
    "            \"temperature\": 0.7,\n",
    "            \"top_p\": 0.9,\n",
    "            \"top_k\": 40,\n",
    "            \"repetition_penalty\": 1.1,\n",
    "            \"pad_token_id\": 50256,  # GPT-2's EOS token\n",
    "        }\n",
    "    ),\n",
    "    RFModelConfig(\n",
    "        model_name=\"gpt2\",\n",
    "        peft_config=peft_configs_lite,\n",
    "        training_args=RFSFTConfig(\n",
    "            learning_rate=2e-4,  # Even more conservative\n",
    "            lr_scheduler_type=\"cosine\",  # Try cosine schedule\n",
    "            per_device_train_batch_size=2,\n",
    "            gradient_accumulation_steps=2,\n",
    "            max_steps=64, # Raise this to see more learning behviors\n",
    "            logging_steps=2,\n",
    "            eval_strategy=\"steps\",\n",
    "            eval_steps=4,\n",
    "            per_device_eval_batch_size=2,\n",
    "            fp16=True,\n",
    "            gradient_checkpointing=True,\n",
    "            report_to=\"none\",  # Disables wandb\n",
    "            warmup_steps=10,  # Add warmup for stability\n",
    "        ),\n",
    "        model_type=\"causal_lm\",\n",
    "        model_kwargs={\n",
    "            \"device_map\": \"auto\",\n",
    "            \"torch_dtype\": \"float16\",\n",
    "            \"use_cache\": False\n",
    "        },\n",
    "        formatting_func=sample_formatting_function,\n",
    "        compute_metrics=sample_compute_metrics,\n",
    "        generation_config={\n",
    "            \"max_new_tokens\": 128,\n",
    "            \"temperature\": 0.7,\n",
    "            \"top_p\": 0.9,\n",
    "            \"top_k\": 40,\n",
    "            \"repetition_penalty\": 1.1,\n",
    "            \"pad_token_id\": 50256,\n",
    "        }\n",
    "    )\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wuo9B8WrAxJl"
   },
   "outputs": [],
   "source": [
    "def sample_create_model(model_config):\n",
    "    \"\"\"Function to create model object with GPT-2 adjustments\"\"\"\n",
    "    from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "    model_name = model_config[\"model_name\"]\n",
    "    model_type = model_config[\"model_type\"]\n",
    "    model_kwargs = model_config[\"model_kwargs\"]\n",
    "\n",
    "    if model_type == \"causal_lm\":\n",
    "        model = AutoModelForCausalLM.from_pretrained(model_name, **model_kwargs)\n",
    "    else:\n",
    "        # Default to causal LM\n",
    "        model = AutoModelForCausalLM.from_pretrained(model_name, **model_kwargs)\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "    # GPT-2 specific: Set pad token (GPT-2 doesn't have one by default)\n",
    "    if \"gpt2\" in model_name.lower():\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "        tokenizer.padding_side = \"left\"  # GPT-2 works better with left padding\n",
    "        model.config.pad_token_id = model.config.eos_token_id\n",
    "\n",
    "    return (model, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "v7NOeq0PAxJl"
   },
   "outputs": [],
   "source": [
    "# Simple grid search across all config combinations: 4 total (2 LoRA configs √ó 2 trainer configs)\n",
    "config_group = RFGridSearch(\n",
    "    configs=config_set_lite,\n",
    "    trainer_type=\"SFT\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KrFU-wjkAxJm"
   },
   "source": [
    "## Launch Interactive Run Controller\n",
    "\n",
    "RapidFire AI provides an Interactive Controller that lets you manage executing runs dynamically in real-time from the notebook:\n",
    "\n",
    "- ‚èπÔ∏è **Stop**: Gracefully stop a running config\n",
    "- ‚ñ∂Ô∏è **Resume**: Resume a stopped run\n",
    "- üóëÔ∏è **Delete**: Remove a run from this experiment\n",
    "- üìã **Clone**: Create a new run by editing the config dictionary of a parent run to try new knob values; optional warm start of parameters\n",
    "- üîÑ **Refresh**: Update run status and metrics\n",
    "\n",
    "The Controller uses ipywidgets and is compatible with both Colab (ipywidgets 7.x) and Jupyter (ipywidgets 8.x)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HNc_MLbGL8xC"
   },
   "outputs": [],
   "source": [
    "# Create Interactive Controller\n",
    "from rapidfireai.fit.utils.interactive_controller import InteractiveController\n",
    "\n",
    "controller = InteractiveController(dispatcher_url=\"http://127.0.0.1:8851\")\n",
    "controller.display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wJfXimPpAxJl"
   },
   "source": [
    "## Start TensorBoard\n",
    "\n",
    "**IMPORTANT: Make sure to start TensorBoard BEFORE invoking run_fit() below so that you can watch metrics appear in real-time!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qVVWU42vKBTN"
   },
   "outputs": [],
   "source": [
    "%tensorboard --logdir {tensorboard_log_dir}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oGOs_rZYAxJm"
   },
   "source": [
    "## Run Training + Validation\n",
    "\n",
    "Now we get to the main function for running multi-config training and evals. The metrics will appear in TensorBoard above in real-time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AykcHp33AxJm"
   },
   "outputs": [],
   "source": [
    "# Launch training\n",
    "experiment.run_fit(\n",
    "    config_group,\n",
    "    sample_create_model,\n",
    "    train_dataset,\n",
    "    eval_dataset,\n",
    "    num_chunks=4,  # 4 chunks for hyperparallel execution\n",
    "    seed=42\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ujpATTaRAxJm"
   },
   "source": [
    "## End Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uOTTI-rVAxJm"
   },
   "outputs": [],
   "source": [
    "experiment.end()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wuiwNvldAxJm"
   },
   "source": [
    "## View TensorBoard Plots and Logs\n",
    "\n",
    "After your experiment is ended, you can still view the full logs in TensorBoard:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rvbsKwE2AxJm"
   },
   "outputs": [],
   "source": [
    "# View final logs\n",
    "%tensorboard --logdir {tensorboard_log_dir}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mXeBD1QjWeLv"
   },
   "source": [
    "# View RapidFire AI Log Files\n",
    "\n",
    "You can track the work being done by the system via the RapidFire AI-produced log files in rapidfire_experiments/ folder. To see the log files, open the Colab terminal and run the commands:\n",
    "\n",
    "```bash\n",
    "tail -n 20 rapidfire_experiments/rapidfire.log\n",
    "tail -n 20 rapidfire_experiments/training.log\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

from jwstnoobfriend.navigation import FileBox
from jwstnoobfriend.utils.environment import load_environment # To load environment variables
from jwstnoobfriend.utils.display import console # Make sure you are use the same console instance.
import os # To access environment variables

## In case you need to do parallel processing, but it is not necessary because the bottleneck is not CPU-bound tasks.
from concurrent.futures import ProcessPoolExecutor

import logging
{% if mute_console %}
## To mute noisy logging messages from the JWST pipeline
{% for logger_name in mute_logger_list %}

{{ logger_name }}_logger = logging.getLogger('{{ logger_name }}')

for handler in {{ logger_name }}_logger.handlers[:]:
    {{ logger_name }}_logger.removeHandler(handler)

{{ logger_name }}_logger.propagate = False
{% endfor %}
### The logger name may change in the future JWST pipeline versions.
### You can always check the logger names and mute them accordingly.
{% endif %}

load_environment()
filebox = FileBox.load()

## Load toml configuration file
import tomllib
with open("{{config_path}}", "rb") as f:
    config = tomllib.load(f)

## Reduction process
from jwst.pipeline import Detector1Pipeline

def reduce_info(info):
    {% if log_record %}
    log_filename = f"./.log/1b_2a/{info.basename}.log"
    file_handler = logging.FileHandler(log_filename, mode='w')
    file_handler.setLevel(logging.INFO)
    formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')
    file_handler.setFormatter(formatter)

    {% for logger_name in log_record_logger_list %}
    {{ logger_name }}_logger.addHandler(file_handler)
    {{ logger_name }}_logger.setLevel(logging.INFO)
    {% endfor %}
    {% endif %}

    try:
        ## Run the pipeline with the config loaded from the toml file
        result = Detector1Pipeline.call(info['1b'].filepath, **config)

        logging.getLogger('processing').addHandler(file_handler)
        logging.getLogger('processing').info(f"Successfully processed {info.basename}")
    except Exception as e:
        logging.getLogger('processing').addHandler(file_handler)
        logging.getLogger('processing').error(f"Error processing {info.basename}: {str(e)}")
        raise
    finally:
        {% for logger_name in log_record_logger_list %}
        {{ logger_name }}_logger.removeHandler(file_handler)
        {% endfor %}
        file_handler.close()
    return True 

def main():
    {% for logger_name in log_record_logger_list %}
    {{ logger_name }}_logger.propagate = False
    {% endfor %}

    console.print("[bold green]Starting reduction process...[/bold green]")
    console.print(f"Using {{ process_count }} processes")
    console.print(f"Configuration loaded from: {{ config_path }}")

    with ProcessPoolExecutor(max_workers={{ process_count }}) as executor:
        futures = [executor.submit(reduce_info, info) for info in filebox.info_list]
        
        successful = 0
        failed = 0

        for i, future in enumerate(futures):
            try:
                future.result()
                successful += 1
                console.print(f"[green] ({i+1}/{len(filebox.info_list)}) Successfully processed: {filebox.info_list[i].basename}[/green]")
            except Exception as e:
                failed += 1
                console.print(f"[red] ({i+1}/{len(filebox.info_list)}) Error processing {filebox.info_list[i].basename}: {str(e)}[/red]")

    console.print(f"[bold green]Reduction process completed![/bold green]")
    console.print(f"[bold green]Successful: {successful}[/bold green]")
    console.print(f"[bold red]Failed: {failed}[/bold red]")

if __name__ == "__main__":
    main()
from abc import ABC, abstractmethod
from selenium import webdriver
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.chrome.service import Service as ChromeService
from webdriver_manager.chrome import ChromeDriverManager
from selenium.webdriver.support.ui import WebDriverWait
from datetime import datetime, timedelta, timezone
import json
import os

class Crawler(ABC):
    def __init__(self, keyword, site, max_pages=5, count=20):
        self.keyword = keyword
        self.search_datetime = ""
        self.max_pages = max_pages
        self.count = count
        self.site = site
        self.driver = None
        self.wait = None
        self.article_links = [ ]
        self.results = []

    # íŒ¨í‚¤ì§€ ë“œë¼ì´ë²„ ì„¸íŒ… 
    def setup_driver(self):
        options = Options()
        options.add_argument('--headless')  # UI ìˆ¨ê¸°ê¸° ì˜µì…˜ ì œê±° 
        options.add_argument('--no-sandbox')
        # options.add_argument('--disable-dev-shm-usage')
        self.driver = webdriver.Chrome(service=ChromeService(ChromeDriverManager().install()),options=options)
        self.wait = WebDriverWait(self.driver, 10)

        # í˜„ì¬ ì‹œê°ì„ ê²€ìƒ‰ ì‹œê°„ìœ¼ë¡œ ì„¤ì •
        KST = timezone(timedelta(hours=9))
        kst_now = datetime.now(KST)
        self.search_datetime = kst_now.strftime("%Y-%m-%d %H:%M:%S")
        print(f"âœ… {self.site} WebDriver ì´ˆê¸°í™” ì™„ë£Œ {self.search_datetime}")

    @abstractmethod
    def collect_article_links(self):
        pass

    @abstractmethod
    def extract_article_data(self, url, index, page):
        pass

    # ê¸°ì‚¬ ìˆ˜ì§‘ í•¨ìˆ˜
    def collect_articles(self):
        for link_dict in self.article_links:
            for idx, url in enumerate(link_dict['links']):
                article = self.extract_article_data(url=url, index=idx, page=link_dict['page'])
                if article != None and len(article.content) > 300: 
                    self.results.append(article)

    # ìˆ˜ì§‘í•œ ë°ì´í„° ì €ì¥í•˜ê¸°
    def save_to_file(self, save_path=None, file_name="result.json"):
        if self.results == []:
            print("ìˆ˜ì§‘ëœ ê¸°ì‚¬ ì—†ìŠµë‹ˆë‹¤.")
            return
        json_data = [article.to_dict() for article in self.results]
        # ğŸŸ¢ ì €ì¥ ê²½ë¡œë¥¼ ì‹¤í–‰ íŒŒì¼ ìœ„ì¹˜ ê¸°ì¤€ìœ¼ë¡œ ê³ ì •: FAIRNESS/data/dumy/
        if save_path is None:
            current_dir = os.path.dirname(os.path.abspath(__file__))  # í˜„ì¬ íŒŒì¼ ìœ„ì¹˜: e.g. FAIRNESS/data/crawler/
            base_dir = os.path.abspath(os.path.join(current_dir, ".."))  # í•œ ë‹¨ê³„ ìœ„ë¡œ: FAIRNESS/data/
            save_path = os.path.join(base_dir, "dumy")  # -> FAIRNESS/data/dumy/
        # ë””ë ‰í† ë¦¬ê°€ ì—†ìœ¼ë©´ ìƒì„±
        os.makedirs(save_path, exist_ok=True)
        full_path = os.path.join(save_path, f"{self.site}_{file_name}")

        with open(full_path, "w", encoding="utf-8") as f:
            json.dump(json_data, f, ensure_ascii=False, indent=2)
        print(f"âœ… {len(json_data)}ê°œ ê¸°ì‚¬ íŒŒì¼ ì €ì¥ ì™„ë£Œ: {full_path}")

    # í¬ë¡¤ë§ ì‹¤í–‰  
    def run(self):
        # ë“œë¼ì´ë²„ ì´ˆê¸° ì„¸íŒ…
        self.setup_driver()
        # ê¸°ì‚¬ ë§í¬ ìˆ˜ì§‘
        self.collect_article_links()
        # ê¸°ì‚¬ ìˆ˜ì§‘
        self.collect_articles()
        # ê¸°ì‚¬ ì¢…ë£Œ
        self.driver.close()
        print(f"ğŸ§¹ {self.site} WebDriver ì¢…ë£Œ ë° ì‘ì—… ì™„ë£Œ ! ì´ ìˆ˜ì§‘ëœ ê¸°ì‚¬ : {len(self.results)}")



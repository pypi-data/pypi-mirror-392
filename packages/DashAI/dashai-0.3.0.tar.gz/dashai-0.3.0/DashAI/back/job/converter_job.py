import logging
from pathlib import Path
from typing import List

import pyarrow as pa
from datasets.arrow_dataset import update_metadata_with_features
from datasets.features import Features
from kink import inject
from sqlalchemy import exc
from sqlalchemy.orm import sessionmaker

from DashAI.back.api.api_v1.schemas.converter_params import ConverterParams
from DashAI.back.dataloaders.classes.dashai_dataset import (
    DashAIDataset,
    load_dataset,
    save_dataset,
)
from DashAI.back.dependencies.database.models import ConverterList
from DashAI.back.dependencies.database.models import Dataset as DatasetModel
from DashAI.back.job.base_job import BaseJob, JobError

logging.basicConfig(level=logging.DEBUG)
log = logging.getLogger(__name__)


def _rebuild_dataset_with_transformed_columns(
    base: DashAIDataset,
    transformed: DashAIDataset,
    scope_column_names: List[str],
    scope_column_indexes: List[int],
) -> DashAIDataset:
    """
    Replaces specific columns in the base dataset with columns from the transformed
    dataset, preserving their original positions. Also appends any additional columns
    that were generated by the transformer at the end. Keeps the features and metadata
    consistent.

    Parameters
    ----------
    base : DashAIDataset
        The original dataset before transformation.

    transformed : DashAIDataset
        The dataset resulting from applying a transformer, containing updated and/or
        new columns.

    scope_column_names : List[str]
        Names of the columns that were originally selected for transformation.

    scope_column_indexes : List[int]
        The indices of the columns in the base dataset that were replaced.
        Must match the order of scope_column_names.

    Returns
    -------
    DashAIDataset
        A new dataset with the specified columns replaced in-place, new columns
        appended, and original metadata and split information preserved.
    """

    original_columns = base.column_names
    original_without_scope = base.remove_columns(scope_column_names)

    transformed_cols = transformed.column_names

    index_to_replacement = dict(zip(scope_column_indexes, scope_column_names))
    index_to_replacement = {
        key: value
        for key, value in index_to_replacement.items()
        if value in transformed_cols and value in original_columns
    }
    new_cols = [col for col in transformed_cols if col not in scope_column_names]

    new_columns_order = []
    for i, col in enumerate(original_columns):
        if i in index_to_replacement:
            new_columns_order.append(index_to_replacement[i])
        else:
            new_columns_order.append(col)
    new_columns_order.extend(new_cols)

    original_table = original_without_scope.arrow_table
    transformed_table = transformed.arrow_table

    new_arrays = []
    final_names = new_columns_order.copy()
    for col in new_columns_order:
        if col in original_table.column_names:
            new_arrays.append(original_table[col])
        elif col in transformed_table.column_names:
            new_arrays.append(transformed_table[col])
        else:
            final_names.remove(col)
    new_columns_order = final_names

    new_table = pa.Table.from_arrays(new_arrays, names=new_columns_order)
    new_dataset = DashAIDataset(new_table, splits=base.splits)

    features = base.features.copy()
    features.update(
        {
            col: transformed.features[col]
            for col in transformed.column_names
            if col in new_columns_order and col in transformed.features
        }
    )
    new_dataset._info.features = Features(
        {col: features[col] for col in new_columns_order if col in features}
    )

    new_dataset._data = update_metadata_with_features(
        new_dataset._data, new_dataset.features
    )

    return new_dataset


class ConverterListJob(BaseJob):
    """ConverterListJob class to modify a dataset by applying a
    sequence of converters."""

    @inject
    def set_status_as_delivered(
        self, session_factory: sessionmaker = lambda di: di["session_factory"]
    ) -> None:
        """Set the status of the list as delivered."""
        converter_list_id = self.kwargs["converter_list_id"]

        with session_factory() as db:
            converter_list = db.get(ConverterList, converter_list_id)
            if converter_list is None:
                raise JobError(
                    f"Converter list with id {converter_list_id} does not exist in DB."
                )

            try:
                converter_list.set_status_as_delivered()
                db.commit()
            except exc.SQLAlchemyError as e:
                log.exception(e)
                raise JobError(
                    "Error setting converter list status as delivered"
                ) from e

    @inject
    def set_status_as_error(
        self, session_factory: sessionmaker = lambda di: di["session_factory"]
    ) -> None:
        """Set the status of the converter list as error."""
        converter_list_id = self.kwargs.get("converter_list_id")
        if converter_list_id is None:
            return

        with session_factory() as db:
            converter_list = db.get(ConverterList, converter_list_id)
            if converter_list is None:
                return

            try:
                converter_list.set_status_as_error()
                db.commit()
            except exc.SQLAlchemyError as e:
                log.exception(e)

    @inject
    def get_job_name(self) -> str:
        """Get a descriptive name for the job."""
        converter_list_id = self.kwargs.get("converter_list_id")
        if not converter_list_id:
            return "Converter Job"

        from kink import di

        session_factory = di["session_factory"]

        try:
            with session_factory() as db:
                converter_list = db.get(ConverterList, converter_list_id)
                if not converter_list:
                    return f"Converter Job #{converter_list_id}"
                converter_name = converter_list.converter

                if hasattr(converter_list, "notebook") and converter_list.notebook:
                    dataset = db.get(DatasetModel, converter_list.notebook.dataset_id)
                    if dataset and dataset.name:
                        return f"{converter_name}: {dataset.name}"

                return f"{converter_name}"
        except Exception as e:
            log.exception(f"Error getting job name: {e}")

        return f"Converter Job #{converter_list_id}"

    @inject
    def run(
        self,
    ) -> None:
        from kink import di

        session_factory = di["session_factory"]
        component_registry = di["component_registry"]

        def instantiate_converters(
            converter_name: str,
            converter_params: ConverterParams,
        ) -> object:
            # Import the converter
            try:
                converter_constructor = component_registry[converter_name]["class"]
            except KeyError as e:
                log.exception(e)
                raise JobError(
                    f"Error importing converter {converter_name}: {e}"
                ) from e

            # Get parameters or empty dict if none
            converter_parameters = converter_params.get("params", {})

            return converter_constructor(**converter_parameters)

        # Extract job parameters
        converter_list_id = self.kwargs["converter_list_id"]
        with session_factory() as db:
            # Validate input parameters
            try:
                if converter_list_id is None:
                    raise JobError("Converter list ID is required")

                converter_list: ConverterList = db.get(ConverterList, converter_list_id)
                if not converter_list:
                    raise JobError(
                        f"Converter list with id {converter_list_id} not found"
                    )

                converter_list.set_status_as_started()
                db.commit()
            except exc.SQLAlchemyError as e:
                log.exception(e)
                raise JobError("Error loading converter list info") from e

            # Get dataset
            try:
                dataset_id = converter_list.notebook.dataset_id
                dataset = db.get(DatasetModel, dataset_id)

                # dataset to edit
                dataset_path = f"{converter_list.notebook.file_path}/dataset"
                loaded_dataset = load_dataset(dataset_path)
                print("Pre target column")
                params = converter_list.parameters or {}
                target_column_index = (
                    params["target"].get("idx")
                    if params.get("target") is not None
                    else None
                )
                print(target_column_index)

                if not loaded_dataset:
                    raise JobError(f"Dataset with path {dataset_path} not found")

            except exc.SQLAlchemyError as e:
                log.exception(e)
                converter_list.set_status_as_error()
                db.commit()
                raise JobError("Error loading dataset info") from e

            # Load dataset
            try:
                # Validate target column index
                if target_column_index is not None and (
                    int(target_column_index) < 1
                    or int(target_column_index) > len(loaded_dataset.features)
                ):
                    raise JobError(
                        f"Target column index {target_column_index} is out of bounds"
                    )
            except Exception as e:
                log.exception(e)
                converter_list.set_status_as_error()
                db.commit()
                raise JobError(f"Cannot load dataset from {dataset_path}") from e

            try:
                # Get the absolute path to the converters directory
                current_file = Path(__file__)
                project_root = (
                    current_file.parent.parent.parent
                )  # Go up three levels to reach project root
                converters_base_path = project_root / "back" / "converters"

                if not converters_base_path.exists():
                    raise JobError(
                        f"Converters directory not found at {converters_base_path}"
                    )

                # Get stored converter configurations
                converters_stored_info = {
                    converter_list.converter: converter_list.parameters
                }
                dataset_original_columns = loaded_dataset.column_names

                # Sort converters by order
                converters_sorted_list = sorted(
                    converters_stored_info.items(), key=lambda x: x[1]["order"]
                )

                i = 0
                converter_instances = []

                while i < len(converters_sorted_list):
                    converter_name = converters_sorted_list[i][0]
                    converter_params = converters_sorted_list[i][1]
                    # Regular converter
                    converter_instance = instantiate_converters(
                        converter_name,
                        converter_params,
                    )

                    # Get scope or use default
                    scope = converter_params.get("scope", {"columns": [], "rows": []})

                    # Add to instances
                    converter_instances.append(
                        {
                            "name": converter_name,
                            "instance": converter_instance,
                            "scope": scope,
                        }
                    )
                    i += 1

                # Apply each converter in sequence
                for converter_info in converter_instances:
                    converter = converter_info["instance"]
                    converter_scope = converter_info["scope"]

                    # Process columns scope
                    columns_scope = [
                        column["idx"] - 1 for column in converter_scope["columns"]
                    ]
                    scope_column_indexes = sorted(set(columns_scope))

                    # If no columns specified, use all columns
                    if not scope_column_indexes:
                        scope_column_indexes = list(range(len(loaded_dataset.features)))

                    scope_column_names = [
                        dataset_original_columns[index]
                        for index in scope_column_indexes
                    ]

                    # Process rows scope
                    rows_scope = [row - 1 for row in converter_scope["rows"]]
                    scope_rows_indexes = sorted(set(rows_scope))

                    # Adjust target column index (0-based internally)
                    y_dataset_fit = None
                    target_column_name = None
                    y_full_transform = None
                    if target_column_index is not None:
                        target_column_index_0based = int(target_column_index) - 1
                        target_column_name = dataset_original_columns[
                            target_column_index_0based
                        ]
                        y_dataset_fit = loaded_dataset.select_columns(
                            [target_column_name]
                        )
                        if scope_rows_indexes:
                            y_dataset_fit = y_dataset_fit.select(scope_rows_indexes)

                        y_full_transform = loaded_dataset.select_columns(
                            [target_column_name]
                        )

                    # Select data for fitting using DashAIDataset operations
                    X_dataset_fit = loaded_dataset.select_columns(scope_column_names)

                    # Select specified rows if provided
                    if scope_rows_indexes:
                        X_dataset_fit = X_dataset_fit.select(scope_rows_indexes)

                    try:
                        converter = converter.fit(X_dataset_fit, y_dataset_fit)
                    except Exception as e:
                        log.exception(e)
                        raise JobError(
                            f"Error fitting converter {converter_name}: {e}"
                        ) from e

                    # Transform data using full dataset for selected columns
                    # Samplers will ignore x_full and y_full, and use internally stored
                    # resampled data.
                    X_full_transform = loaded_dataset.select_columns(scope_column_names)

                    try:
                        transformed_dataset = converter.transform(
                            X_full_transform, y_full_transform
                        )
                    except Exception as e:
                        log.exception(e)
                        raise JobError(
                            f"Error transforming data with {converter_name}: {e}"
                        ) from e

                    if converter.changes_row_count():
                        loaded_dataset = transformed_dataset
                    else:
                        # dataset, preserving their original positions
                        loaded_dataset = _rebuild_dataset_with_transformed_columns(
                            loaded_dataset,
                            transformed_dataset,
                            scope_column_names,
                            scope_column_indexes,
                        )

                dataset_original_columns = loaded_dataset.column_names
                log.info(
                    f"Dataset after {converter_name}: Shape {loaded_dataset.shape}, "
                    f"Columns: {loaded_dataset.column_names}"
                )
                # Save the final dataset
                save_dataset(loaded_dataset, f"{dataset_path}")
                converter_list.set_status_as_finished()
                db.commit()
                db.refresh(dataset)

            except Exception as e:
                log.exception(e)
                converter_list.set_status_as_error()
                db.commit()
                raise JobError(
                    f"Error applying converters to dataset {dataset_id}: {e}"
                ) from e

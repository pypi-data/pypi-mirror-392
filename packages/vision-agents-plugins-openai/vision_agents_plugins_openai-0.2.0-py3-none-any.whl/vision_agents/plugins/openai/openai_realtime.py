import json
from typing import Any, Optional, List, Dict, Union

import aiortc
from openai import AsyncOpenAI
from openai.types.realtime import (
    RateLimitsUpdatedEvent,
    SessionUpdatedEvent,
    RealtimeSessionCreateRequestParam,
    RealtimeAudioConfigParam,
    RealtimeAudioConfigOutputParam,
    RealtimeAudioConfigInputParam,
    AudioTranscriptionParam,
    ResponseAudioTranscriptDoneEvent,
    InputAudioBufferSpeechStartedEvent,
    ConversationItemInputAudioTranscriptionCompletedEvent,
    ResponseDoneEvent,
    SessionCreatedEvent,
    RealtimeSessionCreateRequest,
    RealtimeTranscriptionSessionCreateRequest,
)
from openai.types.realtime.realtime_transcription_session_audio_input_turn_detection_param import (
    SemanticVad,
)


from vision_agents.core.llm import realtime
from vision_agents.core.llm.llm_types import ToolSchema
import logging
from dotenv import load_dotenv
from getstream.video.rtc.track_util import PcmData
from .rtc_manager import RTCManager

from vision_agents.core.edge.types import Participant
from vision_agents.core.processors import Processor
from vision_agents.core.utils.video_forwarder import VideoForwarder

load_dotenv()

logger = logging.getLogger(__name__)

"""
TODO: Future improvements
- Reconnection is currently not easy to do with OpenAI realtime.
"""


class Realtime(realtime.Realtime):
    """
    OpenAI Realtime API implementation for real-time AI audio and video communication over WebRTC.

    Extends the base Realtime class with WebRTC-based audio and optional video
    streaming to OpenAI's servers. Supports speech-to-speech conversation, text
    messaging, multimodal interactions, and function calling with MCP support.

    Args:
        model: OpenAI model to use (e.g., "gpt-realtime").
        voice: Voice for audio responses (e.g., "marin", "alloy").
        realtime_session: Configure RealtimeSessionCreateRequestParam

        api_key: Optionally specify an API key
        client: pass your own AsyncOpenAI client

        This class uses:
        - RTCManager to handle WebRTC connection and media streaming.
        - Output track to forward audio and video to the remote participant.
        - Function calling support for real-time tool execution.
        - MCP integration for external service access.

    """

    def __init__(
        self,
        model: str = "gpt-realtime",
        api_key: Optional[str] = None,
        voice: str = "marin",
        client: Optional[AsyncOpenAI] = None,
        fps: int = 1,
        realtime_session: Optional[RealtimeSessionCreateRequestParam] = None,
        send_video: bool = True,
    ):
        super().__init__(fps)
        self.model = model
        self.voice = voice

        self.realtime_session: RealtimeSessionCreateRequestParam = (
            realtime_session or RealtimeSessionCreateRequestParam(type="realtime")
        )
        self.realtime_session["model"] = self.model

        # Set audio and output if they are None
        if self.realtime_session.get("audio") is None:
            self.realtime_session["audio"] = RealtimeAudioConfigParam(
                input=RealtimeAudioConfigInputParam(
                    transcription=AudioTranscriptionParam(
                        model="gpt-4o-mini-transcribe"
                    ),
                    turn_detection=SemanticVad(type="semantic_vad"),
                )
            )
        if self.realtime_session["audio"].get("output") is None:
            self.realtime_session["audio"]["output"] = RealtimeAudioConfigOutputParam()
        self.realtime_session["audio"]["output"]["voice"] = self.voice

        # Map conversation item_id to participant to handle multi-user scenarios
        self._item_to_participant: Dict[str, Participant] = {}
        self._pending_participant: Optional[Participant] = None

        # Store current session and rate limits
        self.current_session: Optional[
            Union[
                RealtimeSessionCreateRequest, RealtimeTranscriptionSessionCreateRequest
            ]
        ] = None
        self.current_rate_limits: Optional[RateLimitsUpdatedEvent] = None

        # create the client
        if client is not None:
            self.client = client
        elif api_key is not None and api_key != "":
            self.client = AsyncOpenAI(api_key=api_key)
        else:
            self.client = AsyncOpenAI()  # will get it from the env vars

        # Start the realtime connection manager
        self.rtc = RTCManager(
            realtime_session=self.realtime_session,
            client=self.client,
            send_video=send_video,
        )

    async def connect(self):
        """Establish the WebRTC connection to OpenAI's Realtime API.

        Sets up callbacks and connects to OpenAI's servers. Emits connected event
        with session configuration when ready.
        """
        # Wire callbacks so we can emit audio/events upstream
        self.rtc.set_event_callback(self._handle_openai_event)
        self.rtc.set_audio_callback(self._handle_audio_output)
        await self.rtc.connect()

        # Register tools with OpenAI realtime if available
        await self._register_tools_with_openai_realtime()

        # Emit connected/ready
        self._emit_connected_event(
            session_config={"model": self.model, "voice": self.voice},
            capabilities=["text", "audio", "function_calling"],
        )

    async def simple_response(
        self,
        text: str,
        processors: Optional[List[Processor]] = None,
        participant: Optional[Participant] = None,
    ):
        """Send a simple text input to the OpenAI Realtime session.

        This is a convenience wrapper that forwards a text prompt upstream via
        the underlying realtime connection. It does not stream partial deltas
        back; callers should subscribe to the provider's events to receive
        responses.

        Args:
            text: Text prompt to send.
            processors: Optional processors list (not used here; included for
                interface parity with the core `LLM` API).
            participant: Optional participant metadata (ignored here).
        """
        await self.rtc.send_text(text)

    async def simple_audio_response(
        self, audio: PcmData, participant: Optional[Participant] = None
    ):
        """Send a single PCM audio frame to the OpenAI Realtime session.

        The audio should be raw PCM matching the realtime session's expected
        format (typically 48 kHz mono, 16-bit). For continuous audio capture,
        call this repeatedly with consecutive frames.

        Args:
            audio: PCM audio frame to forward upstream.
            participant: Optional participant information for the audio source.
        """
        # Track pending participant for the next conversation item
        self._pending_participant = participant
        await self.rtc.send_audio_pcm(audio)

    async def close(self):
        await self.rtc.close()

    async def _handle_openai_event(self, event: dict) -> None:
        """Process events received from the OpenAI Realtime API.

        Handles OpenAI event types and emits standardized events.

        Args:
            event: Raw event dictionary from OpenAI API.

        Event Handling:
            - response.audio_transcript.done: Emits agent speech transcription
            - conversation.item.input_audio_transcription.completed: Emits user speech transcription
            - input_audio_buffer.speech_started: Flushes output audio track
            - response.tool_call: Handles tool calls from OpenAI realtime

        Note:
            Registered as callback with RTC manager.
        """
        et = event.get("type")

        # code here is weird because OpenAI does something strange
        # see issue: https://github.com/openai/openai-python/issues/2698
        # as a workaround we copy the event and normalize the type to response.output_audio_transcript.done so that
        # ResponseAudioTranscriptDoneEvent.model_validate is happy
        if et in [
            "response.audio_transcript.done",
            "response.output_audio_transcript.done",
        ]:
            # Create a copy and normalize the type field
            event_copy = event.copy()
            event_copy["type"] = "response.output_audio_transcript.done"
            transcript_event: ResponseAudioTranscriptDoneEvent = (
                ResponseAudioTranscriptDoneEvent.model_validate(event_copy)
            )
            self._emit_agent_speech_transcription(
                text=transcript_event.transcript, original=event
            )
            self._emit_response_event(
                text=transcript_event.transcript,
                response_id=transcript_event.response_id,
                is_complete=True,
                conversation_item_id=transcript_event.item_id,
            )
        elif et == "conversation.item.created":
            # When OpenAI creates a conversation item, map it to the participant who sent the audio
            item = event.get("item", {})
            if item.get("type") == "message" and item.get("role") == "user":
                item_id = item.get("id")
                if item_id and self._pending_participant:
                    self._item_to_participant[item_id] = self._pending_participant
                    logger.debug(
                        f"Mapped item {item_id} to participant {self._pending_participant.user_id if self._pending_participant else 'None'}"
                    )
        elif et == "conversation.item.added":
            # Conversation item was added to the conversation
            pass
        elif et == "conversation.item.done":
            # Conversation item is complete
            pass
        elif et == "conversation.item.input_audio_transcription.completed":
            # User input audio transcription completed
            user_transcript_event: ConversationItemInputAudioTranscriptionCompletedEvent = ConversationItemInputAudioTranscriptionCompletedEvent.model_validate(
                event
            )
            item_id = user_transcript_event.item_id

            # Look up the correct participant for this transcription
            participant = self._item_to_participant.get(item_id)

            # Temporarily set the correct participant for this specific transcription
            original_participant = self._current_participant
            self._current_participant = participant
            self._emit_user_speech_transcription(
                text=user_transcript_event.transcript, original=event
            )
            self._current_participant = original_participant

            # Clean up the mapping to avoid memory leaks
            if item_id:
                self._item_to_participant.pop(item_id, None)
        elif et == "input_audio_buffer.speech_started":
            # Validate event but don't need to store it
            InputAudioBufferSpeechStartedEvent.model_validate(event)
            # await self.output_track.flush()
        elif et == "response.output_item.added":
            # Check if this is a function call
            item = event.get("item", {})
            if item.get("type") == "function_call":
                await self._handle_tool_call_event(event)
        elif et == "response.tool_call":
            # Handle tool calls from OpenAI realtime
            await self._handle_tool_call_event(event)
        elif et == "response.created":
            pass
        elif et == "session.created":
            session_event = SessionCreatedEvent(**event)
            self.current_session = session_event.session
            logger.info("Session created %s", event)
        elif et == "rate_limits.updated":
            self.current_rate_limits = RateLimitsUpdatedEvent(**event)
        elif et == "response.done":
            response_done_event = ResponseDoneEvent.model_validate(event)

            if response_done_event.response.status == "failed":
                raise Exception(
                    "OpenAI realtime failure %s", response_done_event.response
                )
        elif et == "session.updated":
            # Update session with new data
            session_updated_event = SessionUpdatedEvent(**event)
            self.current_session = session_updated_event.session
            logger.info("Session updated %s", event)
        elif et == "response.content_part.added":
            # Content part added to response - logged for debugging
            pass
        elif et == "response.audio_transcript.delta":
            # Streaming transcript delta - logged at debug level to avoid clutter
            pass
        elif et == "response.output_audio_transcript.delta":
            # Streaming output audio transcript delta - logged at debug level to avoid clutter
            pass
        elif et == "output_audio_buffer.started":
            # Output audio buffer started - acknowledgment of audio playback start
            pass
        elif et == "output_audio_buffer.stopped":
            # Output audio buffer stopped - acknowledgment of audio playback end
            pass
        elif et == "response.audio.done":
            # Audio generation complete for this response item
            pass
        elif et == "response.output_audio.done":
            # Output audio generation complete for this response item
            pass
        elif et == "response.content_part.done":
            # Content part complete - contains full transcript
            pass
        elif et == "response.output_item.done":
            # Output item complete - logged for debugging
            pass
        else:
            logger.debug(f"Unrecognized OpenAI Realtime event: {et} {event}")

    async def _handle_audio_output(self, pcm: PcmData) -> None:
        """Process audio output received from the OpenAI API.

        Forwards audio data to the output track for playback and emits audio output event.
        """

        # Emit audio output event
        self._emit_audio_output_event(
            audio_data=pcm,
        )

    async def watch_video_track(
        self,
        track: aiortc.mediastreams.MediaStreamTrack,
        shared_forwarder: Optional[VideoForwarder] = None,
    ) -> None:
        """
        Watch the video track and forward data to OpenAI Realtime API.

        Args:
            track: Video track to watch and forward.
            shared_forwarder: Optional shared VideoForwarder instance to use instead
                of creating a new one. Allows multiple consumers to share the same
                video stream.
        """
        await self.rtc.start_video_sender(
            track, self.fps, shared_forwarder=shared_forwarder
        )

    async def _stop_watching_video_track(self) -> None:
        # Video sender will be stopped when connection closes
        pass

    async def _handle_tool_call_event(self, event: dict) -> None:
        """Handle tool call events from OpenAI realtime.

        Args:
            event: Tool call event from OpenAI realtime API
        """
        try:
            # Handle both event structures
            if event.get("type") == "response.output_item.added":
                item = event.get("item", {})
                tool_call_data = item
            else:
                tool_call_data = event.get("tool_call", {})

            if not tool_call_data:
                logger.warning("Received tool call event without tool_call data")
                return

            # Extract tool call details
            tool_call = {
                "type": "tool_call",
                "id": tool_call_data.get("call_id"),
                "name": tool_call_data.get("name", "unknown"),
                "arguments_json": tool_call_data.get("arguments", {}),
            }

            logger.info(
                f"Executing tool call: {tool_call['name']} with args: {tool_call['arguments_json']}"
            )

            # Execute using existing tool execution infrastructure
            tc, result, error = await self._run_one_tool(tool_call, timeout_s=30)

            # Prepare response data
            if error:
                response_data = {"error": str(error)}
                logger.error(f"Tool call {tool_call['name']} failed: {error}")
            else:
                # Ensure response is a dictionary for OpenAI realtime
                if not isinstance(result, dict):
                    response_data = {"result": result}
                else:
                    response_data = result
                logger.info(f"Tool call {tool_call['name']} succeeded: {response_data}")

            # Send tool response back to OpenAI realtime session
            await self._send_tool_response(tool_call["id"], response_data)

        except Exception as e:
            logger.error(f"Error handling tool call event: {e}")
            # Send error response back
            call_id = None
            if event.get("type") == "response.output_item.added":
                call_id = event.get("item", {}).get("call_id")
            else:
                call_id = event.get("tool_call", {}).get("call_id")
            await self._send_tool_response(call_id, {"error": str(e)})

    async def _send_tool_response(
        self, call_id: Optional[str], response_data: Dict[str, Any]
    ) -> None:
        """Send tool response back to OpenAI realtime session.

        Args:
            call_id: The call ID from the original tool call
            response_data: The response data to send back
        """
        if not call_id:
            logger.warning("Cannot send tool response without call_id")
            return

        try:
            # Convert response to string for OpenAI realtime
            response_str = self._sanitize_tool_output(response_data)

            # Send tool response event
            event = {
                "type": "conversation.item.create",
                "item": {
                    "type": "function_call_output",
                    "call_id": call_id,
                    "output": response_str,
                },
            }

            await self.rtc._send_event(event)
            logger.info(f"Sent tool response for call_id {call_id}")

            # Trigger a new response to continue the conversation with audio
            # This ensures the AI responds with audio after receiving the tool result
            await self.rtc._send_event(
                {
                    "type": "response.create",
                    "response": {
                        "modalities": ["text", "audio"],
                        "instructions": "Please respond to the user with the tool results in a conversational way.",
                    },
                }
            )

        except Exception as e:
            logger.error(f"Failed to send tool response: {e}")

    def _set_instructions(self, instructions: str):
        super()._set_instructions(instructions)
        self.realtime_session["instructions"] = (
            self._build_enhanced_instructions() or ""
        )

    def _sanitize_tool_output(self, value: Any, max_chars: int = 60_000) -> str:
        """Sanitize tool output for OpenAI realtime.

        Args:
            value: The tool output to sanitize
            max_chars: Maximum characters allowed (not used in realtime mode)

        Returns:
            Sanitized string output
        """
        if isinstance(value, str):
            return value
        elif isinstance(value, dict):
            return json.dumps(value, ensure_ascii=False)
        else:
            return str(value)

    def _convert_tools_to_openai_realtime_format(
        self, tools: List[ToolSchema]
    ) -> List[Dict[str, Any]]:
        """Convert ToolSchema objects to OpenAI realtime format.

        Args:
            tools: List of ToolSchema objects from the function registry

        Returns:
            List of tools in OpenAI realtime format
        """
        out = []
        for t in tools or []:
            name = t.get("name", "unnamed_tool")
            description = t.get("description", "") or ""
            params = t.get("parameters_schema") or t.get("parameters") or {}
            if not isinstance(params, dict):
                params = {}
            params.setdefault("type", "object")
            params.setdefault("properties", {})
            params.setdefault("additionalProperties", False)

            out.append(
                {
                    "type": "function",
                    "name": name,
                    "description": description,
                    "parameters": params,
                }
            )
        return out

    async def _register_tools_with_openai_realtime(self) -> None:
        """Register available tools with OpenAI realtime session.

        This method registers all available functions and MCP tools with the
        OpenAI realtime session so they can be called during conversations.
        """
        try:
            # Get available tools from function registry
            available_tools = self.get_available_functions()

            if not available_tools:
                logger.info("No tools available to register with OpenAI realtime")
                return

            # Convert tools to OpenAI realtime format
            tools_for_openai = self._convert_tools_to_openai_realtime_format(
                available_tools
            )

            if not tools_for_openai:
                logger.info("No tools converted for OpenAI realtime")
                return

            # Send tools configuration to OpenAI realtime
            tools_event = {
                "type": "session.update",
                "session": {"tools": tools_for_openai},
            }

            await self.rtc._send_event(tools_event)
            logger.info(
                f"Registered {len(tools_for_openai)} tools with OpenAI realtime"
            )

        except Exception as e:
            logger.error(f"Failed to register tools with OpenAI realtime: {e}")
            # Don't raise the exception - tool registration failure shouldn't break the connection

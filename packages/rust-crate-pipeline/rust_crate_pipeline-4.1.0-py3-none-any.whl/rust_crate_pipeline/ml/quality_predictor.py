"""
Machine Learning Quality Predictor for Rust Crates

Uses ML models to predict:
- Code quality scores
- Security risk levels
- Maintenance activity
- Popularity trends
- Dependency health
"""

import json
import logging
import pickle
from dataclasses import dataclass
from pathlib import Path
from typing import Any, Dict, List, Optional

try:
    import numpy as np
    from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor
    from sklearn.feature_extraction.text import TfidfVectorizer
    from sklearn.metrics import classification_report, mean_squared_error
    from sklearn.model_selection import train_test_split
    from sklearn.preprocessing import StandardScaler
    ML_AVAILABLE = True
except ImportError:
    np = None
    RandomForestClassifier = None
    RandomForestRegressor = None
    TfidfVectorizer = None
    classification_report = None
    mean_squared_error = None
    train_test_split = None
    StandardScaler = None
    ML_AVAILABLE = False

from .artifacts import (
    LEGACY_METADATA_FILENAME,
    compute_artifact_hash,
    ensure_required_artifacts_exist,
    load_provenance,
)

@dataclass
class QualityPrediction:
    """Prediction results for crate quality."""

    crate_name: str
    quality_score: float
    security_risk: str
    maintenance_score: float
    popularity_trend: str
    dependency_health: float
    confidence: float
    features_used: List[str]
    model_version: str


class CrateQualityPredictor:
    """ML-based quality predictor for Rust crates."""

    def __init__(self, model_dir: str = "./models"):
        self.model_dir = Path(model_dir)
        self.model_dir.mkdir(parents=True, exist_ok=True)
        self.logger = logging.getLogger(__name__)

        # Models
        self.quality_model: Optional[RandomForestRegressor] = None
        self.security_model: Optional[RandomForestClassifier] = None
        self.maintenance_model: Optional[RandomForestRegressor] = None
        self.popularity_model: Optional[RandomForestClassifier] = None
        self.dependency_model: Optional[RandomForestRegressor] = None

        # Feature processing
        self.text_vectorizer: Optional[TfidfVectorizer] = None
        self.scaler: Optional[StandardScaler] = None

        # Model metadata
        self.model_version = "1.0.0"
        self.feature_names: List[str] = []

        self._load_models()

    def _verify_artifacts(self) -> Dict[str, Any]:
        """Ensure trained artifacts exist and match their recorded hash."""

        try:
            ensure_required_artifacts_exist(self.model_dir)
            provenance = load_provenance(self.model_dir)
        except FileNotFoundError as exc:
            raise RuntimeError(
                "Trained ML model artifacts are missing. "
                "Run `python fix_ml_models.py` before using the predictor."
            ) from exc

        expected_hash = provenance.get("artifact_hash")
        if expected_hash is None:
            raise RuntimeError(
                "Model provenance is missing artifact hash metadata. "
                "Retrain models with `python fix_ml_models.py`."
            )

        actual_hash = compute_artifact_hash(self.model_dir)
        if actual_hash != expected_hash:
            raise RuntimeError(
                "Trained ML model artifacts are stale or corrupted. "
                "Retrain models with `python fix_ml_models.py`."
            )

        return provenance

    def _load_models(self) -> None:
        """Load trained models from disk."""
        model_files = {
            "quality": "quality_model.pkl",
            "security": "security_model.pkl",
            "maintenance": "maintenance_model.pkl",
            "popularity": "popularity_model.pkl",
            "dependency": "dependency_model.pkl",
            "vectorizer": "text_vectorizer.pkl",
            "scaler": "feature_scaler.pkl",
            "metadata": LEGACY_METADATA_FILENAME,
        }

        try:
            provenance = self._verify_artifacts()
        except RuntimeError as exc:
            self.logger.warning("ML model artifacts unavailable: %s", exc)
            provenance = {}
            metadata_file = self.model_dir / model_files["metadata"]
            if metadata_file.exists():
                with open(metadata_file, "r", encoding="utf-8") as f:
                    legacy_metadata = json.load(f)
                self.model_version = legacy_metadata.get("version", self.model_version)
                self.feature_names = legacy_metadata.get("feature_names", [])
        else:
            self.model_version = provenance.get("version", self.model_version)
            self.feature_names = provenance.get(
                "combined_feature_names", provenance.get("feature_names", [])
            )

        for model_name, filename in model_files.items():
            if model_name == "metadata":
                continue

            model_file = self.model_dir / filename
            if not model_file.exists():
                continue

            with open(model_file, "rb") as f:
                model = pickle.load(f)

            if model_name == "quality":
                self.quality_model = model
            elif model_name == "security":
                self.security_model = model
            elif model_name == "maintenance":
                self.maintenance_model = model
            elif model_name == "popularity":
                self.popularity_model = model
            elif model_name == "dependency":
                self.dependency_model = model
            elif model_name == "vectorizer":
                self.text_vectorizer = model
            elif model_name == "scaler":
                self.scaler = model

        if any(
            model is not None
            for model in [
                self.quality_model,
                self.security_model,
                self.maintenance_model,
                self.popularity_model,
                self.dependency_model,
                self.text_vectorizer,
                self.scaler,
            ]
        ):
            self.logger.info("Loaded ML models from %s", self.model_dir)

    def _extract_features(self, crate_data: Dict[str, Any]) -> List[float]:
        """Extract numerical features from crate data."""
        # Feature dimension constants - update these if feature extraction logic changes
        BASIC_FEATURES_COUNT = 8
        TEXT_FEATURES_COUNT = 100
        ANALYSIS_FEATURES_COUNT = 6
        TOTAL_FEATURES_COUNT = BASIC_FEATURES_COUNT + TEXT_FEATURES_COUNT + ANALYSIS_FEATURES_COUNT
        
        try:
            features = []

            # Basic features (8 features)
            description = crate_data.get("description", "")
            features.append(len(description))
            features.append(len(description.split()) if description else 0)

            # Context and reasoning features
            context_sources = crate_data.get("context_sources", [])
            features.append(len(context_sources))

            reasoning_steps = crate_data.get("reasoning_steps", [])
            features.append(len(reasoning_steps))

            # IRL score
            features.append(float(crate_data.get("irl_score", 0.0)))

            # Audit info features
            audit_info = crate_data.get("audit_info", {})

            # Crate analysis features
            crate_analysis = audit_info.get("crate_analysis", {})
            enhanced_analysis = crate_analysis.get("enhanced_analysis", {})

            # Environment features
            environment = enhanced_analysis.get("environment", {})
            features.append(1.0 if environment.get("has_cargo_toml") else 0.0)
            features.append(1.0 if environment.get("has_dependencies") else 0.0)

            # Ensure we have exactly BASIC_FEATURES_COUNT basic features
            while len(features) < BASIC_FEATURES_COUNT:
                features.append(0.0)
            features = features[:BASIC_FEATURES_COUNT]

            # Text features (TEXT_FEATURES_COUNT features from TF-IDF)
            text_features = []
            if self.text_vectorizer is not None:
                try:
                    # Combine relevant text fields
                    text_content = " ".join(
                        [
                            description,
                            str(crate_data.get("readme_content", "")),
                            " ".join(context_sources),
                            " ".join([str(step) for step in reasoning_steps]),
                        ]
                    )

                    if text_content.strip():
                        text_vector = self.text_vectorizer.transform([text_content])
                        text_features = text_vector.toarray()[0].tolist()
                    else:
                        text_features = [0.0] * TEXT_FEATURES_COUNT
                except Exception as e:
                    self.logger.warning(f"Text vectorization failed: {e}")
                    text_features = [0.0] * TEXT_FEATURES_COUNT
            else:
                text_features = [0.0] * TEXT_FEATURES_COUNT

            # Ensure exactly TEXT_FEATURES_COUNT text features
            while len(text_features) < TEXT_FEATURES_COUNT:
                text_features.append(0.0)
            text_features = text_features[:TEXT_FEATURES_COUNT]

            # Analysis features (ANALYSIS_FEATURES_COUNT features)
            analysis_features = []

            # Environment features
            environment = enhanced_analysis.get("environment", {})
            analysis_features.append(
                1.0 if environment.get("has_dev_dependencies") else 0.0
            )
            analysis_features.append(len(environment.get("features", [])))

            # Source statistics
            source_stats = enhanced_analysis.get("source_stats", {})
            analysis_features.append(float(source_stats.get("rust_files", 0)))
            analysis_features.append(float(source_stats.get("rust_lines", 0)))
            analysis_features.append(1.0 if source_stats.get("has_tests") else 0.0)
            analysis_features.append(1.0 if source_stats.get("has_examples") else 0.0)

            # Ensure exactly ANALYSIS_FEATURES_COUNT analysis features
            while len(analysis_features) < ANALYSIS_FEATURES_COUNT:
                analysis_features.append(0.0)
            analysis_features = analysis_features[:ANALYSIS_FEATURES_COUNT]

            # Combine all features: BASIC_FEATURES_COUNT + TEXT_FEATURES_COUNT + ANALYSIS_FEATURES_COUNT = TOTAL_FEATURES_COUNT
            all_features = features + text_features + analysis_features

            # Final validation
            if len(all_features) != TOTAL_FEATURES_COUNT:
                self.logger.warning(
                    f"Feature count mismatch: got {len(all_features)}, expected {TOTAL_FEATURES_COUNT}"
                )
                # Pad or truncate to exactly TOTAL_FEATURES_COUNT
                while len(all_features) < TOTAL_FEATURES_COUNT:
                    all_features.append(0.0)
                all_features = all_features[:TOTAL_FEATURES_COUNT]

            return all_features

        except Exception as e:
            self.logger.warning(f"Feature extraction failed: {e}")
            # Return default feature vector with exactly TOTAL_FEATURES_COUNT features
            return [0.0] * TOTAL_FEATURES_COUNT

    def _initialize_text_vectorizer(self, sample_texts: List[str]) -> None:
        """Initialize text vectorizer with sample data."""
        try:
            self.text_vectorizer = TfidfVectorizer(
                max_features=100,
                stop_words="english",
                lowercase=True,
                ngram_range=(1, 2),
            )
            # Fit on sample texts
            if sample_texts and any(text.strip() for text in sample_texts):
                self.text_vectorizer.fit(sample_texts)
            else:
                # Fit on dummy data if no real text available
                self.text_vectorizer.fit(["sample text", "rust crate", "library"])
            self.logger.info("Initialized text vectorizer")
        except Exception as e:
            self.logger.error(f"Failed to initialize text vectorizer: {e}")
            self.text_vectorizer = None

    def _initialize_scaler(self) -> None:
        """Initialize feature scaler with default parameters."""
        try:
            self.scaler = StandardScaler()
            # We can't fit the scaler without training data
            # It will be fitted during training
            self.logger.info(
                "Initialized feature scaler (requires training data to fit)"
            )
        except Exception as e:
            self.logger.error(f"Failed to initialize scaler: {e}")
            self.scaler = None

    def ensure_models_available(self) -> None:
        """Ensure models are available for predictions."""
        provenance = self._verify_artifacts()

        if any(
            model is None
            for model in [
                self.quality_model,
                self.security_model,
                self.maintenance_model,
                self.popularity_model,
                self.dependency_model,
                self.text_vectorizer,
                self.scaler,
            ]
        ):
            self.logger.info("Reloading ML models from disk")
            self._load_models()

        if any(
            model is None
            for model in [
                self.quality_model,
                self.security_model,
                self.maintenance_model,
                self.popularity_model,
                self.dependency_model,
                self.text_vectorizer,
                self.scaler,
            ]
        ):
            raise RuntimeError(
                "Unable to load trained ML models after verifying artifacts. "
                "Retrain models with `python fix_ml_models.py`."
            )

    def predict_quality(self, crate_data: Dict[str, Any]) -> QualityPrediction:
        """Predict quality metrics for a crate."""
        try:
            # Ensure models are available before prediction
            self.ensure_models_available()

            features = self._extract_features(crate_data)

            # Make predictions
            quality_score = 0.5
            if self.quality_model:
                quality_score = float(self.quality_model.predict([features])[0])
                quality_score = max(0.0, min(1.0, quality_score))

            security_risk = "medium"
            if self.security_model:
                risk_pred = self.security_model.predict([features])[0]
                security_risk = ["low", "medium", "high"][risk_pred]

            maintenance_score = 0.5
            if self.maintenance_model:
                maintenance_score = float(self.maintenance_model.predict([features])[0])
                maintenance_score = max(0.0, min(1.0, maintenance_score))

            popularity_trend = "stable"
            if self.popularity_model:
                trend_pred = self.popularity_model.predict([features])[0]
                popularity_trend = ["declining", "stable", "growing"][trend_pred]

            dependency_health = 0.5
            if self.dependency_model:
                dependency_health = float(self.dependency_model.predict([features])[0])
                dependency_health = max(0.0, min(1.0, dependency_health))

            # Calculate confidence based on model availability
            models_available = sum(
                [
                    self.quality_model is not None,
                    self.security_model is not None,
                    self.maintenance_model is not None,
                    self.popularity_model is not None,
                    self.dependency_model is not None,
                ]
            )
            confidence = min(1.0, models_available / 5.0)

            return QualityPrediction(
                crate_name=crate_data.get("name", "unknown"),
                quality_score=quality_score,
                security_risk=security_risk,
                maintenance_score=maintenance_score,
                popularity_trend=popularity_trend,
                dependency_health=dependency_health,
                confidence=confidence,
                features_used=self.feature_names,
                model_version=self.model_version,
            )

        except Exception as e:
            self.logger.error(f"Prediction failed: {e}")
            return QualityPrediction(
                crate_name=crate_data.get("name", "unknown"),
                quality_score=0.5,
                security_risk="unknown",
                maintenance_score=0.5,
                popularity_trend="unknown",
                dependency_health=0.5,
                confidence=0.0,
                features_used=[],
                model_version=self.model_version,
            )

    def train_models(self, training_data: List[Dict[str, Any]]) -> None:
        """Train models on historical crate data."""
        if not training_data:
            self.logger.warning("No training data provided")
            return

        try:
            # Prepare training data
            X = []
            y_quality = []
            y_security = []
            y_maintenance = []
            y_popularity = []
            y_dependency = []

            for crate in training_data:
                features = self._extract_features(crate)
                X.append(features)

                # Extract labels
                y_quality.append(crate.get("quality_score", 0.5))
                y_security.append(
                    crate.get("security_risk_level", 1)
                )  # 0=low, 1=medium, 2=high
                y_maintenance.append(crate.get("maintenance_score", 0.5))
                y_popularity.append(
                    crate.get("popularity_trend", 1)
                )  # 0=declining, 1=stable, 2=growing
                y_dependency.append(crate.get("dependency_health", 0.5))

            # Check numpy availability before using it
            if np is None:
                raise RuntimeError(
                    "numpy is required for model training but is not installed. "
                    "Install it with: pip install numpy"
                )
            
            X = np.array(X)
            y_quality = np.array(y_quality)
            y_security = np.array(y_security)
            y_maintenance = np.array(y_maintenance)
            y_popularity = np.array(y_popularity)
            y_dependency = np.array(y_dependency)

            # Split data
            X_train, X_test, y_quality_train, y_quality_test = train_test_split(
                X, y_quality, test_size=0.2, random_state=42
            )

            # Train quality model
            self.quality_model = RandomForestRegressor(
                n_estimators=100, random_state=42
            )
            self.quality_model.fit(X_train, y_quality_train)

            # Train security model
            self.security_model = RandomForestClassifier(
                n_estimators=100, random_state=42
            )
            self.security_model.fit(X_train, y_security)

            # Train maintenance model
            self.maintenance_model = RandomForestRegressor(
                n_estimators=100, random_state=42
            )
            self.maintenance_model.fit(X_train, y_maintenance)

            # Train popularity model
            self.popularity_model = RandomForestClassifier(
                n_estimators=100, random_state=42
            )
            self.popularity_model.fit(X_train, y_popularity)

            # Train dependency model
            self.dependency_model = RandomForestRegressor(
                n_estimators=100, random_state=42
            )
            self.dependency_model.fit(X_train, y_dependency)

            # Save models
            self._save_models()

            # Evaluate models
            self._evaluate_models(
                X_test,
                y_quality_test,
                y_security,
                y_maintenance,
                y_popularity,
                y_dependency,
            )

        except Exception as e:
            self.logger.error(f"Model training failed: {e}")

    def _save_models(self) -> None:
        """Save trained models to disk."""
        try:
            models = {
                "quality_model.pkl": self.quality_model,
                "security_model.pkl": self.security_model,
                "maintenance_model.pkl": self.maintenance_model,
                "popularity_model.pkl": self.popularity_model,
                "dependency_model.pkl": self.dependency_model,
                "text_vectorizer.pkl": self.text_vectorizer,
                "feature_scaler.pkl": self.scaler,
            }

            for filename, model in models.items():
                if model is not None:
                    model_file = self.model_dir / filename
                    with open(model_file, "wb") as f:
                        pickle.dump(model, f)

            # Save metadata
            metadata = {
                "version": self.model_version,
                "feature_names": self.feature_names,
                "model_count": len([m for m in models.values() if m is not None]),
            }

            metadata_file = self.model_dir / "model_metadata.json"
            with open(metadata_file, "w") as f:
                json.dump(metadata, f, indent=2)

            self.logger.info("Models saved successfully")

        except Exception as e:
            self.logger.error(f"Failed to save models: {e}")

    def _evaluate_models(
        self,
        X_test: np.ndarray,
        y_quality_test: np.ndarray,
        y_security: np.ndarray,
        y_maintenance: np.ndarray,
        y_popularity: np.ndarray,
        y_dependency: np.ndarray,
    ) -> None:
        """Evaluate model performance."""
        try:
            if self.quality_model is not None:
                y_pred = self.quality_model.predict(X_test)
                mse = mean_squared_error(y_quality_test, y_pred)
                self.logger.info(f"Quality model MSE: {mse:.4f}")

            if self.security_model is not None:
                y_pred = self.security_model.predict(X_test)
                report = classification_report(
                    y_security, y_pred, target_names=["low", "medium", "high"]
                )
                self.logger.info(f"Security model report:\n{report}")

        except Exception as e:
            self.logger.warning(f"Model evaluation failed: {e}")


# Global predictor instance
_global_predictor: Optional[CrateQualityPredictor] = None


def get_predictor() -> CrateQualityPredictor:
    """Get global predictor instance."""
    global _global_predictor
    if _global_predictor is None:
        _global_predictor = CrateQualityPredictor()
    return _global_predictor


def set_predictor(predictor: CrateQualityPredictor) -> None:
    """Set global predictor instance."""
    global _global_predictor
    _global_predictor = predictor

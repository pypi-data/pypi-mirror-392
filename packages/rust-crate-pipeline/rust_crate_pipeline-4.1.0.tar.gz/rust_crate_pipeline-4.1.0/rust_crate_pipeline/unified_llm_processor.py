# unified_llm_processor.py
import logging
from typing import TYPE_CHECKING, Any, Dict, Optional, Union

from .config import CrateMetadata, EnrichedCrate, PipelineConfig
from .llm_client import LLMClient, LLMConfig
from .llm_factory import create_llm_client_from_config

if TYPE_CHECKING:
    pass


class UnifiedLLMProcessor:
    """
    Unified LLM processor that supports multiple providers with a consistent interface.

    This processor replaces the old litellm-based approach with a more robust,
    provider-agnostic implementation that supports:
    - Ollama (local models)
    - OpenAI (cloud models)
    - Azure OpenAI (enterprise models)
    - llama-cpp-python (direct model loading)
    - LiteLLM (fallback for complex scenarios)
    """

    def __init__(self, config: Union[PipelineConfig, LLMConfig]):
        self.logger = logging.getLogger(__name__)

        self.pipeline_config: Optional[PipelineConfig] = None
        self.llm_config: Optional[LLMConfig] = None

        if isinstance(config, PipelineConfig):
            self.pipeline_config = config
            self.llm_client = create_llm_client_from_config(config)
        elif isinstance(config, LLMConfig):
            self.llm_config = config
            self.llm_client = LLMClient(config)
        else:
            raise TypeError(
                "UnifiedLLMProcessor requires a PipelineConfig or LLMConfig instance"
            )

        # Maintain backward-compatible attribute for code paths still expecting
        # a pipeline configuration reference.
        self.config = self.pipeline_config

        # Processing configuration
        self.processing_config = self._load_processing_config()

    def _load_processing_config(self) -> Dict[str, Any]:
        """Load processing configuration from config loader"""
        try:
            from .config_loader import get_config_loader

            config_loader = get_config_loader()
            return config_loader.get_processing_config()
        except Exception as e:
            self.logger.warning(f"Could not load processing config: {e}")
            return {}

    async def process_crate(self, crate: CrateMetadata) -> EnrichedCrate:
        """Process a crate through the unified LLM pipeline"""
        try:
            # Create enriched crate with original data
            enriched = EnrichedCrate(
                name=crate.name,
                version=crate.version,
                description=crate.description,
                repository=crate.repository,
                keywords=crate.keywords,
                categories=crate.categories,
                readme=crate.readme,
                downloads=crate.downloads,
                github_stars=crate.github_stars,
                dependencies=crate.dependencies,
                features=crate.features,
                code_snippets=crate.code_snippets,
                readme_sections=crate.readme_sections,
                librs_downloads=crate.librs_downloads,
                license=crate.license,
                source=crate.source,
                enhanced_scraping=crate.enhanced_scraping,
                enhanced_features=crate.enhanced_features,
                enhanced_dependencies=crate.enhanced_dependencies,
            )

            # Perform AI enrichment
            await self._enrich_crate(enriched)
            return enriched

        except Exception as e:
            self.logger.error(f"Error processing crate {crate.name}: {e}")
            # Return basic enriched crate with error info
            return EnrichedCrate(
                name=crate.name,
                version=crate.version,
                description=crate.description,
                repository=crate.repository,
                keywords=crate.keywords,
                categories=crate.categories,
                readme=crate.readme,
                downloads=crate.downloads,
                github_stars=crate.github_stars,
                dependencies=crate.dependencies,
                features=crate.features,
                code_snippets=crate.code_snippets,
                readme_sections=crate.readme_sections,
                librs_downloads=crate.librs_downloads,
                license=crate.license,
                source=crate.source,
                enhanced_scraping=crate.enhanced_scraping,
                enhanced_features=crate.enhanced_features,
                enhanced_dependencies=crate.enhanced_dependencies,
                readme_summary="Error during processing",
                feature_summary="Error during processing",
                use_case="unknown",
                score=0.0,
                factual_counterfactual="Error during processing",
                source_analysis={"error": str(e)},
                user_behavior={"error": str(e)},
                security={"error": str(e)},
            )

    async def _enrich_crate(self, enriched: EnrichedCrate) -> None:
        """Enrich crate with AI analysis"""

        # Build context for analysis
        context = self._build_context(enriched)

        # Generate summaries
        enriched.readme_summary = await self._generate_readme_summary(enriched)
        enriched.feature_summary = await self._generate_feature_summary(enriched)

        # Perform comprehensive analysis
        analysis_result = await self._perform_comprehensive_analysis(context)

        # Update enriched crate with analysis results
        enriched.source_analysis = analysis_result.get("analysis", {})
        enriched.score = analysis_result.get("quality_score", 0.0)
        enriched.use_case = analysis_result.get("use_case", "unknown")
        enriched.factual_counterfactual = analysis_result.get("factual_pairs", "")
        enriched.user_behavior = analysis_result.get("user_behavior", {})
        enriched.security = analysis_result.get("security", {})

    def _build_context(self, enriched: EnrichedCrate) -> str:
        """Build comprehensive context for LLM analysis"""
        context_parts = []

        # Basic information
        context_parts.append(f"Crate: {enriched.name} v{enriched.version}")
        if enriched.description:
            context_parts.append(f"Description: {enriched.description}")

        # Repository and metadata
        if enriched.repository:
            context_parts.append(f"Repository: {enriched.repository}")
        if enriched.keywords:
            context_parts.append(f"Keywords: {', '.join(enriched.keywords)}")
        if enriched.categories:
            context_parts.append(f"Categories: {', '.join(enriched.categories)}")

        # Dependencies and features
        if enriched.dependencies:
            deps = [
                f"{dep.get('name', 'unknown')} {dep.get('version', '')}"
                for dep in enriched.dependencies
            ]
            context_parts.append(f"Dependencies: {', '.join(deps)}")

        if enriched.features:
            context_parts.append(f"Features: {', '.join(enriched.features.keys())}")

        # Metrics
        if enriched.downloads is not None:
            context_parts.append(f"Downloads: {enriched.downloads}")
        if enriched.github_stars is not None:
            context_parts.append(f"GitHub Stars: {enriched.github_stars}")

        # README content (truncated)
        if enriched.readme:
            readme_preview = (
                enriched.readme[:1500] + "..."
                if len(enriched.readme) > 1500
                else enriched.readme
            )
            context_parts.append(f"README: {readme_preview}")

        return "\n".join(context_parts)

    async def _generate_readme_summary(self, enriched: EnrichedCrate) -> str:
        """Generate README summary"""
        if not enriched.readme:
            return "No README available"

        prompt = f"""Summarize the key aspects of this Rust crate from its README:

Crate: {enriched.name}
Description: {enriched.description}

        README Content:
{enriched.readme[:2000]}

Provide a concise summary (2-3 sentences) focusing on:
- Main purpose and functionality
- Key features and capabilities
- Target use cases

Summary:"""

        try:
            response = await self.llm_client.chat(
                messages=[{"role": "user", "content": prompt}],
                max_tokens=300,  # Increased for more detailed summaries
                temperature=0.3,
            )
            return response.strip()
        except Exception as e:
            self.logger.error(f"Error generating README summary: {e}")
            return "README summary generation failed"

    def _detect_special_capabilities(self, enriched: EnrichedCrate) -> Dict[str, Any]:
        """Detect special capabilities like no_std support and data formats"""
        capabilities = {
            "no_std": False,
            "data_formats": [],
        }
        
        # Check for no_std support
        no_std_indicators = ["no_std", "no-std", "nostd", "no std"]
        text_to_check = " ".join([
            enriched.description.lower(),
            " ".join(enriched.keywords).lower(),
            " ".join(enriched.categories).lower(),
            " ".join(enriched.features.keys()).lower() if enriched.features else "",
        ])
        
        if any(indicator in text_to_check for indicator in no_std_indicators):
            capabilities["no_std"] = True
        
        # Check for data format support
        format_keywords = {
            "json": ["json"],
            "cbor": ["cbor"],
            "yaml": ["yaml", "yml"],
            "toml": ["toml"],
            "xml": ["xml"],
            "messagepack": ["messagepack", "msgpack", "message-pack"],
            "bincode": ["bincode", "bin-code"],
            "postcard": ["postcard"],
            "ron": ["ron"],
            "bson": ["bson"],
        }
        
        for format_name, keywords in format_keywords.items():
            if any(keyword in text_to_check for keyword in keywords):
                capabilities["data_formats"].append(format_name)
        
        return capabilities

    async def _generate_feature_summary(self, enriched: EnrichedCrate) -> str:
        """Generate feature summary"""
        if not enriched.features:
            return "No features defined"

        # Detect special capabilities
        capabilities = self._detect_special_capabilities(enriched)
        
        # Build enhanced prompt
        prompt_parts = [
            "Summarize the features of this Rust crate:",
            "",
            f"Crate: {enriched.name}",
            f"Features: {enriched.features}",
            f"Description: {enriched.description}",
        ]
        
        # Add special capabilities context
        if capabilities["no_std"]:
            prompt_parts.append(
                "\nNote: This crate supports no_std environments (can run without the Rust standard library), "
                "making it suitable for embedded systems, WebAssembly, and other constrained environments."
            )
        
        if capabilities["data_formats"]:
            formats_str = ", ".join(capabilities["data_formats"])
            prompt_parts.append(
                f"\nNote: This crate supports the following data formats: {formats_str}. "
                "Mention these formats in the summary when relevant."
            )
        
        prompt_parts.append(
            "\nProvide a concise summary of the key features and their purposes."
        )
        prompt_parts.append("\nFeature Summary:")
        prompt = "\n".join(prompt_parts)

        try:
            response = await self.llm_client.chat(
                messages=[{"role": "user", "content": prompt}],
                max_tokens=400,  # Increased to allow comprehensive summaries with no_std and format details
                temperature=0.2,
            )
            return response.strip()
        except Exception as e:
            self.logger.error(f"Error generating feature summary: {e}")
            return "Feature summary generation failed"

    async def _perform_comprehensive_analysis(self, context: str) -> Dict[str, Any]:
        """Perform comprehensive analysis of the crate"""
        prompt = (
            f"Analyze this Rust crate comprehensively and provide structured analysis in JSON format:\n\n"
            f"{context}\n\n"
            f"Provide analysis in this JSON format:\n"
            f"{{\n"
            f'    "analysis": {{\n'
            f'        "maintenance_status": "active|inactive|unknown",\n'
            f'        "community_health": "high|medium|low",\n'
            f'        "code_quality": "high|medium|low",\n'
            f'        "documentation_quality": "high|medium|low",\n'
            f'        "security_concerns": ["list", "of", "concerns"],\n'
            f'        "performance_characteristics": "description",\n'
            f'        "use_case_suitability": ["list", "of", "use", "cases"]\n'
            f"    }},\n"
            f'    "quality_score": 0.0-1.0,\n'
            f'    "use_case": "primary use case category",\n'
            f'    "factual_pairs": "3 factual statements and 3 counterfactual statements",\n'
            f'    "user_behavior": {{\n'
            f'        "target_audience": "description",\n'
            f'        "adoption_patterns": "description"\n'
            f"    }},\n"
            f'    "security": {{\n'
            f'        "risk_level": "low|medium|high",\n'
            f'        "vulnerabilities": ["list", "of", "concerns"]\n'
            f"    }}\n"
            f"}}"
        )

        try:
            response = await self.llm_client.chat_json(
                messages=[{"role": "user", "content": prompt}],
                max_tokens=1000,
                temperature=0.2,
            )
            return response
        except Exception as e:
            self.logger.error(f"Error performing comprehensive analysis: {e}")
            return {
                "analysis": {"error": str(e)},
                "quality_score": 0.5,
                "use_case": "unknown",
                "factual_pairs": "Analysis failed",
                "user_behavior": {"error": str(e)},
                "security": {"error": str(e)},
            }

    async def close(self) -> None:
        """Close LLM client resources"""
        await self.llm_client.aclose()

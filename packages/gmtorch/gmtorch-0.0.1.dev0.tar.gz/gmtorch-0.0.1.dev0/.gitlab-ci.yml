# You can override the included template(s) by including variable overrides
# SAST customization: https://docs.gitlab.com/ee/user/application_security/sast/#customizing-the-sast-settings
# Secret Detection customization: https://docs.gitlab.com/user/application_security/secret_detection/pipeline/configure
# Dependency Scanning customization: https://docs.gitlab.com/ee/user/application_security/dependency_scanning/#customizing-the-dependency-scanning-settings
# Container Scanning customization: https://docs.gitlab.com/ee/user/application_security/container_scanning/#customizing-the-container-scanning-settings
# Note that environment variables can be set in several places
# See https://docs.gitlab.com/ee/ci/variables/#cicd-variable-precedence
stages:
  - code-validation
  - test
  - build
  - docs
  - publish

include:
  - template: Security/SAST.gitlab-ci.yml
  - template: Security/Secret-Detection.gitlab-ci.yml
  - template: Jobs/Dependency-Scanning.latest.gitlab-ci.yml

variables:
  SECRET_DETECTION_ENABLED: "true"
  GIT_STRATEGY: clone
  GIT_CLEAN_FLAGS: -ffdx

# Set the tag to the runner you want to use (also works for individual jobs)
# KIT runners: https://docs.gitlab.kit.edu/en/gitlab_runner/

# CODE VALIDATION
secret_detection:
  stage: code-validation

linting:
  stage: code-validation
  image: python:3.12-alpine
  before_script:
    - pip install ruff mypy
    - pip install --no-deps -e .
  script:
    - mkdir -p reports
    - ruff check --force-exclude --extend-fixable=ERA001,F401,F841,T201,T203 --output-format=gitlab . > reports/ruff-codequality.json || true
    - ruff check --force-exclude --extend-fixable=ERA001,F401,F841,T201,T203 .
    - |
      echo "Checking code formatting..."
      if ! ruff format --check --force-exclude .; then
        echo ""
        echo "âŒ FORMATTING ERRORS DETECTED"
        echo "The following files need formatting:"
        ruff format --check --force-exclude . 2>&1 | grep "Would reformat:" || true
        echo ""
        echo "To fix locally, run:"
        echo "  ruff format ."
        echo ""
        exit 1
      fi
    - mypy --junit-xml=reports/mypy.xml src
  artifacts:
    when: always
    reports:
      junit: reports/mypy.xml
      codequality: reports/ruff-codequality.json
    paths:
      - reports/mypy.xml
      - reports/ruff-codequality.json
  allow_failure: false
  rules:
    - when: always

# TEST
sast:
  stage: test

# Tests run unit tests in the GPU-enabled Docker image
tests:
  stage: test
  image: nvidia/cuda:12.8.0-runtime-ubuntu24.04
  variables:
    UV_PYTHON_VERSIONS: "3.10 3.11 3.12 3.13 3.14"
  before_script:
    - apt-get update
    - apt-get install -y --no-install-recommends curl ca-certificates
    - curl -LsSf https://astral.sh/uv/install.sh | sh
    - export PATH="/root/.local/bin:$PATH"
    - uv python install $UV_PYTHON_VERSIONS
  script:
    - for version in $UV_PYTHON_VERSIONS; do
        echo "Running tests with Python $version";
        uv venv --python $version .venv-$version;
        uv pip install --python .venv-$version/bin/python --index-url https://download.pytorch.org/whl/cu128 torch torchvision;
        uv pip install --python .venv-$version/bin/python pytest pytest-xdist;
        uv pip install --python .venv-$version/bin/python -e .;
        .venv-$version/bin/pytest -n auto -v --junitxml=junit-$version.xml tests/ || exit 1;
      done
  artifacts:
    when: on_failure
    expire_in: 1 week
    reports:
      junit: junit-*.xml
    paths:
      - junit-*.xml
  # Change to gpu enabled runner and remove allow_failure once available
  tags:
    - local
  allow_failure: true
  rules:
    - when: always

# BUILD
build_project:
  stage: build
  image: python:3.12-alpine
  tags:
    - kgr2-instance-standard
  before_script:
    - apk add --no-cache git curl jq
    - pip install build
  script:
    - |
      BASE_VERSION=$(grep -m 1 '^version = ' pyproject.toml | tr -s ' ' | tr -d '"' | tr -d "'" | cut -d' ' -f3)
      PACKAGE_NAME=$(grep -m 1 '^name = ' pyproject.toml | tr -s ' ' | tr -d '"' | tr -d "'" | cut -d' ' -f3)
      BUILD_VERSION="$BASE_VERSION"

      echo "Fetching existing versions from PyPI..."
      PYPI_RESPONSE=$(curl -s "https://pypi.org/pypi/$PACKAGE_NAME/json" 2>/dev/null || echo '{"releases":{}}')

      if echo "$PYPI_RESPONSE" | jq -e '.releases' > /dev/null 2>&1; then
        PACKAGE_EXISTS=true
        echo "âœ… Package found on PyPI"
      else
        PACKAGE_EXISTS=false
        echo "â„¹ï¸  Package not yet published on PyPI"
      fi
      echo ""


      if [ "$CI_COMMIT_BRANCH" = "main" ] || [ "$CI_MERGE_REQUEST_TARGET_BRANCH_NAME" = "main" ]; then
        echo "Building stable version: $BUILD_VERSION"

        if [ "$PACKAGE_EXISTS" = "true" ] && echo "$PYPI_RESPONSE" | jq -e ".releases.\"$BUILD_VERSION\"" > /dev/null 2>&1; then
          echo "âŒ ERROR: Version $BUILD_VERSION already exists on PyPI!"
          echo ""
          echo "Please bump the version before merging to main:"
          echo "--> cz bump"
          exit 1
        fi
        
      elif [ "$CI_COMMIT_BRANCH" = "develop" ]; then
        echo "Checking existing dev versions on PyPI..."
        
        if [ "$PACKAGE_EXISTS" = "true" ]; then
          EXISTING_VERSIONS=$(echo "$PYPI_RESPONSE" | jq -r '.releases | keys[]' | grep "^${BASE_VERSION}\.dev" || echo "")
        else
          EXISTING_VERSIONS=""
        fi
        
        if [ -z "$EXISTING_VERSIONS" ]; then
          DEV_NUMBER=0
          echo "No existing dev versions found, starting with dev0"
        else
          echo "Existing dev versions:"
          echo "$EXISTING_VERSIONS"
          
          HIGHEST_DEV=$(echo "$EXISTING_VERSIONS" | sed "s/${BASE_VERSION}\.dev//" | sort -n | tail -1)
          DEV_NUMBER=$((HIGHEST_DEV + 1))
          echo "Highest existing: dev${HIGHEST_DEV}"
          echo "Using: dev${DEV_NUMBER}"
        fi
        
        BUILD_VERSION="${BASE_VERSION}.dev${DEV_NUMBER}"
        echo "Building development version: $BUILD_VERSION"

      else
        SHORT_SHA=$(git rev-parse --short=8 HEAD)
        BUILD_VERSION="${BASE_VERSION}.dev0+${SHORT_SHA}"
        echo "Building feature branch version: $BUILD_VERSION"
        echo "âš ï¸  Note: This version should NOT be published"
      fi

      sed -i "s/^version = .*/version = \"$BUILD_VERSION\"/" pyproject.toml

      echo "Updated pyproject.toml version:"
      grep -m 1 '^version = ' pyproject.toml
      echo ""

      echo "Building package..."
      python -m build --sdist --wheel --outdir dist

      echo "BUILD_VERSION=${BUILD_VERSION}" >> build.env
      echo "PACKAGE_NAME=${PACKAGE_NAME}" >> build.env

      echo ""
      echo "âœ… Successfully built version: $BUILD_VERSION"
      echo ""

  artifacts:
    when: on_success
    expire_in: 1 week
    reports:
      dotenv: build.env
    paths:
      - dist/*.whl
      - dist/*.tar.gz
  rules:
    - if: $CI_COMMIT_BRANCH == "main" || $CI_COMMIT_BRANCH == "develop"

# DOCS
generate_docs:
  stage: docs
  image: python:3.12-slim
  before_script:
    - apt-get update && apt-get install -y --no-install-recommends curl ca-certificates
    - curl -LsSf https://astral.sh/uv/install.sh | sh
    - export PATH="/root/.local/bin:$PATH"
    - uv venv .venv
    - uv pip install sphinx pydata-sphinx-theme
    - uv pip install --index-url https://download.pytorch.org/whl/cpu torch torchvision
    - uv pip install -e .
  script:
    - |
      echo "Generating Sphinx documentation..."
      .venv/bin/sphinx-apidoc -f --private -o docs/source/reference src/gmtorch
      .venv/bin/sphinx-build -b html docs/source docs/_build/html

      echo "âœ… Documentation generated"
  artifacts:
    paths:
      - docs/_build
  rules:
    - if: $CI_COMMIT_BRANCH == "main" || $CI_COMMIT_BRANCH == "develop"

# PUBLISH
publish_docs:
  stage: publish
  image: alpine:latest
  when: manual
  dependencies:
    - generate_docs
  before_script:
    - apk add --no-cache git
    - git config --global user.email "ci@gitlab.com"
    - git config --global user.name "GitLab CI"
  script:
    - |
      echo "Publishing documentation to GitHub Pages..."

      git clone --depth 1 --branch main https://oauth2:${GITHUB_DOCS_TOKEN}@github.com/gmtorch/gmtorch.github.io.git gh-pages
      
      # Remove old documentation
      rm -rf gh-pages/*
      
      # Copy new documentation
      cp -r docs/_build/html/* gh-pages/
      
      # Add .nojekyll file to prevent GitHub from processing with Jekyll
      touch gh-pages/.nojekyll
      
      # Commit and push
      cd gh-pages
      git add .
      git commit -m "Update documentation from GitLab CI - ${CI_COMMIT_SHORT_SHA}" || echo "No changes to commit"
      git push origin main
      
      echo "âœ… Documentation published to GitHub Pages"
  rules:
    - if: $CI_COMMIT_BRANCH == "main" || $CI_COMMIT_BRANCH == "develop"

# Publishes project to GitLab Package Registry and PyPI
publish_project:
  stage: publish
  image: python:3.12-alpine
  when: manual
  before_script:
    - pip install twine
  script:
    - |
      echo "Publishing version $BUILD_VERSION"
      echo "Package: $PACKAGE_NAME"
      
      echo "ðŸ“¦ Publishing to GitLab Package Registry..."
      TWINE_PASSWORD=${CI_JOB_TOKEN} TWINE_USERNAME=gitlab-ci-token python -m twine upload --repository-url ${CI_API_V4_URL}/projects/${CI_PROJECT_ID}/packages/pypi dist/*.whl dist/*.tar.gz
      echo "âœ… GitLab Package Registry upload complete"
      
      echo "ðŸ“¦ Publishing to PyPI..."
      python -m twine upload dist/*.whl dist/*.tar.gz
      echo "âœ… PyPI upload complete"
  variables:
    TWINE_USERNAME: __token__
    TWINE_PASSWORD: ${PYPI_API_TOKEN}
  rules:
    - if: $CI_COMMIT_BRANCH == "main" || $CI_COMMIT_BRANCH == "develop"

publish_docker:
  stage: publish
  tags:
    - kgr2-instance-standard
  image:
    name: ghcr.io/kaniko-build/dist/chainguard-dev-kaniko/executor:latest-debug
    entrypoint: [""]
  when: manual
  variables:
    DOCKER_CONFIG: "/kaniko/.docker"
    DOCKERFILE_PATH: "$CI_PROJECT_DIR/Dockerfile"
    CONTEXT: "$CI_PROJECT_DIR"
    CACHE_REPO: "$DOCKERHUB_REPO"
  before_script:
    - mkdir -p /kaniko/.docker
    - |
      if [ -z "${DOCKERHUB_USERNAME}" ] || [ -z "${DOCKERHUB_PASSWORD}" ]; then
        echo "DOCKERHUB_USERNAME or DOCKERHUB_PASSWORD is not set. Please define them in CI/CD variables." >&2
        exit 1
      fi
      cat > /kaniko/.docker/config.json <<EOF
      {
        "auths": {
          "https://index.docker.io/v1/": {
            "auth": "$(echo -n ${DOCKERHUB_USERNAME}:${DOCKERHUB_PASSWORD} | base64)"
          }
        }
      }
      EOF
  script:
    - |
      # Determine tags based on branch
      if [ "$CI_COMMIT_BRANCH" = "main" ]; then
        echo "Publishing stable Docker image: latest"
        /kaniko/executor \
          --context "$CONTEXT" \
          --dockerfile "$DOCKERFILE_PATH" \
          --destination "$DOCKERHUB_REPO:latest" \
          --cache=true \
          --cache-repo "$CACHE_REPO" \
          --compressed-caching=false \
          --skip-unused-stages \
          --single-snapshot \
          --target base
      elif [ "$CI_COMMIT_BRANCH" = "develop" ]; then
        echo "Publishing development Docker image: develop"
        /kaniko/executor \
          --context "$CONTEXT" \
          --dockerfile "$DOCKERFILE_PATH" \
          --destination "$DOCKERHUB_REPO:develop" \
          --cache=true \
          --cache-repo "$CACHE_REPO" \
          --compressed-caching=false \
          --skip-unused-stages \
          --single-snapshot \
          --target base
      fi
  rules:
    - if: $CI_COMMIT_BRANCH == "main" || $CI_COMMIT_BRANCH == "develop"
[build-system]
requires = ["setuptools>=61.0", "wheel"]
build-backend = "setuptools.build_meta"

[project]
name = "pyspark-storydoc"
dynamic = ["version"]
description = "Business-friendly data lineage documentation for PySpark"
readme = "README.md"
requires-python = ">=3.10"
license = "CC-BY-NC-SA-4.0"
authors = [
    {name = "PySpark StoryDoc Team", email = "team@pysparkstorydoc.com"},
]
maintainers = [
    {name = "PySpark StoryDoc Team", email = "team@pysparkstorydoc.com"},
]
classifiers = [
    "Development Status :: 3 - Alpha",
    "Intended Audience :: Developers",
    "Operating System :: OS Independent",
    "Programming Language :: Python :: 3",
    "Programming Language :: Python :: 3.10",
    "Programming Language :: Python :: 3.11",
    "Programming Language :: Python :: 3.12",
    "Topic :: Software Development :: Libraries :: Python Modules",
    "Topic :: Software Development :: Documentation",
    "Topic :: Scientific/Engineering :: Information Analysis",
]
keywords = ["pyspark", "spark", "lineage", "visualization", "business", "data-science"]
dependencies = [
    "pyspark>=3.5.0",
    "pandas>=2.0.0",
    "numpy>=1.24.0",
    "matplotlib>=3.7.0",
    "seaborn>=0.12.0",
    "pyyaml>=6.0",
    "jinja2>=3.1.0",
]

[project.optional-dependencies]
dev = [
    "pytest>=7.4.0",
    "pytest-cov>=4.1.0",
    "pytest-mock>=3.11.0",
    "pytest-timeout>=2.4.0",
    "black>=23.0.0",
    "isort>=5.12.0",
    "flake8>=6.0.0",
    "mypy>=1.5.0",
    "pre-commit>=3.3.0",
]
docs = [
    "sphinx>=7.1.0",
    "sphinx-rtd-theme>=1.3.0",
    "sphinx-autodoc-typehints>=1.24.0",
    "myst-parser>=2.0.0",
]
visualization = [
    "graphviz>=0.20.0",
]
all = [
    "pyspark-storydoc[dev,docs,visualization]",
]

[project.urls]
Homepage = "https://github.com/pyspark-storydoc/pyspark-storydoc"
Documentation = "https://pyspark-storydoc.readthedocs.io/"
Repository = "https://github.com/pyspark-storydoc/pyspark-storydoc.git"
"Bug Reports" = "https://github.com/pyspark-storydoc/pyspark-storydoc/issues"
Changelog = "https://github.com/pyspark-storydoc/pyspark-storydoc/blob/main/CHANGELOG.md"

[project.scripts]
pyspark-storydoc = "pyspark_storydoc.cli:main"

[tool.setuptools.dynamic]
version = {attr = "pyspark_storydoc.version.__version__"}

[tool.setuptools.packages.find]
where = ["."]
include = ["pyspark_storydoc*"]
exclude = ["tests*"]

[tool.setuptools.package-data]
pyspark_storydoc = [
    "config/*.yml",
    "config/*.yaml",
    "visualization/templates/*.j2",
    "visualization/templates/*.html",
]

# Tool configurations
[tool.black]
line-length = 88
target-version = ['py38', 'py39', 'py310', 'py311']
include = '\.pyi?$'
extend-exclude = '''
/(
  # directories
  \.eggs
  | \.git
  | \.hg
  | \.mypy_cache
  | \.tox
  | \.venv
  | build
  | dist
)/
'''

[tool.isort]
profile = "black"
multi_line_output = 3
line_length = 88
known_first_party = ["pyspark_storydoc"]
known_third_party = ["pyspark", "pandas", "numpy", "yaml", "jinja2"]

[tool.mypy]
python_version = "3.8"
warn_return_any = true
warn_unused_configs = true
disallow_untyped_defs = true
disallow_incomplete_defs = true
check_untyped_defs = true
disallow_untyped_decorators = true
no_implicit_optional = true
warn_redundant_casts = true
warn_unused_ignores = true
warn_no_return = true
warn_unreachable = true
strict_equality = true

[[tool.mypy.overrides]]
module = [
    "pyspark.*",
    "graphviz.*",
]
ignore_missing_imports = true

[tool.pytest.ini_options]
testpaths = ["tests"]
# Exclude standalone demo/manual test scripts that create their own Spark sessions
# These files match test_*.py pattern but are not pytest tests - they're standalone scripts
python_files = ["test_*.py", "!test_fork*.py", "!test_*_tracking.py", "!test_*_lineage.py", "!test_*_capture.py", "!test_*_preservation.py", "!test_pyspark_passthrough.py"]
python_classes = ["Test*"]
python_functions = ["test_*"]
timeout = 1800
timeout_method = "thread"
addopts = [
    "--cov=pyspark_storydoc",
    "--cov-report=html",
    "--cov-report=term-missing",
    "--cov-report=xml",
    "--cov-fail-under=80",
    "--strict-markers",
    "--strict-config",
    "--verbose",
    "-m", "not slow and not manual",
    # Note: Tests run sequentially by default (pytest-xdist not installed)
    # The delays in conftest.py ensure proper Spark initialization
]
markers = [
    "slow: marks tests as slow (deselect with '-m \"not slow\"')",
    "manual: marks tests as manual tests that require long execution time",
    "integration: marks tests as integration tests",
    "unit: marks tests as unit tests",
    "governance: marks tests as governance feature tests",
    "visualization: marks tests as visualization-related tests",
    "spark: marks tests that require an active Spark session (deselect with '-m \"not spark\"')",
]

[tool.coverage.run]
source = ["pyspark_storydoc"]
omit = [
    "*/tests/*",
    "*/test_*",
    "setup.py",
]

[tool.coverage.report]
exclude_lines = [
    "pragma: no cover",
    "def __repr__",
    "if self.debug:",
    "if settings.DEBUG",
    "raise AssertionError",
    "raise NotImplementedError",
    "if 0:",
    "if __name__ == .__main__.:",
    "class .*\\bProtocol\\):",
    "@(abc\\.)?abstractmethod",
]
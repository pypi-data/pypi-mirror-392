{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Python GenAI Demo\n",
    "\n",
    "This notebook demonstrates the Python wrapper for Chrome's built-in AI Prompt API.\n",
    "\n",
    "**Requirements:**\n",
    "- Chrome browser with Prompt API enabled\n",
    "- Running in JupyterLab or Jupyter Notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -q anywidget ipywidgets wiki3_ai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%gui asyncio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c95d5dad70ca4d4687cac464edb69979",
       "version_major": 2,
       "version_minor": 1
      },
      "text/plain": [
       "<__main__.LanguageModelWidget object at 0xffffa4726900>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import anywidget\n",
    "import traitlets\n",
    "\n",
    "\n",
    "class LanguageModelWidget(anywidget.AnyWidget):\n",
    "    _esm = \"\"\"\n",
    "    function render({ model, el }) {\n",
    "      if (!('LanguageModel' in self)) {\n",
    "        model.set(\"result\", `Your browser doesn't support the Prompt API. If you're on Chrome, join the <a href=\"https://goo.gle/chrome-ai-dev-preview-join\">Early Preview Program</a> to enable it.`);\n",
    "      } else {\n",
    "        model.set(\"result\", \"calling\");\n",
    "        self.LanguageModel.availability()\n",
    "          .then(availability => { model.set(\"result\", availability); model.save_changes(); })\n",
    "          .catch(error => { model.set(\"result\", error); model.save_changes(); });\n",
    "      }\n",
    "      model.on(\"change:count\", () => {\n",
    "        model.set(\"result\", \"checking\");\n",
    "        console.log(\"checking\");\n",
    "        self.LanguageModel.availability()\n",
    "          .then(availability => { model.set(\"result\", availability); model.save_changes(); })\n",
    "          .catch(error => { model.set(\"result\", error); model.save_changes(); });\n",
    "      });\n",
    "    }\n",
    "\texport default { render };\n",
    "    \"\"\"\n",
    "    count = traitlets.Int(0).tag(sync=True)\n",
    "    prompt = traitlets.Unicode(\"Foo\").tag(sync=True)\n",
    "    result = traitlets.Unicode(\"Bar\").tag(sync=True)\n",
    "\n",
    "\n",
    "counter = LanguageModelWidget()\n",
    "counter.count = 42\n",
    "counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "42"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "counter.count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'available'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "counter.result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "counter.count = 22"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'available'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "counter.result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the library\n",
    "from wiki3_ai import LanguageModel, Availability, LanguageModelWidget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1e861b5623144afab3381e24881c2fe5",
       "version_major": 2,
       "version_minor": 1
      },
      "text/plain": [
       "<python_genai.language_model.LanguageModelWidget object at 0xffff96bb1bd0>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "widget = LanguageModelWidget()\n",
    "widget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "widget.request"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sending request ID:  fd35cacb-924b-41bb-818d-1121ce0c0111\n",
      "Sent request:  {'id': 'fd35cacb-924b-41bb-818d-1121ce0c0111', 'method': 'availability', 'params': {'options': {}}}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'available'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "options_dict = {}\n",
    "\n",
    "params = {\"options\": options_dict}\n",
    "result = await widget.send_request(\"availability\", params)\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import uuid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sending request ID:  3c3d6047-25e1-4243-a75f-a4293d253c9b\n",
      "Sent request:  {'id': '3c3d6047-25e1-4243-a75f-a4293d253c9b', 'method': 'create', 'params': {'sessionId': 'e092192f-71a7-4c14-9468-e86369b9b106', 'options': {}}}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'sessionId': 'e092192f-71a7-4c14-9468-e86369b9b106',\n",
       " 'topK': 3,\n",
       " 'temperature': 1,\n",
       " 'inputUsage': 0,\n",
       " 'inputQuota': 9216}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "session_id = str(uuid.uuid4())\n",
    "\n",
    "options_dict = {}\n",
    "\n",
    "params = {\"sessionId\": session_id, \"options\": options_dict}\n",
    "result = await widget.send_request(\"create\", params)\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a455e1e621b64858af693500f69d41df",
       "version_major": 2,
       "version_minor": 1
      },
      "text/plain": [
       "<python_genai.language_model.LanguageModelWidget object at 0xffff80ddd160>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lm = LanguageModel()\n",
    "lm.widget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sending request ID:  04882af1-4b3f-424d-a6f5-a2ff45743d65\n",
      "Sent request:  {'id': '04882af1-4b3f-424d-a6f5-a2ff45743d65', 'method': 'create', 'params': {'sessionId': '24259487-6741-41e6-b4e8-6dc87dccd422', 'options': {}}}\n",
      "Created session result:  {'sessionId': '24259487-6741-41e6-b4e8-6dc87dccd422', 'topK': 3, 'temperature': 1, 'inputUsage': 0, 'inputQuota': 9216}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'24259487-6741-41e6-b4e8-6dc87dccd422'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "session_id = await lm.create()\n",
    "session_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"foo\", \"bar\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check Availability\n",
    "\n",
    "First, let's check if the Prompt API is available in your browser."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sending request ID:  e51cf041-55c1-4cfc-bb2e-9532702664ff\n",
      "Sent request:  {'id': 'e51cf041-55c1-4cfc-bb2e-9532702664ff', 'method': 'availability', 'params': {'options': {}}}\n",
      "Model availability: Availability.AVAILABLE\n",
      "\n",
      "✅ API is ready to use!\n"
     ]
    }
   ],
   "source": [
    "# Check if the API is available\n",
    "availability = await lm.availability()\n",
    "print(f\"Model availability: {availability}\")\n",
    "\n",
    "if availability == Availability.UNAVAILABLE:\n",
    "    print(\"\\n❌ Chrome Prompt API is not available.\")\n",
    "    print(\"Make sure you're running in Chrome with the API enabled.\")\n",
    "elif availability == Availability.DOWNLOADING:\n",
    "    print(\"\\n⏳ Model is downloading. Please wait and try again.\")\n",
    "elif availability == Availability.AVAILABLE:\n",
    "    print(\"\\n✅ API is ready to use!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Model Parameters\n",
    "\n",
    "Let's see what parameters the model supports."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sending request ID:  70d9b1af-0e74-4ed6-9ced-f67923f15cab\n",
      "Sent request:  {'id': '70d9b1af-0e74-4ed6-9ced-f67923f15cab', 'method': 'params', 'params': {}}\n"
     ]
    },
    {
     "ename": "Exception",
     "evalue": "DataCloneError: Failed to execute 'structuredClone' on 'Window': LanguageModelParams object could not be cloned.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mException\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[13]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m params = \u001b[38;5;28;01mawait\u001b[39;00m lm.params()\n\u001b[32m      2\u001b[39m params\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspaces/python-ai/python_genai/language_model.py:364\u001b[39m, in \u001b[36mLanguageModel.params\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    362\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mparams\u001b[39m(\u001b[38;5;28mself\u001b[39m) -> Optional[LanguageModelParams]:\n\u001b[32m    363\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Get language model parameters.\"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m364\u001b[39m     result = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m.widget.send_request(\u001b[33m\"\u001b[39m\u001b[33mparams\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    366\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m result \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    367\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspaces/python-ai/python_genai/language_model.py:309\u001b[39m, in \u001b[36mLanguageModelWidget.send_request\u001b[39m\u001b[34m(self, method, params)\u001b[39m\n\u001b[32m    306\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m future.done():\n\u001b[32m    307\u001b[39m     \u001b[38;5;28;01mawait\u001b[39;00m asyncio.sleep(\u001b[32m0\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m309\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m future\n",
      "\u001b[31mException\u001b[39m: DataCloneError: Failed to execute 'structuredClone' on 'Window': LanguageModelParams object could not be cloned."
     ]
    }
   ],
   "source": [
    "params = await lm.params()\n",
    "params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'params' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mparams\u001b[49m:\n\u001b[32m      2\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mDefault temperature: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mparams.default_temperature\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m      3\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mMax temperature: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mparams.max_temperature\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mNameError\u001b[39m: name 'params' is not defined"
     ]
    }
   ],
   "source": [
    "if params:\n",
    "    print(f\"Default temperature: {params.default_temperature}\")\n",
    "    print(f\"Max temperature: {params.max_temperature}\")\n",
    "    print(f\"Default top-K: {params.default_top_k}\")\n",
    "    print(f\"Max top-K: {params.max_top_k}\")\n",
    "else:\n",
    "    print(\"Model parameters not available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple Prompt\n",
    "\n",
    "Create a session and send a simple prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a session\n",
    "session_id = await lm.create()\n",
    "session_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sending request ID:  0b648abf-090a-4f1a-afbf-67d59fb57701\n",
      "Sent request:  {'id': '0b648abf-090a-4f1a-afbf-67d59fb57701', 'method': 'prompt', 'params': {'sessionId': '24259487-6741-41e6-b4e8-6dc87dccd422', 'input': 'Write a haiku about Python programming.', 'options': {}}}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Clean, readable code,\\nLogic flows, a gentle stream,\\nWorlds built with Python. \\n\\n\\n\\n'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Send a prompt\n",
    "result = await lm.prompt(\"Write a haiku about Python programming.\")\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Streaming Response\n",
    "\n",
    "Stream the response as it's generated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Response: \", end=\"\")\n",
    "async for chunk in session.prompt_streaming(\"Tell me a short joke about computers.\"):\n",
    "    print(chunk, end=\"\", flush=True)\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## System Prompt\n",
    "\n",
    "Use a system prompt to set the context for the conversation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a session with a system prompt\n",
    "assistant_session = await LanguageModel.create(\n",
    "    {\n",
    "        \"initialPrompts\": [\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": \"You are a helpful Python programming assistant who gives concise answers.\",\n",
    "            }\n",
    "        ]\n",
    "    }\n",
    ")\n",
    "\n",
    "# Ask a question\n",
    "result = await assistant_session.prompt(\"How do I read a CSV file in Python?\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-turn Conversation\n",
    "\n",
    "Have a conversation with context maintained across prompts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First message\n",
    "result1 = await assistant_session.prompt(\"What is a list comprehension?\")\n",
    "print(\"Assistant:\", result1)\n",
    "print()\n",
    "\n",
    "# Follow-up message (the assistant remembers the context)\n",
    "result2 = await assistant_session.prompt(\"Can you show me an example?\")\n",
    "print(\"Assistant:\", result2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check Token Usage\n",
    "\n",
    "Monitor how many tokens you've used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Current usage: {assistant_session.input_usage}/{assistant_session.input_quota} tokens\")\n",
    "\n",
    "# Measure how many tokens a prompt would use\n",
    "usage = await assistant_session.measure_input_usage(\"What is machine learning?\")\n",
    "print(f\"\\nThis prompt would use approximately {usage} tokens\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Structured Output\n",
    "\n",
    "Use JSON schema to get structured responses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Define a JSON schema\n",
    "schema = {\n",
    "    \"type\": \"object\",\n",
    "    \"required\": [\"sentiment\", \"score\"],\n",
    "    \"properties\": {\n",
    "        \"sentiment\": {\"type\": \"string\", \"enum\": [\"positive\", \"negative\", \"neutral\"]},\n",
    "        \"score\": {\"type\": \"number\", \"minimum\": 0, \"maximum\": 1},\n",
    "    },\n",
    "}\n",
    "\n",
    "# Get structured response\n",
    "result = await session.prompt(\n",
    "    \"Analyze this review: The product exceeded all my expectations! Absolutely amazing!\",\n",
    "    {\"responseConstraint\": schema},\n",
    ")\n",
    "\n",
    "# Parse the JSON response\n",
    "data = json.loads(result)\n",
    "print(f\"Sentiment: {data['sentiment']}\")\n",
    "print(f\"Score: {data['score']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Session Cloning\n",
    "\n",
    "Clone a session to create different conversation branches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start a story\n",
    "story_session = await LanguageModel.create(\n",
    "    {\"initialPrompts\": [{\"role\": \"system\", \"content\": \"You are a creative storyteller.\"}]}\n",
    ")\n",
    "\n",
    "await story_session.prompt(\"Once upon a time, there was a dragon.\")\n",
    "\n",
    "# Create two different story branches\n",
    "branch1 = await story_session.clone()\n",
    "branch2 = await story_session.clone()\n",
    "\n",
    "result1 = await branch1.prompt(\"The dragon was friendly and helpful.\")\n",
    "print(\"Branch 1:\", result1)\n",
    "print()\n",
    "\n",
    "result2 = await branch2.prompt(\"The dragon was fierce and terrifying.\")\n",
    "print(\"Branch 2:\", result2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleanup\n",
    "\n",
    "Destroy sessions when you're done to free up resources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "await session.destroy()\n",
    "await assistant_session.destroy()\n",
    "await story_session.destroy()\n",
    "await branch1.destroy()\n",
    "await branch2.destroy()\n",
    "\n",
    "print(\"✅ All sessions destroyed\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

import os
import json
import time
from typing import List, Dict, Any
import pandas as pd
import matplotlib.pyplot as plt
import gradio as gr
from dotenv import set_key, load_dotenv
from ragmint.autotuner import AutoRAGTuner
from ragmint.tuner import RAGMint
from ragmint.leaderboard import Leaderboard
from ragmint.explainer import explain_results
from ragmint.qa_generator import generate_validation_qa
from matplotlib.ticker import MultipleLocator
import yaml
from pathspec import PathSpec

# ----------------------------------------------------------------------
# CONFIGURATION
# ----------------------------------------------------------------------
DATA_DIR = "data/docs"
VAL_DIR = "data/docs"
LEADERBOARD_PATH = "leaderboard.jsonl"
LOGO_PATH = "https://raw.githubusercontent.com/andyolivers/ragmint/main/src/ragmint/assets/img/ragmint_logo.png"
ENV_PATH = ".env"

BG_COLOR = "#F7F4ED"       # soft beige background
PRIMARY_GREEN = "#1D5C39"  # brand green

os.makedirs(DATA_DIR, exist_ok=True)
os.makedirs(os.path.dirname(LEADERBOARD_PATH) or ".", exist_ok=True)
leaderboard = Leaderboard(storage_path=LEADERBOARD_PATH)

load_dotenv(ENV_PATH)

# ----------------------------------------------------------------------
# UTILITY FUNCTIONS
# ----------------------------------------------------------------------
def generate_qa_dataset():
    try:
        output_path = os.path.join(DATA_DIR, "validation_qa.json")
        generate_validation_qa(
            docs_path=DATA_DIR,                   # folder with .txt documents
            output_path=output_path,              # output JSON
            llm_model="gemini-2.5-flash-lite",    # or "claude-3-opus-20240229"
            batch_size=5,
            sleep_between_batches=2,
            min_q=3,
            max_q=25
        )
        return f"‚úÖ QA dataset generated at {output_path}"
    except Exception as e:
        return f"‚ö†Ô∏è Error generating QA dataset: {str(e)}"

def save_uploaded_files(files):
    saved_files = []
    for f in files:
        # f is a gradio.files.NamedString
        src_path = f.name  # full path to the temp file
        filename = os.path.basename(src_path)  # extract just the name
        dest_path = os.path.join(DATA_DIR, filename)
        with open(src_path, "rb") as src, open(dest_path, "wb") as dst:
            dst.write(src.read())
        saved_files.append(filename)
    return saved_files

def handle_validation_upload(file):
    if not file:
        return "‚ö†Ô∏è Please select a file first."
    dest_path = os.path.join(DATA_DIR, "validation_qa.json")
    with open(file.name, "rb") as src, open(dest_path, "wb") as dst:
        dst.write(src.read())
    return "‚úÖ Validation file saved as validation_qa.json"

# --- Validation toggle ---
def toggle_validation_inputs(choice):
    return (
        gr.update(visible=(choice == "Upload JSON")),
        gr.update(visible=(choice == "HuggingFace Dataset"), interactive=True),
        gr.update(visible=(choice == "LLM Dataset Generator")),
        f"Selected: {choice}"
    )

# --- API key save function with type ---
def save_api_key(api_key: str, provider: str):
    if not api_key:
        return "‚ö†Ô∏è No API key provided."
    if provider == "Google":
        set_key(ENV_PATH, "GOOGLE_API_KEY", api_key)
    elif provider == "Anthropic":
        set_key(ENV_PATH, "ANTHROPIC_API_KEY", api_key)
    return f"‚úÖ {provider} API key saved to .env."

def read_leaderboard_df():
    if not os.path.exists(LEADERBOARD_PATH) or os.path.getsize(LEADERBOARD_PATH) == 0:
        return pd.DataFrame()
    return pd.read_json(LEADERBOARD_PATH, lines=True)


def plot_score_scatter(results: List[Dict[str, Any]], best_index: int = None):
    if not results:
        fig, ax = plt.subplots()
        ax.text(0.5, 0.5, "No results yet", ha="center", va="center")
        return fig

    scores = [r.get("faithfulness", r.get("score", 0)) for r in results]
    fig, ax = plt.subplots()
    ax.scatter(range(len(scores)), scores, color="#1D5C39", label="Trials", alpha=0.7)

    if best_index is not None and 0 <= best_index < len(scores):
        ax.scatter(best_index, scores[best_index], color="gold", s=120, edgecolor="black", label="Best Run")

    ax.set_title("Trial Scores", color="#1D5C39")
    ax.set_xlabel("Trial #")
    ax.set_ylabel("Faithfulness")
    ax.legend()

    # Force integer steps on the X axis
    ax.xaxis.set_major_locator(MultipleLocator(1))
    ax.set_xlim(-0.5, len(scores) - 0.5)  # better spacing on the ends

    plt.tight_layout()
    return fig

def export_best_config(best_json_str: str):
    """
    Export best configuration as config.yaml with only selected fields.
    """
    try:
        best = json.loads(best_json_str)
        allowed_fields = ["retriever", "embedding_model", "reranker", "chunk_size", "overlap", "strategy"]
        config = {k: best[k] for k in allowed_fields if k in best}

        if not config:
            return "‚ö†Ô∏è No valid configuration fields found."

        config_path = os.path.join(DATA_DIR, "config.yaml")
        with open(config_path, "w") as f:
            yaml.dump(config, f, default_flow_style=False, sort_keys=False)

        return f"‚úÖ Exported configuration to {config_path}"

    except Exception as e:
        return f"‚ö†Ô∏è Error exporting config: {str(e)}"


# ----------------------------------------------------------------------
# ACTION HANDLERS
# ----------------------------------------------------------------------
def handle_upload(files):
    if not files:
        return "No files provided."
    saved = save_uploaded_files(files)
    return f"‚úÖ Saved {len(saved)} files to {DATA_DIR}."

def do_auto_tune(
    embedding_model: str,
    num_chunk_pairs: int,
    search_type: str,
    trials: int,
    validation_choice: str,
    hf_dataset: str = None
):
    tuner = AutoRAGTuner(docs_path=DATA_DIR)
    rec = tuner.recommend(embedding_model=embedding_model, num_chunk_pairs=num_chunk_pairs)
    num_chunk_pairs = int(num_chunk_pairs)

    # FIX: use num_chunk_pairs instead of None
    chunk_candidates = tuner.suggest_chunk_sizes(
        model_name=rec["embedding_model"],
        num_pairs=num_chunk_pairs,
        step=20
    )
    chunk_sizes = sorted({c for c, _ in chunk_candidates})
    overlaps = sorted({o for _, o in chunk_candidates})

    rag = RAGMint(
        docs_path=DATA_DIR,
        retrievers=[rec["retriever"]],
        embeddings=[rec["embedding_model"]],
        rerankers=["mmr"],
        chunk_sizes=chunk_sizes,
        overlaps=overlaps,
        strategies=[rec["strategy"]],
    )

    start_time = time.time()

    validation_set = None
    if validation_choice == "Upload JSON" or validation_choice == "LLM Dataset Generator":
        validation_path = os.path.join(DATA_DIR, "validation_qa.json")
        if os.path.exists(validation_path):
            validation_set = validation_path
    elif validation_choice == "HuggingFace Dataset" and hf_dataset:
        validation_set = hf_dataset.strip()


    try:
        best, results = rag.optimize(
            validation_set=validation_set,
            metric="faithfulness",
            search_type=search_type,
            trials=trials,
        )

        elapsed = time.time() - start_time

        run_id = f"run_{int(time.time())}"
        corpus_stats = {
            "num_docs": len(rag.documents),
            "avg_len": sum(len(d.split()) for d in rag.documents) / max(1, len(rag.documents)),
            "corpus_size": sum(len(d) for d in rag.documents),
        }

        leaderboard.upload(
            run_id=run_id,
            best_config=best,
            best_score=best.get("faithfulness", best.get("score", 0.0)),
            all_results=results,
            documents=os.listdir(DATA_DIR),
            model=best.get("embedding_model", rec["embedding_model"]),
            corpus_stats=corpus_stats,
        )

        # --- Plot scatter ---
        best_index = next((i for i, r in enumerate(results)
                           if r.get("faithfulness") == best.get("faithfulness")), None)
        fig = plot_score_scatter(results, best_index)

        # --- Inline explanation (string/markdown) ---
        explanation = explain_results(best, results, corpus_stats=corpus_stats)

        # Return exactly three outputs matching your Gradio components:
        # 1) best configuration JSON (string)
        # 2) Matplotlib figure
        # 3) explanation markdown/string
        return (
            json.dumps(best, indent=2),
            fig,
            explanation
        )

    except Exception as e:
        # Return a placeholder figure if there‚Äôs an error and match output types
        fig, ax = plt.subplots()
        ax.text(0.5, 0.5, str(e), ha="center", va="center", color="red")
        ax.axis("off")
        # 1) error string for best_json  2) placeholder fig  3) explanation/error markdown
        return "Error during tuning", fig, f"‚ö†Ô∏è {str(e)}"

def show_leaderboard_table():
    df = read_leaderboard_df()
    if df.empty:
        return "No runs yet.", ""

    # Convert dict -> pretty JSON string for display
    df["best_config"] = df["best_config"].apply(
        lambda x: json.dumps(x, indent=2) if isinstance(x, dict) else str(x)
    )

    table = df[["run_id", "timestamp", "best_score", "model", "best_config"]] \
                .sort_values("best_score", ascending=False)

    return table, df.to_json(orient="records", indent=2)



def do_explain(run_id: str, llm_model: str = "gemini-2.5-flash-lite"):
    entry = leaderboard.all_results()
    matched = [r for r in entry if r["run_id"] == run_id]
    if not matched:
        return f"Run {run_id} not found."
    record = matched[0]
    best = record["best_config"]
    all_results = record["all_results"]
    corpus_stats = record.get("corpus_stats", {})
    return explain_results(best, all_results, corpus_stats=corpus_stats, model=llm_model)


def analytics_overview():
    df = read_leaderboard_df()
    if df.empty:
        return "No data yet."
    top_score = df["best_score"].max()
    runs = len(df)
    latencies = []
    for row in df["all_results"]:
        for r in row:
            if isinstance(r, dict) and "latency" in r:
                latencies.append(r["latency"])
    avg_latency = sum(latencies) / len(latencies) if latencies else None
    summary = {
        "num_runs": runs,
        "top_score": float(top_score),
        "avg_trial_latency": float(avg_latency) if avg_latency else None,
    }
    return json.dumps(summary, indent=2)

def list_corpus_files():
    try:
        gitignore_path = ".gitignore"

        # Load .gitignore patterns if the file exists
        if os.path.exists(gitignore_path):
            with open(gitignore_path, "r") as f:
                spec = PathSpec.from_lines("gitwildmatch", f)
        else:
            spec = None

        all_items = os.listdir(DATA_DIR)
        files = []

        for item in all_items:
            full_path = os.path.join(DATA_DIR, item)

            # Skip validation file
            if item == "validation_qa.json":
                continue

            # Skip directories
            if os.path.isdir(full_path):
                continue

            # If .gitignore exists ‚Üí filter ignored files
            if spec and spec.match_file(item):
                continue

            files.append(item)

        if not files:
            return "No corpus files yet."

        return "\n".join(files)

    except Exception as e:
        return f"Error reading files: {str(e)}"


# ----------------------------------------------------------------------
# CUSTOM STYLING
# ----------------------------------------------------------------------

custom_css = f"""

#logo {{
    display: flex;
    align-items: center;
    justify-content: center; /* center horizontally */
    padding: 0;            /* remove padding */
    margin: 0;             /* remove margin */
    box-shadow: none;      /* remove shadow */
    border: none;          /* remove any border */
}}

#logo img {{
    height: 80px;
    width: auto;
}}

#logo button {{
    background-color: rgba(29, 92, 57, 0.1); !important;
    color: white !important;
    border-radius: 12px !important;
    font-weight: 600 !important;
}}

.custom-summary {{
    background-color: #f4f4f4;               /* clean light grey background */
    border: 1px solid rgba(0, 0, 0, 0.08);   /* subtle border */
    border-radius: 16px;
    padding: 16px;
    box-shadow: 0 2px 8px rgba(0, 0, 0, 0.05);   /* soft depth */
    transition: all 0.3s ease-in-out;
}}

"""


# ----------------------------------------------------------------------
# BUILD GRADIO APP
# ----------------------------------------------------------------------

with gr.Blocks(css=custom_css, theme=gr.themes.Ocean()) as demo:
    with gr.Row(elem_id="logo"):
        gr.Image(value=LOGO_PATH, show_label=False, interactive=False, elem_id="logo_img")

    gr.Markdown(f"# Ragmint - RAG Automated Tuning")
    gr.Markdown("Auto-tune your RAG pipeline, benchmark performance, visualize results, and get AI-driven insights.")

    # --- Corpus & API Key Upload ---
    with gr.Tab("üìÇ Configuration"):
        with gr.Row():
            # --- LEFT SIDE: Main Steps ---
            with gr.Column(scale=3):
                gr.Markdown("### 1Ô∏è‚É£ Upload Corpus Files")
                uploader = gr.File(label="Upload corpus files", file_count="multiple")
                upload_btn = gr.Button("Upload Files", variant="primary")

                gr.Markdown("### 2Ô∏è‚É£ Add LLM Key")
                api_provider = gr.Radio(
                    label="Provider",
                    choices=["Google", "Anthropic"],
                    value="Google"
                )

                api_key_input = gr.Textbox(
                    label="API Key",
                    placeholder="Paste your API key here",
                    type="password"
                )
                save_api_btn = gr.Button("Save API Key", variant="primary")


                gr.Markdown("### 3Ô∏è‚É£ Validation Dataset (Optional)")
                validation_source = gr.Radio(
                    label="Validation Source",
                    choices=["Default File", "Upload JSON", "HuggingFace Dataset", "LLM Dataset Generator"],
                    value="Default File"
                )

                with gr.Row(visible=False) as llm_generate_row:
                    generate_qa_btn = gr.Button("Generate QA Dataset", variant="primary")
                    generate_qa_status = gr.Textbox(label="Generation Status", interactive=False)

                with gr.Row(visible=False) as validation_upload_row:
                    validation_file = gr.File(
                        label="Upload validation_qa.json",
                        file_count="single",
                        interactive=True
                    )
                    upload_validation_btn = gr.Button("Upload Validation File", variant="primary")

                validation_hf_dataset = gr.Textbox(
                    label="HuggingFace Dataset Name",
                    placeholder="e.g. squad, hotpot_qa, or your own dataset",
                    interactive=True,
                    visible=False
                )



            # --- RIGHT SIDE: Status Summary ---
            with gr.Column(scale=1, elem_classes=["custom-summary"]):
                gr.Markdown("### ‚öôÔ∏è Configuration")
                gr.Markdown("Monitor your current setup below:")
                upload_status = gr.Textbox(label="File Upload Status", interactive=False)
                save_status = gr.Textbox(label="API Key Status", interactive=False)
                validation_status = gr.Textbox(label="Validation Selection", interactive=False)
                corpus_files_box = gr.Textbox(label="Corpus Files", interactive=False)
                corpus_files_box.value = list_corpus_files()

        # --- Event bindings ---
        upload_btn.click(
            fn=handle_upload,
            inputs=[uploader],
            outputs=[upload_status]
        ).then(
            fn=list_corpus_files,
            inputs=None,
            outputs=[corpus_files_box]
        )
        save_api_btn.click(
            fn=save_api_key,
            inputs=[api_key_input, api_provider],
            outputs=[save_status]
        )
        upload_validation_btn.click(
            fn=handle_validation_upload,
            inputs=[validation_file],
            outputs=[validation_status]
        ).then(
            fn=list_corpus_files,
            inputs=None,
            outputs=[corpus_files_box]
        )
        validation_source.change(
            fn=toggle_validation_inputs,
            inputs=[validation_source],
            outputs=[validation_upload_row, validation_hf_dataset, llm_generate_row, validation_status]
        )

        generate_qa_btn.click(
            fn=generate_qa_dataset,
            inputs=None,
            outputs=[generate_qa_status]
        ).then(
            fn=list_corpus_files,
            inputs=None,
            outputs=[corpus_files_box]
        )

    # --- Unified AutoTune ---
    with gr.Tab("ü§ñ AutoTune"):
        # üåø Custom CSS for dashboard-like cards
        gr.HTML("""
        <style>
        .card {
            background-color: #f4f4f4;               /* clean light grey background */
            border: 1px solid rgba(0, 0, 0, 0.08);   /* subtle border */
            border-radius: 16px;
            padding: 16px;
            box-shadow: 0 2px 8px rgba(0, 0, 0, 0.05);   /* soft depth */
            transition: all 0.3s ease-in-out;
        }
        .card:hover {
            box-shadow: 0 4px 12px rgba(0,0,0,0.08);
        }
        .section-title {
            font-size: 1.1rem;
            font-weight: 600;
            color: #1D5C39;
            margin-bottom: 8px;
        }
        </style>
        """)

        # ‚öôÔ∏è Settings
        with gr.Column(elem_classes=["card"]):
            gr.Markdown("‚öôÔ∏è <span class='section-title'>AutoTuner Settings</span>")
            with gr.Accordion("Advanced Settings", open=False):
                embed_model = gr.Textbox(
                    value="sentence-transformers/all-MiniLM-L6-v2",
                    label="Embedding Model",
                    info="Model used to generate text embeddings for retrieval."
                )
                num_pairs = gr.Number(
                    value=5,
                    label="Chunk Candidates",
                    info="Number of chunk size-overlap pairs to test."
                )
                search_type = gr.Dropdown(
                    choices=["random", "grid", "bayesian"],
                    value="grid",
                    label="Search Type",
                    info="Method used for optimization search over hyperparameters."
                )
                trials = gr.Slider(
                    minimum=1, maximum=50, step=1, value=5,
                    label="Trials",
                    info="Number of trials to run during optimization."
                )

            autotune_btn = gr.Button("üöÄ Run AutoTune", elem_id="autotune_btn", variant="primary")

        # üèÜ Best Configuration
        with gr.Row(elem_classes=["card"]):
            with gr.Column():
                gr.Markdown("üèÜ <span class='section-title'>Best Configuration</span>")
                best_json = gr.Textbox(label="", interactive=False, lines=10)

        # üìä Trial Scores
        with gr.Row(elem_classes=["card"]):
            with gr.Column():
                gr.Markdown("üìä <span class='section-title'>Trial Scores</span>")
                score_plot = gr.Plot(label="")

        # üí° Explanation
        with gr.Row(elem_classes=["card"]):
            with gr.Column():
                gr.Markdown("üí° <span class='section-title'>Explanation</span>")
                explanation_md = gr.Markdown(label="")

        export_btn = gr.Button("Export Best Configuration", variant="primary", visible=False)
        export_status = gr.Textbox(label="Export Status", interactive=False, visible=False)

        # üîó Connect button logic
        autotune_btn.click(
            fn=do_auto_tune,
            inputs=[embed_model, num_pairs, search_type, trials, validation_source, validation_hf_dataset],
            outputs=[best_json, score_plot, explanation_md],
            show_progress=True
        ).then(
            fn=lambda: (gr.update(visible=True), gr.update(visible=True)),
            inputs=None,
            outputs=[export_btn, export_status]
        )

        export_btn.click(
            fn=export_best_config,
            inputs=[best_json],
            outputs=[export_status]
        )

    with gr.Tab("üèÜ Leaderboard"):
        show_btn = gr.Button("Refresh",variant="primary")
        lb_table = gr.Dataframe(label="Leaderboard", interactive=False)
        lb_json = gr.Textbox(label="Raw JSON", interactive=False)
        show_btn.click(fn=show_leaderboard_table, outputs=[lb_table, lb_json])


    gr.Markdown(
        f"<center><p font-size:0.9em;'>"
        "Built with ‚ù§Ô∏è using RAGMint ¬∑ ¬© 2025 andyolivers.com</p></center>"
    )

if __name__ == "__main__":
    demo.launch(server_name="0.0.0.0", server_port=7860, show_api=False)
{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e350c961",
   "metadata": {},
   "source": [
    "# Research Example: Generating Procedures for the GSM8K Dataset Using a Genetic Algorithm and OLLaMa Queries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59ad3473",
   "metadata": {},
   "source": [
    "## Step 1: Import packages and necessary application functions & variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08cc3cf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import math\n",
    "from evoproc.ga_scaffold_structured import ProcedureGA, GAConfig\n",
    "from evoproc.validators import validate_procedure_structured\n",
    "from evoproc_procedures.models import Procedure\n",
    "from evoproc_procedures.schemas import get_schema\n",
    "from evoproc_procedures.prompts import create_procedure_prompt\n",
    "from evoproc_procedures.ollama import query, repair_fn_ollama\n",
    "from evoproc_procedures.runners import run_steps_stateful_minimal\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7afda795",
   "metadata": {},
   "source": [
    "## Step 2: Import the GSM8K Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "03411603",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "661e6381",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = load_dataset(\"openai/gsm8k\", \"main\", split=\"train\")\n",
    "test_dataset = load_dataset(\"openai/gsm8k\", \"main\", split=\"test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf261a71",
   "metadata": {},
   "source": [
    "## Step 3: Set variable constants and instantiate necessary functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c8d6ce95",
   "metadata": {},
   "outputs": [],
   "source": [
    "FINAL_SCHEMA = get_schema(\"gsm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2beac706",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_steps_fn(proc_json, question, final_answer_schema, model, print_bool=False):\n",
    "    # use your general runner (backend-agnostic; pass Ollama query fn)\n",
    "    state = run_steps_stateful_minimal(\n",
    "        proc_json,\n",
    "        problem_text=question,\n",
    "        answer_schema=final_answer_schema,\n",
    "        model=model,\n",
    "        query_fn=query,\n",
    "        print_bool=print_bool,\n",
    "    )\n",
    "    return state\n",
    "\n",
    "def _extract_gold_number(gold_answer: str) -> float | None:\n",
    "    # GSM8K gold answers are strings; often last number is the target\n",
    "    nums = re.findall(r\"-?\\d+(?:\\.\\d+)?\", gold_answer)\n",
    "    return float(nums[-1]) if nums else None\n",
    "\n",
    "def eval_fn(state, proc_json) -> float:\n",
    "    \"\"\"Return a fitness score in [0,1].\"\"\"\n",
    "    # prefer model-extracted numeric if present, else try to parse its text\n",
    "    pred_num = state.get(\"answer_numerical\")\n",
    "    if pred_num is None:\n",
    "        try:\n",
    "            pred_num = float(re.findall(r\"-?\\d+(?:\\.\\d+)?\", state.get(\"answer\",\"\"))[-1])\n",
    "        except Exception:\n",
    "            return 0.0\n",
    "    gold_num = state.get(\"_gold_num\")  # we’ll inject this per item\n",
    "    if gold_num is None:\n",
    "        return 0.0\n",
    "    # exact match or close within small tolerance\n",
    "    return 1.0 if math.isclose(pred_num, gold_num, rel_tol=0, abs_tol=1e-6) else 0.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b31c78fa",
   "metadata": {},
   "source": [
    "## Step 3: Instantiate the Procedure Genetic Algorithm Object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4ee1f4bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "ga = ProcedureGA(\n",
    "    model=\"gemma3:latest\",\n",
    "    create_proc_fn=lambda task: create_procedure_prompt(task),\n",
    "    query_fn=query,                                     # backend call\n",
    "    schema_json_fn=lambda: Procedure.model_json_schema(),\n",
    "    validate_fn=validate_procedure_structured,          # pure function\n",
    "    repair_fn=repair_fn_ollama,                         # GA expects (proc, model) -> proc\n",
    "    cfg=GAConfig(population_size=3, max_generations=3, crossover_rate=0.7, mutation_rate=0.3, seed=42),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77e71602",
   "metadata": {},
   "source": [
    "## Step 4: For each Question-Answer pair, run the GA with the question as task_description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ddafe810",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_gsm8k_batch(examples):\n",
    "    \"\"\"\n",
    "    examples: iterable of dicts like {\"id\": ..., \"question\": \"...\", \"answer\": \"...\"} (GSM8K format)\n",
    "    Returns: list of per-item result dicts with procedure, state, and score\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    for ex in examples:\n",
    "        qid = ex.get(\"id\")\n",
    "        question = ex[\"question\"]\n",
    "        gold_text = ex[\"answer\"]\n",
    "        gold_num = _extract_gold_number(gold_text)\n",
    "\n",
    "        # CHOOSE ONE OF THE FOLLOWING:\n",
    "        # 1. Task-eval path: supply all three args so GA uses TaskEval scoring each generation\n",
    "        # best, history = ga.run(\n",
    "        #     task_description=question,\n",
    "        #     final_answer_schema=FINAL_SCHEMA,\n",
    "        #     eval_fn=lambda state, proc: eval_fn({**state, \"_gold_num\": gold_num}, proc),\n",
    "        #     run_steps_fn=run_steps_fn,\n",
    "        #     print_progress=False,\n",
    "        # )\n",
    "\n",
    "        # 2. NO Task-eval path: don't supply all three args so GA uses Hygiene scoring each generation\n",
    "        best, history = ga.run(\n",
    "            task_description=question,\n",
    "            final_answer_schema=FINAL_SCHEMA,\n",
    "            eval_fn=None,\n",
    "            print_progress=False,\n",
    "        )\n",
    "\n",
    "        # After GA finishes, run once more to collect the final state/answer\n",
    "        final_state = run_steps_fn(best.proc, question, FINAL_SCHEMA, ga.model, print_bool=False)\n",
    "\n",
    "        results.append({\n",
    "            \"id\": qid,\n",
    "            \"question\": question,\n",
    "            \"gold_answer\": gold_text,\n",
    "            \"gold_num\": gold_num,\n",
    "            \"fitness\": best.fitness,\n",
    "            \"procedure\": best.proc,            # JSON dict\n",
    "            \"state\": final_state,              # includes \"answer\" and \"answer_numerical\"\n",
    "            \"pred_answer\": final_state.get(\"final_answer\"),\n",
    "            \"pred_num\": final_state.get(\"final_answer_numerical\"),\n",
    "            \"correct\": bool(eval_fn({**final_state, \"_gold_num\": gold_num}, best.proc) >= 1.0),\n",
    "            \"steps\": len(best.proc.get(\"steps\", [])),\n",
    "        })\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "581ef386",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For testing purposes, just grab first 10 as this will take a long time to run\n",
    "first_two = train_dataset.select(range(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "8a2263dc",
   "metadata": {},
   "outputs": [
    {
     "ename": "ConnectionError",
     "evalue": "Failed to connect to Ollama. Please check that Ollama is downloaded, running and accessible. https://ollama.com/download",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mConnectionError\u001b[39m                           Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[35]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mrun_gsm8k_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfirst_two\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[18]\u001b[39m\u001b[32m, line 24\u001b[39m, in \u001b[36mrun_gsm8k_batch\u001b[39m\u001b[34m(examples)\u001b[39m\n\u001b[32m     11\u001b[39m gold_num = _extract_gold_number(gold_text)\n\u001b[32m     13\u001b[39m \u001b[38;5;66;03m# CHOOSE ONE OF THE FOLLOWING:\u001b[39;00m\n\u001b[32m     14\u001b[39m \u001b[38;5;66;03m# 1. Task-eval path: supply all three args so GA uses TaskEval scoring each generation\u001b[39;00m\n\u001b[32m     15\u001b[39m \u001b[38;5;66;03m# best, history = ga.run(\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     22\u001b[39m \n\u001b[32m     23\u001b[39m \u001b[38;5;66;03m# 2. NO Task-eval path: don't supply all three args so GA uses Hygiene scoring each generation\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m24\u001b[39m best, history = \u001b[43mga\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     25\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtask_description\u001b[49m\u001b[43m=\u001b[49m\u001b[43mquestion\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     26\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfinal_answer_schema\u001b[49m\u001b[43m=\u001b[49m\u001b[43mFINAL_SCHEMA\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     27\u001b[39m \u001b[43m    \u001b[49m\u001b[43meval_fn\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m     28\u001b[39m \u001b[43m    \u001b[49m\u001b[43mprint_progress\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m     29\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     31\u001b[39m \u001b[38;5;66;03m# After GA finishes, run once more to collect the final state/answer\u001b[39;00m\n\u001b[32m     32\u001b[39m final_state = run_steps_fn(best.proc, question, FINAL_SCHEMA, ga.model, print_bool=\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Malia/procedural-llms/projects/core/src/llm_procedure_generation_ga/ga_scaffold_structured.py:640\u001b[39m, in \u001b[36mProcedureGA.run\u001b[39m\u001b[34m(self, task_description, final_answer_schema, eval_fn, run_steps_fn, print_progress)\u001b[39m\n\u001b[32m    616\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mrun\u001b[39m(\n\u001b[32m    617\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    618\u001b[39m     task_description: \u001b[38;5;28mstr\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    622\u001b[39m     print_progress: \u001b[38;5;28mbool\u001b[39m = \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m    623\u001b[39m ) -> Tuple[Individual, List[Individual]]:\n\u001b[32m    624\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Execute the full GA loop and return the best individual plus history of elites.\u001b[39;00m\n\u001b[32m    625\u001b[39m \n\u001b[32m    626\u001b[39m \u001b[33;03m    If ``final_answer_schema``, ``eval_fn``, and ``run_steps_fn`` are all provided,\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    638\u001b[39m \u001b[33;03m        A tuple of ``(best_individual, elites_history)``.\u001b[39;00m\n\u001b[32m    639\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m640\u001b[39m     pop = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43minitialize_population\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtask_description\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    641\u001b[39m     history: List[Individual] = []\n\u001b[32m    643\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m gen \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m.cfg.max_generations):\n\u001b[32m    644\u001b[39m         \u001b[38;5;66;03m# Choose scorer (task-eval if fully provided; else structural)\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Malia/procedural-llms/projects/core/src/llm_procedure_generation_ga/ga_scaffold_structured.py:559\u001b[39m, in \u001b[36mProcedureGA.initialize_population\u001b[39m\u001b[34m(self, task_description)\u001b[39m\n\u001b[32m    553\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34minitialize_population\u001b[39m(\u001b[38;5;28mself\u001b[39m, task_description: \u001b[38;5;28mstr\u001b[39m) -> List[Individual]:\n\u001b[32m    554\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Generate the initial population by repeatedly calling ``_generate_one``.\u001b[39;00m\n\u001b[32m    555\u001b[39m \n\u001b[32m    556\u001b[39m \u001b[33;03m    Returns:\u001b[39;00m\n\u001b[32m    557\u001b[39m \u001b[33;03m        A list of :class:`Individual` with ``proc`` populated.\u001b[39;00m\n\u001b[32m    558\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m559\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m [Individual(\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_generate_one\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtask_description\u001b[49m\u001b[43m)\u001b[49m) \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m.cfg.population_size)]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Malia/procedural-llms/projects/core/src/llm_procedure_generation_ga/ga_scaffold_structured.py:537\u001b[39m, in \u001b[36mProcedureGA._generate_one\u001b[39m\u001b[34m(self, task_description)\u001b[39m\n\u001b[32m    525\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Create a single, valid procedure by prompting, repairing, and normalizing.\u001b[39;00m\n\u001b[32m    526\u001b[39m \n\u001b[32m    527\u001b[39m \u001b[33;03mSteps:\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    534\u001b[39m \u001b[33;03m    A schema‑conforming procedure JSON (best‑effort).\u001b[39;00m\n\u001b[32m    535\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    536\u001b[39m prompt = \u001b[38;5;28mself\u001b[39m.create_proc_fn(task_description)\n\u001b[32m--> \u001b[39m\u001b[32m537\u001b[39m raw = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mquery_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfmt\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mschema_json_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mseed\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1234\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    538\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    539\u001b[39m     proc = json.loads(raw) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(raw, \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m raw\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Malia/procedural-llms/projects/procedures/src/procedures/ollama.py:83\u001b[39m, in \u001b[36mquery\u001b[39m\u001b[34m(prompt, model, fmt, seed)\u001b[39m\n\u001b[32m     63\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mquery\u001b[39m(\n\u001b[32m     64\u001b[39m     prompt: \u001b[38;5;28mstr\u001b[39m,\n\u001b[32m     65\u001b[39m     model: \u001b[38;5;28mstr\u001b[39m,\n\u001b[32m     66\u001b[39m     fmt: Optional[Dict[\u001b[38;5;28mstr\u001b[39m, Any]] = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m     67\u001b[39m     seed: Optional[\u001b[38;5;28mint\u001b[39m] = \u001b[32m1234\u001b[39m,\n\u001b[32m     68\u001b[39m ) -> \u001b[38;5;28mstr\u001b[39m:\n\u001b[32m     69\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"General-purpose Ollama call.\u001b[39;00m\n\u001b[32m     70\u001b[39m \n\u001b[32m     71\u001b[39m \u001b[33;03m    This is your default structured call. When `fmt` is supplied, Ollama\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     81\u001b[39m \u001b[33;03m        Raw string response (often JSON text when `fmt` is provided).\u001b[39;00m\n\u001b[32m     82\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m83\u001b[39m     res = \u001b[43mollama\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     84\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     85\u001b[39m \u001b[43m        \u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     86\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mformat\u001b[39;49m\u001b[43m=\u001b[49m\u001b[43mfmt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     87\u001b[39m \u001b[43m        \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m=\u001b[49m\u001b[43m{\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtemperature\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mseed\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mseed\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     88\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     89\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m res[\u001b[33m\"\u001b[39m\u001b[33mresponse\u001b[39m\u001b[33m\"\u001b[39m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Malia/procedural-llms/venv/lib/python3.13/site-packages/ollama/_client.py:256\u001b[39m, in \u001b[36mClient.generate\u001b[39m\u001b[34m(self, model, prompt, suffix, system, template, context, stream, think, raw, format, images, options, keep_alive)\u001b[39m\n\u001b[32m    229\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mgenerate\u001b[39m(\n\u001b[32m    230\u001b[39m   \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    231\u001b[39m   model: \u001b[38;5;28mstr\u001b[39m = \u001b[33m'\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    244\u001b[39m   keep_alive: Optional[Union[\u001b[38;5;28mfloat\u001b[39m, \u001b[38;5;28mstr\u001b[39m]] = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m    245\u001b[39m ) -> Union[GenerateResponse, Iterator[GenerateResponse]]:\n\u001b[32m    246\u001b[39m \u001b[38;5;250m  \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    247\u001b[39m \u001b[33;03m  Create a response using the requested model.\u001b[39;00m\n\u001b[32m    248\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m    253\u001b[39m \u001b[33;03m  Returns `GenerateResponse` if `stream` is `False`, otherwise returns a `GenerateResponse` generator.\u001b[39;00m\n\u001b[32m    254\u001b[39m \u001b[33;03m  \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m256\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    257\u001b[39m \u001b[43m    \u001b[49m\u001b[43mGenerateResponse\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    258\u001b[39m \u001b[43m    \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mPOST\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    259\u001b[39m \u001b[43m    \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43m/api/generate\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    260\u001b[39m \u001b[43m    \u001b[49m\u001b[43mjson\u001b[49m\u001b[43m=\u001b[49m\u001b[43mGenerateRequest\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    261\u001b[39m \u001b[43m      \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    262\u001b[39m \u001b[43m      \u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    263\u001b[39m \u001b[43m      \u001b[49m\u001b[43msuffix\u001b[49m\u001b[43m=\u001b[49m\u001b[43msuffix\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    264\u001b[39m \u001b[43m      \u001b[49m\u001b[43msystem\u001b[49m\u001b[43m=\u001b[49m\u001b[43msystem\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    265\u001b[39m \u001b[43m      \u001b[49m\u001b[43mtemplate\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtemplate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    266\u001b[39m \u001b[43m      \u001b[49m\u001b[43mcontext\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcontext\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    267\u001b[39m \u001b[43m      \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    268\u001b[39m \u001b[43m      \u001b[49m\u001b[43mthink\u001b[49m\u001b[43m=\u001b[49m\u001b[43mthink\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    269\u001b[39m \u001b[43m      \u001b[49m\u001b[43mraw\u001b[49m\u001b[43m=\u001b[49m\u001b[43mraw\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    270\u001b[39m \u001b[43m      \u001b[49m\u001b[38;5;28;43mformat\u001b[39;49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mformat\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    271\u001b[39m \u001b[43m      \u001b[49m\u001b[43mimages\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m_copy_images\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimages\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mimages\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    272\u001b[39m \u001b[43m      \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m=\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    273\u001b[39m \u001b[43m      \u001b[49m\u001b[43mkeep_alive\u001b[49m\u001b[43m=\u001b[49m\u001b[43mkeep_alive\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    274\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmodel_dump\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexclude_none\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    275\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    276\u001b[39m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Malia/procedural-llms/venv/lib/python3.13/site-packages/ollama/_client.py:189\u001b[39m, in \u001b[36mClient._request\u001b[39m\u001b[34m(self, cls, stream, *args, **kwargs)\u001b[39m\n\u001b[32m    185\u001b[39m         \u001b[38;5;28;01myield\u001b[39;00m \u001b[38;5;28mcls\u001b[39m(**part)\n\u001b[32m    187\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m inner()\n\u001b[32m--> \u001b[39m\u001b[32m189\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mcls\u001b[39m(**\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_request_raw\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m.json())\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Malia/procedural-llms/venv/lib/python3.13/site-packages/ollama/_client.py:135\u001b[39m, in \u001b[36mClient._request_raw\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    133\u001b[39m   \u001b[38;5;28;01mraise\u001b[39;00m ResponseError(e.response.text, e.response.status_code) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    134\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m httpx.ConnectError:\n\u001b[32m--> \u001b[39m\u001b[32m135\u001b[39m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m(CONNECTION_ERROR_MESSAGE) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[31mConnectionError\u001b[39m: Failed to connect to Ollama. Please check that Ollama is downloaded, running and accessible. https://ollama.com/download"
     ]
    }
   ],
   "source": [
    "run_gsm8k_batch(first_two)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09f7c84d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.13.5)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

"""Code generated by Speakeasy (https://speakeasy.com). DO NOT EDIT."""

from __future__ import annotations
from cribl_control_plane import models, utils
from cribl_control_plane.types import BaseModel
from cribl_control_plane.utils import get_discriminator, validate_open_enum
from enum import Enum
import pydantic
from pydantic import ConfigDict, Discriminator, Tag, field_serializer
from pydantic.functional_validators import PlainValidator
from typing import Any, Dict, List, Optional, Union
from typing_extensions import Annotated, NotRequired, TypeAliasType, TypedDict


class TypeCloudflareHec(str, Enum):
    CLOUDFLARE_HEC = "cloudflare_hec"


class ConnectionCloudflareHecTypedDict(TypedDict):
    output: str
    pipeline: NotRequired[str]


class ConnectionCloudflareHec(BaseModel):
    output: str

    pipeline: Optional[str] = None


class ModeCloudflareHec(str, Enum, metaclass=utils.OpenEnumMeta):
    r"""With Smart mode, PQ will write events to the filesystem only when it detects backpressure from the processing engine. With Always On mode, PQ will always write events directly to the queue before forwarding them to the processing engine."""

    # Smart
    SMART = "smart"
    # Always On
    ALWAYS = "always"


class CompressionCloudflareHec(str, Enum, metaclass=utils.OpenEnumMeta):
    r"""Codec to use to compress the persisted data"""

    # None
    NONE = "none"
    # Gzip
    GZIP = "gzip"


class PqControlsCloudflareHecTypedDict(TypedDict):
    pass


class PqControlsCloudflareHec(BaseModel):
    pass


class PqCloudflareHecTypedDict(TypedDict):
    mode: NotRequired[ModeCloudflareHec]
    r"""With Smart mode, PQ will write events to the filesystem only when it detects backpressure from the processing engine. With Always On mode, PQ will always write events directly to the queue before forwarding them to the processing engine."""
    max_buffer_size: NotRequired[float]
    r"""The maximum number of events to hold in memory before writing the events to disk"""
    commit_frequency: NotRequired[float]
    r"""The number of events to send downstream before committing that Stream has read them"""
    max_file_size: NotRequired[str]
    r"""The maximum size to store in each queue file before closing and optionally compressing. Enter a numeral with units of KB, MB, etc."""
    max_size: NotRequired[str]
    r"""The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc."""
    path: NotRequired[str]
    r"""The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/inputs/<input-id>"""
    compress: NotRequired[CompressionCloudflareHec]
    r"""Codec to use to compress the persisted data"""
    pq_controls: NotRequired[PqControlsCloudflareHecTypedDict]


class PqCloudflareHec(BaseModel):
    mode: Annotated[
        Optional[ModeCloudflareHec], PlainValidator(validate_open_enum(False))
    ] = ModeCloudflareHec.ALWAYS
    r"""With Smart mode, PQ will write events to the filesystem only when it detects backpressure from the processing engine. With Always On mode, PQ will always write events directly to the queue before forwarding them to the processing engine."""

    max_buffer_size: Annotated[
        Optional[float], pydantic.Field(alias="maxBufferSize")
    ] = 1000
    r"""The maximum number of events to hold in memory before writing the events to disk"""

    commit_frequency: Annotated[
        Optional[float], pydantic.Field(alias="commitFrequency")
    ] = 42
    r"""The number of events to send downstream before committing that Stream has read them"""

    max_file_size: Annotated[Optional[str], pydantic.Field(alias="maxFileSize")] = (
        "1 MB"
    )
    r"""The maximum size to store in each queue file before closing and optionally compressing. Enter a numeral with units of KB, MB, etc."""

    max_size: Annotated[Optional[str], pydantic.Field(alias="maxSize")] = "5GB"
    r"""The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc."""

    path: Optional[str] = "$CRIBL_HOME/state/queues"
    r"""The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/inputs/<input-id>"""

    compress: Annotated[
        Optional[CompressionCloudflareHec], PlainValidator(validate_open_enum(False))
    ] = CompressionCloudflareHec.NONE
    r"""Codec to use to compress the persisted data"""

    pq_controls: Annotated[
        Optional[PqControlsCloudflareHec], pydantic.Field(alias="pqControls")
    ] = None

    @field_serializer("mode")
    def serialize_mode(self, value):
        if isinstance(value, str):
            try:
                return models.ModeCloudflareHec(value)
            except ValueError:
                return value
        return value

    @field_serializer("compress")
    def serialize_compress(self, value):
        if isinstance(value, str):
            try:
                return models.CompressionCloudflareHec(value)
            except ValueError:
                return value
        return value


class AuthenticationMethodCloudflareHec(str, Enum, metaclass=utils.OpenEnumMeta):
    r"""Select Secret to use a text secret to authenticate"""

    SECRET = "secret"


class AuthTokenMetadatumCloudflareHecTypedDict(TypedDict):
    name: str
    value: str
    r"""JavaScript expression to compute field's value, enclosed in quotes or backticks. (Can evaluate to a constant.)"""


class AuthTokenMetadatumCloudflareHec(BaseModel):
    name: str

    value: str
    r"""JavaScript expression to compute field's value, enclosed in quotes or backticks. (Can evaluate to a constant.)"""


class AuthTokenCloudflareHecTypedDict(TypedDict):
    auth_type: NotRequired[AuthenticationMethodCloudflareHec]
    r"""Select Secret to use a text secret to authenticate"""
    token_secret: NotRequired[Any]
    token: NotRequired[Any]
    enabled: NotRequired[bool]
    description: NotRequired[str]
    allowed_indexes_at_token: NotRequired[List[str]]
    r"""Enter the values you want to allow in the HEC event index field at the token level. Supports wildcards. To skip validation, leave blank."""
    metadata: NotRequired[List[AuthTokenMetadatumCloudflareHecTypedDict]]
    r"""Fields to add to events referencing this token"""


class AuthTokenCloudflareHec(BaseModel):
    auth_type: Annotated[
        Annotated[
            Optional[AuthenticationMethodCloudflareHec],
            PlainValidator(validate_open_enum(False)),
        ],
        pydantic.Field(alias="authType"),
    ] = AuthenticationMethodCloudflareHec.SECRET
    r"""Select Secret to use a text secret to authenticate"""

    token_secret: Annotated[Optional[Any], pydantic.Field(alias="tokenSecret")] = None

    token: Optional[Any] = None

    enabled: Optional[bool] = True

    description: Optional[str] = None

    allowed_indexes_at_token: Annotated[
        Optional[List[str]], pydantic.Field(alias="allowedIndexesAtToken")
    ] = None
    r"""Enter the values you want to allow in the HEC event index field at the token level. Supports wildcards. To skip validation, leave blank."""

    metadata: Optional[List[AuthTokenMetadatumCloudflareHec]] = None
    r"""Fields to add to events referencing this token"""

    @field_serializer("auth_type")
    def serialize_auth_type(self, value):
        if isinstance(value, str):
            try:
                return models.AuthenticationMethodCloudflareHec(value)
            except ValueError:
                return value
        return value


class MinimumTLSVersionCloudflareHec(str, Enum, metaclass=utils.OpenEnumMeta):
    TL_SV1 = "TLSv1"
    TL_SV1_1 = "TLSv1.1"
    TL_SV1_2 = "TLSv1.2"
    TL_SV1_3 = "TLSv1.3"


class MaximumTLSVersionCloudflareHec(str, Enum, metaclass=utils.OpenEnumMeta):
    TL_SV1 = "TLSv1"
    TL_SV1_1 = "TLSv1.1"
    TL_SV1_2 = "TLSv1.2"
    TL_SV1_3 = "TLSv1.3"


class TLSSettingsServerSideCloudflareHecTypedDict(TypedDict):
    disabled: NotRequired[bool]
    request_cert: NotRequired[bool]
    r"""Require clients to present their certificates. Used to perform client authentication using SSL certs."""
    reject_unauthorized: NotRequired[bool]
    r"""Reject certificates not authorized by a CA in the CA certificate path or by another trusted CA (such as the system's)"""
    common_name_regex: NotRequired[str]
    r"""Regex matching allowable common names in peer certificates' subject attribute"""
    certificate_name: NotRequired[str]
    r"""The name of the predefined certificate"""
    priv_key_path: NotRequired[str]
    r"""Path on server containing the private key to use. PEM format. Can reference $ENV_VARS."""
    passphrase: NotRequired[str]
    r"""Passphrase to use to decrypt private key"""
    cert_path: NotRequired[str]
    r"""Path on server containing certificates to use. PEM format. Can reference $ENV_VARS."""
    ca_path: NotRequired[str]
    r"""Path on server containing CA certificates to use. PEM format. Can reference $ENV_VARS."""
    min_version: NotRequired[MinimumTLSVersionCloudflareHec]
    max_version: NotRequired[MaximumTLSVersionCloudflareHec]


class TLSSettingsServerSideCloudflareHec(BaseModel):
    disabled: Optional[bool] = True

    request_cert: Annotated[Optional[bool], pydantic.Field(alias="requestCert")] = False
    r"""Require clients to present their certificates. Used to perform client authentication using SSL certs."""

    reject_unauthorized: Annotated[
        Optional[bool], pydantic.Field(alias="rejectUnauthorized")
    ] = True
    r"""Reject certificates not authorized by a CA in the CA certificate path or by another trusted CA (such as the system's)"""

    common_name_regex: Annotated[
        Optional[str], pydantic.Field(alias="commonNameRegex")
    ] = "/.*/"
    r"""Regex matching allowable common names in peer certificates' subject attribute"""

    certificate_name: Annotated[
        Optional[str], pydantic.Field(alias="certificateName")
    ] = None
    r"""The name of the predefined certificate"""

    priv_key_path: Annotated[Optional[str], pydantic.Field(alias="privKeyPath")] = None
    r"""Path on server containing the private key to use. PEM format. Can reference $ENV_VARS."""

    passphrase: Optional[str] = None
    r"""Passphrase to use to decrypt private key"""

    cert_path: Annotated[Optional[str], pydantic.Field(alias="certPath")] = None
    r"""Path on server containing certificates to use. PEM format. Can reference $ENV_VARS."""

    ca_path: Annotated[Optional[str], pydantic.Field(alias="caPath")] = None
    r"""Path on server containing CA certificates to use. PEM format. Can reference $ENV_VARS."""

    min_version: Annotated[
        Annotated[
            Optional[MinimumTLSVersionCloudflareHec],
            PlainValidator(validate_open_enum(False)),
        ],
        pydantic.Field(alias="minVersion"),
    ] = None

    max_version: Annotated[
        Annotated[
            Optional[MaximumTLSVersionCloudflareHec],
            PlainValidator(validate_open_enum(False)),
        ],
        pydantic.Field(alias="maxVersion"),
    ] = None

    @field_serializer("min_version")
    def serialize_min_version(self, value):
        if isinstance(value, str):
            try:
                return models.MinimumTLSVersionCloudflareHec(value)
            except ValueError:
                return value
        return value

    @field_serializer("max_version")
    def serialize_max_version(self, value):
        if isinstance(value, str):
            try:
                return models.MaximumTLSVersionCloudflareHec(value)
            except ValueError:
                return value
        return value


class MetadatumCloudflareHecTypedDict(TypedDict):
    name: str
    value: str
    r"""JavaScript expression to compute field's value, enclosed in quotes or backticks. (Can evaluate to a constant.)"""


class MetadatumCloudflareHec(BaseModel):
    name: str

    value: str
    r"""JavaScript expression to compute field's value, enclosed in quotes or backticks. (Can evaluate to a constant.)"""


class InputCloudflareHecTypedDict(TypedDict):
    type: TypeCloudflareHec
    port: float
    r"""Port to listen on"""
    hec_api: str
    r"""Absolute path on which to listen for the Cloudflare HTTP Event Collector API requests. This input supports the /event endpoint."""
    id: NotRequired[str]
    r"""Unique ID for this input"""
    disabled: NotRequired[bool]
    pipeline: NotRequired[str]
    r"""Pipeline to process data from this Source before sending it through the Routes"""
    send_to_routes: NotRequired[bool]
    r"""Select whether to send data to Routes, or directly to Destinations."""
    environment: NotRequired[str]
    r"""Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere."""
    pq_enabled: NotRequired[bool]
    r"""Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers)."""
    streamtags: NotRequired[List[str]]
    r"""Tags for filtering and grouping in @{product}"""
    connections: NotRequired[List[ConnectionCloudflareHecTypedDict]]
    r"""Direct connections to Destinations, and optionally via a Pipeline or a Pack"""
    pq: NotRequired[PqCloudflareHecTypedDict]
    host: NotRequired[str]
    r"""Address to bind on. Defaults to 0.0.0.0 (all addresses)."""
    auth_tokens: NotRequired[List[AuthTokenCloudflareHecTypedDict]]
    r"""Shared secrets to be provided by any client (Authorization: <token>). If empty, unauthorized access is permitted."""
    tls: NotRequired[TLSSettingsServerSideCloudflareHecTypedDict]
    max_active_req: NotRequired[float]
    r"""Maximum number of active requests allowed per Worker Process. Set to 0 for unlimited. Caution: Increasing the limit above the default value, or setting it to unlimited, may degrade performance and reduce throughput."""
    max_requests_per_socket: NotRequired[int]
    r"""Maximum number of requests per socket before @{product} instructs the client to close the connection. Default is 0 (unlimited)."""
    enable_proxy_header: NotRequired[bool]
    r"""Extract the client IP and port from PROXY protocol v1/v2. When enabled, the X-Forwarded-For header is ignored. Disable to use the X-Forwarded-For header for client IP extraction."""
    capture_headers: NotRequired[bool]
    r"""Add request headers to events, in the __headers field"""
    activity_log_sample_rate: NotRequired[float]
    r"""How often request activity is logged at the `info` level. A value of 1 would log every request, 10 every 10th request, etc."""
    request_timeout: NotRequired[float]
    r"""How long to wait for an incoming request to complete before aborting it. Use 0 to disable."""
    socket_timeout: NotRequired[float]
    r"""How long @{product} should wait before assuming that an inactive socket has timed out. To wait forever, set to 0."""
    keep_alive_timeout: NotRequired[float]
    r"""After the last response is sent, @{product} will wait this long for additional data before closing the socket connection. Minimum 1 second, maximum 600 seconds (10 minutes)."""
    enable_health_check: NotRequired[Any]
    ip_allowlist_regex: NotRequired[str]
    r"""Messages from matched IP addresses will be processed, unless also matched by the denylist"""
    ip_denylist_regex: NotRequired[str]
    r"""Messages from matched IP addresses will be ignored. This takes precedence over the allowlist."""
    metadata: NotRequired[List[MetadatumCloudflareHecTypedDict]]
    r"""Fields to add to every event. May be overridden by fields added at the token or request level."""
    allowed_indexes: NotRequired[List[str]]
    r"""List values allowed in HEC event index field. Leave blank to skip validation. Supports wildcards. The values here can expand index validation at the token level."""
    breaker_rulesets: NotRequired[List[str]]
    r"""A list of event-breaking rulesets that will be applied, in order, to the input data stream"""
    stale_channel_flush_ms: NotRequired[float]
    r"""How long (in milliseconds) the Event Breaker will wait for new data to be sent to a specific channel before flushing the data stream out, as is, to the Pipelines"""
    access_control_allow_origin: NotRequired[List[str]]
    r"""HTTP origins to which @{product} should send CORS (cross-origin resource sharing) Access-Control-Allow-* headers. Supports wildcards."""
    access_control_allow_headers: NotRequired[List[str]]
    r"""HTTP headers that @{product} will send to allowed origins as \"Access-Control-Allow-Headers\" in a CORS preflight response. Use \"*\" to allow all headers."""
    emit_token_metrics: NotRequired[bool]
    r"""Emit per-token (<prefix>.http.perToken) and summary (<prefix>.http.summary) request metrics"""
    description: NotRequired[str]


class InputCloudflareHec(BaseModel):
    model_config = ConfigDict(
        populate_by_name=True, arbitrary_types_allowed=True, extra="allow"
    )
    __pydantic_extra__: Dict[str, Any] = pydantic.Field(init=False)

    type: TypeCloudflareHec

    port: float
    r"""Port to listen on"""

    hec_api: Annotated[str, pydantic.Field(alias="hecAPI")]
    r"""Absolute path on which to listen for the Cloudflare HTTP Event Collector API requests. This input supports the /event endpoint."""

    id: Optional[str] = None
    r"""Unique ID for this input"""

    disabled: Optional[bool] = False

    pipeline: Optional[str] = None
    r"""Pipeline to process data from this Source before sending it through the Routes"""

    send_to_routes: Annotated[Optional[bool], pydantic.Field(alias="sendToRoutes")] = (
        True
    )
    r"""Select whether to send data to Routes, or directly to Destinations."""

    environment: Optional[str] = None
    r"""Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere."""

    pq_enabled: Annotated[Optional[bool], pydantic.Field(alias="pqEnabled")] = False
    r"""Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers)."""

    streamtags: Optional[List[str]] = None
    r"""Tags for filtering and grouping in @{product}"""

    connections: Optional[List[ConnectionCloudflareHec]] = None
    r"""Direct connections to Destinations, and optionally via a Pipeline or a Pack"""

    pq: Optional[PqCloudflareHec] = None

    host: Optional[str] = "0.0.0.0"
    r"""Address to bind on. Defaults to 0.0.0.0 (all addresses)."""

    auth_tokens: Annotated[
        Optional[List[AuthTokenCloudflareHec]], pydantic.Field(alias="authTokens")
    ] = None
    r"""Shared secrets to be provided by any client (Authorization: <token>). If empty, unauthorized access is permitted."""

    tls: Optional[TLSSettingsServerSideCloudflareHec] = None

    max_active_req: Annotated[Optional[float], pydantic.Field(alias="maxActiveReq")] = (
        256
    )
    r"""Maximum number of active requests allowed per Worker Process. Set to 0 for unlimited. Caution: Increasing the limit above the default value, or setting it to unlimited, may degrade performance and reduce throughput."""

    max_requests_per_socket: Annotated[
        Optional[int], pydantic.Field(alias="maxRequestsPerSocket")
    ] = 0
    r"""Maximum number of requests per socket before @{product} instructs the client to close the connection. Default is 0 (unlimited)."""

    enable_proxy_header: Annotated[
        Optional[bool], pydantic.Field(alias="enableProxyHeader")
    ] = False
    r"""Extract the client IP and port from PROXY protocol v1/v2. When enabled, the X-Forwarded-For header is ignored. Disable to use the X-Forwarded-For header for client IP extraction."""

    capture_headers: Annotated[
        Optional[bool], pydantic.Field(alias="captureHeaders")
    ] = False
    r"""Add request headers to events, in the __headers field"""

    activity_log_sample_rate: Annotated[
        Optional[float], pydantic.Field(alias="activityLogSampleRate")
    ] = 100
    r"""How often request activity is logged at the `info` level. A value of 1 would log every request, 10 every 10th request, etc."""

    request_timeout: Annotated[
        Optional[float], pydantic.Field(alias="requestTimeout")
    ] = 0
    r"""How long to wait for an incoming request to complete before aborting it. Use 0 to disable."""

    socket_timeout: Annotated[
        Optional[float], pydantic.Field(alias="socketTimeout")
    ] = 0
    r"""How long @{product} should wait before assuming that an inactive socket has timed out. To wait forever, set to 0."""

    keep_alive_timeout: Annotated[
        Optional[float], pydantic.Field(alias="keepAliveTimeout")
    ] = 5
    r"""After the last response is sent, @{product} will wait this long for additional data before closing the socket connection. Minimum 1 second, maximum 600 seconds (10 minutes)."""

    enable_health_check: Annotated[
        Optional[Any], pydantic.Field(alias="enableHealthCheck")
    ] = None

    ip_allowlist_regex: Annotated[
        Optional[str], pydantic.Field(alias="ipAllowlistRegex")
    ] = "/.*/"
    r"""Messages from matched IP addresses will be processed, unless also matched by the denylist"""

    ip_denylist_regex: Annotated[
        Optional[str], pydantic.Field(alias="ipDenylistRegex")
    ] = "/^$/"
    r"""Messages from matched IP addresses will be ignored. This takes precedence over the allowlist."""

    metadata: Optional[List[MetadatumCloudflareHec]] = None
    r"""Fields to add to every event. May be overridden by fields added at the token or request level."""

    allowed_indexes: Annotated[
        Optional[List[str]], pydantic.Field(alias="allowedIndexes")
    ] = None
    r"""List values allowed in HEC event index field. Leave blank to skip validation. Supports wildcards. The values here can expand index validation at the token level."""

    breaker_rulesets: Annotated[
        Optional[List[str]], pydantic.Field(alias="breakerRulesets")
    ] = None
    r"""A list of event-breaking rulesets that will be applied, in order, to the input data stream"""

    stale_channel_flush_ms: Annotated[
        Optional[float], pydantic.Field(alias="staleChannelFlushMs")
    ] = 10000
    r"""How long (in milliseconds) the Event Breaker will wait for new data to be sent to a specific channel before flushing the data stream out, as is, to the Pipelines"""

    access_control_allow_origin: Annotated[
        Optional[List[str]], pydantic.Field(alias="accessControlAllowOrigin")
    ] = None
    r"""HTTP origins to which @{product} should send CORS (cross-origin resource sharing) Access-Control-Allow-* headers. Supports wildcards."""

    access_control_allow_headers: Annotated[
        Optional[List[str]], pydantic.Field(alias="accessControlAllowHeaders")
    ] = None
    r"""HTTP headers that @{product} will send to allowed origins as \"Access-Control-Allow-Headers\" in a CORS preflight response. Use \"*\" to allow all headers."""

    emit_token_metrics: Annotated[
        Optional[bool], pydantic.Field(alias="emitTokenMetrics")
    ] = False
    r"""Emit per-token (<prefix>.http.perToken) and summary (<prefix>.http.summary) request metrics"""

    description: Optional[str] = None

    @property
    def additional_properties(self):
        return self.__pydantic_extra__

    @additional_properties.setter
    def additional_properties(self, value):
        self.__pydantic_extra__ = value  # pyright: ignore[reportIncompatibleVariableOverride]


class TypeZscalerHec(str, Enum):
    ZSCALER_HEC = "zscaler_hec"


class ConnectionZscalerHecTypedDict(TypedDict):
    output: str
    pipeline: NotRequired[str]


class ConnectionZscalerHec(BaseModel):
    output: str

    pipeline: Optional[str] = None


class ModeZscalerHec(str, Enum, metaclass=utils.OpenEnumMeta):
    r"""With Smart mode, PQ will write events to the filesystem only when it detects backpressure from the processing engine. With Always On mode, PQ will always write events directly to the queue before forwarding them to the processing engine."""

    # Smart
    SMART = "smart"
    # Always On
    ALWAYS = "always"


class CompressionZscalerHec(str, Enum, metaclass=utils.OpenEnumMeta):
    r"""Codec to use to compress the persisted data"""

    # None
    NONE = "none"
    # Gzip
    GZIP = "gzip"


class PqControlsZscalerHecTypedDict(TypedDict):
    pass


class PqControlsZscalerHec(BaseModel):
    pass


class PqZscalerHecTypedDict(TypedDict):
    mode: NotRequired[ModeZscalerHec]
    r"""With Smart mode, PQ will write events to the filesystem only when it detects backpressure from the processing engine. With Always On mode, PQ will always write events directly to the queue before forwarding them to the processing engine."""
    max_buffer_size: NotRequired[float]
    r"""The maximum number of events to hold in memory before writing the events to disk"""
    commit_frequency: NotRequired[float]
    r"""The number of events to send downstream before committing that Stream has read them"""
    max_file_size: NotRequired[str]
    r"""The maximum size to store in each queue file before closing and optionally compressing. Enter a numeral with units of KB, MB, etc."""
    max_size: NotRequired[str]
    r"""The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc."""
    path: NotRequired[str]
    r"""The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/inputs/<input-id>"""
    compress: NotRequired[CompressionZscalerHec]
    r"""Codec to use to compress the persisted data"""
    pq_controls: NotRequired[PqControlsZscalerHecTypedDict]


class PqZscalerHec(BaseModel):
    mode: Annotated[
        Optional[ModeZscalerHec], PlainValidator(validate_open_enum(False))
    ] = ModeZscalerHec.ALWAYS
    r"""With Smart mode, PQ will write events to the filesystem only when it detects backpressure from the processing engine. With Always On mode, PQ will always write events directly to the queue before forwarding them to the processing engine."""

    max_buffer_size: Annotated[
        Optional[float], pydantic.Field(alias="maxBufferSize")
    ] = 1000
    r"""The maximum number of events to hold in memory before writing the events to disk"""

    commit_frequency: Annotated[
        Optional[float], pydantic.Field(alias="commitFrequency")
    ] = 42
    r"""The number of events to send downstream before committing that Stream has read them"""

    max_file_size: Annotated[Optional[str], pydantic.Field(alias="maxFileSize")] = (
        "1 MB"
    )
    r"""The maximum size to store in each queue file before closing and optionally compressing. Enter a numeral with units of KB, MB, etc."""

    max_size: Annotated[Optional[str], pydantic.Field(alias="maxSize")] = "5GB"
    r"""The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc."""

    path: Optional[str] = "$CRIBL_HOME/state/queues"
    r"""The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/inputs/<input-id>"""

    compress: Annotated[
        Optional[CompressionZscalerHec], PlainValidator(validate_open_enum(False))
    ] = CompressionZscalerHec.NONE
    r"""Codec to use to compress the persisted data"""

    pq_controls: Annotated[
        Optional[PqControlsZscalerHec], pydantic.Field(alias="pqControls")
    ] = None

    @field_serializer("mode")
    def serialize_mode(self, value):
        if isinstance(value, str):
            try:
                return models.ModeZscalerHec(value)
            except ValueError:
                return value
        return value

    @field_serializer("compress")
    def serialize_compress(self, value):
        if isinstance(value, str):
            try:
                return models.CompressionZscalerHec(value)
            except ValueError:
                return value
        return value


class AuthenticationMethodZscalerHec(str, Enum, metaclass=utils.OpenEnumMeta):
    r"""Select Manual to enter an auth token directly, or select Secret to use a text secret to authenticate"""

    MANUAL = "manual"
    SECRET = "secret"


class AuthTokenMetadatumZscalerHecTypedDict(TypedDict):
    name: str
    value: str
    r"""JavaScript expression to compute field's value, enclosed in quotes or backticks. (Can evaluate to a constant.)"""


class AuthTokenMetadatumZscalerHec(BaseModel):
    name: str

    value: str
    r"""JavaScript expression to compute field's value, enclosed in quotes or backticks. (Can evaluate to a constant.)"""


class AuthTokenZscalerHecTypedDict(TypedDict):
    token: Any
    auth_type: NotRequired[AuthenticationMethodZscalerHec]
    r"""Select Manual to enter an auth token directly, or select Secret to use a text secret to authenticate"""
    token_secret: NotRequired[Any]
    enabled: NotRequired[bool]
    description: NotRequired[str]
    allowed_indexes_at_token: NotRequired[List[str]]
    r"""Enter the values you want to allow in the HEC event index field at the token level. Supports wildcards. To skip validation, leave blank."""
    metadata: NotRequired[List[AuthTokenMetadatumZscalerHecTypedDict]]
    r"""Fields to add to events referencing this token"""


class AuthTokenZscalerHec(BaseModel):
    token: Any

    auth_type: Annotated[
        Annotated[
            Optional[AuthenticationMethodZscalerHec],
            PlainValidator(validate_open_enum(False)),
        ],
        pydantic.Field(alias="authType"),
    ] = AuthenticationMethodZscalerHec.MANUAL
    r"""Select Manual to enter an auth token directly, or select Secret to use a text secret to authenticate"""

    token_secret: Annotated[Optional[Any], pydantic.Field(alias="tokenSecret")] = None

    enabled: Optional[bool] = True

    description: Optional[str] = None

    allowed_indexes_at_token: Annotated[
        Optional[List[str]], pydantic.Field(alias="allowedIndexesAtToken")
    ] = None
    r"""Enter the values you want to allow in the HEC event index field at the token level. Supports wildcards. To skip validation, leave blank."""

    metadata: Optional[List[AuthTokenMetadatumZscalerHec]] = None
    r"""Fields to add to events referencing this token"""

    @field_serializer("auth_type")
    def serialize_auth_type(self, value):
        if isinstance(value, str):
            try:
                return models.AuthenticationMethodZscalerHec(value)
            except ValueError:
                return value
        return value


class MinimumTLSVersionZscalerHec(str, Enum, metaclass=utils.OpenEnumMeta):
    TL_SV1 = "TLSv1"
    TL_SV1_1 = "TLSv1.1"
    TL_SV1_2 = "TLSv1.2"
    TL_SV1_3 = "TLSv1.3"


class MaximumTLSVersionZscalerHec(str, Enum, metaclass=utils.OpenEnumMeta):
    TL_SV1 = "TLSv1"
    TL_SV1_1 = "TLSv1.1"
    TL_SV1_2 = "TLSv1.2"
    TL_SV1_3 = "TLSv1.3"


class TLSSettingsServerSideZscalerHecTypedDict(TypedDict):
    disabled: NotRequired[bool]
    request_cert: NotRequired[bool]
    r"""Require clients to present their certificates. Used to perform client authentication using SSL certs."""
    reject_unauthorized: NotRequired[bool]
    r"""Reject certificates not authorized by a CA in the CA certificate path or by another trusted CA (such as the system's)"""
    common_name_regex: NotRequired[str]
    r"""Regex matching allowable common names in peer certificates' subject attribute"""
    certificate_name: NotRequired[str]
    r"""The name of the predefined certificate"""
    priv_key_path: NotRequired[str]
    r"""Path on server containing the private key to use. PEM format. Can reference $ENV_VARS."""
    passphrase: NotRequired[str]
    r"""Passphrase to use to decrypt private key"""
    cert_path: NotRequired[str]
    r"""Path on server containing certificates to use. PEM format. Can reference $ENV_VARS."""
    ca_path: NotRequired[str]
    r"""Path on server containing CA certificates to use. PEM format. Can reference $ENV_VARS."""
    min_version: NotRequired[MinimumTLSVersionZscalerHec]
    max_version: NotRequired[MaximumTLSVersionZscalerHec]


class TLSSettingsServerSideZscalerHec(BaseModel):
    disabled: Optional[bool] = True

    request_cert: Annotated[Optional[bool], pydantic.Field(alias="requestCert")] = False
    r"""Require clients to present their certificates. Used to perform client authentication using SSL certs."""

    reject_unauthorized: Annotated[
        Optional[bool], pydantic.Field(alias="rejectUnauthorized")
    ] = True
    r"""Reject certificates not authorized by a CA in the CA certificate path or by another trusted CA (such as the system's)"""

    common_name_regex: Annotated[
        Optional[str], pydantic.Field(alias="commonNameRegex")
    ] = "/.*/"
    r"""Regex matching allowable common names in peer certificates' subject attribute"""

    certificate_name: Annotated[
        Optional[str], pydantic.Field(alias="certificateName")
    ] = None
    r"""The name of the predefined certificate"""

    priv_key_path: Annotated[Optional[str], pydantic.Field(alias="privKeyPath")] = None
    r"""Path on server containing the private key to use. PEM format. Can reference $ENV_VARS."""

    passphrase: Optional[str] = None
    r"""Passphrase to use to decrypt private key"""

    cert_path: Annotated[Optional[str], pydantic.Field(alias="certPath")] = None
    r"""Path on server containing certificates to use. PEM format. Can reference $ENV_VARS."""

    ca_path: Annotated[Optional[str], pydantic.Field(alias="caPath")] = None
    r"""Path on server containing CA certificates to use. PEM format. Can reference $ENV_VARS."""

    min_version: Annotated[
        Annotated[
            Optional[MinimumTLSVersionZscalerHec],
            PlainValidator(validate_open_enum(False)),
        ],
        pydantic.Field(alias="minVersion"),
    ] = None

    max_version: Annotated[
        Annotated[
            Optional[MaximumTLSVersionZscalerHec],
            PlainValidator(validate_open_enum(False)),
        ],
        pydantic.Field(alias="maxVersion"),
    ] = None

    @field_serializer("min_version")
    def serialize_min_version(self, value):
        if isinstance(value, str):
            try:
                return models.MinimumTLSVersionZscalerHec(value)
            except ValueError:
                return value
        return value

    @field_serializer("max_version")
    def serialize_max_version(self, value):
        if isinstance(value, str):
            try:
                return models.MaximumTLSVersionZscalerHec(value)
            except ValueError:
                return value
        return value


class MetadatumZscalerHecTypedDict(TypedDict):
    name: str
    value: str
    r"""JavaScript expression to compute field's value, enclosed in quotes or backticks. (Can evaluate to a constant.)"""


class MetadatumZscalerHec(BaseModel):
    name: str

    value: str
    r"""JavaScript expression to compute field's value, enclosed in quotes or backticks. (Can evaluate to a constant.)"""


class InputZscalerHecTypedDict(TypedDict):
    type: TypeZscalerHec
    port: float
    r"""Port to listen on"""
    id: NotRequired[str]
    r"""Unique ID for this input"""
    disabled: NotRequired[bool]
    pipeline: NotRequired[str]
    r"""Pipeline to process data from this Source before sending it through the Routes"""
    send_to_routes: NotRequired[bool]
    r"""Select whether to send data to Routes, or directly to Destinations."""
    environment: NotRequired[str]
    r"""Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere."""
    pq_enabled: NotRequired[bool]
    r"""Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers)."""
    streamtags: NotRequired[List[str]]
    r"""Tags for filtering and grouping in @{product}"""
    connections: NotRequired[List[ConnectionZscalerHecTypedDict]]
    r"""Direct connections to Destinations, and optionally via a Pipeline or a Pack"""
    pq: NotRequired[PqZscalerHecTypedDict]
    host: NotRequired[str]
    r"""Address to bind on. Defaults to 0.0.0.0 (all addresses)."""
    auth_tokens: NotRequired[List[AuthTokenZscalerHecTypedDict]]
    r"""Shared secrets to be provided by any client (Authorization: <token>). If empty, unauthorized access is permitted."""
    tls: NotRequired[TLSSettingsServerSideZscalerHecTypedDict]
    max_active_req: NotRequired[float]
    r"""Maximum number of active requests allowed per Worker Process. Set to 0 for unlimited. Caution: Increasing the limit above the default value, or setting it to unlimited, may degrade performance and reduce throughput."""
    max_requests_per_socket: NotRequired[int]
    r"""Maximum number of requests per socket before @{product} instructs the client to close the connection. Default is 0 (unlimited)."""
    enable_proxy_header: NotRequired[bool]
    r"""Extract the client IP and port from PROXY protocol v1/v2. When enabled, the X-Forwarded-For header is ignored. Disable to use the X-Forwarded-For header for client IP extraction."""
    capture_headers: NotRequired[bool]
    r"""Add request headers to events, in the __headers field"""
    activity_log_sample_rate: NotRequired[float]
    r"""How often request activity is logged at the `info` level. A value of 1 would log every request, 10 every 10th request, etc."""
    request_timeout: NotRequired[float]
    r"""How long to wait for an incoming request to complete before aborting it. Use 0 to disable."""
    socket_timeout: NotRequired[float]
    r"""How long @{product} should wait before assuming that an inactive socket has timed out. To wait forever, set to 0."""
    keep_alive_timeout: NotRequired[float]
    r"""After the last response is sent, @{product} will wait this long for additional data before closing the socket connection. Minimum 1 second, maximum 600 seconds (10 minutes)."""
    enable_health_check: NotRequired[Any]
    ip_allowlist_regex: NotRequired[str]
    r"""Messages from matched IP addresses will be processed, unless also matched by the denylist"""
    ip_denylist_regex: NotRequired[str]
    r"""Messages from matched IP addresses will be ignored. This takes precedence over the allowlist."""
    hec_api: NotRequired[str]
    r"""Absolute path on which to listen for the Zscaler HTTP Event Collector API requests. This input supports the /event endpoint."""
    metadata: NotRequired[List[MetadatumZscalerHecTypedDict]]
    r"""Fields to add to every event. May be overridden by fields added at the token or request level."""
    allowed_indexes: NotRequired[List[str]]
    r"""List values allowed in HEC event index field. Leave blank to skip validation. Supports wildcards. The values here can expand index validation at the token level."""
    hec_acks: NotRequired[bool]
    r"""Whether to enable Zscaler HEC acknowledgements"""
    access_control_allow_origin: NotRequired[List[str]]
    r"""Optionally, list HTTP origins to which @{product} should send CORS (cross-origin resource sharing) Access-Control-Allow-* headers. Supports wildcards."""
    access_control_allow_headers: NotRequired[List[str]]
    r"""Optionally, list HTTP headers that @{product} will send to allowed origins as \"Access-Control-Allow-Headers\" in a CORS preflight response. Use \"*\" to allow all headers."""
    emit_token_metrics: NotRequired[bool]
    r"""Enable to emit per-token (<prefix>.http.perToken) and summary (<prefix>.http.summary) request metrics"""
    description: NotRequired[str]


class InputZscalerHec(BaseModel):
    model_config = ConfigDict(
        populate_by_name=True, arbitrary_types_allowed=True, extra="allow"
    )
    __pydantic_extra__: Dict[str, Any] = pydantic.Field(init=False)

    type: TypeZscalerHec

    port: float
    r"""Port to listen on"""

    id: Optional[str] = None
    r"""Unique ID for this input"""

    disabled: Optional[bool] = False

    pipeline: Optional[str] = None
    r"""Pipeline to process data from this Source before sending it through the Routes"""

    send_to_routes: Annotated[Optional[bool], pydantic.Field(alias="sendToRoutes")] = (
        True
    )
    r"""Select whether to send data to Routes, or directly to Destinations."""

    environment: Optional[str] = None
    r"""Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere."""

    pq_enabled: Annotated[Optional[bool], pydantic.Field(alias="pqEnabled")] = False
    r"""Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers)."""

    streamtags: Optional[List[str]] = None
    r"""Tags for filtering and grouping in @{product}"""

    connections: Optional[List[ConnectionZscalerHec]] = None
    r"""Direct connections to Destinations, and optionally via a Pipeline or a Pack"""

    pq: Optional[PqZscalerHec] = None

    host: Optional[str] = "0.0.0.0"
    r"""Address to bind on. Defaults to 0.0.0.0 (all addresses)."""

    auth_tokens: Annotated[
        Optional[List[AuthTokenZscalerHec]], pydantic.Field(alias="authTokens")
    ] = None
    r"""Shared secrets to be provided by any client (Authorization: <token>). If empty, unauthorized access is permitted."""

    tls: Optional[TLSSettingsServerSideZscalerHec] = None

    max_active_req: Annotated[Optional[float], pydantic.Field(alias="maxActiveReq")] = (
        256
    )
    r"""Maximum number of active requests allowed per Worker Process. Set to 0 for unlimited. Caution: Increasing the limit above the default value, or setting it to unlimited, may degrade performance and reduce throughput."""

    max_requests_per_socket: Annotated[
        Optional[int], pydantic.Field(alias="maxRequestsPerSocket")
    ] = 0
    r"""Maximum number of requests per socket before @{product} instructs the client to close the connection. Default is 0 (unlimited)."""

    enable_proxy_header: Annotated[
        Optional[bool], pydantic.Field(alias="enableProxyHeader")
    ] = False
    r"""Extract the client IP and port from PROXY protocol v1/v2. When enabled, the X-Forwarded-For header is ignored. Disable to use the X-Forwarded-For header for client IP extraction."""

    capture_headers: Annotated[
        Optional[bool], pydantic.Field(alias="captureHeaders")
    ] = False
    r"""Add request headers to events, in the __headers field"""

    activity_log_sample_rate: Annotated[
        Optional[float], pydantic.Field(alias="activityLogSampleRate")
    ] = 100
    r"""How often request activity is logged at the `info` level. A value of 1 would log every request, 10 every 10th request, etc."""

    request_timeout: Annotated[
        Optional[float], pydantic.Field(alias="requestTimeout")
    ] = 0
    r"""How long to wait for an incoming request to complete before aborting it. Use 0 to disable."""

    socket_timeout: Annotated[
        Optional[float], pydantic.Field(alias="socketTimeout")
    ] = 0
    r"""How long @{product} should wait before assuming that an inactive socket has timed out. To wait forever, set to 0."""

    keep_alive_timeout: Annotated[
        Optional[float], pydantic.Field(alias="keepAliveTimeout")
    ] = 5
    r"""After the last response is sent, @{product} will wait this long for additional data before closing the socket connection. Minimum 1 second, maximum 600 seconds (10 minutes)."""

    enable_health_check: Annotated[
        Optional[Any], pydantic.Field(alias="enableHealthCheck")
    ] = None

    ip_allowlist_regex: Annotated[
        Optional[str], pydantic.Field(alias="ipAllowlistRegex")
    ] = "/.*/"
    r"""Messages from matched IP addresses will be processed, unless also matched by the denylist"""

    ip_denylist_regex: Annotated[
        Optional[str], pydantic.Field(alias="ipDenylistRegex")
    ] = "/^$/"
    r"""Messages from matched IP addresses will be ignored. This takes precedence over the allowlist."""

    hec_api: Annotated[Optional[str], pydantic.Field(alias="hecAPI")] = (
        "/services/collector"
    )
    r"""Absolute path on which to listen for the Zscaler HTTP Event Collector API requests. This input supports the /event endpoint."""

    metadata: Optional[List[MetadatumZscalerHec]] = None
    r"""Fields to add to every event. May be overridden by fields added at the token or request level."""

    allowed_indexes: Annotated[
        Optional[List[str]], pydantic.Field(alias="allowedIndexes")
    ] = None
    r"""List values allowed in HEC event index field. Leave blank to skip validation. Supports wildcards. The values here can expand index validation at the token level."""

    hec_acks: Annotated[Optional[bool], pydantic.Field(alias="hecAcks")] = False
    r"""Whether to enable Zscaler HEC acknowledgements"""

    access_control_allow_origin: Annotated[
        Optional[List[str]], pydantic.Field(alias="accessControlAllowOrigin")
    ] = None
    r"""Optionally, list HTTP origins to which @{product} should send CORS (cross-origin resource sharing) Access-Control-Allow-* headers. Supports wildcards."""

    access_control_allow_headers: Annotated[
        Optional[List[str]], pydantic.Field(alias="accessControlAllowHeaders")
    ] = None
    r"""Optionally, list HTTP headers that @{product} will send to allowed origins as \"Access-Control-Allow-Headers\" in a CORS preflight response. Use \"*\" to allow all headers."""

    emit_token_metrics: Annotated[
        Optional[bool], pydantic.Field(alias="emitTokenMetrics")
    ] = False
    r"""Enable to emit per-token (<prefix>.http.perToken) and summary (<prefix>.http.summary) request metrics"""

    description: Optional[str] = None

    @property
    def additional_properties(self):
        return self.__pydantic_extra__

    @additional_properties.setter
    def additional_properties(self, value):
        self.__pydantic_extra__ = value  # pyright: ignore[reportIncompatibleVariableOverride]


class InputTypeSecurityLake(str, Enum):
    SECURITY_LAKE = "security_lake"


class ConnectionSecurityLakeTypedDict(TypedDict):
    output: str
    pipeline: NotRequired[str]


class ConnectionSecurityLake(BaseModel):
    output: str

    pipeline: Optional[str] = None


class ModeSecurityLake(str, Enum, metaclass=utils.OpenEnumMeta):
    r"""With Smart mode, PQ will write events to the filesystem only when it detects backpressure from the processing engine. With Always On mode, PQ will always write events directly to the queue before forwarding them to the processing engine."""

    # Smart
    SMART = "smart"
    # Always On
    ALWAYS = "always"


class CompressionSecurityLake(str, Enum, metaclass=utils.OpenEnumMeta):
    r"""Codec to use to compress the persisted data"""

    # None
    NONE = "none"
    # Gzip
    GZIP = "gzip"


class PqControlsSecurityLakeTypedDict(TypedDict):
    pass


class PqControlsSecurityLake(BaseModel):
    pass


class PqSecurityLakeTypedDict(TypedDict):
    mode: NotRequired[ModeSecurityLake]
    r"""With Smart mode, PQ will write events to the filesystem only when it detects backpressure from the processing engine. With Always On mode, PQ will always write events directly to the queue before forwarding them to the processing engine."""
    max_buffer_size: NotRequired[float]
    r"""The maximum number of events to hold in memory before writing the events to disk"""
    commit_frequency: NotRequired[float]
    r"""The number of events to send downstream before committing that Stream has read them"""
    max_file_size: NotRequired[str]
    r"""The maximum size to store in each queue file before closing and optionally compressing. Enter a numeral with units of KB, MB, etc."""
    max_size: NotRequired[str]
    r"""The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc."""
    path: NotRequired[str]
    r"""The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/inputs/<input-id>"""
    compress: NotRequired[CompressionSecurityLake]
    r"""Codec to use to compress the persisted data"""
    pq_controls: NotRequired[PqControlsSecurityLakeTypedDict]


class PqSecurityLake(BaseModel):
    mode: Annotated[
        Optional[ModeSecurityLake], PlainValidator(validate_open_enum(False))
    ] = ModeSecurityLake.ALWAYS
    r"""With Smart mode, PQ will write events to the filesystem only when it detects backpressure from the processing engine. With Always On mode, PQ will always write events directly to the queue before forwarding them to the processing engine."""

    max_buffer_size: Annotated[
        Optional[float], pydantic.Field(alias="maxBufferSize")
    ] = 1000
    r"""The maximum number of events to hold in memory before writing the events to disk"""

    commit_frequency: Annotated[
        Optional[float], pydantic.Field(alias="commitFrequency")
    ] = 42
    r"""The number of events to send downstream before committing that Stream has read them"""

    max_file_size: Annotated[Optional[str], pydantic.Field(alias="maxFileSize")] = (
        "1 MB"
    )
    r"""The maximum size to store in each queue file before closing and optionally compressing. Enter a numeral with units of KB, MB, etc."""

    max_size: Annotated[Optional[str], pydantic.Field(alias="maxSize")] = "5GB"
    r"""The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc."""

    path: Optional[str] = "$CRIBL_HOME/state/queues"
    r"""The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/inputs/<input-id>"""

    compress: Annotated[
        Optional[CompressionSecurityLake], PlainValidator(validate_open_enum(False))
    ] = CompressionSecurityLake.NONE
    r"""Codec to use to compress the persisted data"""

    pq_controls: Annotated[
        Optional[PqControlsSecurityLake], pydantic.Field(alias="pqControls")
    ] = None

    @field_serializer("mode")
    def serialize_mode(self, value):
        if isinstance(value, str):
            try:
                return models.ModeSecurityLake(value)
            except ValueError:
                return value
        return value

    @field_serializer("compress")
    def serialize_compress(self, value):
        if isinstance(value, str):
            try:
                return models.CompressionSecurityLake(value)
            except ValueError:
                return value
        return value


class InputAuthenticationMethodSecurityLake(str, Enum, metaclass=utils.OpenEnumMeta):
    r"""AWS authentication method. Choose Auto to use IAM roles."""

    # Auto
    AUTO = "auto"
    # Manual
    MANUAL = "manual"
    # Secret Key pair
    SECRET = "secret"


class InputSignatureVersionSecurityLake(str, Enum, metaclass=utils.OpenEnumMeta):
    r"""Signature version to use for signing S3 requests"""

    V2 = "v2"
    V4 = "v4"


class PreprocessSecurityLakeTypedDict(TypedDict):
    disabled: NotRequired[bool]
    command: NotRequired[str]
    r"""Command to feed the data through (via stdin) and process its output (stdout)"""
    args: NotRequired[List[str]]
    r"""Arguments to be added to the custom command"""


class PreprocessSecurityLake(BaseModel):
    disabled: Optional[bool] = True

    command: Optional[str] = None
    r"""Command to feed the data through (via stdin) and process its output (stdout)"""

    args: Optional[List[str]] = None
    r"""Arguments to be added to the custom command"""


class MetadatumSecurityLakeTypedDict(TypedDict):
    name: str
    value: str
    r"""JavaScript expression to compute field's value, enclosed in quotes or backticks. (Can evaluate to a constant.)"""


class MetadatumSecurityLake(BaseModel):
    name: str

    value: str
    r"""JavaScript expression to compute field's value, enclosed in quotes or backticks. (Can evaluate to a constant.)"""


class CheckpointingSecurityLakeTypedDict(TypedDict):
    enabled: NotRequired[bool]
    r"""Resume processing files after an interruption"""
    retries: NotRequired[float]
    r"""The number of times to retry processing when a processing error occurs. If Skip file on error is enabled, this setting is ignored."""


class CheckpointingSecurityLake(BaseModel):
    enabled: Optional[bool] = False
    r"""Resume processing files after an interruption"""

    retries: Optional[float] = 5
    r"""The number of times to retry processing when a processing error occurs. If Skip file on error is enabled, this setting is ignored."""


class TagAfterProcessingSecurityLake(str, Enum, metaclass=utils.OpenEnumMeta):
    FALSE = "false"
    TRUE = "true"


class InputSecurityLakeTypedDict(TypedDict):
    type: InputTypeSecurityLake
    queue_name: str
    r"""The name, URL, or ARN of the SQS queue to read notifications from. When a non-AWS URL is specified, format must be: '{url}/myQueueName'. Example: 'https://host:port/myQueueName'. Value must be a JavaScript expression (which can evaluate to a constant value), enclosed in quotes or backticks. Can be evaluated only at init time. Example referencing a Global Variable: `https://host:port/myQueue-${C.vars.myVar}`."""
    id: NotRequired[str]
    r"""Unique ID for this input"""
    disabled: NotRequired[bool]
    pipeline: NotRequired[str]
    r"""Pipeline to process data from this Source before sending it through the Routes"""
    send_to_routes: NotRequired[bool]
    r"""Select whether to send data to Routes, or directly to Destinations."""
    environment: NotRequired[str]
    r"""Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere."""
    pq_enabled: NotRequired[bool]
    r"""Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers)."""
    streamtags: NotRequired[List[str]]
    r"""Tags for filtering and grouping in @{product}"""
    connections: NotRequired[List[ConnectionSecurityLakeTypedDict]]
    r"""Direct connections to Destinations, and optionally via a Pipeline or a Pack"""
    pq: NotRequired[PqSecurityLakeTypedDict]
    file_filter: NotRequired[str]
    r"""Regex matching file names to download and process. Defaults to: .*"""
    aws_account_id: NotRequired[str]
    r"""SQS queue owner's AWS account ID. Leave empty if SQS queue is in same AWS account."""
    aws_authentication_method: NotRequired[InputAuthenticationMethodSecurityLake]
    r"""AWS authentication method. Choose Auto to use IAM roles."""
    aws_secret_key: NotRequired[str]
    region: NotRequired[str]
    r"""AWS Region where the S3 bucket and SQS queue are located. Required, unless the Queue entry is a URL or ARN that includes a Region."""
    endpoint: NotRequired[str]
    r"""S3 service endpoint. If empty, defaults to the AWS Region-specific endpoint. Otherwise, it must point to S3-compatible endpoint."""
    signature_version: NotRequired[InputSignatureVersionSecurityLake]
    r"""Signature version to use for signing S3 requests"""
    reuse_connections: NotRequired[bool]
    r"""Reuse connections between requests, which can improve performance"""
    reject_unauthorized: NotRequired[bool]
    r"""Reject certificates that cannot be verified against a valid CA, such as self-signed certificates"""
    breaker_rulesets: NotRequired[List[str]]
    r"""A list of event-breaking rulesets that will be applied, in order, to the input data stream"""
    stale_channel_flush_ms: NotRequired[float]
    r"""How long (in milliseconds) the Event Breaker will wait for new data to be sent to a specific channel before flushing the data stream out, as is, to the Pipelines"""
    max_messages: NotRequired[float]
    r"""The maximum number of messages SQS should return in a poll request. Amazon SQS never returns more messages than this value (however, fewer messages might be returned). Valid values: 1 to 10."""
    visibility_timeout: NotRequired[float]
    r"""After messages are retrieved by a ReceiveMessage request, @{product} will hide them from subsequent retrieve requests for at least this duration. You can set this as high as 43200 sec. (12 hours)."""
    num_receivers: NotRequired[float]
    r"""How many receiver processes to run. The higher the number, the better the throughput - at the expense of CPU overhead."""
    socket_timeout: NotRequired[float]
    r"""Socket inactivity timeout (in seconds). Increase this value if timeouts occur due to backpressure."""
    skip_on_error: NotRequired[bool]
    r"""Skip files that trigger a processing error. Disabled by default, which allows retries after processing errors."""
    include_sqs_metadata: NotRequired[bool]
    r"""Attach SQS notification metadata to a __sqsMetadata field on each event"""
    enable_assume_role: NotRequired[bool]
    r"""Use Assume Role credentials to access Amazon S3"""
    assume_role_arn: NotRequired[str]
    r"""Amazon Resource Name (ARN) of the role to assume"""
    assume_role_external_id: NotRequired[str]
    r"""External ID to use when assuming role"""
    duration_seconds: NotRequired[float]
    r"""Duration of the assumed role's session, in seconds. Minimum is 900 (15 minutes), default is 3600 (1 hour), and maximum is 43200 (12 hours)."""
    enable_sqs_assume_role: NotRequired[bool]
    r"""Use Assume Role credentials when accessing Amazon SQS"""
    preprocess: NotRequired[PreprocessSecurityLakeTypedDict]
    metadata: NotRequired[List[MetadatumSecurityLakeTypedDict]]
    r"""Fields to add to events from this input"""
    parquet_chunk_size_mb: NotRequired[float]
    r"""Maximum file size for each Parquet chunk"""
    parquet_chunk_download_timeout: NotRequired[float]
    r"""The maximum time allowed for downloading a Parquet chunk. Processing will stop if a chunk cannot be downloaded within the time specified."""
    checkpointing: NotRequired[CheckpointingSecurityLakeTypedDict]
    poll_timeout: NotRequired[float]
    r"""How long to wait for events before trying polling again. The lower the number the higher the AWS bill. The higher the number the longer it will take for the source to react to configuration changes and system restarts."""
    encoding: NotRequired[str]
    r"""Character encoding to use when parsing ingested data. When not set, @{product} will default to UTF-8 but may incorrectly interpret multi-byte characters."""
    description: NotRequired[str]
    aws_api_key: NotRequired[str]
    aws_secret: NotRequired[str]
    r"""Select or create a stored secret that references your access key and secret key"""
    tag_after_processing: NotRequired[TagAfterProcessingSecurityLake]
    processed_tag_key: NotRequired[str]
    r"""The key for the S3 object tag applied after processing. This field accepts an expression for dynamic generation."""
    processed_tag_value: NotRequired[str]
    r"""The value for the S3 object tag applied after processing. This field accepts an expression for dynamic generation."""


class InputSecurityLake(BaseModel):
    model_config = ConfigDict(
        populate_by_name=True, arbitrary_types_allowed=True, extra="allow"
    )
    __pydantic_extra__: Dict[str, Any] = pydantic.Field(init=False)

    type: InputTypeSecurityLake

    queue_name: Annotated[str, pydantic.Field(alias="queueName")]
    r"""The name, URL, or ARN of the SQS queue to read notifications from. When a non-AWS URL is specified, format must be: '{url}/myQueueName'. Example: 'https://host:port/myQueueName'. Value must be a JavaScript expression (which can evaluate to a constant value), enclosed in quotes or backticks. Can be evaluated only at init time. Example referencing a Global Variable: `https://host:port/myQueue-${C.vars.myVar}`."""

    id: Optional[str] = None
    r"""Unique ID for this input"""

    disabled: Optional[bool] = False

    pipeline: Optional[str] = None
    r"""Pipeline to process data from this Source before sending it through the Routes"""

    send_to_routes: Annotated[Optional[bool], pydantic.Field(alias="sendToRoutes")] = (
        True
    )
    r"""Select whether to send data to Routes, or directly to Destinations."""

    environment: Optional[str] = None
    r"""Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere."""

    pq_enabled: Annotated[Optional[bool], pydantic.Field(alias="pqEnabled")] = False
    r"""Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers)."""

    streamtags: Optional[List[str]] = None
    r"""Tags for filtering and grouping in @{product}"""

    connections: Optional[List[ConnectionSecurityLake]] = None
    r"""Direct connections to Destinations, and optionally via a Pipeline or a Pack"""

    pq: Optional[PqSecurityLake] = None

    file_filter: Annotated[Optional[str], pydantic.Field(alias="fileFilter")] = "/.*/"
    r"""Regex matching file names to download and process. Defaults to: .*"""

    aws_account_id: Annotated[Optional[str], pydantic.Field(alias="awsAccountId")] = (
        None
    )
    r"""SQS queue owner's AWS account ID. Leave empty if SQS queue is in same AWS account."""

    aws_authentication_method: Annotated[
        Annotated[
            Optional[InputAuthenticationMethodSecurityLake],
            PlainValidator(validate_open_enum(False)),
        ],
        pydantic.Field(alias="awsAuthenticationMethod"),
    ] = InputAuthenticationMethodSecurityLake.AUTO
    r"""AWS authentication method. Choose Auto to use IAM roles."""

    aws_secret_key: Annotated[Optional[str], pydantic.Field(alias="awsSecretKey")] = (
        None
    )

    region: Optional[str] = None
    r"""AWS Region where the S3 bucket and SQS queue are located. Required, unless the Queue entry is a URL or ARN that includes a Region."""

    endpoint: Optional[str] = None
    r"""S3 service endpoint. If empty, defaults to the AWS Region-specific endpoint. Otherwise, it must point to S3-compatible endpoint."""

    signature_version: Annotated[
        Annotated[
            Optional[InputSignatureVersionSecurityLake],
            PlainValidator(validate_open_enum(False)),
        ],
        pydantic.Field(alias="signatureVersion"),
    ] = InputSignatureVersionSecurityLake.V4
    r"""Signature version to use for signing S3 requests"""

    reuse_connections: Annotated[
        Optional[bool], pydantic.Field(alias="reuseConnections")
    ] = True
    r"""Reuse connections between requests, which can improve performance"""

    reject_unauthorized: Annotated[
        Optional[bool], pydantic.Field(alias="rejectUnauthorized")
    ] = True
    r"""Reject certificates that cannot be verified against a valid CA, such as self-signed certificates"""

    breaker_rulesets: Annotated[
        Optional[List[str]], pydantic.Field(alias="breakerRulesets")
    ] = None
    r"""A list of event-breaking rulesets that will be applied, in order, to the input data stream"""

    stale_channel_flush_ms: Annotated[
        Optional[float], pydantic.Field(alias="staleChannelFlushMs")
    ] = 10000
    r"""How long (in milliseconds) the Event Breaker will wait for new data to be sent to a specific channel before flushing the data stream out, as is, to the Pipelines"""

    max_messages: Annotated[Optional[float], pydantic.Field(alias="maxMessages")] = 1
    r"""The maximum number of messages SQS should return in a poll request. Amazon SQS never returns more messages than this value (however, fewer messages might be returned). Valid values: 1 to 10."""

    visibility_timeout: Annotated[
        Optional[float], pydantic.Field(alias="visibilityTimeout")
    ] = 600
    r"""After messages are retrieved by a ReceiveMessage request, @{product} will hide them from subsequent retrieve requests for at least this duration. You can set this as high as 43200 sec. (12 hours)."""

    num_receivers: Annotated[Optional[float], pydantic.Field(alias="numReceivers")] = 1
    r"""How many receiver processes to run. The higher the number, the better the throughput - at the expense of CPU overhead."""

    socket_timeout: Annotated[
        Optional[float], pydantic.Field(alias="socketTimeout")
    ] = 300
    r"""Socket inactivity timeout (in seconds). Increase this value if timeouts occur due to backpressure."""

    skip_on_error: Annotated[Optional[bool], pydantic.Field(alias="skipOnError")] = (
        False
    )
    r"""Skip files that trigger a processing error. Disabled by default, which allows retries after processing errors."""

    include_sqs_metadata: Annotated[
        Optional[bool], pydantic.Field(alias="includeSqsMetadata")
    ] = False
    r"""Attach SQS notification metadata to a __sqsMetadata field on each event"""

    enable_assume_role: Annotated[
        Optional[bool], pydantic.Field(alias="enableAssumeRole")
    ] = True
    r"""Use Assume Role credentials to access Amazon S3"""

    assume_role_arn: Annotated[Optional[str], pydantic.Field(alias="assumeRoleArn")] = (
        None
    )
    r"""Amazon Resource Name (ARN) of the role to assume"""

    assume_role_external_id: Annotated[
        Optional[str], pydantic.Field(alias="assumeRoleExternalId")
    ] = None
    r"""External ID to use when assuming role"""

    duration_seconds: Annotated[
        Optional[float], pydantic.Field(alias="durationSeconds")
    ] = 3600
    r"""Duration of the assumed role's session, in seconds. Minimum is 900 (15 minutes), default is 3600 (1 hour), and maximum is 43200 (12 hours)."""

    enable_sqs_assume_role: Annotated[
        Optional[bool], pydantic.Field(alias="enableSQSAssumeRole")
    ] = False
    r"""Use Assume Role credentials when accessing Amazon SQS"""

    preprocess: Optional[PreprocessSecurityLake] = None

    metadata: Optional[List[MetadatumSecurityLake]] = None
    r"""Fields to add to events from this input"""

    parquet_chunk_size_mb: Annotated[
        Optional[float], pydantic.Field(alias="parquetChunkSizeMB")
    ] = 5
    r"""Maximum file size for each Parquet chunk"""

    parquet_chunk_download_timeout: Annotated[
        Optional[float], pydantic.Field(alias="parquetChunkDownloadTimeout")
    ] = 600
    r"""The maximum time allowed for downloading a Parquet chunk. Processing will stop if a chunk cannot be downloaded within the time specified."""

    checkpointing: Optional[CheckpointingSecurityLake] = None

    poll_timeout: Annotated[Optional[float], pydantic.Field(alias="pollTimeout")] = 10
    r"""How long to wait for events before trying polling again. The lower the number the higher the AWS bill. The higher the number the longer it will take for the source to react to configuration changes and system restarts."""

    encoding: Optional[str] = None
    r"""Character encoding to use when parsing ingested data. When not set, @{product} will default to UTF-8 but may incorrectly interpret multi-byte characters."""

    description: Optional[str] = None

    aws_api_key: Annotated[Optional[str], pydantic.Field(alias="awsApiKey")] = None

    aws_secret: Annotated[Optional[str], pydantic.Field(alias="awsSecret")] = None
    r"""Select or create a stored secret that references your access key and secret key"""

    tag_after_processing: Annotated[
        Annotated[
            Optional[TagAfterProcessingSecurityLake],
            PlainValidator(validate_open_enum(False)),
        ],
        pydantic.Field(alias="tagAfterProcessing"),
    ] = None

    processed_tag_key: Annotated[
        Optional[str], pydantic.Field(alias="processedTagKey")
    ] = None
    r"""The key for the S3 object tag applied after processing. This field accepts an expression for dynamic generation."""

    processed_tag_value: Annotated[
        Optional[str], pydantic.Field(alias="processedTagValue")
    ] = None
    r"""The value for the S3 object tag applied after processing. This field accepts an expression for dynamic generation."""

    @property
    def additional_properties(self):
        return self.__pydantic_extra__

    @additional_properties.setter
    def additional_properties(self, value):
        self.__pydantic_extra__ = value  # pyright: ignore[reportIncompatibleVariableOverride]

    @field_serializer("aws_authentication_method")
    def serialize_aws_authentication_method(self, value):
        if isinstance(value, str):
            try:
                return models.InputAuthenticationMethodSecurityLake(value)
            except ValueError:
                return value
        return value

    @field_serializer("signature_version")
    def serialize_signature_version(self, value):
        if isinstance(value, str):
            try:
                return models.InputSignatureVersionSecurityLake(value)
            except ValueError:
                return value
        return value

    @field_serializer("tag_after_processing")
    def serialize_tag_after_processing(self, value):
        if isinstance(value, str):
            try:
                return models.TagAfterProcessingSecurityLake(value)
            except ValueError:
                return value
        return value


class InputTypeNetflow(str, Enum):
    NETFLOW = "netflow"


class ConnectionNetflowTypedDict(TypedDict):
    output: str
    pipeline: NotRequired[str]


class ConnectionNetflow(BaseModel):
    output: str

    pipeline: Optional[str] = None


class ModeNetflow(str, Enum, metaclass=utils.OpenEnumMeta):
    r"""With Smart mode, PQ will write events to the filesystem only when it detects backpressure from the processing engine. With Always On mode, PQ will always write events directly to the queue before forwarding them to the processing engine."""

    # Smart
    SMART = "smart"
    # Always On
    ALWAYS = "always"


class CompressionNetflow(str, Enum, metaclass=utils.OpenEnumMeta):
    r"""Codec to use to compress the persisted data"""

    # None
    NONE = "none"
    # Gzip
    GZIP = "gzip"


class PqControlsNetflowTypedDict(TypedDict):
    pass


class PqControlsNetflow(BaseModel):
    pass


class PqNetflowTypedDict(TypedDict):
    mode: NotRequired[ModeNetflow]
    r"""With Smart mode, PQ will write events to the filesystem only when it detects backpressure from the processing engine. With Always On mode, PQ will always write events directly to the queue before forwarding them to the processing engine."""
    max_buffer_size: NotRequired[float]
    r"""The maximum number of events to hold in memory before writing the events to disk"""
    commit_frequency: NotRequired[float]
    r"""The number of events to send downstream before committing that Stream has read them"""
    max_file_size: NotRequired[str]
    r"""The maximum size to store in each queue file before closing and optionally compressing. Enter a numeral with units of KB, MB, etc."""
    max_size: NotRequired[str]
    r"""The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc."""
    path: NotRequired[str]
    r"""The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/inputs/<input-id>"""
    compress: NotRequired[CompressionNetflow]
    r"""Codec to use to compress the persisted data"""
    pq_controls: NotRequired[PqControlsNetflowTypedDict]


class PqNetflow(BaseModel):
    mode: Annotated[
        Optional[ModeNetflow], PlainValidator(validate_open_enum(False))
    ] = ModeNetflow.ALWAYS
    r"""With Smart mode, PQ will write events to the filesystem only when it detects backpressure from the processing engine. With Always On mode, PQ will always write events directly to the queue before forwarding them to the processing engine."""

    max_buffer_size: Annotated[
        Optional[float], pydantic.Field(alias="maxBufferSize")
    ] = 1000
    r"""The maximum number of events to hold in memory before writing the events to disk"""

    commit_frequency: Annotated[
        Optional[float], pydantic.Field(alias="commitFrequency")
    ] = 42
    r"""The number of events to send downstream before committing that Stream has read them"""

    max_file_size: Annotated[Optional[str], pydantic.Field(alias="maxFileSize")] = (
        "1 MB"
    )
    r"""The maximum size to store in each queue file before closing and optionally compressing. Enter a numeral with units of KB, MB, etc."""

    max_size: Annotated[Optional[str], pydantic.Field(alias="maxSize")] = "5GB"
    r"""The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc."""

    path: Optional[str] = "$CRIBL_HOME/state/queues"
    r"""The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/inputs/<input-id>"""

    compress: Annotated[
        Optional[CompressionNetflow], PlainValidator(validate_open_enum(False))
    ] = CompressionNetflow.NONE
    r"""Codec to use to compress the persisted data"""

    pq_controls: Annotated[
        Optional[PqControlsNetflow], pydantic.Field(alias="pqControls")
    ] = None

    @field_serializer("mode")
    def serialize_mode(self, value):
        if isinstance(value, str):
            try:
                return models.ModeNetflow(value)
            except ValueError:
                return value
        return value

    @field_serializer("compress")
    def serialize_compress(self, value):
        if isinstance(value, str):
            try:
                return models.CompressionNetflow(value)
            except ValueError:
                return value
        return value


class MetadatumNetflowTypedDict(TypedDict):
    name: str
    value: str
    r"""JavaScript expression to compute field's value, enclosed in quotes or backticks. (Can evaluate to a constant.)"""


class MetadatumNetflow(BaseModel):
    name: str

    value: str
    r"""JavaScript expression to compute field's value, enclosed in quotes or backticks. (Can evaluate to a constant.)"""


class InputNetflowTypedDict(TypedDict):
    type: InputTypeNetflow
    id: NotRequired[str]
    r"""Unique ID for this input"""
    disabled: NotRequired[bool]
    pipeline: NotRequired[str]
    r"""Pipeline to process data from this Source before sending it through the Routes"""
    send_to_routes: NotRequired[bool]
    r"""Select whether to send data to Routes, or directly to Destinations."""
    environment: NotRequired[str]
    r"""Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere."""
    pq_enabled: NotRequired[bool]
    r"""Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers)."""
    streamtags: NotRequired[List[str]]
    r"""Tags for filtering and grouping in @{product}"""
    connections: NotRequired[List[ConnectionNetflowTypedDict]]
    r"""Direct connections to Destinations, and optionally via a Pipeline or a Pack"""
    pq: NotRequired[PqNetflowTypedDict]
    host: NotRequired[str]
    r"""Address to bind on. For IPv4 (all addresses), use the default '0.0.0.0'. For IPv6, enter '::' (all addresses) or specify an IP address."""
    port: NotRequired[float]
    r"""Port to listen on"""
    enable_pass_through: NotRequired[bool]
    r"""Allow forwarding of events to a NetFlow destination. Enabling this feature will generate an extra event containing __netflowRaw which can be routed to a NetFlow destination. Note that these events will not count against ingest quota."""
    ip_allowlist_regex: NotRequired[str]
    r"""Messages from matched IP addresses will be processed, unless also matched by the denylist."""
    ip_denylist_regex: NotRequired[str]
    r"""Messages from matched IP addresses will be ignored. This takes precedence over the allowlist."""
    udp_socket_rx_buf_size: NotRequired[float]
    r"""Optionally, set the SO_RCVBUF socket option for the UDP socket. This value tells the operating system how many bytes can be buffered in the kernel before events are dropped. Leave blank to use the OS default. Caution: Increasing this value will affect OS memory utilization."""
    template_cache_minutes: NotRequired[float]
    r"""Specifies how many minutes NetFlow v9 templates are cached before being discarded if not refreshed. Adjust based on your network's template update frequency to optimize performance and memory usage."""
    v5_enabled: NotRequired[bool]
    r"""Accept messages in Netflow V5 format."""
    v9_enabled: NotRequired[bool]
    r"""Accept messages in Netflow V9 format."""
    ipfix_enabled: NotRequired[bool]
    r"""Accept messages in IPFIX format."""
    metadata: NotRequired[List[MetadatumNetflowTypedDict]]
    r"""Fields to add to events from this input"""
    description: NotRequired[str]


class InputNetflow(BaseModel):
    model_config = ConfigDict(
        populate_by_name=True, arbitrary_types_allowed=True, extra="allow"
    )
    __pydantic_extra__: Dict[str, Any] = pydantic.Field(init=False)

    type: InputTypeNetflow

    id: Optional[str] = None
    r"""Unique ID for this input"""

    disabled: Optional[bool] = False

    pipeline: Optional[str] = None
    r"""Pipeline to process data from this Source before sending it through the Routes"""

    send_to_routes: Annotated[Optional[bool], pydantic.Field(alias="sendToRoutes")] = (
        True
    )
    r"""Select whether to send data to Routes, or directly to Destinations."""

    environment: Optional[str] = None
    r"""Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere."""

    pq_enabled: Annotated[Optional[bool], pydantic.Field(alias="pqEnabled")] = False
    r"""Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers)."""

    streamtags: Optional[List[str]] = None
    r"""Tags for filtering and grouping in @{product}"""

    connections: Optional[List[ConnectionNetflow]] = None
    r"""Direct connections to Destinations, and optionally via a Pipeline or a Pack"""

    pq: Optional[PqNetflow] = None

    host: Optional[str] = "0.0.0.0"
    r"""Address to bind on. For IPv4 (all addresses), use the default '0.0.0.0'. For IPv6, enter '::' (all addresses) or specify an IP address."""

    port: Optional[float] = 2055
    r"""Port to listen on"""

    enable_pass_through: Annotated[
        Optional[bool], pydantic.Field(alias="enablePassThrough")
    ] = False
    r"""Allow forwarding of events to a NetFlow destination. Enabling this feature will generate an extra event containing __netflowRaw which can be routed to a NetFlow destination. Note that these events will not count against ingest quota."""

    ip_allowlist_regex: Annotated[
        Optional[str], pydantic.Field(alias="ipAllowlistRegex")
    ] = "/.*/"
    r"""Messages from matched IP addresses will be processed, unless also matched by the denylist."""

    ip_denylist_regex: Annotated[
        Optional[str], pydantic.Field(alias="ipDenylistRegex")
    ] = "/^$/"
    r"""Messages from matched IP addresses will be ignored. This takes precedence over the allowlist."""

    udp_socket_rx_buf_size: Annotated[
        Optional[float], pydantic.Field(alias="udpSocketRxBufSize")
    ] = None
    r"""Optionally, set the SO_RCVBUF socket option for the UDP socket. This value tells the operating system how many bytes can be buffered in the kernel before events are dropped. Leave blank to use the OS default. Caution: Increasing this value will affect OS memory utilization."""

    template_cache_minutes: Annotated[
        Optional[float], pydantic.Field(alias="templateCacheMinutes")
    ] = 30
    r"""Specifies how many minutes NetFlow v9 templates are cached before being discarded if not refreshed. Adjust based on your network's template update frequency to optimize performance and memory usage."""

    v5_enabled: Annotated[Optional[bool], pydantic.Field(alias="v5Enabled")] = True
    r"""Accept messages in Netflow V5 format."""

    v9_enabled: Annotated[Optional[bool], pydantic.Field(alias="v9Enabled")] = True
    r"""Accept messages in Netflow V9 format."""

    ipfix_enabled: Annotated[Optional[bool], pydantic.Field(alias="ipfixEnabled")] = (
        False
    )
    r"""Accept messages in IPFIX format."""

    metadata: Optional[List[MetadatumNetflow]] = None
    r"""Fields to add to events from this input"""

    description: Optional[str] = None

    @property
    def additional_properties(self):
        return self.__pydantic_extra__

    @additional_properties.setter
    def additional_properties(self, value):
        self.__pydantic_extra__ = value  # pyright: ignore[reportIncompatibleVariableOverride]


class TypeWizWebhook(str, Enum):
    WIZ_WEBHOOK = "wiz_webhook"


class ConnectionWizWebhookTypedDict(TypedDict):
    output: str
    pipeline: NotRequired[str]


class ConnectionWizWebhook(BaseModel):
    output: str

    pipeline: Optional[str] = None


class ModeWizWebhook(str, Enum, metaclass=utils.OpenEnumMeta):
    r"""With Smart mode, PQ will write events to the filesystem only when it detects backpressure from the processing engine. With Always On mode, PQ will always write events directly to the queue before forwarding them to the processing engine."""

    # Smart
    SMART = "smart"
    # Always On
    ALWAYS = "always"


class CompressionWizWebhook(str, Enum, metaclass=utils.OpenEnumMeta):
    r"""Codec to use to compress the persisted data"""

    # None
    NONE = "none"
    # Gzip
    GZIP = "gzip"


class PqControlsWizWebhookTypedDict(TypedDict):
    pass


class PqControlsWizWebhook(BaseModel):
    pass


class PqWizWebhookTypedDict(TypedDict):
    mode: NotRequired[ModeWizWebhook]
    r"""With Smart mode, PQ will write events to the filesystem only when it detects backpressure from the processing engine. With Always On mode, PQ will always write events directly to the queue before forwarding them to the processing engine."""
    max_buffer_size: NotRequired[float]
    r"""The maximum number of events to hold in memory before writing the events to disk"""
    commit_frequency: NotRequired[float]
    r"""The number of events to send downstream before committing that Stream has read them"""
    max_file_size: NotRequired[str]
    r"""The maximum size to store in each queue file before closing and optionally compressing. Enter a numeral with units of KB, MB, etc."""
    max_size: NotRequired[str]
    r"""The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc."""
    path: NotRequired[str]
    r"""The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/inputs/<input-id>"""
    compress: NotRequired[CompressionWizWebhook]
    r"""Codec to use to compress the persisted data"""
    pq_controls: NotRequired[PqControlsWizWebhookTypedDict]


class PqWizWebhook(BaseModel):
    mode: Annotated[
        Optional[ModeWizWebhook], PlainValidator(validate_open_enum(False))
    ] = ModeWizWebhook.ALWAYS
    r"""With Smart mode, PQ will write events to the filesystem only when it detects backpressure from the processing engine. With Always On mode, PQ will always write events directly to the queue before forwarding them to the processing engine."""

    max_buffer_size: Annotated[
        Optional[float], pydantic.Field(alias="maxBufferSize")
    ] = 1000
    r"""The maximum number of events to hold in memory before writing the events to disk"""

    commit_frequency: Annotated[
        Optional[float], pydantic.Field(alias="commitFrequency")
    ] = 42
    r"""The number of events to send downstream before committing that Stream has read them"""

    max_file_size: Annotated[Optional[str], pydantic.Field(alias="maxFileSize")] = (
        "1 MB"
    )
    r"""The maximum size to store in each queue file before closing and optionally compressing. Enter a numeral with units of KB, MB, etc."""

    max_size: Annotated[Optional[str], pydantic.Field(alias="maxSize")] = "5GB"
    r"""The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc."""

    path: Optional[str] = "$CRIBL_HOME/state/queues"
    r"""The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/inputs/<input-id>"""

    compress: Annotated[
        Optional[CompressionWizWebhook], PlainValidator(validate_open_enum(False))
    ] = CompressionWizWebhook.NONE
    r"""Codec to use to compress the persisted data"""

    pq_controls: Annotated[
        Optional[PqControlsWizWebhook], pydantic.Field(alias="pqControls")
    ] = None

    @field_serializer("mode")
    def serialize_mode(self, value):
        if isinstance(value, str):
            try:
                return models.ModeWizWebhook(value)
            except ValueError:
                return value
        return value

    @field_serializer("compress")
    def serialize_compress(self, value):
        if isinstance(value, str):
            try:
                return models.CompressionWizWebhook(value)
            except ValueError:
                return value
        return value


class MinimumTLSVersionWizWebhook(str, Enum, metaclass=utils.OpenEnumMeta):
    TL_SV1 = "TLSv1"
    TL_SV1_1 = "TLSv1.1"
    TL_SV1_2 = "TLSv1.2"
    TL_SV1_3 = "TLSv1.3"


class MaximumTLSVersionWizWebhook(str, Enum, metaclass=utils.OpenEnumMeta):
    TL_SV1 = "TLSv1"
    TL_SV1_1 = "TLSv1.1"
    TL_SV1_2 = "TLSv1.2"
    TL_SV1_3 = "TLSv1.3"


class TLSSettingsServerSideWizWebhookTypedDict(TypedDict):
    disabled: NotRequired[bool]
    request_cert: NotRequired[bool]
    r"""Require clients to present their certificates. Used to perform client authentication using SSL certs."""
    reject_unauthorized: NotRequired[bool]
    r"""Reject certificates not authorized by a CA in the CA certificate path or by another trusted CA (such as the system's)"""
    common_name_regex: NotRequired[str]
    r"""Regex matching allowable common names in peer certificates' subject attribute"""
    certificate_name: NotRequired[str]
    r"""The name of the predefined certificate"""
    priv_key_path: NotRequired[str]
    r"""Path on server containing the private key to use. PEM format. Can reference $ENV_VARS."""
    passphrase: NotRequired[str]
    r"""Passphrase to use to decrypt private key"""
    cert_path: NotRequired[str]
    r"""Path on server containing certificates to use. PEM format. Can reference $ENV_VARS."""
    ca_path: NotRequired[str]
    r"""Path on server containing CA certificates to use. PEM format. Can reference $ENV_VARS."""
    min_version: NotRequired[MinimumTLSVersionWizWebhook]
    max_version: NotRequired[MaximumTLSVersionWizWebhook]


class TLSSettingsServerSideWizWebhook(BaseModel):
    disabled: Optional[bool] = True

    request_cert: Annotated[Optional[bool], pydantic.Field(alias="requestCert")] = False
    r"""Require clients to present their certificates. Used to perform client authentication using SSL certs."""

    reject_unauthorized: Annotated[
        Optional[bool], pydantic.Field(alias="rejectUnauthorized")
    ] = True
    r"""Reject certificates not authorized by a CA in the CA certificate path or by another trusted CA (such as the system's)"""

    common_name_regex: Annotated[
        Optional[str], pydantic.Field(alias="commonNameRegex")
    ] = "/.*/"
    r"""Regex matching allowable common names in peer certificates' subject attribute"""

    certificate_name: Annotated[
        Optional[str], pydantic.Field(alias="certificateName")
    ] = None
    r"""The name of the predefined certificate"""

    priv_key_path: Annotated[Optional[str], pydantic.Field(alias="privKeyPath")] = None
    r"""Path on server containing the private key to use. PEM format. Can reference $ENV_VARS."""

    passphrase: Optional[str] = None
    r"""Passphrase to use to decrypt private key"""

    cert_path: Annotated[Optional[str], pydantic.Field(alias="certPath")] = None
    r"""Path on server containing certificates to use. PEM format. Can reference $ENV_VARS."""

    ca_path: Annotated[Optional[str], pydantic.Field(alias="caPath")] = None
    r"""Path on server containing CA certificates to use. PEM format. Can reference $ENV_VARS."""

    min_version: Annotated[
        Annotated[
            Optional[MinimumTLSVersionWizWebhook],
            PlainValidator(validate_open_enum(False)),
        ],
        pydantic.Field(alias="minVersion"),
    ] = None

    max_version: Annotated[
        Annotated[
            Optional[MaximumTLSVersionWizWebhook],
            PlainValidator(validate_open_enum(False)),
        ],
        pydantic.Field(alias="maxVersion"),
    ] = None

    @field_serializer("min_version")
    def serialize_min_version(self, value):
        if isinstance(value, str):
            try:
                return models.MinimumTLSVersionWizWebhook(value)
            except ValueError:
                return value
        return value

    @field_serializer("max_version")
    def serialize_max_version(self, value):
        if isinstance(value, str):
            try:
                return models.MaximumTLSVersionWizWebhook(value)
            except ValueError:
                return value
        return value


class MetadatumWizWebhookTypedDict(TypedDict):
    name: str
    value: str
    r"""JavaScript expression to compute field's value, enclosed in quotes or backticks. (Can evaluate to a constant.)"""


class MetadatumWizWebhook(BaseModel):
    name: str

    value: str
    r"""JavaScript expression to compute field's value, enclosed in quotes or backticks. (Can evaluate to a constant.)"""


class AuthTokensExtMetadatumWizWebhookTypedDict(TypedDict):
    name: str
    value: str
    r"""JavaScript expression to compute field's value, enclosed in quotes or backticks. (Can evaluate to a constant.)"""


class AuthTokensExtMetadatumWizWebhook(BaseModel):
    name: str

    value: str
    r"""JavaScript expression to compute field's value, enclosed in quotes or backticks. (Can evaluate to a constant.)"""


class AuthTokensExtWizWebhookTypedDict(TypedDict):
    token: str
    r"""Shared secret to be provided by any client (Authorization: <token>)"""
    description: NotRequired[str]
    metadata: NotRequired[List[AuthTokensExtMetadatumWizWebhookTypedDict]]
    r"""Fields to add to events referencing this token"""


class AuthTokensExtWizWebhook(BaseModel):
    token: str
    r"""Shared secret to be provided by any client (Authorization: <token>)"""

    description: Optional[str] = None

    metadata: Optional[List[AuthTokensExtMetadatumWizWebhook]] = None
    r"""Fields to add to events referencing this token"""


class InputWizWebhookTypedDict(TypedDict):
    type: TypeWizWebhook
    port: float
    r"""Port to listen on"""
    id: NotRequired[str]
    r"""Unique ID for this input"""
    disabled: NotRequired[bool]
    pipeline: NotRequired[str]
    r"""Pipeline to process data from this Source before sending it through the Routes"""
    send_to_routes: NotRequired[bool]
    r"""Select whether to send data to Routes, or directly to Destinations."""
    environment: NotRequired[str]
    r"""Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere."""
    pq_enabled: NotRequired[bool]
    r"""Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers)."""
    streamtags: NotRequired[List[str]]
    r"""Tags for filtering and grouping in @{product}"""
    connections: NotRequired[List[ConnectionWizWebhookTypedDict]]
    r"""Direct connections to Destinations, and optionally via a Pipeline or a Pack"""
    pq: NotRequired[PqWizWebhookTypedDict]
    host: NotRequired[str]
    r"""Address to bind on. Defaults to 0.0.0.0 (all addresses)."""
    auth_tokens: NotRequired[List[str]]
    r"""Shared secrets to be provided by any client (Authorization: <token>). If empty, unauthorized access is permitted."""
    tls: NotRequired[TLSSettingsServerSideWizWebhookTypedDict]
    max_active_req: NotRequired[float]
    r"""Maximum number of active requests allowed per Worker Process. Set to 0 for unlimited. Caution: Increasing the limit above the default value, or setting it to unlimited, may degrade performance and reduce throughput."""
    max_requests_per_socket: NotRequired[int]
    r"""Maximum number of requests per socket before @{product} instructs the client to close the connection. Default is 0 (unlimited)."""
    enable_proxy_header: NotRequired[bool]
    r"""Extract the client IP and port from PROXY protocol v1/v2. When enabled, the X-Forwarded-For header is ignored. Disable to use the X-Forwarded-For header for client IP extraction."""
    capture_headers: NotRequired[bool]
    r"""Add request headers to events, in the __headers field"""
    activity_log_sample_rate: NotRequired[float]
    r"""How often request activity is logged at the `info` level. A value of 1 would log every request, 10 every 10th request, etc."""
    request_timeout: NotRequired[float]
    r"""How long to wait for an incoming request to complete before aborting it. Use 0 to disable."""
    socket_timeout: NotRequired[float]
    r"""How long @{product} should wait before assuming that an inactive socket has timed out. To wait forever, set to 0."""
    keep_alive_timeout: NotRequired[float]
    r"""After the last response is sent, @{product} will wait this long for additional data before closing the socket connection. Minimum 1 second, maximum 600 seconds (10 minutes)."""
    enable_health_check: NotRequired[bool]
    r"""Expose the /cribl_health endpoint, which returns 200 OK when this Source is healthy"""
    ip_allowlist_regex: NotRequired[str]
    r"""Messages from matched IP addresses will be processed, unless also matched by the denylist"""
    ip_denylist_regex: NotRequired[str]
    r"""Messages from matched IP addresses will be ignored. This takes precedence over the allowlist."""
    breaker_rulesets: NotRequired[List[str]]
    r"""A list of event-breaking rulesets that will be applied, in order, to the input data stream"""
    stale_channel_flush_ms: NotRequired[float]
    r"""How long (in milliseconds) the Event Breaker will wait for new data to be sent to a specific channel before flushing the data stream out, as is, to the Pipelines"""
    metadata: NotRequired[List[MetadatumWizWebhookTypedDict]]
    r"""Fields to add to events from this input"""
    allowed_paths: NotRequired[List[str]]
    r"""List of URI paths accepted by this input. Wildcards are supported (such as /api/v*/hook). Defaults to allow all."""
    allowed_methods: NotRequired[List[str]]
    r"""List of HTTP methods accepted by this input. Wildcards are supported (such as P*, GET). Defaults to allow all."""
    auth_tokens_ext: NotRequired[List[AuthTokensExtWizWebhookTypedDict]]
    r"""Shared secrets to be provided by any client (Authorization: <token>). If empty, unauthorized access is permitted."""
    description: NotRequired[str]


class InputWizWebhook(BaseModel):
    model_config = ConfigDict(
        populate_by_name=True, arbitrary_types_allowed=True, extra="allow"
    )
    __pydantic_extra__: Dict[str, Any] = pydantic.Field(init=False)

    type: TypeWizWebhook

    port: float
    r"""Port to listen on"""

    id: Optional[str] = None
    r"""Unique ID for this input"""

    disabled: Optional[bool] = False

    pipeline: Optional[str] = None
    r"""Pipeline to process data from this Source before sending it through the Routes"""

    send_to_routes: Annotated[Optional[bool], pydantic.Field(alias="sendToRoutes")] = (
        True
    )
    r"""Select whether to send data to Routes, or directly to Destinations."""

    environment: Optional[str] = None
    r"""Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere."""

    pq_enabled: Annotated[Optional[bool], pydantic.Field(alias="pqEnabled")] = False
    r"""Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers)."""

    streamtags: Optional[List[str]] = None
    r"""Tags for filtering and grouping in @{product}"""

    connections: Optional[List[ConnectionWizWebhook]] = None
    r"""Direct connections to Destinations, and optionally via a Pipeline or a Pack"""

    pq: Optional[PqWizWebhook] = None

    host: Optional[str] = "0.0.0.0"
    r"""Address to bind on. Defaults to 0.0.0.0 (all addresses)."""

    auth_tokens: Annotated[Optional[List[str]], pydantic.Field(alias="authTokens")] = (
        None
    )
    r"""Shared secrets to be provided by any client (Authorization: <token>). If empty, unauthorized access is permitted."""

    tls: Optional[TLSSettingsServerSideWizWebhook] = None

    max_active_req: Annotated[Optional[float], pydantic.Field(alias="maxActiveReq")] = (
        256
    )
    r"""Maximum number of active requests allowed per Worker Process. Set to 0 for unlimited. Caution: Increasing the limit above the default value, or setting it to unlimited, may degrade performance and reduce throughput."""

    max_requests_per_socket: Annotated[
        Optional[int], pydantic.Field(alias="maxRequestsPerSocket")
    ] = 0
    r"""Maximum number of requests per socket before @{product} instructs the client to close the connection. Default is 0 (unlimited)."""

    enable_proxy_header: Annotated[
        Optional[bool], pydantic.Field(alias="enableProxyHeader")
    ] = False
    r"""Extract the client IP and port from PROXY protocol v1/v2. When enabled, the X-Forwarded-For header is ignored. Disable to use the X-Forwarded-For header for client IP extraction."""

    capture_headers: Annotated[
        Optional[bool], pydantic.Field(alias="captureHeaders")
    ] = False
    r"""Add request headers to events, in the __headers field"""

    activity_log_sample_rate: Annotated[
        Optional[float], pydantic.Field(alias="activityLogSampleRate")
    ] = 100
    r"""How often request activity is logged at the `info` level. A value of 1 would log every request, 10 every 10th request, etc."""

    request_timeout: Annotated[
        Optional[float], pydantic.Field(alias="requestTimeout")
    ] = 0
    r"""How long to wait for an incoming request to complete before aborting it. Use 0 to disable."""

    socket_timeout: Annotated[
        Optional[float], pydantic.Field(alias="socketTimeout")
    ] = 0
    r"""How long @{product} should wait before assuming that an inactive socket has timed out. To wait forever, set to 0."""

    keep_alive_timeout: Annotated[
        Optional[float], pydantic.Field(alias="keepAliveTimeout")
    ] = 5
    r"""After the last response is sent, @{product} will wait this long for additional data before closing the socket connection. Minimum 1 second, maximum 600 seconds (10 minutes)."""

    enable_health_check: Annotated[
        Optional[bool], pydantic.Field(alias="enableHealthCheck")
    ] = False
    r"""Expose the /cribl_health endpoint, which returns 200 OK when this Source is healthy"""

    ip_allowlist_regex: Annotated[
        Optional[str], pydantic.Field(alias="ipAllowlistRegex")
    ] = "/.*/"
    r"""Messages from matched IP addresses will be processed, unless also matched by the denylist"""

    ip_denylist_regex: Annotated[
        Optional[str], pydantic.Field(alias="ipDenylistRegex")
    ] = "/^$/"
    r"""Messages from matched IP addresses will be ignored. This takes precedence over the allowlist."""

    breaker_rulesets: Annotated[
        Optional[List[str]], pydantic.Field(alias="breakerRulesets")
    ] = None
    r"""A list of event-breaking rulesets that will be applied, in order, to the input data stream"""

    stale_channel_flush_ms: Annotated[
        Optional[float], pydantic.Field(alias="staleChannelFlushMs")
    ] = 10000
    r"""How long (in milliseconds) the Event Breaker will wait for new data to be sent to a specific channel before flushing the data stream out, as is, to the Pipelines"""

    metadata: Optional[List[MetadatumWizWebhook]] = None
    r"""Fields to add to events from this input"""

    allowed_paths: Annotated[
        Optional[List[str]], pydantic.Field(alias="allowedPaths")
    ] = None
    r"""List of URI paths accepted by this input. Wildcards are supported (such as /api/v*/hook). Defaults to allow all."""

    allowed_methods: Annotated[
        Optional[List[str]], pydantic.Field(alias="allowedMethods")
    ] = None
    r"""List of HTTP methods accepted by this input. Wildcards are supported (such as P*, GET). Defaults to allow all."""

    auth_tokens_ext: Annotated[
        Optional[List[AuthTokensExtWizWebhook]], pydantic.Field(alias="authTokensExt")
    ] = None
    r"""Shared secrets to be provided by any client (Authorization: <token>). If empty, unauthorized access is permitted."""

    description: Optional[str] = None

    @property
    def additional_properties(self):
        return self.__pydantic_extra__

    @additional_properties.setter
    def additional_properties(self, value):
        self.__pydantic_extra__ = value  # pyright: ignore[reportIncompatibleVariableOverride]


class TypeWiz(str, Enum):
    WIZ = "wiz"


class ConnectionWizTypedDict(TypedDict):
    output: str
    pipeline: NotRequired[str]


class ConnectionWiz(BaseModel):
    output: str

    pipeline: Optional[str] = None


class ModeWiz(str, Enum, metaclass=utils.OpenEnumMeta):
    r"""With Smart mode, PQ will write events to the filesystem only when it detects backpressure from the processing engine. With Always On mode, PQ will always write events directly to the queue before forwarding them to the processing engine."""

    # Smart
    SMART = "smart"
    # Always On
    ALWAYS = "always"


class CompressionWiz(str, Enum, metaclass=utils.OpenEnumMeta):
    r"""Codec to use to compress the persisted data"""

    # None
    NONE = "none"
    # Gzip
    GZIP = "gzip"


class PqControlsWizTypedDict(TypedDict):
    pass


class PqControlsWiz(BaseModel):
    pass


class PqWizTypedDict(TypedDict):
    mode: NotRequired[ModeWiz]
    r"""With Smart mode, PQ will write events to the filesystem only when it detects backpressure from the processing engine. With Always On mode, PQ will always write events directly to the queue before forwarding them to the processing engine."""
    max_buffer_size: NotRequired[float]
    r"""The maximum number of events to hold in memory before writing the events to disk"""
    commit_frequency: NotRequired[float]
    r"""The number of events to send downstream before committing that Stream has read them"""
    max_file_size: NotRequired[str]
    r"""The maximum size to store in each queue file before closing and optionally compressing. Enter a numeral with units of KB, MB, etc."""
    max_size: NotRequired[str]
    r"""The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc."""
    path: NotRequired[str]
    r"""The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/inputs/<input-id>"""
    compress: NotRequired[CompressionWiz]
    r"""Codec to use to compress the persisted data"""
    pq_controls: NotRequired[PqControlsWizTypedDict]


class PqWiz(BaseModel):
    mode: Annotated[Optional[ModeWiz], PlainValidator(validate_open_enum(False))] = (
        ModeWiz.ALWAYS
    )
    r"""With Smart mode, PQ will write events to the filesystem only when it detects backpressure from the processing engine. With Always On mode, PQ will always write events directly to the queue before forwarding them to the processing engine."""

    max_buffer_size: Annotated[
        Optional[float], pydantic.Field(alias="maxBufferSize")
    ] = 1000
    r"""The maximum number of events to hold in memory before writing the events to disk"""

    commit_frequency: Annotated[
        Optional[float], pydantic.Field(alias="commitFrequency")
    ] = 42
    r"""The number of events to send downstream before committing that Stream has read them"""

    max_file_size: Annotated[Optional[str], pydantic.Field(alias="maxFileSize")] = (
        "1 MB"
    )
    r"""The maximum size to store in each queue file before closing and optionally compressing. Enter a numeral with units of KB, MB, etc."""

    max_size: Annotated[Optional[str], pydantic.Field(alias="maxSize")] = "5GB"
    r"""The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc."""

    path: Optional[str] = "$CRIBL_HOME/state/queues"
    r"""The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/inputs/<input-id>"""

    compress: Annotated[
        Optional[CompressionWiz], PlainValidator(validate_open_enum(False))
    ] = CompressionWiz.NONE
    r"""Codec to use to compress the persisted data"""

    pq_controls: Annotated[
        Optional[PqControlsWiz], pydantic.Field(alias="pqControls")
    ] = None

    @field_serializer("mode")
    def serialize_mode(self, value):
        if isinstance(value, str):
            try:
                return models.ModeWiz(value)
            except ValueError:
                return value
        return value

    @field_serializer("compress")
    def serialize_compress(self, value):
        if isinstance(value, str):
            try:
                return models.CompressionWiz(value)
            except ValueError:
                return value
        return value


class ContentConfigWizTypedDict(TypedDict):
    content_type: str
    r"""The name of the Wiz query"""
    content_description: NotRequired[str]
    enabled: NotRequired[bool]


class ContentConfigWiz(BaseModel):
    content_type: Annotated[str, pydantic.Field(alias="contentType")]
    r"""The name of the Wiz query"""

    content_description: Annotated[
        Optional[str], pydantic.Field(alias="contentDescription")
    ] = None

    enabled: Optional[bool] = False


class MetadatumWizTypedDict(TypedDict):
    name: str
    value: str
    r"""JavaScript expression to compute field's value, enclosed in quotes or backticks. (Can evaluate to a constant.)"""


class MetadatumWiz(BaseModel):
    name: str

    value: str
    r"""JavaScript expression to compute field's value, enclosed in quotes or backticks. (Can evaluate to a constant.)"""


class RetryTypeWiz(str, Enum, metaclass=utils.OpenEnumMeta):
    r"""The algorithm to use when performing HTTP retries"""

    # Disabled
    NONE = "none"
    # Backoff
    BACKOFF = "backoff"
    # Static
    STATIC = "static"


class RetryRulesWizTypedDict(TypedDict):
    type: NotRequired[RetryTypeWiz]
    r"""The algorithm to use when performing HTTP retries"""
    interval: NotRequired[float]
    r"""Time interval between failed request and first retry (kickoff). Maximum allowed value is 20,000 ms (1/3 minute)."""
    limit: NotRequired[float]
    r"""The maximum number of times to retry a failed HTTP request"""
    multiplier: NotRequired[float]
    r"""Base for exponential backoff, e.g., base 2 means that retries will occur after 2, then 4, then 8 seconds, and so on"""
    codes: NotRequired[List[float]]
    r"""List of HTTP codes that trigger a retry. Leave empty to use the default list of 429 and 503."""
    enable_header: NotRequired[bool]
    r"""Honor any Retry-After header that specifies a delay (in seconds) or a timestamp after which to retry the request. The delay is limited to 20 seconds, even if the Retry-After header specifies a longer delay. When disabled, all Retry-After headers are ignored."""
    retry_connect_timeout: NotRequired[bool]
    r"""Make a single retry attempt when a connection timeout (ETIMEDOUT) error occurs"""
    retry_connect_reset: NotRequired[bool]
    r"""Retry request when a connection reset (ECONNRESET) error occurs"""


class RetryRulesWiz(BaseModel):
    type: Annotated[
        Optional[RetryTypeWiz], PlainValidator(validate_open_enum(False))
    ] = RetryTypeWiz.BACKOFF
    r"""The algorithm to use when performing HTTP retries"""

    interval: Optional[float] = 1000
    r"""Time interval between failed request and first retry (kickoff). Maximum allowed value is 20,000 ms (1/3 minute)."""

    limit: Optional[float] = 5
    r"""The maximum number of times to retry a failed HTTP request"""

    multiplier: Optional[float] = 2
    r"""Base for exponential backoff, e.g., base 2 means that retries will occur after 2, then 4, then 8 seconds, and so on"""

    codes: Optional[List[float]] = None
    r"""List of HTTP codes that trigger a retry. Leave empty to use the default list of 429 and 503."""

    enable_header: Annotated[Optional[bool], pydantic.Field(alias="enableHeader")] = (
        True
    )
    r"""Honor any Retry-After header that specifies a delay (in seconds) or a timestamp after which to retry the request. The delay is limited to 20 seconds, even if the Retry-After header specifies a longer delay. When disabled, all Retry-After headers are ignored."""

    retry_connect_timeout: Annotated[
        Optional[bool], pydantic.Field(alias="retryConnectTimeout")
    ] = False
    r"""Make a single retry attempt when a connection timeout (ETIMEDOUT) error occurs"""

    retry_connect_reset: Annotated[
        Optional[bool], pydantic.Field(alias="retryConnectReset")
    ] = False
    r"""Retry request when a connection reset (ECONNRESET) error occurs"""

    @field_serializer("type")
    def serialize_type(self, value):
        if isinstance(value, str):
            try:
                return models.RetryTypeWiz(value)
            except ValueError:
                return value
        return value


class AuthenticationMethodWiz(str, Enum, metaclass=utils.OpenEnumMeta):
    r"""Enter client secret directly, or select a stored secret"""

    MANUAL = "manual"
    SECRET = "secret"


class InputWizTypedDict(TypedDict):
    type: TypeWiz
    auth_url: str
    r"""The authentication URL to generate an OAuth token"""
    client_id: str
    r"""The client ID of the Wiz application"""
    content_config: List[ContentConfigWizTypedDict]
    id: NotRequired[str]
    r"""Unique ID for this input"""
    disabled: NotRequired[bool]
    pipeline: NotRequired[str]
    r"""Pipeline to process data from this Source before sending it through the Routes"""
    send_to_routes: NotRequired[bool]
    r"""Select whether to send data to Routes, or directly to Destinations."""
    environment: NotRequired[str]
    r"""Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere."""
    pq_enabled: NotRequired[bool]
    r"""Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers)."""
    streamtags: NotRequired[List[str]]
    r"""Tags for filtering and grouping in @{product}"""
    connections: NotRequired[List[ConnectionWizTypedDict]]
    r"""Direct connections to Destinations, and optionally via a Pipeline or a Pack"""
    pq: NotRequired[PqWizTypedDict]
    endpoint: NotRequired[str]
    r"""The Wiz GraphQL API endpoint. Example: https://api.us1.app.wiz.io/graphql"""
    auth_audience_override: NotRequired[str]
    r"""The audience to use when requesting an OAuth token for a custom auth URL. When not specified, `wiz-api` will be used."""
    request_timeout: NotRequired[float]
    r"""HTTP request inactivity timeout. Use 0 to disable."""
    keep_alive_time: NotRequired[float]
    r"""How often workers should check in with the scheduler to keep job subscription alive"""
    max_missed_keep_alives: NotRequired[float]
    r"""The number of Keep Alive Time periods before an inactive worker will have its job subscription revoked."""
    ttl: NotRequired[str]
    r"""Time to keep the job's artifacts on disk after job completion. This also affects how long a job is listed in the Job Inspector."""
    ignore_group_jobs_limit: NotRequired[bool]
    r"""When enabled, this job's artifacts are not counted toward the Worker Group's finished job artifacts limit. Artifacts will be removed only after the Collector's configured time to live."""
    metadata: NotRequired[List[MetadatumWizTypedDict]]
    r"""Fields to add to events from this input"""
    retry_rules: NotRequired[RetryRulesWizTypedDict]
    auth_type: NotRequired[AuthenticationMethodWiz]
    r"""Enter client secret directly, or select a stored secret"""
    description: NotRequired[str]
    client_secret: NotRequired[str]
    r"""The client secret of the Wiz application"""
    text_secret: NotRequired[str]
    r"""Select or create a stored text secret"""


class InputWiz(BaseModel):
    model_config = ConfigDict(
        populate_by_name=True, arbitrary_types_allowed=True, extra="allow"
    )
    __pydantic_extra__: Dict[str, Any] = pydantic.Field(init=False)

    type: TypeWiz

    auth_url: Annotated[str, pydantic.Field(alias="authUrl")]
    r"""The authentication URL to generate an OAuth token"""

    client_id: Annotated[str, pydantic.Field(alias="clientId")]
    r"""The client ID of the Wiz application"""

    content_config: Annotated[
        List[ContentConfigWiz], pydantic.Field(alias="contentConfig")
    ]

    id: Optional[str] = None
    r"""Unique ID for this input"""

    disabled: Optional[bool] = False

    pipeline: Optional[str] = None
    r"""Pipeline to process data from this Source before sending it through the Routes"""

    send_to_routes: Annotated[Optional[bool], pydantic.Field(alias="sendToRoutes")] = (
        True
    )
    r"""Select whether to send data to Routes, or directly to Destinations."""

    environment: Optional[str] = None
    r"""Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere."""

    pq_enabled: Annotated[Optional[bool], pydantic.Field(alias="pqEnabled")] = False
    r"""Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers)."""

    streamtags: Optional[List[str]] = None
    r"""Tags for filtering and grouping in @{product}"""

    connections: Optional[List[ConnectionWiz]] = None
    r"""Direct connections to Destinations, and optionally via a Pipeline or a Pack"""

    pq: Optional[PqWiz] = None

    endpoint: Optional[str] = "https://api.<region>.app.wiz.io/graphql"
    r"""The Wiz GraphQL API endpoint. Example: https://api.us1.app.wiz.io/graphql"""

    auth_audience_override: Annotated[
        Optional[str], pydantic.Field(alias="authAudienceOverride")
    ] = None
    r"""The audience to use when requesting an OAuth token for a custom auth URL. When not specified, `wiz-api` will be used."""

    request_timeout: Annotated[
        Optional[float], pydantic.Field(alias="requestTimeout")
    ] = 300
    r"""HTTP request inactivity timeout. Use 0 to disable."""

    keep_alive_time: Annotated[
        Optional[float], pydantic.Field(alias="keepAliveTime")
    ] = 30
    r"""How often workers should check in with the scheduler to keep job subscription alive"""

    max_missed_keep_alives: Annotated[
        Optional[float], pydantic.Field(alias="maxMissedKeepAlives")
    ] = 3
    r"""The number of Keep Alive Time periods before an inactive worker will have its job subscription revoked."""

    ttl: Optional[str] = "4h"
    r"""Time to keep the job's artifacts on disk after job completion. This also affects how long a job is listed in the Job Inspector."""

    ignore_group_jobs_limit: Annotated[
        Optional[bool], pydantic.Field(alias="ignoreGroupJobsLimit")
    ] = False
    r"""When enabled, this job's artifacts are not counted toward the Worker Group's finished job artifacts limit. Artifacts will be removed only after the Collector's configured time to live."""

    metadata: Optional[List[MetadatumWiz]] = None
    r"""Fields to add to events from this input"""

    retry_rules: Annotated[
        Optional[RetryRulesWiz], pydantic.Field(alias="retryRules")
    ] = None

    auth_type: Annotated[
        Annotated[
            Optional[AuthenticationMethodWiz], PlainValidator(validate_open_enum(False))
        ],
        pydantic.Field(alias="authType"),
    ] = AuthenticationMethodWiz.MANUAL
    r"""Enter client secret directly, or select a stored secret"""

    description: Optional[str] = None

    client_secret: Annotated[Optional[str], pydantic.Field(alias="clientSecret")] = None
    r"""The client secret of the Wiz application"""

    text_secret: Annotated[Optional[str], pydantic.Field(alias="textSecret")] = None
    r"""Select or create a stored text secret"""

    @property
    def additional_properties(self):
        return self.__pydantic_extra__

    @additional_properties.setter
    def additional_properties(self, value):
        self.__pydantic_extra__ = value  # pyright: ignore[reportIncompatibleVariableOverride]

    @field_serializer("auth_type")
    def serialize_auth_type(self, value):
        if isinstance(value, str):
            try:
                return models.AuthenticationMethodWiz(value)
            except ValueError:
                return value
        return value


class InputJournalFilesType(str, Enum):
    JOURNAL_FILES = "journal_files"


class InputJournalFilesConnectionTypedDict(TypedDict):
    output: str
    pipeline: NotRequired[str]


class InputJournalFilesConnection(BaseModel):
    output: str

    pipeline: Optional[str] = None


class InputJournalFilesMode(str, Enum, metaclass=utils.OpenEnumMeta):
    r"""With Smart mode, PQ will write events to the filesystem only when it detects backpressure from the processing engine. With Always On mode, PQ will always write events directly to the queue before forwarding them to the processing engine."""

    # Smart
    SMART = "smart"
    # Always On
    ALWAYS = "always"


class InputJournalFilesCompression(str, Enum, metaclass=utils.OpenEnumMeta):
    r"""Codec to use to compress the persisted data"""

    # None
    NONE = "none"
    # Gzip
    GZIP = "gzip"


class InputJournalFilesPqControlsTypedDict(TypedDict):
    pass


class InputJournalFilesPqControls(BaseModel):
    pass


class InputJournalFilesPqTypedDict(TypedDict):
    mode: NotRequired[InputJournalFilesMode]
    r"""With Smart mode, PQ will write events to the filesystem only when it detects backpressure from the processing engine. With Always On mode, PQ will always write events directly to the queue before forwarding them to the processing engine."""
    max_buffer_size: NotRequired[float]
    r"""The maximum number of events to hold in memory before writing the events to disk"""
    commit_frequency: NotRequired[float]
    r"""The number of events to send downstream before committing that Stream has read them"""
    max_file_size: NotRequired[str]
    r"""The maximum size to store in each queue file before closing and optionally compressing. Enter a numeral with units of KB, MB, etc."""
    max_size: NotRequired[str]
    r"""The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc."""
    path: NotRequired[str]
    r"""The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/inputs/<input-id>"""
    compress: NotRequired[InputJournalFilesCompression]
    r"""Codec to use to compress the persisted data"""
    pq_controls: NotRequired[InputJournalFilesPqControlsTypedDict]


class InputJournalFilesPq(BaseModel):
    mode: Annotated[
        Optional[InputJournalFilesMode], PlainValidator(validate_open_enum(False))
    ] = InputJournalFilesMode.ALWAYS
    r"""With Smart mode, PQ will write events to the filesystem only when it detects backpressure from the processing engine. With Always On mode, PQ will always write events directly to the queue before forwarding them to the processing engine."""

    max_buffer_size: Annotated[
        Optional[float], pydantic.Field(alias="maxBufferSize")
    ] = 1000
    r"""The maximum number of events to hold in memory before writing the events to disk"""

    commit_frequency: Annotated[
        Optional[float], pydantic.Field(alias="commitFrequency")
    ] = 42
    r"""The number of events to send downstream before committing that Stream has read them"""

    max_file_size: Annotated[Optional[str], pydantic.Field(alias="maxFileSize")] = (
        "1 MB"
    )
    r"""The maximum size to store in each queue file before closing and optionally compressing. Enter a numeral with units of KB, MB, etc."""

    max_size: Annotated[Optional[str], pydantic.Field(alias="maxSize")] = "5GB"
    r"""The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc."""

    path: Optional[str] = "$CRIBL_HOME/state/queues"
    r"""The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/inputs/<input-id>"""

    compress: Annotated[
        Optional[InputJournalFilesCompression],
        PlainValidator(validate_open_enum(False)),
    ] = InputJournalFilesCompression.NONE
    r"""Codec to use to compress the persisted data"""

    pq_controls: Annotated[
        Optional[InputJournalFilesPqControls], pydantic.Field(alias="pqControls")
    ] = None

    @field_serializer("mode")
    def serialize_mode(self, value):
        if isinstance(value, str):
            try:
                return models.InputJournalFilesMode(value)
            except ValueError:
                return value
        return value

    @field_serializer("compress")
    def serialize_compress(self, value):
        if isinstance(value, str):
            try:
                return models.InputJournalFilesCompression(value)
            except ValueError:
                return value
        return value


class InputJournalFilesRuleTypedDict(TypedDict):
    filter_: str
    r"""JavaScript expression applied to Journal objects. Return 'true' to include it."""
    description: NotRequired[str]
    r"""Optional description of this rule's purpose"""


class InputJournalFilesRule(BaseModel):
    filter_: Annotated[str, pydantic.Field(alias="filter")]
    r"""JavaScript expression applied to Journal objects. Return 'true' to include it."""

    description: Optional[str] = None
    r"""Optional description of this rule's purpose"""


class InputJournalFilesMetadatumTypedDict(TypedDict):
    name: str
    value: str
    r"""JavaScript expression to compute field's value, enclosed in quotes or backticks. (Can evaluate to a constant.)"""


class InputJournalFilesMetadatum(BaseModel):
    name: str

    value: str
    r"""JavaScript expression to compute field's value, enclosed in quotes or backticks. (Can evaluate to a constant.)"""


class InputJournalFilesTypedDict(TypedDict):
    type: InputJournalFilesType
    path: str
    r"""Directory path to search for journals. Environment variables will be resolved, e.g. $CRIBL_EDGE_FS_ROOT/var/log/journal/$MACHINE_ID."""
    journals: List[str]
    r"""The full path of discovered journals are matched against this wildcard list."""
    id: NotRequired[str]
    r"""Unique ID for this input"""
    disabled: NotRequired[bool]
    pipeline: NotRequired[str]
    r"""Pipeline to process data from this Source before sending it through the Routes"""
    send_to_routes: NotRequired[bool]
    r"""Select whether to send data to Routes, or directly to Destinations."""
    environment: NotRequired[str]
    r"""Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere."""
    pq_enabled: NotRequired[bool]
    r"""Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers)."""
    streamtags: NotRequired[List[str]]
    r"""Tags for filtering and grouping in @{product}"""
    connections: NotRequired[List[InputJournalFilesConnectionTypedDict]]
    r"""Direct connections to Destinations, and optionally via a Pipeline or a Pack"""
    pq: NotRequired[InputJournalFilesPqTypedDict]
    interval: NotRequired[float]
    r"""Time, in seconds, between scanning for journals."""
    rules: NotRequired[List[InputJournalFilesRuleTypedDict]]
    r"""Add rules to decide which journal objects to allow. Events are generated if no rules are given or if all the rules' expressions evaluate to true."""
    current_boot: NotRequired[bool]
    r"""Skip log messages that are not part of the current boot session."""
    max_age_dur: NotRequired[str]
    r"""The maximum log message age, in duration form (e.g,: 60s, 4h, 3d, 1w).  Default of no value will apply no max age filters."""
    metadata: NotRequired[List[InputJournalFilesMetadatumTypedDict]]
    r"""Fields to add to events from this input"""
    description: NotRequired[str]


class InputJournalFiles(BaseModel):
    model_config = ConfigDict(
        populate_by_name=True, arbitrary_types_allowed=True, extra="allow"
    )
    __pydantic_extra__: Dict[str, Any] = pydantic.Field(init=False)

    type: InputJournalFilesType

    path: str
    r"""Directory path to search for journals. Environment variables will be resolved, e.g. $CRIBL_EDGE_FS_ROOT/var/log/journal/$MACHINE_ID."""

    journals: List[str]
    r"""The full path of discovered journals are matched against this wildcard list."""

    id: Optional[str] = None
    r"""Unique ID for this input"""

    disabled: Optional[bool] = False

    pipeline: Optional[str] = None
    r"""Pipeline to process data from this Source before sending it through the Routes"""

    send_to_routes: Annotated[Optional[bool], pydantic.Field(alias="sendToRoutes")] = (
        True
    )
    r"""Select whether to send data to Routes, or directly to Destinations."""

    environment: Optional[str] = None
    r"""Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere."""

    pq_enabled: Annotated[Optional[bool], pydantic.Field(alias="pqEnabled")] = False
    r"""Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers)."""

    streamtags: Optional[List[str]] = None
    r"""Tags for filtering and grouping in @{product}"""

    connections: Optional[List[InputJournalFilesConnection]] = None
    r"""Direct connections to Destinations, and optionally via a Pipeline or a Pack"""

    pq: Optional[InputJournalFilesPq] = None

    interval: Optional[float] = 10
    r"""Time, in seconds, between scanning for journals."""

    rules: Optional[List[InputJournalFilesRule]] = None
    r"""Add rules to decide which journal objects to allow. Events are generated if no rules are given or if all the rules' expressions evaluate to true."""

    current_boot: Annotated[Optional[bool], pydantic.Field(alias="currentBoot")] = False
    r"""Skip log messages that are not part of the current boot session."""

    max_age_dur: Annotated[Optional[str], pydantic.Field(alias="maxAgeDur")] = None
    r"""The maximum log message age, in duration form (e.g,: 60s, 4h, 3d, 1w).  Default of no value will apply no max age filters."""

    metadata: Optional[List[InputJournalFilesMetadatum]] = None
    r"""Fields to add to events from this input"""

    description: Optional[str] = None

    @property
    def additional_properties(self):
        return self.__pydantic_extra__

    @additional_properties.setter
    def additional_properties(self, value):
        self.__pydantic_extra__ = value  # pyright: ignore[reportIncompatibleVariableOverride]


class TypeRawUDP(str, Enum):
    RAW_UDP = "raw_udp"


class ConnectionRawUDPTypedDict(TypedDict):
    output: str
    pipeline: NotRequired[str]


class ConnectionRawUDP(BaseModel):
    output: str

    pipeline: Optional[str] = None


class ModeRawUDP(str, Enum, metaclass=utils.OpenEnumMeta):
    r"""With Smart mode, PQ will write events to the filesystem only when it detects backpressure from the processing engine. With Always On mode, PQ will always write events directly to the queue before forwarding them to the processing engine."""

    # Smart
    SMART = "smart"
    # Always On
    ALWAYS = "always"


class CompressionRawUDP(str, Enum, metaclass=utils.OpenEnumMeta):
    r"""Codec to use to compress the persisted data"""

    # None
    NONE = "none"
    # Gzip
    GZIP = "gzip"


class PqControlsRawUDPTypedDict(TypedDict):
    pass


class PqControlsRawUDP(BaseModel):
    pass


class PqRawUDPTypedDict(TypedDict):
    mode: NotRequired[ModeRawUDP]
    r"""With Smart mode, PQ will write events to the filesystem only when it detects backpressure from the processing engine. With Always On mode, PQ will always write events directly to the queue before forwarding them to the processing engine."""
    max_buffer_size: NotRequired[float]
    r"""The maximum number of events to hold in memory before writing the events to disk"""
    commit_frequency: NotRequired[float]
    r"""The number of events to send downstream before committing that Stream has read them"""
    max_file_size: NotRequired[str]
    r"""The maximum size to store in each queue file before closing and optionally compressing. Enter a numeral with units of KB, MB, etc."""
    max_size: NotRequired[str]
    r"""The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc."""
    path: NotRequired[str]
    r"""The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/inputs/<input-id>"""
    compress: NotRequired[CompressionRawUDP]
    r"""Codec to use to compress the persisted data"""
    pq_controls: NotRequired[PqControlsRawUDPTypedDict]


class PqRawUDP(BaseModel):
    mode: Annotated[Optional[ModeRawUDP], PlainValidator(validate_open_enum(False))] = (
        ModeRawUDP.ALWAYS
    )
    r"""With Smart mode, PQ will write events to the filesystem only when it detects backpressure from the processing engine. With Always On mode, PQ will always write events directly to the queue before forwarding them to the processing engine."""

    max_buffer_size: Annotated[
        Optional[float], pydantic.Field(alias="maxBufferSize")
    ] = 1000
    r"""The maximum number of events to hold in memory before writing the events to disk"""

    commit_frequency: Annotated[
        Optional[float], pydantic.Field(alias="commitFrequency")
    ] = 42
    r"""The number of events to send downstream before committing that Stream has read them"""

    max_file_size: Annotated[Optional[str], pydantic.Field(alias="maxFileSize")] = (
        "1 MB"
    )
    r"""The maximum size to store in each queue file before closing and optionally compressing. Enter a numeral with units of KB, MB, etc."""

    max_size: Annotated[Optional[str], pydantic.Field(alias="maxSize")] = "5GB"
    r"""The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc."""

    path: Optional[str] = "$CRIBL_HOME/state/queues"
    r"""The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/inputs/<input-id>"""

    compress: Annotated[
        Optional[CompressionRawUDP], PlainValidator(validate_open_enum(False))
    ] = CompressionRawUDP.NONE
    r"""Codec to use to compress the persisted data"""

    pq_controls: Annotated[
        Optional[PqControlsRawUDP], pydantic.Field(alias="pqControls")
    ] = None

    @field_serializer("mode")
    def serialize_mode(self, value):
        if isinstance(value, str):
            try:
                return models.ModeRawUDP(value)
            except ValueError:
                return value
        return value

    @field_serializer("compress")
    def serialize_compress(self, value):
        if isinstance(value, str):
            try:
                return models.CompressionRawUDP(value)
            except ValueError:
                return value
        return value


class MetadatumRawUDPTypedDict(TypedDict):
    name: str
    value: str
    r"""JavaScript expression to compute field's value, enclosed in quotes or backticks. (Can evaluate to a constant.)"""


class MetadatumRawUDP(BaseModel):
    name: str

    value: str
    r"""JavaScript expression to compute field's value, enclosed in quotes or backticks. (Can evaluate to a constant.)"""


class InputRawUDPTypedDict(TypedDict):
    type: TypeRawUDP
    port: float
    r"""Port to listen on"""
    id: NotRequired[str]
    r"""Unique ID for this input"""
    disabled: NotRequired[bool]
    pipeline: NotRequired[str]
    r"""Pipeline to process data from this Source before sending it through the Routes"""
    send_to_routes: NotRequired[bool]
    r"""Select whether to send data to Routes, or directly to Destinations."""
    environment: NotRequired[str]
    r"""Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere."""
    pq_enabled: NotRequired[bool]
    r"""Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers)."""
    streamtags: NotRequired[List[str]]
    r"""Tags for filtering and grouping in @{product}"""
    connections: NotRequired[List[ConnectionRawUDPTypedDict]]
    r"""Direct connections to Destinations, and optionally via a Pipeline or a Pack"""
    pq: NotRequired[PqRawUDPTypedDict]
    host: NotRequired[str]
    r"""Address to bind on. For IPv4 (all addresses), use the default '0.0.0.0'. For IPv6, enter '::' (all addresses) or specify an IP address."""
    max_buffer_size: NotRequired[float]
    r"""Maximum number of events to buffer when downstream is blocking."""
    ip_whitelist_regex: NotRequired[str]
    r"""Regex matching IP addresses that are allowed to send data"""
    single_msg_udp_packets: NotRequired[bool]
    r"""If true, each UDP packet is assumed to contain a single message. If false, each UDP packet is assumed to contain multiple messages, separated by newlines."""
    ingest_raw_bytes: NotRequired[bool]
    r"""If true, a __rawBytes field will be added to each event containing the raw bytes of the datagram."""
    udp_socket_rx_buf_size: NotRequired[float]
    r"""Optionally, set the SO_RCVBUF socket option for the UDP socket. This value tells the operating system how many bytes can be buffered in the kernel before events are dropped. Leave blank to use the OS default. Caution: Increasing this value will affect OS memory utilization."""
    metadata: NotRequired[List[MetadatumRawUDPTypedDict]]
    r"""Fields to add to events from this input"""
    description: NotRequired[str]


class InputRawUDP(BaseModel):
    model_config = ConfigDict(
        populate_by_name=True, arbitrary_types_allowed=True, extra="allow"
    )
    __pydantic_extra__: Dict[str, Any] = pydantic.Field(init=False)

    type: TypeRawUDP

    port: float
    r"""Port to listen on"""

    id: Optional[str] = None
    r"""Unique ID for this input"""

    disabled: Optional[bool] = False

    pipeline: Optional[str] = None
    r"""Pipeline to process data from this Source before sending it through the Routes"""

    send_to_routes: Annotated[Optional[bool], pydantic.Field(alias="sendToRoutes")] = (
        True
    )
    r"""Select whether to send data to Routes, or directly to Destinations."""

    environment: Optional[str] = None
    r"""Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere."""

    pq_enabled: Annotated[Optional[bool], pydantic.Field(alias="pqEnabled")] = False
    r"""Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers)."""

    streamtags: Optional[List[str]] = None
    r"""Tags for filtering and grouping in @{product}"""

    connections: Optional[List[ConnectionRawUDP]] = None
    r"""Direct connections to Destinations, and optionally via a Pipeline or a Pack"""

    pq: Optional[PqRawUDP] = None

    host: Optional[str] = "0.0.0.0"
    r"""Address to bind on. For IPv4 (all addresses), use the default '0.0.0.0'. For IPv6, enter '::' (all addresses) or specify an IP address."""

    max_buffer_size: Annotated[
        Optional[float], pydantic.Field(alias="maxBufferSize")
    ] = 1000
    r"""Maximum number of events to buffer when downstream is blocking."""

    ip_whitelist_regex: Annotated[
        Optional[str], pydantic.Field(alias="ipWhitelistRegex")
    ] = "/.*/"
    r"""Regex matching IP addresses that are allowed to send data"""

    single_msg_udp_packets: Annotated[
        Optional[bool], pydantic.Field(alias="singleMsgUdpPackets")
    ] = False
    r"""If true, each UDP packet is assumed to contain a single message. If false, each UDP packet is assumed to contain multiple messages, separated by newlines."""

    ingest_raw_bytes: Annotated[
        Optional[bool], pydantic.Field(alias="ingestRawBytes")
    ] = False
    r"""If true, a __rawBytes field will be added to each event containing the raw bytes of the datagram."""

    udp_socket_rx_buf_size: Annotated[
        Optional[float], pydantic.Field(alias="udpSocketRxBufSize")
    ] = None
    r"""Optionally, set the SO_RCVBUF socket option for the UDP socket. This value tells the operating system how many bytes can be buffered in the kernel before events are dropped. Leave blank to use the OS default. Caution: Increasing this value will affect OS memory utilization."""

    metadata: Optional[List[MetadatumRawUDP]] = None
    r"""Fields to add to events from this input"""

    description: Optional[str] = None

    @property
    def additional_properties(self):
        return self.__pydantic_extra__

    @additional_properties.setter
    def additional_properties(self, value):
        self.__pydantic_extra__ = value  # pyright: ignore[reportIncompatibleVariableOverride]


class TypeWinEventLogs(str, Enum):
    WIN_EVENT_LOGS = "win_event_logs"


class ConnectionWinEventLogsTypedDict(TypedDict):
    output: str
    pipeline: NotRequired[str]


class ConnectionWinEventLogs(BaseModel):
    output: str

    pipeline: Optional[str] = None


class ModeWinEventLogs(str, Enum, metaclass=utils.OpenEnumMeta):
    r"""With Smart mode, PQ will write events to the filesystem only when it detects backpressure from the processing engine. With Always On mode, PQ will always write events directly to the queue before forwarding them to the processing engine."""

    # Smart
    SMART = "smart"
    # Always On
    ALWAYS = "always"


class CompressionWinEventLogs(str, Enum, metaclass=utils.OpenEnumMeta):
    r"""Codec to use to compress the persisted data"""

    # None
    NONE = "none"
    # Gzip
    GZIP = "gzip"


class PqControlsWinEventLogsTypedDict(TypedDict):
    pass


class PqControlsWinEventLogs(BaseModel):
    pass


class PqWinEventLogsTypedDict(TypedDict):
    mode: NotRequired[ModeWinEventLogs]
    r"""With Smart mode, PQ will write events to the filesystem only when it detects backpressure from the processing engine. With Always On mode, PQ will always write events directly to the queue before forwarding them to the processing engine."""
    max_buffer_size: NotRequired[float]
    r"""The maximum number of events to hold in memory before writing the events to disk"""
    commit_frequency: NotRequired[float]
    r"""The number of events to send downstream before committing that Stream has read them"""
    max_file_size: NotRequired[str]
    r"""The maximum size to store in each queue file before closing and optionally compressing. Enter a numeral with units of KB, MB, etc."""
    max_size: NotRequired[str]
    r"""The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc."""
    path: NotRequired[str]
    r"""The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/inputs/<input-id>"""
    compress: NotRequired[CompressionWinEventLogs]
    r"""Codec to use to compress the persisted data"""
    pq_controls: NotRequired[PqControlsWinEventLogsTypedDict]


class PqWinEventLogs(BaseModel):
    mode: Annotated[
        Optional[ModeWinEventLogs], PlainValidator(validate_open_enum(False))
    ] = ModeWinEventLogs.ALWAYS
    r"""With Smart mode, PQ will write events to the filesystem only when it detects backpressure from the processing engine. With Always On mode, PQ will always write events directly to the queue before forwarding them to the processing engine."""

    max_buffer_size: Annotated[
        Optional[float], pydantic.Field(alias="maxBufferSize")
    ] = 1000
    r"""The maximum number of events to hold in memory before writing the events to disk"""

    commit_frequency: Annotated[
        Optional[float], pydantic.Field(alias="commitFrequency")
    ] = 42
    r"""The number of events to send downstream before committing that Stream has read them"""

    max_file_size: Annotated[Optional[str], pydantic.Field(alias="maxFileSize")] = (
        "1 MB"
    )
    r"""The maximum size to store in each queue file before closing and optionally compressing. Enter a numeral with units of KB, MB, etc."""

    max_size: Annotated[Optional[str], pydantic.Field(alias="maxSize")] = "5GB"
    r"""The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc."""

    path: Optional[str] = "$CRIBL_HOME/state/queues"
    r"""The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/inputs/<input-id>"""

    compress: Annotated[
        Optional[CompressionWinEventLogs], PlainValidator(validate_open_enum(False))
    ] = CompressionWinEventLogs.NONE
    r"""Codec to use to compress the persisted data"""

    pq_controls: Annotated[
        Optional[PqControlsWinEventLogs], pydantic.Field(alias="pqControls")
    ] = None

    @field_serializer("mode")
    def serialize_mode(self, value):
        if isinstance(value, str):
            try:
                return models.ModeWinEventLogs(value)
            except ValueError:
                return value
        return value

    @field_serializer("compress")
    def serialize_compress(self, value):
        if isinstance(value, str):
            try:
                return models.CompressionWinEventLogs(value)
            except ValueError:
                return value
        return value


class ReadMode(str, Enum, metaclass=utils.OpenEnumMeta):
    r"""Read all stored and future event logs, or only future events"""

    # Entire log
    OLDEST = "oldest"
    # From last entry
    NEWEST = "newest"


class EventFormat(str, Enum, metaclass=utils.OpenEnumMeta):
    r"""Format of individual events"""

    # JSON
    JSON = "json"
    # XML
    XML = "xml"


class MetadatumWinEventLogsTypedDict(TypedDict):
    name: str
    value: str
    r"""JavaScript expression to compute field's value, enclosed in quotes or backticks. (Can evaluate to a constant.)"""


class MetadatumWinEventLogs(BaseModel):
    name: str

    value: str
    r"""JavaScript expression to compute field's value, enclosed in quotes or backticks. (Can evaluate to a constant.)"""


class InputWinEventLogsTypedDict(TypedDict):
    type: TypeWinEventLogs
    log_names: List[str]
    r"""Enter the event logs to collect. Run \"Get-WinEvent -ListLog *\" in PowerShell to see the available logs."""
    id: NotRequired[str]
    r"""Unique ID for this input"""
    disabled: NotRequired[bool]
    pipeline: NotRequired[str]
    r"""Pipeline to process data from this Source before sending it through the Routes"""
    send_to_routes: NotRequired[bool]
    r"""Select whether to send data to Routes, or directly to Destinations."""
    environment: NotRequired[str]
    r"""Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere."""
    pq_enabled: NotRequired[bool]
    r"""Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers)."""
    streamtags: NotRequired[List[str]]
    r"""Tags for filtering and grouping in @{product}"""
    connections: NotRequired[List[ConnectionWinEventLogsTypedDict]]
    r"""Direct connections to Destinations, and optionally via a Pipeline or a Pack"""
    pq: NotRequired[PqWinEventLogsTypedDict]
    read_mode: NotRequired[ReadMode]
    r"""Read all stored and future event logs, or only future events"""
    event_format: NotRequired[EventFormat]
    r"""Format of individual events"""
    disable_native_module: NotRequired[bool]
    r"""Enable to use built-in tools (PowerShell for JSON, wevtutil for XML) to collect event logs instead of native API (default) [Learn more](https://docs.cribl.io/edge/sources-windows-event-logs/#advanced-settings)"""
    interval: NotRequired[float]
    r"""Time, in seconds, between checking for new entries (Applicable for pre-4.8.0 nodes that use Windows Tools)"""
    batch_size: NotRequired[float]
    r"""The maximum number of events to read in one polling interval. A batch size higher than 500 can cause delays when pulling from multiple event logs. (Applicable for pre-4.8.0 nodes that use Windows Tools)"""
    metadata: NotRequired[List[MetadatumWinEventLogsTypedDict]]
    r"""Fields to add to events from this input"""
    max_event_bytes: NotRequired[float]
    r"""The maximum number of bytes in an event before it is flushed to the pipelines"""
    description: NotRequired[str]
    disable_json_rendering: NotRequired[bool]
    r"""Enable/disable the rendering of localized event message strings (Applicable for 4.8.0 nodes and newer that use the Native API)"""
    disable_xml_rendering: NotRequired[bool]
    r"""Enable/disable the rendering of localized event message strings (Applicable for 4.8.0 nodes and newer that use the Native API)"""


class InputWinEventLogs(BaseModel):
    model_config = ConfigDict(
        populate_by_name=True, arbitrary_types_allowed=True, extra="allow"
    )
    __pydantic_extra__: Dict[str, Any] = pydantic.Field(init=False)

    type: TypeWinEventLogs

    log_names: Annotated[List[str], pydantic.Field(alias="logNames")]
    r"""Enter the event logs to collect. Run \"Get-WinEvent -ListLog *\" in PowerShell to see the available logs."""

    id: Optional[str] = None
    r"""Unique ID for this input"""

    disabled: Optional[bool] = False

    pipeline: Optional[str] = None
    r"""Pipeline to process data from this Source before sending it through the Routes"""

    send_to_routes: Annotated[Optional[bool], pydantic.Field(alias="sendToRoutes")] = (
        True
    )
    r"""Select whether to send data to Routes, or directly to Destinations."""

    environment: Optional[str] = None
    r"""Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere."""

    pq_enabled: Annotated[Optional[bool], pydantic.Field(alias="pqEnabled")] = False
    r"""Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers)."""

    streamtags: Optional[List[str]] = None
    r"""Tags for filtering and grouping in @{product}"""

    connections: Optional[List[ConnectionWinEventLogs]] = None
    r"""Direct connections to Destinations, and optionally via a Pipeline or a Pack"""

    pq: Optional[PqWinEventLogs] = None

    read_mode: Annotated[
        Annotated[Optional[ReadMode], PlainValidator(validate_open_enum(False))],
        pydantic.Field(alias="readMode"),
    ] = ReadMode.NEWEST
    r"""Read all stored and future event logs, or only future events"""

    event_format: Annotated[
        Annotated[Optional[EventFormat], PlainValidator(validate_open_enum(False))],
        pydantic.Field(alias="eventFormat"),
    ] = EventFormat.JSON
    r"""Format of individual events"""

    disable_native_module: Annotated[
        Optional[bool], pydantic.Field(alias="disableNativeModule")
    ] = False
    r"""Enable to use built-in tools (PowerShell for JSON, wevtutil for XML) to collect event logs instead of native API (default) [Learn more](https://docs.cribl.io/edge/sources-windows-event-logs/#advanced-settings)"""

    interval: Optional[float] = 10
    r"""Time, in seconds, between checking for new entries (Applicable for pre-4.8.0 nodes that use Windows Tools)"""

    batch_size: Annotated[Optional[float], pydantic.Field(alias="batchSize")] = 500
    r"""The maximum number of events to read in one polling interval. A batch size higher than 500 can cause delays when pulling from multiple event logs. (Applicable for pre-4.8.0 nodes that use Windows Tools)"""

    metadata: Optional[List[MetadatumWinEventLogs]] = None
    r"""Fields to add to events from this input"""

    max_event_bytes: Annotated[
        Optional[float], pydantic.Field(alias="maxEventBytes")
    ] = 51200
    r"""The maximum number of bytes in an event before it is flushed to the pipelines"""

    description: Optional[str] = None

    disable_json_rendering: Annotated[
        Optional[bool], pydantic.Field(alias="disableJsonRendering")
    ] = False
    r"""Enable/disable the rendering of localized event message strings (Applicable for 4.8.0 nodes and newer that use the Native API)"""

    disable_xml_rendering: Annotated[
        Optional[bool], pydantic.Field(alias="disableXmlRendering")
    ] = True
    r"""Enable/disable the rendering of localized event message strings (Applicable for 4.8.0 nodes and newer that use the Native API)"""

    @property
    def additional_properties(self):
        return self.__pydantic_extra__

    @additional_properties.setter
    def additional_properties(self, value):
        self.__pydantic_extra__ = value  # pyright: ignore[reportIncompatibleVariableOverride]

    @field_serializer("read_mode")
    def serialize_read_mode(self, value):
        if isinstance(value, str):
            try:
                return models.ReadMode(value)
            except ValueError:
                return value
        return value

    @field_serializer("event_format")
    def serialize_event_format(self, value):
        if isinstance(value, str):
            try:
                return models.EventFormat(value)
            except ValueError:
                return value
        return value


class TypeWef(str, Enum):
    WEF = "wef"


class ConnectionWefTypedDict(TypedDict):
    output: str
    pipeline: NotRequired[str]


class ConnectionWef(BaseModel):
    output: str

    pipeline: Optional[str] = None


class ModeWef(str, Enum, metaclass=utils.OpenEnumMeta):
    r"""With Smart mode, PQ will write events to the filesystem only when it detects backpressure from the processing engine. With Always On mode, PQ will always write events directly to the queue before forwarding them to the processing engine."""

    # Smart
    SMART = "smart"
    # Always On
    ALWAYS = "always"


class CompressionWef(str, Enum, metaclass=utils.OpenEnumMeta):
    r"""Codec to use to compress the persisted data"""

    # None
    NONE = "none"
    # Gzip
    GZIP = "gzip"


class PqControlsWefTypedDict(TypedDict):
    pass


class PqControlsWef(BaseModel):
    pass


class PqWefTypedDict(TypedDict):
    mode: NotRequired[ModeWef]
    r"""With Smart mode, PQ will write events to the filesystem only when it detects backpressure from the processing engine. With Always On mode, PQ will always write events directly to the queue before forwarding them to the processing engine."""
    max_buffer_size: NotRequired[float]
    r"""The maximum number of events to hold in memory before writing the events to disk"""
    commit_frequency: NotRequired[float]
    r"""The number of events to send downstream before committing that Stream has read them"""
    max_file_size: NotRequired[str]
    r"""The maximum size to store in each queue file before closing and optionally compressing. Enter a numeral with units of KB, MB, etc."""
    max_size: NotRequired[str]
    r"""The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc."""
    path: NotRequired[str]
    r"""The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/inputs/<input-id>"""
    compress: NotRequired[CompressionWef]
    r"""Codec to use to compress the persisted data"""
    pq_controls: NotRequired[PqControlsWefTypedDict]


class PqWef(BaseModel):
    mode: Annotated[Optional[ModeWef], PlainValidator(validate_open_enum(False))] = (
        ModeWef.ALWAYS
    )
    r"""With Smart mode, PQ will write events to the filesystem only when it detects backpressure from the processing engine. With Always On mode, PQ will always write events directly to the queue before forwarding them to the processing engine."""

    max_buffer_size: Annotated[
        Optional[float], pydantic.Field(alias="maxBufferSize")
    ] = 1000
    r"""The maximum number of events to hold in memory before writing the events to disk"""

    commit_frequency: Annotated[
        Optional[float], pydantic.Field(alias="commitFrequency")
    ] = 42
    r"""The number of events to send downstream before committing that Stream has read them"""

    max_file_size: Annotated[Optional[str], pydantic.Field(alias="maxFileSize")] = (
        "1 MB"
    )
    r"""The maximum size to store in each queue file before closing and optionally compressing. Enter a numeral with units of KB, MB, etc."""

    max_size: Annotated[Optional[str], pydantic.Field(alias="maxSize")] = "5GB"
    r"""The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc."""

    path: Optional[str] = "$CRIBL_HOME/state/queues"
    r"""The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/inputs/<input-id>"""

    compress: Annotated[
        Optional[CompressionWef], PlainValidator(validate_open_enum(False))
    ] = CompressionWef.NONE
    r"""Codec to use to compress the persisted data"""

    pq_controls: Annotated[
        Optional[PqControlsWef], pydantic.Field(alias="pqControls")
    ] = None

    @field_serializer("mode")
    def serialize_mode(self, value):
        if isinstance(value, str):
            try:
                return models.ModeWef(value)
            except ValueError:
                return value
        return value

    @field_serializer("compress")
    def serialize_compress(self, value):
        if isinstance(value, str):
            try:
                return models.CompressionWef(value)
            except ValueError:
                return value
        return value


class AuthMethodAuthenticationMethod(str, Enum, metaclass=utils.OpenEnumMeta):
    r"""How to authenticate incoming client connections"""

    # Client certificate
    CLIENT_CERT = "clientCert"
    # Kerberos
    KERBEROS = "kerberos"


class MinimumTLSVersionWef(str, Enum, metaclass=utils.OpenEnumMeta):
    TL_SV1 = "TLSv1"
    TL_SV1_1 = "TLSv1.1"
    TL_SV1_2 = "TLSv1.2"
    TL_SV1_3 = "TLSv1.3"


class MaximumTLSVersionWef(str, Enum, metaclass=utils.OpenEnumMeta):
    TL_SV1 = "TLSv1"
    TL_SV1_1 = "TLSv1.1"
    TL_SV1_2 = "TLSv1.2"
    TL_SV1_3 = "TLSv1.3"


class MTLSSettingsTypedDict(TypedDict):
    priv_key_path: str
    r"""Path on server containing the private key to use. PEM format. Can reference $ENV_VARS."""
    cert_path: str
    r"""Path on server containing certificates to use. PEM format. Can reference $ENV_VARS."""
    ca_path: str
    r"""Server path containing CA certificates (in PEM format) to use. Can reference $ENV_VARS. If multiple certificates are present in a .pem, each must directly certify the one preceding it."""
    disabled: NotRequired[bool]
    r"""Enable TLS"""
    reject_unauthorized: NotRequired[bool]
    r"""Required for WEF certificate authentication"""
    request_cert: NotRequired[bool]
    r"""Required for WEF certificate authentication"""
    certificate_name: NotRequired[str]
    r"""Name of the predefined certificate"""
    passphrase: NotRequired[str]
    r"""Passphrase to use to decrypt private key"""
    common_name_regex: NotRequired[str]
    r"""Regex matching allowable common names in peer certificates' subject attribute"""
    min_version: NotRequired[MinimumTLSVersionWef]
    max_version: NotRequired[MaximumTLSVersionWef]
    ocsp_check: NotRequired[bool]
    r"""Enable OCSP check of certificate"""
    keytab: NotRequired[Any]
    principal: NotRequired[Any]
    ocsp_check_fail_close: NotRequired[bool]
    r"""If enabled, checks will fail on any OCSP error. Otherwise, checks will fail only when a certificate is revoked, ignoring other errors."""


class MTLSSettings(BaseModel):
    priv_key_path: Annotated[str, pydantic.Field(alias="privKeyPath")]
    r"""Path on server containing the private key to use. PEM format. Can reference $ENV_VARS."""

    cert_path: Annotated[str, pydantic.Field(alias="certPath")]
    r"""Path on server containing certificates to use. PEM format. Can reference $ENV_VARS."""

    ca_path: Annotated[str, pydantic.Field(alias="caPath")]
    r"""Server path containing CA certificates (in PEM format) to use. Can reference $ENV_VARS. If multiple certificates are present in a .pem, each must directly certify the one preceding it."""

    disabled: Optional[bool] = False
    r"""Enable TLS"""

    reject_unauthorized: Annotated[
        Optional[bool], pydantic.Field(alias="rejectUnauthorized")
    ] = True
    r"""Required for WEF certificate authentication"""

    request_cert: Annotated[Optional[bool], pydantic.Field(alias="requestCert")] = True
    r"""Required for WEF certificate authentication"""

    certificate_name: Annotated[
        Optional[str], pydantic.Field(alias="certificateName")
    ] = None
    r"""Name of the predefined certificate"""

    passphrase: Optional[str] = None
    r"""Passphrase to use to decrypt private key"""

    common_name_regex: Annotated[
        Optional[str], pydantic.Field(alias="commonNameRegex")
    ] = "/.*/"
    r"""Regex matching allowable common names in peer certificates' subject attribute"""

    min_version: Annotated[
        Annotated[
            Optional[MinimumTLSVersionWef], PlainValidator(validate_open_enum(False))
        ],
        pydantic.Field(alias="minVersion"),
    ] = None

    max_version: Annotated[
        Annotated[
            Optional[MaximumTLSVersionWef], PlainValidator(validate_open_enum(False))
        ],
        pydantic.Field(alias="maxVersion"),
    ] = None

    ocsp_check: Annotated[Optional[bool], pydantic.Field(alias="ocspCheck")] = False
    r"""Enable OCSP check of certificate"""

    keytab: Optional[Any] = None

    principal: Optional[Any] = None

    ocsp_check_fail_close: Annotated[
        Optional[bool], pydantic.Field(alias="ocspCheckFailClose")
    ] = False
    r"""If enabled, checks will fail on any OCSP error. Otherwise, checks will fail only when a certificate is revoked, ignoring other errors."""

    @field_serializer("min_version")
    def serialize_min_version(self, value):
        if isinstance(value, str):
            try:
                return models.MinimumTLSVersionWef(value)
            except ValueError:
                return value
        return value

    @field_serializer("max_version")
    def serialize_max_version(self, value):
        if isinstance(value, str):
            try:
                return models.MaximumTLSVersionWef(value)
            except ValueError:
                return value
        return value


class InputFormat(str, Enum, metaclass=utils.OpenEnumMeta):
    r"""Content format in which the endpoint should deliver events"""

    RAW = "Raw"
    RENDERED_TEXT = "RenderedText"


class QueryBuilderMode(str, Enum, metaclass=utils.OpenEnumMeta):
    SIMPLE = "simple"
    XML = "xml"


class SubscriptionMetadatumTypedDict(TypedDict):
    name: str
    value: str
    r"""JavaScript expression to compute field's value, enclosed in quotes or backticks. (Can evaluate to a constant.)"""


class SubscriptionMetadatum(BaseModel):
    name: str

    value: str
    r"""JavaScript expression to compute field's value, enclosed in quotes or backticks. (Can evaluate to a constant.)"""


class SubscriptionTypedDict(TypedDict):
    subscription_name: str
    targets: List[str]
    r"""The DNS names of the endpoints that should forward these events. You may use wildcards, such as *.mydomain.com"""
    version: NotRequired[str]
    r"""Version UUID for this subscription. If any subscription parameters are modified, this value will change."""
    content_format: NotRequired[InputFormat]
    r"""Content format in which the endpoint should deliver events"""
    heartbeat_interval: NotRequired[float]
    r"""Maximum time (in seconds) between endpoint checkins before considering it unavailable"""
    batch_timeout: NotRequired[float]
    r"""Interval (in seconds) over which the endpoint should collect events before sending them to Stream"""
    read_existing_events: NotRequired[bool]
    r"""Newly subscribed endpoints will send previously existing events. Disable to receive new events only."""
    send_bookmarks: NotRequired[bool]
    r"""Keep track of which events have been received, resuming from that point after a re-subscription. This setting takes precedence over 'Read existing events'. See [Cribl Docs](https://docs.cribl.io/stream/sources-wef/#subscriptions) for more details."""
    compress: NotRequired[bool]
    r"""Receive compressed events from the source"""
    locale: NotRequired[str]
    r"""The RFC-3066 locale the Windows clients should use when sending events. Defaults to \"en-US\"."""
    query_selector: NotRequired[QueryBuilderMode]
    metadata: NotRequired[List[SubscriptionMetadatumTypedDict]]
    r"""Fields to add to events ingested under this subscription"""


class Subscription(BaseModel):
    subscription_name: Annotated[str, pydantic.Field(alias="subscriptionName")]

    targets: List[str]
    r"""The DNS names of the endpoints that should forward these events. You may use wildcards, such as *.mydomain.com"""

    version: Optional[str] = None
    r"""Version UUID for this subscription. If any subscription parameters are modified, this value will change."""

    content_format: Annotated[
        Annotated[Optional[InputFormat], PlainValidator(validate_open_enum(False))],
        pydantic.Field(alias="contentFormat"),
    ] = InputFormat.RAW
    r"""Content format in which the endpoint should deliver events"""

    heartbeat_interval: Annotated[
        Optional[float], pydantic.Field(alias="heartbeatInterval")
    ] = 60
    r"""Maximum time (in seconds) between endpoint checkins before considering it unavailable"""

    batch_timeout: Annotated[Optional[float], pydantic.Field(alias="batchTimeout")] = 60
    r"""Interval (in seconds) over which the endpoint should collect events before sending them to Stream"""

    read_existing_events: Annotated[
        Optional[bool], pydantic.Field(alias="readExistingEvents")
    ] = False
    r"""Newly subscribed endpoints will send previously existing events. Disable to receive new events only."""

    send_bookmarks: Annotated[Optional[bool], pydantic.Field(alias="sendBookmarks")] = (
        True
    )
    r"""Keep track of which events have been received, resuming from that point after a re-subscription. This setting takes precedence over 'Read existing events'. See [Cribl Docs](https://docs.cribl.io/stream/sources-wef/#subscriptions) for more details."""

    compress: Optional[bool] = True
    r"""Receive compressed events from the source"""

    locale: Optional[str] = "en-US"
    r"""The RFC-3066 locale the Windows clients should use when sending events. Defaults to \"en-US\"."""

    query_selector: Annotated[
        Annotated[
            Optional[QueryBuilderMode], PlainValidator(validate_open_enum(False))
        ],
        pydantic.Field(alias="querySelector"),
    ] = QueryBuilderMode.SIMPLE

    metadata: Optional[List[SubscriptionMetadatum]] = None
    r"""Fields to add to events ingested under this subscription"""

    @field_serializer("content_format")
    def serialize_content_format(self, value):
        if isinstance(value, str):
            try:
                return models.InputFormat(value)
            except ValueError:
                return value
        return value

    @field_serializer("query_selector")
    def serialize_query_selector(self, value):
        if isinstance(value, str):
            try:
                return models.QueryBuilderMode(value)
            except ValueError:
                return value
        return value


class MetadatumWefTypedDict(TypedDict):
    name: str
    value: str
    r"""JavaScript expression to compute field's value, enclosed in quotes or backticks. (Can evaluate to a constant.)"""


class MetadatumWef(BaseModel):
    name: str

    value: str
    r"""JavaScript expression to compute field's value, enclosed in quotes or backticks. (Can evaluate to a constant.)"""


class InputWefTypedDict(TypedDict):
    type: TypeWef
    subscriptions: List[SubscriptionTypedDict]
    r"""Subscriptions to events on forwarding endpoints"""
    id: NotRequired[str]
    r"""Unique ID for this input"""
    disabled: NotRequired[bool]
    pipeline: NotRequired[str]
    r"""Pipeline to process data from this Source before sending it through the Routes"""
    send_to_routes: NotRequired[bool]
    r"""Select whether to send data to Routes, or directly to Destinations."""
    environment: NotRequired[str]
    r"""Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere."""
    pq_enabled: NotRequired[bool]
    r"""Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers)."""
    streamtags: NotRequired[List[str]]
    r"""Tags for filtering and grouping in @{product}"""
    connections: NotRequired[List[ConnectionWefTypedDict]]
    r"""Direct connections to Destinations, and optionally via a Pipeline or a Pack"""
    pq: NotRequired[PqWefTypedDict]
    host: NotRequired[str]
    r"""Address to bind on. Defaults to 0.0.0.0 (all addresses)."""
    port: NotRequired[float]
    r"""Port to listen on"""
    auth_method: NotRequired[AuthMethodAuthenticationMethod]
    r"""How to authenticate incoming client connections"""
    tls: NotRequired[MTLSSettingsTypedDict]
    max_active_req: NotRequired[float]
    r"""Maximum number of active requests allowed per Worker Process. Set to 0 for unlimited. Caution: Increasing the limit above the default value, or setting it to unlimited, may degrade performance and reduce throughput."""
    max_requests_per_socket: NotRequired[int]
    r"""Maximum number of requests per socket before @{product} instructs the client to close the connection. Default is 0 (unlimited)."""
    enable_proxy_header: NotRequired[bool]
    r"""Preserve the client’s original IP address in the __srcIpPort field when connecting through an HTTP proxy that supports the X-Forwarded-For header. This does not apply to TCP-layer Proxy Protocol v1/v2."""
    capture_headers: NotRequired[bool]
    r"""Add request headers to events in the __headers field"""
    keep_alive_timeout: NotRequired[float]
    r"""After the last response is sent, @{product} will wait this long for additional data before closing the socket connection. Minimum 1 second, maximum 600 seconds (10 minutes)."""
    enable_health_check: NotRequired[bool]
    r"""Expose the /cribl_health endpoint, which returns 200 OK when this Source is healthy"""
    ip_allowlist_regex: NotRequired[str]
    r"""Messages from matched IP addresses will be processed, unless also matched by the denylist"""
    ip_denylist_regex: NotRequired[str]
    r"""Messages from matched IP addresses will be ignored. This takes precedence over the allowlist."""
    socket_timeout: NotRequired[float]
    r"""How long @{product} should wait before assuming that an inactive socket has timed out. To wait forever, set to 0."""
    ca_fingerprint: NotRequired[str]
    r"""SHA1 fingerprint expected by the client, if it does not match the first certificate in the configured CA chain"""
    keytab: NotRequired[str]
    r"""Path to the keytab file containing the service principal credentials. @{product} will use `/etc/krb5.keytab` if not provided."""
    principal: NotRequired[str]
    r"""Kerberos principal used for authentication, typically in the form HTTP/<hostname>@<REALM>"""
    allow_machine_id_mismatch: NotRequired[bool]
    r"""Allow events to be ingested even if their MachineID does not match the client certificate CN"""
    metadata: NotRequired[List[MetadatumWefTypedDict]]
    r"""Fields to add to events from this input"""
    description: NotRequired[str]
    log_fingerprint_mismatch: NotRequired[bool]
    r"""Log a warning if the client certificate authority (CA) fingerprint does not match the expected value. A mismatch prevents Cribl from receiving events from the Windows Event Forwarder."""


class InputWef(BaseModel):
    model_config = ConfigDict(
        populate_by_name=True, arbitrary_types_allowed=True, extra="allow"
    )
    __pydantic_extra__: Dict[str, Any] = pydantic.Field(init=False)

    type: TypeWef

    subscriptions: List[Subscription]
    r"""Subscriptions to events on forwarding endpoints"""

    id: Optional[str] = None
    r"""Unique ID for this input"""

    disabled: Optional[bool] = False

    pipeline: Optional[str] = None
    r"""Pipeline to process data from this Source before sending it through the Routes"""

    send_to_routes: Annotated[Optional[bool], pydantic.Field(alias="sendToRoutes")] = (
        True
    )
    r"""Select whether to send data to Routes, or directly to Destinations."""

    environment: Optional[str] = None
    r"""Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere."""

    pq_enabled: Annotated[Optional[bool], pydantic.Field(alias="pqEnabled")] = False
    r"""Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers)."""

    streamtags: Optional[List[str]] = None
    r"""Tags for filtering and grouping in @{product}"""

    connections: Optional[List[ConnectionWef]] = None
    r"""Direct connections to Destinations, and optionally via a Pipeline or a Pack"""

    pq: Optional[PqWef] = None

    host: Optional[str] = "0.0.0.0"
    r"""Address to bind on. Defaults to 0.0.0.0 (all addresses)."""

    port: Optional[float] = 5986
    r"""Port to listen on"""

    auth_method: Annotated[
        Annotated[
            Optional[AuthMethodAuthenticationMethod],
            PlainValidator(validate_open_enum(False)),
        ],
        pydantic.Field(alias="authMethod"),
    ] = AuthMethodAuthenticationMethod.CLIENT_CERT
    r"""How to authenticate incoming client connections"""

    tls: Optional[MTLSSettings] = None

    max_active_req: Annotated[Optional[float], pydantic.Field(alias="maxActiveReq")] = (
        256
    )
    r"""Maximum number of active requests allowed per Worker Process. Set to 0 for unlimited. Caution: Increasing the limit above the default value, or setting it to unlimited, may degrade performance and reduce throughput."""

    max_requests_per_socket: Annotated[
        Optional[int], pydantic.Field(alias="maxRequestsPerSocket")
    ] = 0
    r"""Maximum number of requests per socket before @{product} instructs the client to close the connection. Default is 0 (unlimited)."""

    enable_proxy_header: Annotated[
        Optional[bool], pydantic.Field(alias="enableProxyHeader")
    ] = False
    r"""Preserve the client’s original IP address in the __srcIpPort field when connecting through an HTTP proxy that supports the X-Forwarded-For header. This does not apply to TCP-layer Proxy Protocol v1/v2."""

    capture_headers: Annotated[
        Optional[bool], pydantic.Field(alias="captureHeaders")
    ] = False
    r"""Add request headers to events in the __headers field"""

    keep_alive_timeout: Annotated[
        Optional[float], pydantic.Field(alias="keepAliveTimeout")
    ] = 90
    r"""After the last response is sent, @{product} will wait this long for additional data before closing the socket connection. Minimum 1 second, maximum 600 seconds (10 minutes)."""

    enable_health_check: Annotated[
        Optional[bool], pydantic.Field(alias="enableHealthCheck")
    ] = False
    r"""Expose the /cribl_health endpoint, which returns 200 OK when this Source is healthy"""

    ip_allowlist_regex: Annotated[
        Optional[str], pydantic.Field(alias="ipAllowlistRegex")
    ] = "/.*/"
    r"""Messages from matched IP addresses will be processed, unless also matched by the denylist"""

    ip_denylist_regex: Annotated[
        Optional[str], pydantic.Field(alias="ipDenylistRegex")
    ] = "/^$/"
    r"""Messages from matched IP addresses will be ignored. This takes precedence over the allowlist."""

    socket_timeout: Annotated[
        Optional[float], pydantic.Field(alias="socketTimeout")
    ] = 0
    r"""How long @{product} should wait before assuming that an inactive socket has timed out. To wait forever, set to 0."""

    ca_fingerprint: Annotated[Optional[str], pydantic.Field(alias="caFingerprint")] = (
        None
    )
    r"""SHA1 fingerprint expected by the client, if it does not match the first certificate in the configured CA chain"""

    keytab: Optional[str] = None
    r"""Path to the keytab file containing the service principal credentials. @{product} will use `/etc/krb5.keytab` if not provided."""

    principal: Optional[str] = None
    r"""Kerberos principal used for authentication, typically in the form HTTP/<hostname>@<REALM>"""

    allow_machine_id_mismatch: Annotated[
        Optional[bool], pydantic.Field(alias="allowMachineIdMismatch")
    ] = False
    r"""Allow events to be ingested even if their MachineID does not match the client certificate CN"""

    metadata: Optional[List[MetadatumWef]] = None
    r"""Fields to add to events from this input"""

    description: Optional[str] = None

    log_fingerprint_mismatch: Annotated[
        Optional[bool], pydantic.Field(alias="logFingerprintMismatch")
    ] = False
    r"""Log a warning if the client certificate authority (CA) fingerprint does not match the expected value. A mismatch prevents Cribl from receiving events from the Windows Event Forwarder."""

    @property
    def additional_properties(self):
        return self.__pydantic_extra__

    @additional_properties.setter
    def additional_properties(self, value):
        self.__pydantic_extra__ = value  # pyright: ignore[reportIncompatibleVariableOverride]

    @field_serializer("auth_method")
    def serialize_auth_method(self, value):
        if isinstance(value, str):
            try:
                return models.AuthMethodAuthenticationMethod(value)
            except ValueError:
                return value
        return value


class TypeAppscope(str, Enum):
    APPSCOPE = "appscope"


class ConnectionAppscopeTypedDict(TypedDict):
    output: str
    pipeline: NotRequired[str]


class ConnectionAppscope(BaseModel):
    output: str

    pipeline: Optional[str] = None


class ModeAppscope(str, Enum, metaclass=utils.OpenEnumMeta):
    r"""With Smart mode, PQ will write events to the filesystem only when it detects backpressure from the processing engine. With Always On mode, PQ will always write events directly to the queue before forwarding them to the processing engine."""

    # Smart
    SMART = "smart"
    # Always On
    ALWAYS = "always"


class CompressionAppscope(str, Enum, metaclass=utils.OpenEnumMeta):
    r"""Codec to use to compress the persisted data"""

    # None
    NONE = "none"
    # Gzip
    GZIP = "gzip"


class PqControlsAppscopeTypedDict(TypedDict):
    pass


class PqControlsAppscope(BaseModel):
    pass


class PqAppscopeTypedDict(TypedDict):
    mode: NotRequired[ModeAppscope]
    r"""With Smart mode, PQ will write events to the filesystem only when it detects backpressure from the processing engine. With Always On mode, PQ will always write events directly to the queue before forwarding them to the processing engine."""
    max_buffer_size: NotRequired[float]
    r"""The maximum number of events to hold in memory before writing the events to disk"""
    commit_frequency: NotRequired[float]
    r"""The number of events to send downstream before committing that Stream has read them"""
    max_file_size: NotRequired[str]
    r"""The maximum size to store in each queue file before closing and optionally compressing. Enter a numeral with units of KB, MB, etc."""
    max_size: NotRequired[str]
    r"""The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc."""
    path: NotRequired[str]
    r"""The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/inputs/<input-id>"""
    compress: NotRequired[CompressionAppscope]
    r"""Codec to use to compress the persisted data"""
    pq_controls: NotRequired[PqControlsAppscopeTypedDict]


class PqAppscope(BaseModel):
    mode: Annotated[
        Optional[ModeAppscope], PlainValidator(validate_open_enum(False))
    ] = ModeAppscope.ALWAYS
    r"""With Smart mode, PQ will write events to the filesystem only when it detects backpressure from the processing engine. With Always On mode, PQ will always write events directly to the queue before forwarding them to the processing engine."""

    max_buffer_size: Annotated[
        Optional[float], pydantic.Field(alias="maxBufferSize")
    ] = 1000
    r"""The maximum number of events to hold in memory before writing the events to disk"""

    commit_frequency: Annotated[
        Optional[float], pydantic.Field(alias="commitFrequency")
    ] = 42
    r"""The number of events to send downstream before committing that Stream has read them"""

    max_file_size: Annotated[Optional[str], pydantic.Field(alias="maxFileSize")] = (
        "1 MB"
    )
    r"""The maximum size to store in each queue file before closing and optionally compressing. Enter a numeral with units of KB, MB, etc."""

    max_size: Annotated[Optional[str], pydantic.Field(alias="maxSize")] = "5GB"
    r"""The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc."""

    path: Optional[str] = "$CRIBL_HOME/state/queues"
    r"""The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/inputs/<input-id>"""

    compress: Annotated[
        Optional[CompressionAppscope], PlainValidator(validate_open_enum(False))
    ] = CompressionAppscope.NONE
    r"""Codec to use to compress the persisted data"""

    pq_controls: Annotated[
        Optional[PqControlsAppscope], pydantic.Field(alias="pqControls")
    ] = None

    @field_serializer("mode")
    def serialize_mode(self, value):
        if isinstance(value, str):
            try:
                return models.ModeAppscope(value)
            except ValueError:
                return value
        return value

    @field_serializer("compress")
    def serialize_compress(self, value):
        if isinstance(value, str):
            try:
                return models.CompressionAppscope(value)
            except ValueError:
                return value
        return value


class MetadatumAppscopeTypedDict(TypedDict):
    name: str
    value: str
    r"""JavaScript expression to compute field's value, enclosed in quotes or backticks. (Can evaluate to a constant.)"""


class MetadatumAppscope(BaseModel):
    name: str

    value: str
    r"""JavaScript expression to compute field's value, enclosed in quotes or backticks. (Can evaluate to a constant.)"""


class AllowTypedDict(TypedDict):
    procname: str
    r"""Specify the name of a process or family of processes."""
    config: str
    r"""Choose a config to apply to processes that match the process name and/or argument."""
    arg: NotRequired[str]
    r"""Specify a string to substring-match against process command-line."""


class Allow(BaseModel):
    procname: str
    r"""Specify the name of a process or family of processes."""

    config: str
    r"""Choose a config to apply to processes that match the process name and/or argument."""

    arg: Optional[str] = None
    r"""Specify a string to substring-match against process command-line."""


class FilterAppscopeTypedDict(TypedDict):
    allow: NotRequired[List[AllowTypedDict]]
    r"""Specify processes that AppScope should be loaded into, and the config to use."""
    transport_url: NotRequired[str]
    r"""To override the UNIX domain socket or address/port specified in General Settings (while leaving Authentication settings as is), enter a URL."""


class FilterAppscope(BaseModel):
    allow: Optional[List[Allow]] = None
    r"""Specify processes that AppScope should be loaded into, and the config to use."""

    transport_url: Annotated[Optional[str], pydantic.Field(alias="transportURL")] = None
    r"""To override the UNIX domain socket or address/port specified in General Settings (while leaving Authentication settings as is), enter a URL."""


class DataCompressionFormatAppscope(str, Enum, metaclass=utils.OpenEnumMeta):
    NONE = "none"
    GZIP = "gzip"


class PersistenceAppscopeTypedDict(TypedDict):
    enable: NotRequired[bool]
    r"""Spool events and metrics on disk for Cribl Edge and Search"""
    time_window: NotRequired[str]
    r"""Time span for each file bucket"""
    max_data_size: NotRequired[str]
    r"""Maximum disk space allowed to be consumed (examples: 420MB, 4GB). When limit is reached, older data will be deleted."""
    max_data_time: NotRequired[str]
    r"""Maximum amount of time to retain data (examples: 2h, 4d). When limit is reached, older data will be deleted."""
    compress: NotRequired[DataCompressionFormatAppscope]
    dest_path: NotRequired[str]
    r"""Path to use to write metrics. Defaults to $CRIBL_HOME/state/appscope"""


class PersistenceAppscope(BaseModel):
    enable: Optional[bool] = False
    r"""Spool events and metrics on disk for Cribl Edge and Search"""

    time_window: Annotated[Optional[str], pydantic.Field(alias="timeWindow")] = "10m"
    r"""Time span for each file bucket"""

    max_data_size: Annotated[Optional[str], pydantic.Field(alias="maxDataSize")] = "1GB"
    r"""Maximum disk space allowed to be consumed (examples: 420MB, 4GB). When limit is reached, older data will be deleted."""

    max_data_time: Annotated[Optional[str], pydantic.Field(alias="maxDataTime")] = "24h"
    r"""Maximum amount of time to retain data (examples: 2h, 4d). When limit is reached, older data will be deleted."""

    compress: Annotated[
        Optional[DataCompressionFormatAppscope],
        PlainValidator(validate_open_enum(False)),
    ] = DataCompressionFormatAppscope.GZIP

    dest_path: Annotated[Optional[str], pydantic.Field(alias="destPath")] = (
        "$CRIBL_HOME/state/appscope"
    )
    r"""Path to use to write metrics. Defaults to $CRIBL_HOME/state/appscope"""

    @field_serializer("compress")
    def serialize_compress(self, value):
        if isinstance(value, str):
            try:
                return models.DataCompressionFormatAppscope(value)
            except ValueError:
                return value
        return value


class AuthenticationMethodAppscope(str, Enum, metaclass=utils.OpenEnumMeta):
    r"""Select Manual to enter an auth token directly, or select Secret to use a text secret to authenticate"""

    MANUAL = "manual"
    SECRET = "secret"


class MinimumTLSVersionAppscope(str, Enum, metaclass=utils.OpenEnumMeta):
    TL_SV1 = "TLSv1"
    TL_SV1_1 = "TLSv1.1"
    TL_SV1_2 = "TLSv1.2"
    TL_SV1_3 = "TLSv1.3"


class MaximumTLSVersionAppscope(str, Enum, metaclass=utils.OpenEnumMeta):
    TL_SV1 = "TLSv1"
    TL_SV1_1 = "TLSv1.1"
    TL_SV1_2 = "TLSv1.2"
    TL_SV1_3 = "TLSv1.3"


class TLSSettingsServerSideAppscopeTypedDict(TypedDict):
    disabled: NotRequired[bool]
    request_cert: NotRequired[bool]
    r"""Require clients to present their certificates. Used to perform client authentication using SSL certs."""
    reject_unauthorized: NotRequired[bool]
    r"""Reject certificates not authorized by a CA in the CA certificate path or by another trusted CA (such as the system's)"""
    common_name_regex: NotRequired[str]
    r"""Regex matching allowable common names in peer certificates' subject attribute"""
    certificate_name: NotRequired[str]
    r"""The name of the predefined certificate"""
    priv_key_path: NotRequired[str]
    r"""Path on server containing the private key to use. PEM format. Can reference $ENV_VARS."""
    passphrase: NotRequired[str]
    r"""Passphrase to use to decrypt private key"""
    cert_path: NotRequired[str]
    r"""Path on server containing certificates to use. PEM format. Can reference $ENV_VARS."""
    ca_path: NotRequired[str]
    r"""Path on server containing CA certificates to use. PEM format. Can reference $ENV_VARS."""
    min_version: NotRequired[MinimumTLSVersionAppscope]
    max_version: NotRequired[MaximumTLSVersionAppscope]


class TLSSettingsServerSideAppscope(BaseModel):
    disabled: Optional[bool] = True

    request_cert: Annotated[Optional[bool], pydantic.Field(alias="requestCert")] = False
    r"""Require clients to present their certificates. Used to perform client authentication using SSL certs."""

    reject_unauthorized: Annotated[
        Optional[bool], pydantic.Field(alias="rejectUnauthorized")
    ] = True
    r"""Reject certificates not authorized by a CA in the CA certificate path or by another trusted CA (such as the system's)"""

    common_name_regex: Annotated[
        Optional[str], pydantic.Field(alias="commonNameRegex")
    ] = "/.*/"
    r"""Regex matching allowable common names in peer certificates' subject attribute"""

    certificate_name: Annotated[
        Optional[str], pydantic.Field(alias="certificateName")
    ] = None
    r"""The name of the predefined certificate"""

    priv_key_path: Annotated[Optional[str], pydantic.Field(alias="privKeyPath")] = None
    r"""Path on server containing the private key to use. PEM format. Can reference $ENV_VARS."""

    passphrase: Optional[str] = None
    r"""Passphrase to use to decrypt private key"""

    cert_path: Annotated[Optional[str], pydantic.Field(alias="certPath")] = None
    r"""Path on server containing certificates to use. PEM format. Can reference $ENV_VARS."""

    ca_path: Annotated[Optional[str], pydantic.Field(alias="caPath")] = None
    r"""Path on server containing CA certificates to use. PEM format. Can reference $ENV_VARS."""

    min_version: Annotated[
        Annotated[
            Optional[MinimumTLSVersionAppscope],
            PlainValidator(validate_open_enum(False)),
        ],
        pydantic.Field(alias="minVersion"),
    ] = None

    max_version: Annotated[
        Annotated[
            Optional[MaximumTLSVersionAppscope],
            PlainValidator(validate_open_enum(False)),
        ],
        pydantic.Field(alias="maxVersion"),
    ] = None

    @field_serializer("min_version")
    def serialize_min_version(self, value):
        if isinstance(value, str):
            try:
                return models.MinimumTLSVersionAppscope(value)
            except ValueError:
                return value
        return value

    @field_serializer("max_version")
    def serialize_max_version(self, value):
        if isinstance(value, str):
            try:
                return models.MaximumTLSVersionAppscope(value)
            except ValueError:
                return value
        return value


class InputAppscopeTypedDict(TypedDict):
    type: TypeAppscope
    id: NotRequired[str]
    r"""Unique ID for this input"""
    disabled: NotRequired[bool]
    pipeline: NotRequired[str]
    r"""Pipeline to process data from this Source before sending it through the Routes"""
    send_to_routes: NotRequired[bool]
    r"""Select whether to send data to Routes, or directly to Destinations."""
    environment: NotRequired[str]
    r"""Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere."""
    pq_enabled: NotRequired[bool]
    r"""Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers)."""
    streamtags: NotRequired[List[str]]
    r"""Tags for filtering and grouping in @{product}"""
    connections: NotRequired[List[ConnectionAppscopeTypedDict]]
    r"""Direct connections to Destinations, and optionally via a Pipeline or a Pack"""
    pq: NotRequired[PqAppscopeTypedDict]
    ip_whitelist_regex: NotRequired[str]
    r"""Regex matching IP addresses that are allowed to establish a connection"""
    max_active_cxn: NotRequired[float]
    r"""Maximum number of active connections allowed per Worker Process. Use 0 for unlimited."""
    socket_idle_timeout: NotRequired[float]
    r"""How long @{product} should wait before assuming that an inactive socket has timed out. After this time, the connection will be closed. Leave at 0 for no inactive socket monitoring."""
    socket_ending_max_wait: NotRequired[float]
    r"""How long the server will wait after initiating a closure for a client to close its end of the connection. If the client doesn't close the connection within this time, the server will forcefully terminate the socket to prevent resource leaks and ensure efficient connection cleanup and system stability. Leave at 0 for no inactive socket monitoring."""
    socket_max_lifespan: NotRequired[float]
    r"""The maximum duration a socket can remain open, even if active. This helps manage resources and mitigate issues caused by TCP pinning. Set to 0 to disable."""
    enable_proxy_header: NotRequired[bool]
    r"""Enable if the connection is proxied by a device that supports proxy protocol v1 or v2"""
    metadata: NotRequired[List[MetadatumAppscopeTypedDict]]
    r"""Fields to add to events from this input"""
    breaker_rulesets: NotRequired[List[str]]
    r"""A list of event-breaking rulesets that will be applied, in order, to the input data stream"""
    stale_channel_flush_ms: NotRequired[float]
    r"""How long (in milliseconds) the Event Breaker will wait for new data to be sent to a specific channel before flushing the data stream out, as is, to the Pipelines"""
    enable_unix_path: NotRequired[bool]
    r"""Toggle to Yes to specify a file-backed UNIX domain socket connection, instead of a network host and port."""
    filter_: NotRequired[FilterAppscopeTypedDict]
    persistence: NotRequired[PersistenceAppscopeTypedDict]
    auth_type: NotRequired[AuthenticationMethodAppscope]
    r"""Select Manual to enter an auth token directly, or select Secret to use a text secret to authenticate"""
    description: NotRequired[str]
    host: NotRequired[str]
    r"""Address to bind on. Defaults to 0.0.0.0 (all addresses)."""
    port: NotRequired[float]
    r"""Port to listen on"""
    tls: NotRequired[TLSSettingsServerSideAppscopeTypedDict]
    unix_socket_path: NotRequired[str]
    r"""Path to the UNIX domain socket to listen on."""
    unix_socket_perms: NotRequired[str]
    r"""Permissions to set for socket e.g., 777. If empty, falls back to the runtime user's default permissions."""
    auth_token: NotRequired[str]
    r"""Shared secret to be provided by any client (in authToken header field). If empty, unauthorized access is permitted."""
    text_secret: NotRequired[str]
    r"""Select or create a stored text secret"""


class InputAppscope(BaseModel):
    model_config = ConfigDict(
        populate_by_name=True, arbitrary_types_allowed=True, extra="allow"
    )
    __pydantic_extra__: Dict[str, Any] = pydantic.Field(init=False)

    type: TypeAppscope

    id: Optional[str] = None
    r"""Unique ID for this input"""

    disabled: Optional[bool] = False

    pipeline: Optional[str] = None
    r"""Pipeline to process data from this Source before sending it through the Routes"""

    send_to_routes: Annotated[Optional[bool], pydantic.Field(alias="sendToRoutes")] = (
        True
    )
    r"""Select whether to send data to Routes, or directly to Destinations."""

    environment: Optional[str] = None
    r"""Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere."""

    pq_enabled: Annotated[Optional[bool], pydantic.Field(alias="pqEnabled")] = False
    r"""Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers)."""

    streamtags: Optional[List[str]] = None
    r"""Tags for filtering and grouping in @{product}"""

    connections: Optional[List[ConnectionAppscope]] = None
    r"""Direct connections to Destinations, and optionally via a Pipeline or a Pack"""

    pq: Optional[PqAppscope] = None

    ip_whitelist_regex: Annotated[
        Optional[str], pydantic.Field(alias="ipWhitelistRegex")
    ] = "/.*/"
    r"""Regex matching IP addresses that are allowed to establish a connection"""

    max_active_cxn: Annotated[Optional[float], pydantic.Field(alias="maxActiveCxn")] = (
        1000
    )
    r"""Maximum number of active connections allowed per Worker Process. Use 0 for unlimited."""

    socket_idle_timeout: Annotated[
        Optional[float], pydantic.Field(alias="socketIdleTimeout")
    ] = 0
    r"""How long @{product} should wait before assuming that an inactive socket has timed out. After this time, the connection will be closed. Leave at 0 for no inactive socket monitoring."""

    socket_ending_max_wait: Annotated[
        Optional[float], pydantic.Field(alias="socketEndingMaxWait")
    ] = 30
    r"""How long the server will wait after initiating a closure for a client to close its end of the connection. If the client doesn't close the connection within this time, the server will forcefully terminate the socket to prevent resource leaks and ensure efficient connection cleanup and system stability. Leave at 0 for no inactive socket monitoring."""

    socket_max_lifespan: Annotated[
        Optional[float], pydantic.Field(alias="socketMaxLifespan")
    ] = 0
    r"""The maximum duration a socket can remain open, even if active. This helps manage resources and mitigate issues caused by TCP pinning. Set to 0 to disable."""

    enable_proxy_header: Annotated[
        Optional[bool], pydantic.Field(alias="enableProxyHeader")
    ] = False
    r"""Enable if the connection is proxied by a device that supports proxy protocol v1 or v2"""

    metadata: Optional[List[MetadatumAppscope]] = None
    r"""Fields to add to events from this input"""

    breaker_rulesets: Annotated[
        Optional[List[str]], pydantic.Field(alias="breakerRulesets")
    ] = None
    r"""A list of event-breaking rulesets that will be applied, in order, to the input data stream"""

    stale_channel_flush_ms: Annotated[
        Optional[float], pydantic.Field(alias="staleChannelFlushMs")
    ] = 10000
    r"""How long (in milliseconds) the Event Breaker will wait for new data to be sent to a specific channel before flushing the data stream out, as is, to the Pipelines"""

    enable_unix_path: Annotated[
        Optional[bool], pydantic.Field(alias="enableUnixPath")
    ] = False
    r"""Toggle to Yes to specify a file-backed UNIX domain socket connection, instead of a network host and port."""

    filter_: Annotated[Optional[FilterAppscope], pydantic.Field(alias="filter")] = None

    persistence: Optional[PersistenceAppscope] = None

    auth_type: Annotated[
        Annotated[
            Optional[AuthenticationMethodAppscope],
            PlainValidator(validate_open_enum(False)),
        ],
        pydantic.Field(alias="authType"),
    ] = AuthenticationMethodAppscope.MANUAL
    r"""Select Manual to enter an auth token directly, or select Secret to use a text secret to authenticate"""

    description: Optional[str] = None

    host: Optional[str] = None
    r"""Address to bind on. Defaults to 0.0.0.0 (all addresses)."""

    port: Optional[float] = None
    r"""Port to listen on"""

    tls: Optional[TLSSettingsServerSideAppscope] = None

    unix_socket_path: Annotated[
        Optional[str], pydantic.Field(alias="unixSocketPath")
    ] = "$CRIBL_HOME/state/appscope.sock"
    r"""Path to the UNIX domain socket to listen on."""

    unix_socket_perms: Annotated[
        Optional[str], pydantic.Field(alias="unixSocketPerms")
    ] = None
    r"""Permissions to set for socket e.g., 777. If empty, falls back to the runtime user's default permissions."""

    auth_token: Annotated[Optional[str], pydantic.Field(alias="authToken")] = ""
    r"""Shared secret to be provided by any client (in authToken header field). If empty, unauthorized access is permitted."""

    text_secret: Annotated[Optional[str], pydantic.Field(alias="textSecret")] = None
    r"""Select or create a stored text secret"""

    @property
    def additional_properties(self):
        return self.__pydantic_extra__

    @additional_properties.setter
    def additional_properties(self, value):
        self.__pydantic_extra__ = value  # pyright: ignore[reportIncompatibleVariableOverride]

    @field_serializer("auth_type")
    def serialize_auth_type(self, value):
        if isinstance(value, str):
            try:
                return models.AuthenticationMethodAppscope(value)
            except ValueError:
                return value
        return value


class TypeTCP(str, Enum):
    TCP = "tcp"


class ConnectionTCPTypedDict(TypedDict):
    output: str
    pipeline: NotRequired[str]


class ConnectionTCP(BaseModel):
    output: str

    pipeline: Optional[str] = None


class ModeTCP(str, Enum, metaclass=utils.OpenEnumMeta):
    r"""With Smart mode, PQ will write events to the filesystem only when it detects backpressure from the processing engine. With Always On mode, PQ will always write events directly to the queue before forwarding them to the processing engine."""

    # Smart
    SMART = "smart"
    # Always On
    ALWAYS = "always"


class CompressionTCP(str, Enum, metaclass=utils.OpenEnumMeta):
    r"""Codec to use to compress the persisted data"""

    # None
    NONE = "none"
    # Gzip
    GZIP = "gzip"


class PqControlsTCPTypedDict(TypedDict):
    pass


class PqControlsTCP(BaseModel):
    pass


class PqTCPTypedDict(TypedDict):
    mode: NotRequired[ModeTCP]
    r"""With Smart mode, PQ will write events to the filesystem only when it detects backpressure from the processing engine. With Always On mode, PQ will always write events directly to the queue before forwarding them to the processing engine."""
    max_buffer_size: NotRequired[float]
    r"""The maximum number of events to hold in memory before writing the events to disk"""
    commit_frequency: NotRequired[float]
    r"""The number of events to send downstream before committing that Stream has read them"""
    max_file_size: NotRequired[str]
    r"""The maximum size to store in each queue file before closing and optionally compressing. Enter a numeral with units of KB, MB, etc."""
    max_size: NotRequired[str]
    r"""The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc."""
    path: NotRequired[str]
    r"""The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/inputs/<input-id>"""
    compress: NotRequired[CompressionTCP]
    r"""Codec to use to compress the persisted data"""
    pq_controls: NotRequired[PqControlsTCPTypedDict]


class PqTCP(BaseModel):
    mode: Annotated[Optional[ModeTCP], PlainValidator(validate_open_enum(False))] = (
        ModeTCP.ALWAYS
    )
    r"""With Smart mode, PQ will write events to the filesystem only when it detects backpressure from the processing engine. With Always On mode, PQ will always write events directly to the queue before forwarding them to the processing engine."""

    max_buffer_size: Annotated[
        Optional[float], pydantic.Field(alias="maxBufferSize")
    ] = 1000
    r"""The maximum number of events to hold in memory before writing the events to disk"""

    commit_frequency: Annotated[
        Optional[float], pydantic.Field(alias="commitFrequency")
    ] = 42
    r"""The number of events to send downstream before committing that Stream has read them"""

    max_file_size: Annotated[Optional[str], pydantic.Field(alias="maxFileSize")] = (
        "1 MB"
    )
    r"""The maximum size to store in each queue file before closing and optionally compressing. Enter a numeral with units of KB, MB, etc."""

    max_size: Annotated[Optional[str], pydantic.Field(alias="maxSize")] = "5GB"
    r"""The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc."""

    path: Optional[str] = "$CRIBL_HOME/state/queues"
    r"""The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/inputs/<input-id>"""

    compress: Annotated[
        Optional[CompressionTCP], PlainValidator(validate_open_enum(False))
    ] = CompressionTCP.NONE
    r"""Codec to use to compress the persisted data"""

    pq_controls: Annotated[
        Optional[PqControlsTCP], pydantic.Field(alias="pqControls")
    ] = None

    @field_serializer("mode")
    def serialize_mode(self, value):
        if isinstance(value, str):
            try:
                return models.ModeTCP(value)
            except ValueError:
                return value
        return value

    @field_serializer("compress")
    def serialize_compress(self, value):
        if isinstance(value, str):
            try:
                return models.CompressionTCP(value)
            except ValueError:
                return value
        return value


class MinimumTLSVersionTCP(str, Enum, metaclass=utils.OpenEnumMeta):
    TL_SV1 = "TLSv1"
    TL_SV1_1 = "TLSv1.1"
    TL_SV1_2 = "TLSv1.2"
    TL_SV1_3 = "TLSv1.3"


class MaximumTLSVersionTCP(str, Enum, metaclass=utils.OpenEnumMeta):
    TL_SV1 = "TLSv1"
    TL_SV1_1 = "TLSv1.1"
    TL_SV1_2 = "TLSv1.2"
    TL_SV1_3 = "TLSv1.3"


class TLSSettingsServerSideTCPTypedDict(TypedDict):
    disabled: NotRequired[bool]
    request_cert: NotRequired[bool]
    r"""Require clients to present their certificates. Used to perform client authentication using SSL certs."""
    reject_unauthorized: NotRequired[bool]
    r"""Reject certificates not authorized by a CA in the CA certificate path or by another trusted CA (such as the system's)"""
    common_name_regex: NotRequired[str]
    r"""Regex matching allowable common names in peer certificates' subject attribute"""
    certificate_name: NotRequired[str]
    r"""The name of the predefined certificate"""
    priv_key_path: NotRequired[str]
    r"""Path on server containing the private key to use. PEM format. Can reference $ENV_VARS."""
    passphrase: NotRequired[str]
    r"""Passphrase to use to decrypt private key"""
    cert_path: NotRequired[str]
    r"""Path on server containing certificates to use. PEM format. Can reference $ENV_VARS."""
    ca_path: NotRequired[str]
    r"""Path on server containing CA certificates to use. PEM format. Can reference $ENV_VARS."""
    min_version: NotRequired[MinimumTLSVersionTCP]
    max_version: NotRequired[MaximumTLSVersionTCP]


class TLSSettingsServerSideTCP(BaseModel):
    disabled: Optional[bool] = True

    request_cert: Annotated[Optional[bool], pydantic.Field(alias="requestCert")] = False
    r"""Require clients to present their certificates. Used to perform client authentication using SSL certs."""

    reject_unauthorized: Annotated[
        Optional[bool], pydantic.Field(alias="rejectUnauthorized")
    ] = True
    r"""Reject certificates not authorized by a CA in the CA certificate path or by another trusted CA (such as the system's)"""

    common_name_regex: Annotated[
        Optional[str], pydantic.Field(alias="commonNameRegex")
    ] = "/.*/"
    r"""Regex matching allowable common names in peer certificates' subject attribute"""

    certificate_name: Annotated[
        Optional[str], pydantic.Field(alias="certificateName")
    ] = None
    r"""The name of the predefined certificate"""

    priv_key_path: Annotated[Optional[str], pydantic.Field(alias="privKeyPath")] = None
    r"""Path on server containing the private key to use. PEM format. Can reference $ENV_VARS."""

    passphrase: Optional[str] = None
    r"""Passphrase to use to decrypt private key"""

    cert_path: Annotated[Optional[str], pydantic.Field(alias="certPath")] = None
    r"""Path on server containing certificates to use. PEM format. Can reference $ENV_VARS."""

    ca_path: Annotated[Optional[str], pydantic.Field(alias="caPath")] = None
    r"""Path on server containing CA certificates to use. PEM format. Can reference $ENV_VARS."""

    min_version: Annotated[
        Annotated[
            Optional[MinimumTLSVersionTCP], PlainValidator(validate_open_enum(False))
        ],
        pydantic.Field(alias="minVersion"),
    ] = None

    max_version: Annotated[
        Annotated[
            Optional[MaximumTLSVersionTCP], PlainValidator(validate_open_enum(False))
        ],
        pydantic.Field(alias="maxVersion"),
    ] = None

    @field_serializer("min_version")
    def serialize_min_version(self, value):
        if isinstance(value, str):
            try:
                return models.MinimumTLSVersionTCP(value)
            except ValueError:
                return value
        return value

    @field_serializer("max_version")
    def serialize_max_version(self, value):
        if isinstance(value, str):
            try:
                return models.MaximumTLSVersionTCP(value)
            except ValueError:
                return value
        return value


class MetadatumTCPTypedDict(TypedDict):
    name: str
    value: str
    r"""JavaScript expression to compute field's value, enclosed in quotes or backticks. (Can evaluate to a constant.)"""


class MetadatumTCP(BaseModel):
    name: str

    value: str
    r"""JavaScript expression to compute field's value, enclosed in quotes or backticks. (Can evaluate to a constant.)"""


class PreprocessTCPTypedDict(TypedDict):
    disabled: NotRequired[bool]
    command: NotRequired[str]
    r"""Command to feed the data through (via stdin) and process its output (stdout)"""
    args: NotRequired[List[str]]
    r"""Arguments to be added to the custom command"""


class PreprocessTCP(BaseModel):
    disabled: Optional[bool] = True

    command: Optional[str] = None
    r"""Command to feed the data through (via stdin) and process its output (stdout)"""

    args: Optional[List[str]] = None
    r"""Arguments to be added to the custom command"""


class AuthenticationMethodTCP(str, Enum, metaclass=utils.OpenEnumMeta):
    r"""Select Manual to enter an auth token directly, or select Secret to use a text secret to authenticate"""

    MANUAL = "manual"
    SECRET = "secret"


class InputTCPTypedDict(TypedDict):
    type: TypeTCP
    port: float
    r"""Port to listen on"""
    id: NotRequired[str]
    r"""Unique ID for this input"""
    disabled: NotRequired[bool]
    pipeline: NotRequired[str]
    r"""Pipeline to process data from this Source before sending it through the Routes"""
    send_to_routes: NotRequired[bool]
    r"""Select whether to send data to Routes, or directly to Destinations."""
    environment: NotRequired[str]
    r"""Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere."""
    pq_enabled: NotRequired[bool]
    r"""Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers)."""
    streamtags: NotRequired[List[str]]
    r"""Tags for filtering and grouping in @{product}"""
    connections: NotRequired[List[ConnectionTCPTypedDict]]
    r"""Direct connections to Destinations, and optionally via a Pipeline or a Pack"""
    pq: NotRequired[PqTCPTypedDict]
    host: NotRequired[str]
    r"""Address to bind on. Defaults to 0.0.0.0 (all addresses)."""
    tls: NotRequired[TLSSettingsServerSideTCPTypedDict]
    ip_whitelist_regex: NotRequired[str]
    r"""Regex matching IP addresses that are allowed to establish a connection"""
    max_active_cxn: NotRequired[float]
    r"""Maximum number of active connections allowed per Worker Process. Use 0 for unlimited."""
    socket_idle_timeout: NotRequired[float]
    r"""How long @{product} should wait before assuming that an inactive socket has timed out. After this time, the connection will be closed. Leave at 0 for no inactive socket monitoring."""
    socket_ending_max_wait: NotRequired[float]
    r"""How long the server will wait after initiating a closure for a client to close its end of the connection. If the client doesn't close the connection within this time, the server will forcefully terminate the socket to prevent resource leaks and ensure efficient connection cleanup and system stability. Leave at 0 for no inactive socket monitoring."""
    socket_max_lifespan: NotRequired[float]
    r"""The maximum duration a socket can remain open, even if active. This helps manage resources and mitigate issues caused by TCP pinning. Set to 0 to disable."""
    enable_proxy_header: NotRequired[bool]
    r"""Enable if the connection is proxied by a device that supports proxy protocol v1 or v2"""
    metadata: NotRequired[List[MetadatumTCPTypedDict]]
    r"""Fields to add to events from this input"""
    breaker_rulesets: NotRequired[List[str]]
    r"""A list of event-breaking rulesets that will be applied, in order, to the input data stream"""
    stale_channel_flush_ms: NotRequired[float]
    r"""How long (in milliseconds) the Event Breaker will wait for new data to be sent to a specific channel before flushing the data stream out, as is, to the Pipelines"""
    enable_header: NotRequired[bool]
    r"""Client will pass the header record with every new connection. The header can contain an authToken, and an object with a list of fields and values to add to every event. These fields can be used to simplify Event Breaker selection, routing, etc. Header has this format, and must be followed by a newline: { \"authToken\" : \"myToken\", \"fields\": { \"field1\": \"value1\", \"field2\": \"value2\" } }"""
    preprocess: NotRequired[PreprocessTCPTypedDict]
    description: NotRequired[str]
    auth_token: NotRequired[str]
    r"""Shared secret to be provided by any client (in authToken header field). If empty, unauthorized access is permitted."""
    auth_type: NotRequired[AuthenticationMethodTCP]
    r"""Select Manual to enter an auth token directly, or select Secret to use a text secret to authenticate"""
    text_secret: NotRequired[str]
    r"""Select or create a stored text secret"""


class InputTCP(BaseModel):
    model_config = ConfigDict(
        populate_by_name=True, arbitrary_types_allowed=True, extra="allow"
    )
    __pydantic_extra__: Dict[str, Any] = pydantic.Field(init=False)

    type: TypeTCP

    port: float
    r"""Port to listen on"""

    id: Optional[str] = None
    r"""Unique ID for this input"""

    disabled: Optional[bool] = False

    pipeline: Optional[str] = None
    r"""Pipeline to process data from this Source before sending it through the Routes"""

    send_to_routes: Annotated[Optional[bool], pydantic.Field(alias="sendToRoutes")] = (
        True
    )
    r"""Select whether to send data to Routes, or directly to Destinations."""

    environment: Optional[str] = None
    r"""Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere."""

    pq_enabled: Annotated[Optional[bool], pydantic.Field(alias="pqEnabled")] = False
    r"""Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers)."""

    streamtags: Optional[List[str]] = None
    r"""Tags for filtering and grouping in @{product}"""

    connections: Optional[List[ConnectionTCP]] = None
    r"""Direct connections to Destinations, and optionally via a Pipeline or a Pack"""

    pq: Optional[PqTCP] = None

    host: Optional[str] = "0.0.0.0"
    r"""Address to bind on. Defaults to 0.0.0.0 (all addresses)."""

    tls: Optional[TLSSettingsServerSideTCP] = None

    ip_whitelist_regex: Annotated[
        Optional[str], pydantic.Field(alias="ipWhitelistRegex")
    ] = "/.*/"
    r"""Regex matching IP addresses that are allowed to establish a connection"""

    max_active_cxn: Annotated[Optional[float], pydantic.Field(alias="maxActiveCxn")] = (
        1000
    )
    r"""Maximum number of active connections allowed per Worker Process. Use 0 for unlimited."""

    socket_idle_timeout: Annotated[
        Optional[float], pydantic.Field(alias="socketIdleTimeout")
    ] = 0
    r"""How long @{product} should wait before assuming that an inactive socket has timed out. After this time, the connection will be closed. Leave at 0 for no inactive socket monitoring."""

    socket_ending_max_wait: Annotated[
        Optional[float], pydantic.Field(alias="socketEndingMaxWait")
    ] = 30
    r"""How long the server will wait after initiating a closure for a client to close its end of the connection. If the client doesn't close the connection within this time, the server will forcefully terminate the socket to prevent resource leaks and ensure efficient connection cleanup and system stability. Leave at 0 for no inactive socket monitoring."""

    socket_max_lifespan: Annotated[
        Optional[float], pydantic.Field(alias="socketMaxLifespan")
    ] = 0
    r"""The maximum duration a socket can remain open, even if active. This helps manage resources and mitigate issues caused by TCP pinning. Set to 0 to disable."""

    enable_proxy_header: Annotated[
        Optional[bool], pydantic.Field(alias="enableProxyHeader")
    ] = False
    r"""Enable if the connection is proxied by a device that supports proxy protocol v1 or v2"""

    metadata: Optional[List[MetadatumTCP]] = None
    r"""Fields to add to events from this input"""

    breaker_rulesets: Annotated[
        Optional[List[str]], pydantic.Field(alias="breakerRulesets")
    ] = None
    r"""A list of event-breaking rulesets that will be applied, in order, to the input data stream"""

    stale_channel_flush_ms: Annotated[
        Optional[float], pydantic.Field(alias="staleChannelFlushMs")
    ] = 10000
    r"""How long (in milliseconds) the Event Breaker will wait for new data to be sent to a specific channel before flushing the data stream out, as is, to the Pipelines"""

    enable_header: Annotated[Optional[bool], pydantic.Field(alias="enableHeader")] = (
        False
    )
    r"""Client will pass the header record with every new connection. The header can contain an authToken, and an object with a list of fields and values to add to every event. These fields can be used to simplify Event Breaker selection, routing, etc. Header has this format, and must be followed by a newline: { \"authToken\" : \"myToken\", \"fields\": { \"field1\": \"value1\", \"field2\": \"value2\" } }"""

    preprocess: Optional[PreprocessTCP] = None

    description: Optional[str] = None

    auth_token: Annotated[Optional[str], pydantic.Field(alias="authToken")] = ""
    r"""Shared secret to be provided by any client (in authToken header field). If empty, unauthorized access is permitted."""

    auth_type: Annotated[
        Annotated[
            Optional[AuthenticationMethodTCP], PlainValidator(validate_open_enum(False))
        ],
        pydantic.Field(alias="authType"),
    ] = AuthenticationMethodTCP.MANUAL
    r"""Select Manual to enter an auth token directly, or select Secret to use a text secret to authenticate"""

    text_secret: Annotated[Optional[str], pydantic.Field(alias="textSecret")] = None
    r"""Select or create a stored text secret"""

    @property
    def additional_properties(self):
        return self.__pydantic_extra__

    @additional_properties.setter
    def additional_properties(self, value):
        self.__pydantic_extra__ = value  # pyright: ignore[reportIncompatibleVariableOverride]

    @field_serializer("auth_type")
    def serialize_auth_type(self, value):
        if isinstance(value, str):
            try:
                return models.AuthenticationMethodTCP(value)
            except ValueError:
                return value
        return value


class InputFileType(str, Enum):
    FILE = "file"


class InputFileConnectionTypedDict(TypedDict):
    output: str
    pipeline: NotRequired[str]


class InputFileConnection(BaseModel):
    output: str

    pipeline: Optional[str] = None


class InputFilePqMode(str, Enum, metaclass=utils.OpenEnumMeta):
    r"""With Smart mode, PQ will write events to the filesystem only when it detects backpressure from the processing engine. With Always On mode, PQ will always write events directly to the queue before forwarding them to the processing engine."""

    # Smart
    SMART = "smart"
    # Always On
    ALWAYS = "always"


class InputFileCompression(str, Enum, metaclass=utils.OpenEnumMeta):
    r"""Codec to use to compress the persisted data"""

    # None
    NONE = "none"
    # Gzip
    GZIP = "gzip"


class InputFilePqControlsTypedDict(TypedDict):
    pass


class InputFilePqControls(BaseModel):
    pass


class InputFilePqTypedDict(TypedDict):
    mode: NotRequired[InputFilePqMode]
    r"""With Smart mode, PQ will write events to the filesystem only when it detects backpressure from the processing engine. With Always On mode, PQ will always write events directly to the queue before forwarding them to the processing engine."""
    max_buffer_size: NotRequired[float]
    r"""The maximum number of events to hold in memory before writing the events to disk"""
    commit_frequency: NotRequired[float]
    r"""The number of events to send downstream before committing that Stream has read them"""
    max_file_size: NotRequired[str]
    r"""The maximum size to store in each queue file before closing and optionally compressing. Enter a numeral with units of KB, MB, etc."""
    max_size: NotRequired[str]
    r"""The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc."""
    path: NotRequired[str]
    r"""The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/inputs/<input-id>"""
    compress: NotRequired[InputFileCompression]
    r"""Codec to use to compress the persisted data"""
    pq_controls: NotRequired[InputFilePqControlsTypedDict]


class InputFilePq(BaseModel):
    mode: Annotated[
        Optional[InputFilePqMode], PlainValidator(validate_open_enum(False))
    ] = InputFilePqMode.ALWAYS
    r"""With Smart mode, PQ will write events to the filesystem only when it detects backpressure from the processing engine. With Always On mode, PQ will always write events directly to the queue before forwarding them to the processing engine."""

    max_buffer_size: Annotated[
        Optional[float], pydantic.Field(alias="maxBufferSize")
    ] = 1000
    r"""The maximum number of events to hold in memory before writing the events to disk"""

    commit_frequency: Annotated[
        Optional[float], pydantic.Field(alias="commitFrequency")
    ] = 42
    r"""The number of events to send downstream before committing that Stream has read them"""

    max_file_size: Annotated[Optional[str], pydantic.Field(alias="maxFileSize")] = (
        "1 MB"
    )
    r"""The maximum size to store in each queue file before closing and optionally compressing. Enter a numeral with units of KB, MB, etc."""

    max_size: Annotated[Optional[str], pydantic.Field(alias="maxSize")] = "5GB"
    r"""The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc."""

    path: Optional[str] = "$CRIBL_HOME/state/queues"
    r"""The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/inputs/<input-id>"""

    compress: Annotated[
        Optional[InputFileCompression], PlainValidator(validate_open_enum(False))
    ] = InputFileCompression.NONE
    r"""Codec to use to compress the persisted data"""

    pq_controls: Annotated[
        Optional[InputFilePqControls], pydantic.Field(alias="pqControls")
    ] = None

    @field_serializer("mode")
    def serialize_mode(self, value):
        if isinstance(value, str):
            try:
                return models.InputFilePqMode(value)
            except ValueError:
                return value
        return value

    @field_serializer("compress")
    def serialize_compress(self, value):
        if isinstance(value, str):
            try:
                return models.InputFileCompression(value)
            except ValueError:
                return value
        return value


class InputFileMode(str, Enum, metaclass=utils.OpenEnumMeta):
    r"""Choose how to discover files to monitor"""

    # Manual
    MANUAL = "manual"
    # Auto
    AUTO = "auto"


class InputFileMetadatumTypedDict(TypedDict):
    name: str
    value: str
    r"""JavaScript expression to compute field's value, enclosed in quotes or backticks. (Can evaluate to a constant.)"""


class InputFileMetadatum(BaseModel):
    name: str

    value: str
    r"""JavaScript expression to compute field's value, enclosed in quotes or backticks. (Can evaluate to a constant.)"""


class InputFileTypedDict(TypedDict):
    type: InputFileType
    id: NotRequired[str]
    r"""Unique ID for this input"""
    disabled: NotRequired[bool]
    pipeline: NotRequired[str]
    r"""Pipeline to process data from this Source before sending it through the Routes"""
    send_to_routes: NotRequired[bool]
    r"""Select whether to send data to Routes, or directly to Destinations."""
    environment: NotRequired[str]
    r"""Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere."""
    pq_enabled: NotRequired[bool]
    r"""Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers)."""
    streamtags: NotRequired[List[str]]
    r"""Tags for filtering and grouping in @{product}"""
    connections: NotRequired[List[InputFileConnectionTypedDict]]
    r"""Direct connections to Destinations, and optionally via a Pipeline or a Pack"""
    pq: NotRequired[InputFilePqTypedDict]
    mode: NotRequired[InputFileMode]
    r"""Choose how to discover files to monitor"""
    interval: NotRequired[float]
    r"""Time, in seconds, between scanning for files"""
    filenames: NotRequired[List[str]]
    r"""The full path of discovered files are matched against this wildcard list"""
    filter_archived_files: NotRequired[bool]
    r"""Apply filename allowlist to file entries in archive file types, like tar or zip."""
    tail_only: NotRequired[bool]
    r"""Read only new entries at the end of all files discovered at next startup. @{product} will then read newly discovered files from the head. Disable this to resume reading all files from head."""
    idle_timeout: NotRequired[float]
    r"""Time, in seconds, before an idle file is closed"""
    min_age_dur: NotRequired[str]
    r"""The minimum age of files to monitor. Format examples: 30s, 15m, 1h. Age is relative to file modification time. Leave empty to apply no age filters."""
    max_age_dur: NotRequired[str]
    r"""The maximum age of event timestamps to collect. Format examples: 60s, 4h, 3d, 1w. Can be used in conjuction with \"Check file modification times\". Leave empty to apply no age filters."""
    check_file_mod_time: NotRequired[bool]
    r"""Skip files with modification times earlier than the maximum age duration"""
    force_text: NotRequired[bool]
    r"""Forces files containing binary data to be streamed as text"""
    hash_len: NotRequired[float]
    r"""Length of file header bytes to use in hash for unique file identification"""
    metadata: NotRequired[List[InputFileMetadatumTypedDict]]
    r"""Fields to add to events from this input"""
    breaker_rulesets: NotRequired[List[str]]
    r"""A list of event-breaking rulesets that will be applied, in order, to the input data stream"""
    stale_channel_flush_ms: NotRequired[float]
    r"""How long (in milliseconds) the Event Breaker will wait for new data to be sent to a specific channel before flushing the data stream out, as is, to the Pipelines"""
    description: NotRequired[str]
    path: NotRequired[str]
    r"""Directory path to search for files. Environment variables will be resolved, e.g. $CRIBL_HOME/log/."""
    depth: NotRequired[float]
    r"""Set how many subdirectories deep to search. Use 0 to search only files in the given path, 1 to also look in its immediate subdirectories, etc. Leave it empty for unlimited depth."""
    suppress_missing_path_errors: NotRequired[bool]
    delete_files: NotRequired[bool]
    r"""Delete files after they have been collected"""
    include_unidentifiable_binary: NotRequired[bool]
    r"""Stream binary files as Base64-encoded chunks."""


class InputFile(BaseModel):
    model_config = ConfigDict(
        populate_by_name=True, arbitrary_types_allowed=True, extra="allow"
    )
    __pydantic_extra__: Dict[str, Any] = pydantic.Field(init=False)

    type: InputFileType

    id: Optional[str] = None
    r"""Unique ID for this input"""

    disabled: Optional[bool] = False

    pipeline: Optional[str] = None
    r"""Pipeline to process data from this Source before sending it through the Routes"""

    send_to_routes: Annotated[Optional[bool], pydantic.Field(alias="sendToRoutes")] = (
        True
    )
    r"""Select whether to send data to Routes, or directly to Destinations."""

    environment: Optional[str] = None
    r"""Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere."""

    pq_enabled: Annotated[Optional[bool], pydantic.Field(alias="pqEnabled")] = False
    r"""Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers)."""

    streamtags: Optional[List[str]] = None
    r"""Tags for filtering and grouping in @{product}"""

    connections: Optional[List[InputFileConnection]] = None
    r"""Direct connections to Destinations, and optionally via a Pipeline or a Pack"""

    pq: Optional[InputFilePq] = None

    mode: Annotated[
        Optional[InputFileMode], PlainValidator(validate_open_enum(False))
    ] = InputFileMode.MANUAL
    r"""Choose how to discover files to monitor"""

    interval: Optional[float] = 10
    r"""Time, in seconds, between scanning for files"""

    filenames: Optional[List[str]] = None
    r"""The full path of discovered files are matched against this wildcard list"""

    filter_archived_files: Annotated[
        Optional[bool], pydantic.Field(alias="filterArchivedFiles")
    ] = False
    r"""Apply filename allowlist to file entries in archive file types, like tar or zip."""

    tail_only: Annotated[Optional[bool], pydantic.Field(alias="tailOnly")] = True
    r"""Read only new entries at the end of all files discovered at next startup. @{product} will then read newly discovered files from the head. Disable this to resume reading all files from head."""

    idle_timeout: Annotated[Optional[float], pydantic.Field(alias="idleTimeout")] = 300
    r"""Time, in seconds, before an idle file is closed"""

    min_age_dur: Annotated[Optional[str], pydantic.Field(alias="minAgeDur")] = None
    r"""The minimum age of files to monitor. Format examples: 30s, 15m, 1h. Age is relative to file modification time. Leave empty to apply no age filters."""

    max_age_dur: Annotated[Optional[str], pydantic.Field(alias="maxAgeDur")] = None
    r"""The maximum age of event timestamps to collect. Format examples: 60s, 4h, 3d, 1w. Can be used in conjuction with \"Check file modification times\". Leave empty to apply no age filters."""

    check_file_mod_time: Annotated[
        Optional[bool], pydantic.Field(alias="checkFileModTime")
    ] = False
    r"""Skip files with modification times earlier than the maximum age duration"""

    force_text: Annotated[Optional[bool], pydantic.Field(alias="forceText")] = False
    r"""Forces files containing binary data to be streamed as text"""

    hash_len: Annotated[Optional[float], pydantic.Field(alias="hashLen")] = 256
    r"""Length of file header bytes to use in hash for unique file identification"""

    metadata: Optional[List[InputFileMetadatum]] = None
    r"""Fields to add to events from this input"""

    breaker_rulesets: Annotated[
        Optional[List[str]], pydantic.Field(alias="breakerRulesets")
    ] = None
    r"""A list of event-breaking rulesets that will be applied, in order, to the input data stream"""

    stale_channel_flush_ms: Annotated[
        Optional[float], pydantic.Field(alias="staleChannelFlushMs")
    ] = 10000
    r"""How long (in milliseconds) the Event Breaker will wait for new data to be sent to a specific channel before flushing the data stream out, as is, to the Pipelines"""

    description: Optional[str] = None

    path: Optional[str] = None
    r"""Directory path to search for files. Environment variables will be resolved, e.g. $CRIBL_HOME/log/."""

    depth: Optional[float] = None
    r"""Set how many subdirectories deep to search. Use 0 to search only files in the given path, 1 to also look in its immediate subdirectories, etc. Leave it empty for unlimited depth."""

    suppress_missing_path_errors: Annotated[
        Optional[bool], pydantic.Field(alias="suppressMissingPathErrors")
    ] = False

    delete_files: Annotated[Optional[bool], pydantic.Field(alias="deleteFiles")] = False
    r"""Delete files after they have been collected"""

    include_unidentifiable_binary: Annotated[
        Optional[bool], pydantic.Field(alias="includeUnidentifiableBinary")
    ] = False
    r"""Stream binary files as Base64-encoded chunks."""

    @property
    def additional_properties(self):
        return self.__pydantic_extra__

    @additional_properties.setter
    def additional_properties(self, value):
        self.__pydantic_extra__ = value  # pyright: ignore[reportIncompatibleVariableOverride]

    @field_serializer("mode")
    def serialize_mode(self, value):
        if isinstance(value, str):
            try:
                return models.InputFileMode(value)
            except ValueError:
                return value
        return value


class InputSyslogType2(str, Enum):
    SYSLOG = "syslog"


class InputSyslogConnection2TypedDict(TypedDict):
    output: str
    pipeline: NotRequired[str]


class InputSyslogConnection2(BaseModel):
    output: str

    pipeline: Optional[str] = None


class InputSyslogMode2(str, Enum, metaclass=utils.OpenEnumMeta):
    r"""With Smart mode, PQ will write events to the filesystem only when it detects backpressure from the processing engine. With Always On mode, PQ will always write events directly to the queue before forwarding them to the processing engine."""

    # Smart
    SMART = "smart"
    # Always On
    ALWAYS = "always"


class InputSyslogCompression2(str, Enum, metaclass=utils.OpenEnumMeta):
    r"""Codec to use to compress the persisted data"""

    # None
    NONE = "none"
    # Gzip
    GZIP = "gzip"


class InputSyslogPqControls2TypedDict(TypedDict):
    pass


class InputSyslogPqControls2(BaseModel):
    pass


class InputSyslogPq2TypedDict(TypedDict):
    mode: NotRequired[InputSyslogMode2]
    r"""With Smart mode, PQ will write events to the filesystem only when it detects backpressure from the processing engine. With Always On mode, PQ will always write events directly to the queue before forwarding them to the processing engine."""
    max_buffer_size: NotRequired[float]
    r"""The maximum number of events to hold in memory before writing the events to disk"""
    commit_frequency: NotRequired[float]
    r"""The number of events to send downstream before committing that Stream has read them"""
    max_file_size: NotRequired[str]
    r"""The maximum size to store in each queue file before closing and optionally compressing. Enter a numeral with units of KB, MB, etc."""
    max_size: NotRequired[str]
    r"""The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc."""
    path: NotRequired[str]
    r"""The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/inputs/<input-id>"""
    compress: NotRequired[InputSyslogCompression2]
    r"""Codec to use to compress the persisted data"""
    pq_controls: NotRequired[InputSyslogPqControls2TypedDict]


class InputSyslogPq2(BaseModel):
    mode: Annotated[
        Optional[InputSyslogMode2], PlainValidator(validate_open_enum(False))
    ] = InputSyslogMode2.ALWAYS
    r"""With Smart mode, PQ will write events to the filesystem only when it detects backpressure from the processing engine. With Always On mode, PQ will always write events directly to the queue before forwarding them to the processing engine."""

    max_buffer_size: Annotated[
        Optional[float], pydantic.Field(alias="maxBufferSize")
    ] = 1000
    r"""The maximum number of events to hold in memory before writing the events to disk"""

    commit_frequency: Annotated[
        Optional[float], pydantic.Field(alias="commitFrequency")
    ] = 42
    r"""The number of events to send downstream before committing that Stream has read them"""

    max_file_size: Annotated[Optional[str], pydantic.Field(alias="maxFileSize")] = (
        "1 MB"
    )
    r"""The maximum size to store in each queue file before closing and optionally compressing. Enter a numeral with units of KB, MB, etc."""

    max_size: Annotated[Optional[str], pydantic.Field(alias="maxSize")] = "5GB"
    r"""The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc."""

    path: Optional[str] = "$CRIBL_HOME/state/queues"
    r"""The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/inputs/<input-id>"""

    compress: Annotated[
        Optional[InputSyslogCompression2], PlainValidator(validate_open_enum(False))
    ] = InputSyslogCompression2.NONE
    r"""Codec to use to compress the persisted data"""

    pq_controls: Annotated[
        Optional[InputSyslogPqControls2], pydantic.Field(alias="pqControls")
    ] = None

    @field_serializer("mode")
    def serialize_mode(self, value):
        if isinstance(value, str):
            try:
                return models.InputSyslogMode2(value)
            except ValueError:
                return value
        return value

    @field_serializer("compress")
    def serialize_compress(self, value):
        if isinstance(value, str):
            try:
                return models.InputSyslogCompression2(value)
            except ValueError:
                return value
        return value


class InputSyslogMinimumTLSVersion2(str, Enum, metaclass=utils.OpenEnumMeta):
    TL_SV1 = "TLSv1"
    TL_SV1_1 = "TLSv1.1"
    TL_SV1_2 = "TLSv1.2"
    TL_SV1_3 = "TLSv1.3"


class InputSyslogMaximumTLSVersion2(str, Enum, metaclass=utils.OpenEnumMeta):
    TL_SV1 = "TLSv1"
    TL_SV1_1 = "TLSv1.1"
    TL_SV1_2 = "TLSv1.2"
    TL_SV1_3 = "TLSv1.3"


class InputSyslogTLSSettingsServerSide2TypedDict(TypedDict):
    disabled: NotRequired[bool]
    request_cert: NotRequired[bool]
    r"""Require clients to present their certificates. Used to perform client authentication using SSL certs."""
    reject_unauthorized: NotRequired[bool]
    r"""Reject certificates not authorized by a CA in the CA certificate path or by another trusted CA (such as the system's)"""
    common_name_regex: NotRequired[str]
    r"""Regex matching allowable common names in peer certificates' subject attribute"""
    certificate_name: NotRequired[str]
    r"""The name of the predefined certificate"""
    priv_key_path: NotRequired[str]
    r"""Path on server containing the private key to use. PEM format. Can reference $ENV_VARS."""
    passphrase: NotRequired[str]
    r"""Passphrase to use to decrypt private key"""
    cert_path: NotRequired[str]
    r"""Path on server containing certificates to use. PEM format. Can reference $ENV_VARS."""
    ca_path: NotRequired[str]
    r"""Path on server containing CA certificates to use. PEM format. Can reference $ENV_VARS."""
    min_version: NotRequired[InputSyslogMinimumTLSVersion2]
    max_version: NotRequired[InputSyslogMaximumTLSVersion2]


class InputSyslogTLSSettingsServerSide2(BaseModel):
    disabled: Optional[bool] = True

    request_cert: Annotated[Optional[bool], pydantic.Field(alias="requestCert")] = False
    r"""Require clients to present their certificates. Used to perform client authentication using SSL certs."""

    reject_unauthorized: Annotated[
        Optional[bool], pydantic.Field(alias="rejectUnauthorized")
    ] = True
    r"""Reject certificates not authorized by a CA in the CA certificate path or by another trusted CA (such as the system's)"""

    common_name_regex: Annotated[
        Optional[str], pydantic.Field(alias="commonNameRegex")
    ] = "/.*/"
    r"""Regex matching allowable common names in peer certificates' subject attribute"""

    certificate_name: Annotated[
        Optional[str], pydantic.Field(alias="certificateName")
    ] = None
    r"""The name of the predefined certificate"""

    priv_key_path: Annotated[Optional[str], pydantic.Field(alias="privKeyPath")] = None
    r"""Path on server containing the private key to use. PEM format. Can reference $ENV_VARS."""

    passphrase: Optional[str] = None
    r"""Passphrase to use to decrypt private key"""

    cert_path: Annotated[Optional[str], pydantic.Field(alias="certPath")] = None
    r"""Path on server containing certificates to use. PEM format. Can reference $ENV_VARS."""

    ca_path: Annotated[Optional[str], pydantic.Field(alias="caPath")] = None
    r"""Path on server containing CA certificates to use. PEM format. Can reference $ENV_VARS."""

    min_version: Annotated[
        Annotated[
            Optional[InputSyslogMinimumTLSVersion2],
            PlainValidator(validate_open_enum(False)),
        ],
        pydantic.Field(alias="minVersion"),
    ] = None

    max_version: Annotated[
        Annotated[
            Optional[InputSyslogMaximumTLSVersion2],
            PlainValidator(validate_open_enum(False)),
        ],
        pydantic.Field(alias="maxVersion"),
    ] = None

    @field_serializer("min_version")
    def serialize_min_version(self, value):
        if isinstance(value, str):
            try:
                return models.InputSyslogMinimumTLSVersion2(value)
            except ValueError:
                return value
        return value

    @field_serializer("max_version")
    def serialize_max_version(self, value):
        if isinstance(value, str):
            try:
                return models.InputSyslogMaximumTLSVersion2(value)
            except ValueError:
                return value
        return value


class InputSyslogMetadatum2TypedDict(TypedDict):
    name: str
    value: str
    r"""JavaScript expression to compute field's value, enclosed in quotes or backticks. (Can evaluate to a constant.)"""


class InputSyslogMetadatum2(BaseModel):
    name: str

    value: str
    r"""JavaScript expression to compute field's value, enclosed in quotes or backticks. (Can evaluate to a constant.)"""


class InputSyslogSyslog2TypedDict(TypedDict):
    type: InputSyslogType2
    tcp_port: float
    r"""Enter TCP port number to listen on. Not required if listening on UDP."""
    id: NotRequired[str]
    r"""Unique ID for this input"""
    disabled: NotRequired[bool]
    pipeline: NotRequired[str]
    r"""Pipeline to process data from this Source before sending it through the Routes"""
    send_to_routes: NotRequired[bool]
    r"""Select whether to send data to Routes, or directly to Destinations."""
    environment: NotRequired[str]
    r"""Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere."""
    pq_enabled: NotRequired[bool]
    r"""Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers)."""
    streamtags: NotRequired[List[str]]
    r"""Tags for filtering and grouping in @{product}"""
    connections: NotRequired[List[InputSyslogConnection2TypedDict]]
    r"""Direct connections to Destinations, and optionally via a Pipeline or a Pack"""
    pq: NotRequired[InputSyslogPq2TypedDict]
    host: NotRequired[str]
    r"""Address to bind on. For IPv4 (all addresses), use the default '0.0.0.0'. For IPv6, enter '::' (all addresses) or specify an IP address."""
    udp_port: NotRequired[float]
    r"""Enter UDP port number to listen on. Not required if listening on TCP."""
    max_buffer_size: NotRequired[float]
    r"""Maximum number of events to buffer when downstream is blocking. Only applies to UDP."""
    ip_whitelist_regex: NotRequired[str]
    r"""Regex matching IP addresses that are allowed to send data"""
    timestamp_timezone: NotRequired[str]
    r"""Timezone to assign to timestamps without timezone info"""
    single_msg_udp_packets: NotRequired[bool]
    r"""Treat UDP packet data received as full syslog message"""
    enable_proxy_header: NotRequired[bool]
    r"""Enable if the connection is proxied by a device that supports Proxy Protocol V1 or V2"""
    keep_fields_list: NotRequired[List[str]]
    r"""Wildcard list of fields to keep from source data; * = ALL (default)"""
    octet_counting: NotRequired[bool]
    r"""Enable if incoming messages use octet counting per RFC 6587."""
    infer_framing: NotRequired[bool]
    r"""Enable if we should infer the syslog framing of the incoming messages."""
    strictly_infer_octet_counting: NotRequired[bool]
    r"""Enable if we should infer octet counting only if the messages comply with RFC 5424."""
    allow_non_standard_app_name: NotRequired[bool]
    r"""Enable if RFC 3164-formatted messages have hyphens in the app name portion of the TAG section. If disabled, only alphanumeric characters and underscores are allowed. Ignored for RFC 5424-formatted messages."""
    max_active_cxn: NotRequired[float]
    r"""Maximum number of active connections allowed per Worker Process for TCP connections. Use 0 for unlimited."""
    socket_idle_timeout: NotRequired[float]
    r"""How long @{product} should wait before assuming that an inactive socket has timed out. After this time, the connection will be closed. Leave at 0 for no inactive socket monitoring."""
    socket_ending_max_wait: NotRequired[float]
    r"""How long the server will wait after initiating a closure for a client to close its end of the connection. If the client doesn't close the connection within this time, the server will forcefully terminate the socket to prevent resource leaks and ensure efficient connection cleanup and system stability. Leave at 0 for no inactive socket monitoring."""
    socket_max_lifespan: NotRequired[float]
    r"""The maximum duration a socket can remain open, even if active. This helps manage resources and mitigate issues caused by TCP pinning. Set to 0 to disable."""
    tls: NotRequired[InputSyslogTLSSettingsServerSide2TypedDict]
    metadata: NotRequired[List[InputSyslogMetadatum2TypedDict]]
    r"""Fields to add to events from this input"""
    udp_socket_rx_buf_size: NotRequired[float]
    r"""Optionally, set the SO_RCVBUF socket option for the UDP socket. This value tells the operating system how many bytes can be buffered in the kernel before events are dropped. Leave blank to use the OS default. Caution: Increasing this value will affect OS memory utilization."""
    enable_load_balancing: NotRequired[bool]
    r"""Load balance traffic across all Worker Processes"""
    description: NotRequired[str]
    enable_enhanced_proxy_header_parsing: NotRequired[bool]
    r"""When enabled, parses PROXY protocol headers during the TLS handshake. Disable if compatibility issues arise."""


class InputSyslogSyslog2(BaseModel):
    model_config = ConfigDict(
        populate_by_name=True, arbitrary_types_allowed=True, extra="allow"
    )
    __pydantic_extra__: Dict[str, Any] = pydantic.Field(init=False)

    type: InputSyslogType2

    tcp_port: Annotated[float, pydantic.Field(alias="tcpPort")]
    r"""Enter TCP port number to listen on. Not required if listening on UDP."""

    id: Optional[str] = None
    r"""Unique ID for this input"""

    disabled: Optional[bool] = False

    pipeline: Optional[str] = None
    r"""Pipeline to process data from this Source before sending it through the Routes"""

    send_to_routes: Annotated[Optional[bool], pydantic.Field(alias="sendToRoutes")] = (
        True
    )
    r"""Select whether to send data to Routes, or directly to Destinations."""

    environment: Optional[str] = None
    r"""Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere."""

    pq_enabled: Annotated[Optional[bool], pydantic.Field(alias="pqEnabled")] = False
    r"""Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers)."""

    streamtags: Optional[List[str]] = None
    r"""Tags for filtering and grouping in @{product}"""

    connections: Optional[List[InputSyslogConnection2]] = None
    r"""Direct connections to Destinations, and optionally via a Pipeline or a Pack"""

    pq: Optional[InputSyslogPq2] = None

    host: Optional[str] = "0.0.0.0"
    r"""Address to bind on. For IPv4 (all addresses), use the default '0.0.0.0'. For IPv6, enter '::' (all addresses) or specify an IP address."""

    udp_port: Annotated[Optional[float], pydantic.Field(alias="udpPort")] = None
    r"""Enter UDP port number to listen on. Not required if listening on TCP."""

    max_buffer_size: Annotated[
        Optional[float], pydantic.Field(alias="maxBufferSize")
    ] = 1000
    r"""Maximum number of events to buffer when downstream is blocking. Only applies to UDP."""

    ip_whitelist_regex: Annotated[
        Optional[str], pydantic.Field(alias="ipWhitelistRegex")
    ] = "/.*/"
    r"""Regex matching IP addresses that are allowed to send data"""

    timestamp_timezone: Annotated[
        Optional[str], pydantic.Field(alias="timestampTimezone")
    ] = "local"
    r"""Timezone to assign to timestamps without timezone info"""

    single_msg_udp_packets: Annotated[
        Optional[bool], pydantic.Field(alias="singleMsgUdpPackets")
    ] = False
    r"""Treat UDP packet data received as full syslog message"""

    enable_proxy_header: Annotated[
        Optional[bool], pydantic.Field(alias="enableProxyHeader")
    ] = False
    r"""Enable if the connection is proxied by a device that supports Proxy Protocol V1 or V2"""

    keep_fields_list: Annotated[
        Optional[List[str]], pydantic.Field(alias="keepFieldsList")
    ] = None
    r"""Wildcard list of fields to keep from source data; * = ALL (default)"""

    octet_counting: Annotated[Optional[bool], pydantic.Field(alias="octetCounting")] = (
        False
    )
    r"""Enable if incoming messages use octet counting per RFC 6587."""

    infer_framing: Annotated[Optional[bool], pydantic.Field(alias="inferFraming")] = (
        True
    )
    r"""Enable if we should infer the syslog framing of the incoming messages."""

    strictly_infer_octet_counting: Annotated[
        Optional[bool], pydantic.Field(alias="strictlyInferOctetCounting")
    ] = True
    r"""Enable if we should infer octet counting only if the messages comply with RFC 5424."""

    allow_non_standard_app_name: Annotated[
        Optional[bool], pydantic.Field(alias="allowNonStandardAppName")
    ] = False
    r"""Enable if RFC 3164-formatted messages have hyphens in the app name portion of the TAG section. If disabled, only alphanumeric characters and underscores are allowed. Ignored for RFC 5424-formatted messages."""

    max_active_cxn: Annotated[Optional[float], pydantic.Field(alias="maxActiveCxn")] = (
        1000
    )
    r"""Maximum number of active connections allowed per Worker Process for TCP connections. Use 0 for unlimited."""

    socket_idle_timeout: Annotated[
        Optional[float], pydantic.Field(alias="socketIdleTimeout")
    ] = 0
    r"""How long @{product} should wait before assuming that an inactive socket has timed out. After this time, the connection will be closed. Leave at 0 for no inactive socket monitoring."""

    socket_ending_max_wait: Annotated[
        Optional[float], pydantic.Field(alias="socketEndingMaxWait")
    ] = 30
    r"""How long the server will wait after initiating a closure for a client to close its end of the connection. If the client doesn't close the connection within this time, the server will forcefully terminate the socket to prevent resource leaks and ensure efficient connection cleanup and system stability. Leave at 0 for no inactive socket monitoring."""

    socket_max_lifespan: Annotated[
        Optional[float], pydantic.Field(alias="socketMaxLifespan")
    ] = 0
    r"""The maximum duration a socket can remain open, even if active. This helps manage resources and mitigate issues caused by TCP pinning. Set to 0 to disable."""

    tls: Optional[InputSyslogTLSSettingsServerSide2] = None

    metadata: Optional[List[InputSyslogMetadatum2]] = None
    r"""Fields to add to events from this input"""

    udp_socket_rx_buf_size: Annotated[
        Optional[float], pydantic.Field(alias="udpSocketRxBufSize")
    ] = None
    r"""Optionally, set the SO_RCVBUF socket option for the UDP socket. This value tells the operating system how many bytes can be buffered in the kernel before events are dropped. Leave blank to use the OS default. Caution: Increasing this value will affect OS memory utilization."""

    enable_load_balancing: Annotated[
        Optional[bool], pydantic.Field(alias="enableLoadBalancing")
    ] = False
    r"""Load balance traffic across all Worker Processes"""

    description: Optional[str] = None

    enable_enhanced_proxy_header_parsing: Annotated[
        Optional[bool], pydantic.Field(alias="enableEnhancedProxyHeaderParsing")
    ] = None
    r"""When enabled, parses PROXY protocol headers during the TLS handshake. Disable if compatibility issues arise."""

    @property
    def additional_properties(self):
        return self.__pydantic_extra__

    @additional_properties.setter
    def additional_properties(self, value):
        self.__pydantic_extra__ = value  # pyright: ignore[reportIncompatibleVariableOverride]


class InputSyslogType1(str, Enum):
    SYSLOG = "syslog"


class InputSyslogConnection1TypedDict(TypedDict):
    output: str
    pipeline: NotRequired[str]


class InputSyslogConnection1(BaseModel):
    output: str

    pipeline: Optional[str] = None


class InputSyslogMode1(str, Enum, metaclass=utils.OpenEnumMeta):
    r"""With Smart mode, PQ will write events to the filesystem only when it detects backpressure from the processing engine. With Always On mode, PQ will always write events directly to the queue before forwarding them to the processing engine."""

    # Smart
    SMART = "smart"
    # Always On
    ALWAYS = "always"


class InputSyslogCompression1(str, Enum, metaclass=utils.OpenEnumMeta):
    r"""Codec to use to compress the persisted data"""

    # None
    NONE = "none"
    # Gzip
    GZIP = "gzip"


class InputSyslogPqControls1TypedDict(TypedDict):
    pass


class InputSyslogPqControls1(BaseModel):
    pass


class InputSyslogPq1TypedDict(TypedDict):
    mode: NotRequired[InputSyslogMode1]
    r"""With Smart mode, PQ will write events to the filesystem only when it detects backpressure from the processing engine. With Always On mode, PQ will always write events directly to the queue before forwarding them to the processing engine."""
    max_buffer_size: NotRequired[float]
    r"""The maximum number of events to hold in memory before writing the events to disk"""
    commit_frequency: NotRequired[float]
    r"""The number of events to send downstream before committing that Stream has read them"""
    max_file_size: NotRequired[str]
    r"""The maximum size to store in each queue file before closing and optionally compressing. Enter a numeral with units of KB, MB, etc."""
    max_size: NotRequired[str]
    r"""The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc."""
    path: NotRequired[str]
    r"""The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/inputs/<input-id>"""
    compress: NotRequired[InputSyslogCompression1]
    r"""Codec to use to compress the persisted data"""
    pq_controls: NotRequired[InputSyslogPqControls1TypedDict]


class InputSyslogPq1(BaseModel):
    mode: Annotated[
        Optional[InputSyslogMode1], PlainValidator(validate_open_enum(False))
    ] = InputSyslogMode1.ALWAYS
    r"""With Smart mode, PQ will write events to the filesystem only when it detects backpressure from the processing engine. With Always On mode, PQ will always write events directly to the queue before forwarding them to the processing engine."""

    max_buffer_size: Annotated[
        Optional[float], pydantic.Field(alias="maxBufferSize")
    ] = 1000
    r"""The maximum number of events to hold in memory before writing the events to disk"""

    commit_frequency: Annotated[
        Optional[float], pydantic.Field(alias="commitFrequency")
    ] = 42
    r"""The number of events to send downstream before committing that Stream has read them"""

    max_file_size: Annotated[Optional[str], pydantic.Field(alias="maxFileSize")] = (
        "1 MB"
    )
    r"""The maximum size to store in each queue file before closing and optionally compressing. Enter a numeral with units of KB, MB, etc."""

    max_size: Annotated[Optional[str], pydantic.Field(alias="maxSize")] = "5GB"
    r"""The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc."""

    path: Optional[str] = "$CRIBL_HOME/state/queues"
    r"""The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/inputs/<input-id>"""

    compress: Annotated[
        Optional[InputSyslogCompression1], PlainValidator(validate_open_enum(False))
    ] = InputSyslogCompression1.NONE
    r"""Codec to use to compress the persisted data"""

    pq_controls: Annotated[
        Optional[InputSyslogPqControls1], pydantic.Field(alias="pqControls")
    ] = None

    @field_serializer("mode")
    def serialize_mode(self, value):
        if isinstance(value, str):
            try:
                return models.InputSyslogMode1(value)
            except ValueError:
                return value
        return value

    @field_serializer("compress")
    def serialize_compress(self, value):
        if isinstance(value, str):
            try:
                return models.InputSyslogCompression1(value)
            except ValueError:
                return value
        return value


class InputSyslogMinimumTLSVersion1(str, Enum, metaclass=utils.OpenEnumMeta):
    TL_SV1 = "TLSv1"
    TL_SV1_1 = "TLSv1.1"
    TL_SV1_2 = "TLSv1.2"
    TL_SV1_3 = "TLSv1.3"


class InputSyslogMaximumTLSVersion1(str, Enum, metaclass=utils.OpenEnumMeta):
    TL_SV1 = "TLSv1"
    TL_SV1_1 = "TLSv1.1"
    TL_SV1_2 = "TLSv1.2"
    TL_SV1_3 = "TLSv1.3"


class InputSyslogTLSSettingsServerSide1TypedDict(TypedDict):
    disabled: NotRequired[bool]
    request_cert: NotRequired[bool]
    r"""Require clients to present their certificates. Used to perform client authentication using SSL certs."""
    reject_unauthorized: NotRequired[bool]
    r"""Reject certificates not authorized by a CA in the CA certificate path or by another trusted CA (such as the system's)"""
    common_name_regex: NotRequired[str]
    r"""Regex matching allowable common names in peer certificates' subject attribute"""
    certificate_name: NotRequired[str]
    r"""The name of the predefined certificate"""
    priv_key_path: NotRequired[str]
    r"""Path on server containing the private key to use. PEM format. Can reference $ENV_VARS."""
    passphrase: NotRequired[str]
    r"""Passphrase to use to decrypt private key"""
    cert_path: NotRequired[str]
    r"""Path on server containing certificates to use. PEM format. Can reference $ENV_VARS."""
    ca_path: NotRequired[str]
    r"""Path on server containing CA certificates to use. PEM format. Can reference $ENV_VARS."""
    min_version: NotRequired[InputSyslogMinimumTLSVersion1]
    max_version: NotRequired[InputSyslogMaximumTLSVersion1]


class InputSyslogTLSSettingsServerSide1(BaseModel):
    disabled: Optional[bool] = True

    request_cert: Annotated[Optional[bool], pydantic.Field(alias="requestCert")] = False
    r"""Require clients to present their certificates. Used to perform client authentication using SSL certs."""

    reject_unauthorized: Annotated[
        Optional[bool], pydantic.Field(alias="rejectUnauthorized")
    ] = True
    r"""Reject certificates not authorized by a CA in the CA certificate path or by another trusted CA (such as the system's)"""

    common_name_regex: Annotated[
        Optional[str], pydantic.Field(alias="commonNameRegex")
    ] = "/.*/"
    r"""Regex matching allowable common names in peer certificates' subject attribute"""

    certificate_name: Annotated[
        Optional[str], pydantic.Field(alias="certificateName")
    ] = None
    r"""The name of the predefined certificate"""

    priv_key_path: Annotated[Optional[str], pydantic.Field(alias="privKeyPath")] = None
    r"""Path on server containing the private key to use. PEM format. Can reference $ENV_VARS."""

    passphrase: Optional[str] = None
    r"""Passphrase to use to decrypt private key"""

    cert_path: Annotated[Optional[str], pydantic.Field(alias="certPath")] = None
    r"""Path on server containing certificates to use. PEM format. Can reference $ENV_VARS."""

    ca_path: Annotated[Optional[str], pydantic.Field(alias="caPath")] = None
    r"""Path on server containing CA certificates to use. PEM format. Can reference $ENV_VARS."""

    min_version: Annotated[
        Annotated[
            Optional[InputSyslogMinimumTLSVersion1],
            PlainValidator(validate_open_enum(False)),
        ],
        pydantic.Field(alias="minVersion"),
    ] = None

    max_version: Annotated[
        Annotated[
            Optional[InputSyslogMaximumTLSVersion1],
            PlainValidator(validate_open_enum(False)),
        ],
        pydantic.Field(alias="maxVersion"),
    ] = None

    @field_serializer("min_version")
    def serialize_min_version(self, value):
        if isinstance(value, str):
            try:
                return models.InputSyslogMinimumTLSVersion1(value)
            except ValueError:
                return value
        return value

    @field_serializer("max_version")
    def serialize_max_version(self, value):
        if isinstance(value, str):
            try:
                return models.InputSyslogMaximumTLSVersion1(value)
            except ValueError:
                return value
        return value


class InputSyslogMetadatum1TypedDict(TypedDict):
    name: str
    value: str
    r"""JavaScript expression to compute field's value, enclosed in quotes or backticks. (Can evaluate to a constant.)"""


class InputSyslogMetadatum1(BaseModel):
    name: str

    value: str
    r"""JavaScript expression to compute field's value, enclosed in quotes or backticks. (Can evaluate to a constant.)"""


class InputSyslogSyslog1TypedDict(TypedDict):
    type: InputSyslogType1
    udp_port: float
    r"""Enter UDP port number to listen on. Not required if listening on TCP."""
    id: NotRequired[str]
    r"""Unique ID for this input"""
    disabled: NotRequired[bool]
    pipeline: NotRequired[str]
    r"""Pipeline to process data from this Source before sending it through the Routes"""
    send_to_routes: NotRequired[bool]
    r"""Select whether to send data to Routes, or directly to Destinations."""
    environment: NotRequired[str]
    r"""Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere."""
    pq_enabled: NotRequired[bool]
    r"""Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers)."""
    streamtags: NotRequired[List[str]]
    r"""Tags for filtering and grouping in @{product}"""
    connections: NotRequired[List[InputSyslogConnection1TypedDict]]
    r"""Direct connections to Destinations, and optionally via a Pipeline or a Pack"""
    pq: NotRequired[InputSyslogPq1TypedDict]
    host: NotRequired[str]
    r"""Address to bind on. For IPv4 (all addresses), use the default '0.0.0.0'. For IPv6, enter '::' (all addresses) or specify an IP address."""
    tcp_port: NotRequired[float]
    r"""Enter TCP port number to listen on. Not required if listening on UDP."""
    max_buffer_size: NotRequired[float]
    r"""Maximum number of events to buffer when downstream is blocking. Only applies to UDP."""
    ip_whitelist_regex: NotRequired[str]
    r"""Regex matching IP addresses that are allowed to send data"""
    timestamp_timezone: NotRequired[str]
    r"""Timezone to assign to timestamps without timezone info"""
    single_msg_udp_packets: NotRequired[bool]
    r"""Treat UDP packet data received as full syslog message"""
    enable_proxy_header: NotRequired[bool]
    r"""Enable if the connection is proxied by a device that supports Proxy Protocol V1 or V2"""
    keep_fields_list: NotRequired[List[str]]
    r"""Wildcard list of fields to keep from source data; * = ALL (default)"""
    octet_counting: NotRequired[bool]
    r"""Enable if incoming messages use octet counting per RFC 6587."""
    infer_framing: NotRequired[bool]
    r"""Enable if we should infer the syslog framing of the incoming messages."""
    strictly_infer_octet_counting: NotRequired[bool]
    r"""Enable if we should infer octet counting only if the messages comply with RFC 5424."""
    allow_non_standard_app_name: NotRequired[bool]
    r"""Enable if RFC 3164-formatted messages have hyphens in the app name portion of the TAG section. If disabled, only alphanumeric characters and underscores are allowed. Ignored for RFC 5424-formatted messages."""
    max_active_cxn: NotRequired[float]
    r"""Maximum number of active connections allowed per Worker Process for TCP connections. Use 0 for unlimited."""
    socket_idle_timeout: NotRequired[float]
    r"""How long @{product} should wait before assuming that an inactive socket has timed out. After this time, the connection will be closed. Leave at 0 for no inactive socket monitoring."""
    socket_ending_max_wait: NotRequired[float]
    r"""How long the server will wait after initiating a closure for a client to close its end of the connection. If the client doesn't close the connection within this time, the server will forcefully terminate the socket to prevent resource leaks and ensure efficient connection cleanup and system stability. Leave at 0 for no inactive socket monitoring."""
    socket_max_lifespan: NotRequired[float]
    r"""The maximum duration a socket can remain open, even if active. This helps manage resources and mitigate issues caused by TCP pinning. Set to 0 to disable."""
    tls: NotRequired[InputSyslogTLSSettingsServerSide1TypedDict]
    metadata: NotRequired[List[InputSyslogMetadatum1TypedDict]]
    r"""Fields to add to events from this input"""
    udp_socket_rx_buf_size: NotRequired[float]
    r"""Optionally, set the SO_RCVBUF socket option for the UDP socket. This value tells the operating system how many bytes can be buffered in the kernel before events are dropped. Leave blank to use the OS default. Caution: Increasing this value will affect OS memory utilization."""
    enable_load_balancing: NotRequired[bool]
    r"""Load balance traffic across all Worker Processes"""
    description: NotRequired[str]
    enable_enhanced_proxy_header_parsing: NotRequired[bool]
    r"""When enabled, parses PROXY protocol headers during the TLS handshake. Disable if compatibility issues arise."""


class InputSyslogSyslog1(BaseModel):
    model_config = ConfigDict(
        populate_by_name=True, arbitrary_types_allowed=True, extra="allow"
    )
    __pydantic_extra__: Dict[str, Any] = pydantic.Field(init=False)

    type: InputSyslogType1

    udp_port: Annotated[float, pydantic.Field(alias="udpPort")]
    r"""Enter UDP port number to listen on. Not required if listening on TCP."""

    id: Optional[str] = None
    r"""Unique ID for this input"""

    disabled: Optional[bool] = False

    pipeline: Optional[str] = None
    r"""Pipeline to process data from this Source before sending it through the Routes"""

    send_to_routes: Annotated[Optional[bool], pydantic.Field(alias="sendToRoutes")] = (
        True
    )
    r"""Select whether to send data to Routes, or directly to Destinations."""

    environment: Optional[str] = None
    r"""Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere."""

    pq_enabled: Annotated[Optional[bool], pydantic.Field(alias="pqEnabled")] = False
    r"""Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers)."""

    streamtags: Optional[List[str]] = None
    r"""Tags for filtering and grouping in @{product}"""

    connections: Optional[List[InputSyslogConnection1]] = None
    r"""Direct connections to Destinations, and optionally via a Pipeline or a Pack"""

    pq: Optional[InputSyslogPq1] = None

    host: Optional[str] = "0.0.0.0"
    r"""Address to bind on. For IPv4 (all addresses), use the default '0.0.0.0'. For IPv6, enter '::' (all addresses) or specify an IP address."""

    tcp_port: Annotated[Optional[float], pydantic.Field(alias="tcpPort")] = None
    r"""Enter TCP port number to listen on. Not required if listening on UDP."""

    max_buffer_size: Annotated[
        Optional[float], pydantic.Field(alias="maxBufferSize")
    ] = 1000
    r"""Maximum number of events to buffer when downstream is blocking. Only applies to UDP."""

    ip_whitelist_regex: Annotated[
        Optional[str], pydantic.Field(alias="ipWhitelistRegex")
    ] = "/.*/"
    r"""Regex matching IP addresses that are allowed to send data"""

    timestamp_timezone: Annotated[
        Optional[str], pydantic.Field(alias="timestampTimezone")
    ] = "local"
    r"""Timezone to assign to timestamps without timezone info"""

    single_msg_udp_packets: Annotated[
        Optional[bool], pydantic.Field(alias="singleMsgUdpPackets")
    ] = False
    r"""Treat UDP packet data received as full syslog message"""

    enable_proxy_header: Annotated[
        Optional[bool], pydantic.Field(alias="enableProxyHeader")
    ] = False
    r"""Enable if the connection is proxied by a device that supports Proxy Protocol V1 or V2"""

    keep_fields_list: Annotated[
        Optional[List[str]], pydantic.Field(alias="keepFieldsList")
    ] = None
    r"""Wildcard list of fields to keep from source data; * = ALL (default)"""

    octet_counting: Annotated[Optional[bool], pydantic.Field(alias="octetCounting")] = (
        False
    )
    r"""Enable if incoming messages use octet counting per RFC 6587."""

    infer_framing: Annotated[Optional[bool], pydantic.Field(alias="inferFraming")] = (
        True
    )
    r"""Enable if we should infer the syslog framing of the incoming messages."""

    strictly_infer_octet_counting: Annotated[
        Optional[bool], pydantic.Field(alias="strictlyInferOctetCounting")
    ] = True
    r"""Enable if we should infer octet counting only if the messages comply with RFC 5424."""

    allow_non_standard_app_name: Annotated[
        Optional[bool], pydantic.Field(alias="allowNonStandardAppName")
    ] = False
    r"""Enable if RFC 3164-formatted messages have hyphens in the app name portion of the TAG section. If disabled, only alphanumeric characters and underscores are allowed. Ignored for RFC 5424-formatted messages."""

    max_active_cxn: Annotated[Optional[float], pydantic.Field(alias="maxActiveCxn")] = (
        1000
    )
    r"""Maximum number of active connections allowed per Worker Process for TCP connections. Use 0 for unlimited."""

    socket_idle_timeout: Annotated[
        Optional[float], pydantic.Field(alias="socketIdleTimeout")
    ] = 0
    r"""How long @{product} should wait before assuming that an inactive socket has timed out. After this time, the connection will be closed. Leave at 0 for no inactive socket monitoring."""

    socket_ending_max_wait: Annotated[
        Optional[float], pydantic.Field(alias="socketEndingMaxWait")
    ] = 30
    r"""How long the server will wait after initiating a closure for a client to close its end of the connection. If the client doesn't close the connection within this time, the server will forcefully terminate the socket to prevent resource leaks and ensure efficient connection cleanup and system stability. Leave at 0 for no inactive socket monitoring."""

    socket_max_lifespan: Annotated[
        Optional[float], pydantic.Field(alias="socketMaxLifespan")
    ] = 0
    r"""The maximum duration a socket can remain open, even if active. This helps manage resources and mitigate issues caused by TCP pinning. Set to 0 to disable."""

    tls: Optional[InputSyslogTLSSettingsServerSide1] = None

    metadata: Optional[List[InputSyslogMetadatum1]] = None
    r"""Fields to add to events from this input"""

    udp_socket_rx_buf_size: Annotated[
        Optional[float], pydantic.Field(alias="udpSocketRxBufSize")
    ] = None
    r"""Optionally, set the SO_RCVBUF socket option for the UDP socket. This value tells the operating system how many bytes can be buffered in the kernel before events are dropped. Leave blank to use the OS default. Caution: Increasing this value will affect OS memory utilization."""

    enable_load_balancing: Annotated[
        Optional[bool], pydantic.Field(alias="enableLoadBalancing")
    ] = False
    r"""Load balance traffic across all Worker Processes"""

    description: Optional[str] = None

    enable_enhanced_proxy_header_parsing: Annotated[
        Optional[bool], pydantic.Field(alias="enableEnhancedProxyHeaderParsing")
    ] = None
    r"""When enabled, parses PROXY protocol headers during the TLS handshake. Disable if compatibility issues arise."""

    @property
    def additional_properties(self):
        return self.__pydantic_extra__

    @additional_properties.setter
    def additional_properties(self, value):
        self.__pydantic_extra__ = value  # pyright: ignore[reportIncompatibleVariableOverride]


InputSyslogTypedDict = TypeAliasType(
    "InputSyslogTypedDict",
    Union[InputSyslogSyslog1TypedDict, InputSyslogSyslog2TypedDict],
)


InputSyslog = TypeAliasType(
    "InputSyslog", Union[InputSyslogSyslog1, InputSyslogSyslog2]
)


class InputTypeSqs(str, Enum):
    SQS = "sqs"


class ConnectionSqsTypedDict(TypedDict):
    output: str
    pipeline: NotRequired[str]


class ConnectionSqs(BaseModel):
    output: str

    pipeline: Optional[str] = None


class PqModeSqs(str, Enum, metaclass=utils.OpenEnumMeta):
    r"""With Smart mode, PQ will write events to the filesystem only when it detects backpressure from the processing engine. With Always On mode, PQ will always write events directly to the queue before forwarding them to the processing engine."""

    # Smart
    SMART = "smart"
    # Always On
    ALWAYS = "always"


class PqCompressionSqs(str, Enum, metaclass=utils.OpenEnumMeta):
    r"""Codec to use to compress the persisted data"""

    # None
    NONE = "none"
    # Gzip
    GZIP = "gzip"


class InputPqControlsSqsTypedDict(TypedDict):
    pass


class InputPqControlsSqs(BaseModel):
    pass


class PqSqsTypedDict(TypedDict):
    mode: NotRequired[PqModeSqs]
    r"""With Smart mode, PQ will write events to the filesystem only when it detects backpressure from the processing engine. With Always On mode, PQ will always write events directly to the queue before forwarding them to the processing engine."""
    max_buffer_size: NotRequired[float]
    r"""The maximum number of events to hold in memory before writing the events to disk"""
    commit_frequency: NotRequired[float]
    r"""The number of events to send downstream before committing that Stream has read them"""
    max_file_size: NotRequired[str]
    r"""The maximum size to store in each queue file before closing and optionally compressing. Enter a numeral with units of KB, MB, etc."""
    max_size: NotRequired[str]
    r"""The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc."""
    path: NotRequired[str]
    r"""The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/inputs/<input-id>"""
    compress: NotRequired[PqCompressionSqs]
    r"""Codec to use to compress the persisted data"""
    pq_controls: NotRequired[InputPqControlsSqsTypedDict]


class PqSqs(BaseModel):
    mode: Annotated[Optional[PqModeSqs], PlainValidator(validate_open_enum(False))] = (
        PqModeSqs.ALWAYS
    )
    r"""With Smart mode, PQ will write events to the filesystem only when it detects backpressure from the processing engine. With Always On mode, PQ will always write events directly to the queue before forwarding them to the processing engine."""

    max_buffer_size: Annotated[
        Optional[float], pydantic.Field(alias="maxBufferSize")
    ] = 1000
    r"""The maximum number of events to hold in memory before writing the events to disk"""

    commit_frequency: Annotated[
        Optional[float], pydantic.Field(alias="commitFrequency")
    ] = 42
    r"""The number of events to send downstream before committing that Stream has read them"""

    max_file_size: Annotated[Optional[str], pydantic.Field(alias="maxFileSize")] = (
        "1 MB"
    )
    r"""The maximum size to store in each queue file before closing and optionally compressing. Enter a numeral with units of KB, MB, etc."""

    max_size: Annotated[Optional[str], pydantic.Field(alias="maxSize")] = "5GB"
    r"""The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc."""

    path: Optional[str] = "$CRIBL_HOME/state/queues"
    r"""The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/inputs/<input-id>"""

    compress: Annotated[
        Optional[PqCompressionSqs], PlainValidator(validate_open_enum(False))
    ] = PqCompressionSqs.NONE
    r"""Codec to use to compress the persisted data"""

    pq_controls: Annotated[
        Optional[InputPqControlsSqs], pydantic.Field(alias="pqControls")
    ] = None

    @field_serializer("mode")
    def serialize_mode(self, value):
        if isinstance(value, str):
            try:
                return models.PqModeSqs(value)
            except ValueError:
                return value
        return value

    @field_serializer("compress")
    def serialize_compress(self, value):
        if isinstance(value, str):
            try:
                return models.PqCompressionSqs(value)
            except ValueError:
                return value
        return value


class InputQueueType(str, Enum, metaclass=utils.OpenEnumMeta):
    r"""The queue type used (or created)"""

    # Standard
    STANDARD = "standard"
    # FIFO
    FIFO = "fifo"


class InputAuthenticationMethodSqs(str, Enum, metaclass=utils.OpenEnumMeta):
    r"""AWS authentication method. Choose Auto to use IAM roles."""

    # Auto
    AUTO = "auto"
    # Manual
    MANUAL = "manual"
    # Secret Key pair
    SECRET = "secret"


class InputSignatureVersionSqs(str, Enum, metaclass=utils.OpenEnumMeta):
    r"""Signature version to use for signing SQS requests"""

    V2 = "v2"
    V4 = "v4"


class MetadatumSqsTypedDict(TypedDict):
    name: str
    value: str
    r"""JavaScript expression to compute field's value, enclosed in quotes or backticks. (Can evaluate to a constant.)"""


class MetadatumSqs(BaseModel):
    name: str

    value: str
    r"""JavaScript expression to compute field's value, enclosed in quotes or backticks. (Can evaluate to a constant.)"""


class InputSqsTypedDict(TypedDict):
    type: InputTypeSqs
    queue_name: str
    r"""The name, URL, or ARN of the SQS queue to read events from. When a non-AWS URL is specified, format must be: '{url}/myQueueName'. Example: 'https://host:port/myQueueName'. Value must be a JavaScript expression (which can evaluate to a constant value), enclosed in quotes or backticks. Can only be evaluated at init time. Example referencing a Global Variable: `https://host:port/myQueue-${C.vars.myVar}`."""
    queue_type: InputQueueType
    r"""The queue type used (or created)"""
    id: NotRequired[str]
    r"""Unique ID for this input"""
    disabled: NotRequired[bool]
    pipeline: NotRequired[str]
    r"""Pipeline to process data from this Source before sending it through the Routes"""
    send_to_routes: NotRequired[bool]
    r"""Select whether to send data to Routes, or directly to Destinations."""
    environment: NotRequired[str]
    r"""Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere."""
    pq_enabled: NotRequired[bool]
    r"""Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers)."""
    streamtags: NotRequired[List[str]]
    r"""Tags for filtering and grouping in @{product}"""
    connections: NotRequired[List[ConnectionSqsTypedDict]]
    r"""Direct connections to Destinations, and optionally via a Pipeline or a Pack"""
    pq: NotRequired[PqSqsTypedDict]
    aws_account_id: NotRequired[str]
    r"""SQS queue owner's AWS account ID. Leave empty if SQS queue is in same AWS account."""
    create_queue: NotRequired[bool]
    r"""Create queue if it does not exist"""
    aws_authentication_method: NotRequired[InputAuthenticationMethodSqs]
    r"""AWS authentication method. Choose Auto to use IAM roles."""
    aws_secret_key: NotRequired[str]
    region: NotRequired[str]
    r"""AWS Region where the SQS queue is located. Required, unless the Queue entry is a URL or ARN that includes a Region."""
    endpoint: NotRequired[str]
    r"""SQS service endpoint. If empty, defaults to the AWS Region-specific endpoint. Otherwise, it must point to SQS-compatible endpoint."""
    signature_version: NotRequired[InputSignatureVersionSqs]
    r"""Signature version to use for signing SQS requests"""
    reuse_connections: NotRequired[bool]
    r"""Reuse connections between requests, which can improve performance"""
    reject_unauthorized: NotRequired[bool]
    r"""Reject certificates that cannot be verified against a valid CA, such as self-signed certificates"""
    enable_assume_role: NotRequired[bool]
    r"""Use Assume Role credentials to access SQS"""
    assume_role_arn: NotRequired[str]
    r"""Amazon Resource Name (ARN) of the role to assume"""
    assume_role_external_id: NotRequired[str]
    r"""External ID to use when assuming role"""
    duration_seconds: NotRequired[float]
    r"""Duration of the assumed role's session, in seconds. Minimum is 900 (15 minutes), default is 3600 (1 hour), and maximum is 43200 (12 hours)."""
    max_messages: NotRequired[float]
    r"""The maximum number of messages SQS should return in a poll request. Amazon SQS never returns more messages than this value (however, fewer messages might be returned). Valid values: 1 to 10."""
    visibility_timeout: NotRequired[float]
    r"""After messages are retrieved by a ReceiveMessage request, @{product} will hide them from subsequent retrieve requests for at least this duration. You can set this as high as 43200 sec. (12 hours)."""
    metadata: NotRequired[List[MetadatumSqsTypedDict]]
    r"""Fields to add to events from this input"""
    poll_timeout: NotRequired[float]
    r"""How long to wait for events before trying polling again. The lower the number the higher the AWS bill. The higher the number the longer it will take for the source to react to configuration changes and system restarts."""
    description: NotRequired[str]
    aws_api_key: NotRequired[str]
    aws_secret: NotRequired[str]
    r"""Select or create a stored secret that references your access key and secret key"""
    num_receivers: NotRequired[float]
    r"""How many receiver processes to run. The higher the number, the better the throughput - at the expense of CPU overhead."""


class InputSqs(BaseModel):
    model_config = ConfigDict(
        populate_by_name=True, arbitrary_types_allowed=True, extra="allow"
    )
    __pydantic_extra__: Dict[str, Any] = pydantic.Field(init=False)

    type: InputTypeSqs

    queue_name: Annotated[str, pydantic.Field(alias="queueName")]
    r"""The name, URL, or ARN of the SQS queue to read events from. When a non-AWS URL is specified, format must be: '{url}/myQueueName'. Example: 'https://host:port/myQueueName'. Value must be a JavaScript expression (which can evaluate to a constant value), enclosed in quotes or backticks. Can only be evaluated at init time. Example referencing a Global Variable: `https://host:port/myQueue-${C.vars.myVar}`."""

    queue_type: Annotated[
        Annotated[InputQueueType, PlainValidator(validate_open_enum(False))],
        pydantic.Field(alias="queueType"),
    ]
    r"""The queue type used (or created)"""

    id: Optional[str] = None
    r"""Unique ID for this input"""

    disabled: Optional[bool] = False

    pipeline: Optional[str] = None
    r"""Pipeline to process data from this Source before sending it through the Routes"""

    send_to_routes: Annotated[Optional[bool], pydantic.Field(alias="sendToRoutes")] = (
        True
    )
    r"""Select whether to send data to Routes, or directly to Destinations."""

    environment: Optional[str] = None
    r"""Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere."""

    pq_enabled: Annotated[Optional[bool], pydantic.Field(alias="pqEnabled")] = False
    r"""Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers)."""

    streamtags: Optional[List[str]] = None
    r"""Tags for filtering and grouping in @{product}"""

    connections: Optional[List[ConnectionSqs]] = None
    r"""Direct connections to Destinations, and optionally via a Pipeline or a Pack"""

    pq: Optional[PqSqs] = None

    aws_account_id: Annotated[Optional[str], pydantic.Field(alias="awsAccountId")] = (
        None
    )
    r"""SQS queue owner's AWS account ID. Leave empty if SQS queue is in same AWS account."""

    create_queue: Annotated[Optional[bool], pydantic.Field(alias="createQueue")] = False
    r"""Create queue if it does not exist"""

    aws_authentication_method: Annotated[
        Annotated[
            Optional[InputAuthenticationMethodSqs],
            PlainValidator(validate_open_enum(False)),
        ],
        pydantic.Field(alias="awsAuthenticationMethod"),
    ] = InputAuthenticationMethodSqs.AUTO
    r"""AWS authentication method. Choose Auto to use IAM roles."""

    aws_secret_key: Annotated[Optional[str], pydantic.Field(alias="awsSecretKey")] = (
        None
    )

    region: Optional[str] = None
    r"""AWS Region where the SQS queue is located. Required, unless the Queue entry is a URL or ARN that includes a Region."""

    endpoint: Optional[str] = None
    r"""SQS service endpoint. If empty, defaults to the AWS Region-specific endpoint. Otherwise, it must point to SQS-compatible endpoint."""

    signature_version: Annotated[
        Annotated[
            Optional[InputSignatureVersionSqs],
            PlainValidator(validate_open_enum(False)),
        ],
        pydantic.Field(alias="signatureVersion"),
    ] = InputSignatureVersionSqs.V4
    r"""Signature version to use for signing SQS requests"""

    reuse_connections: Annotated[
        Optional[bool], pydantic.Field(alias="reuseConnections")
    ] = True
    r"""Reuse connections between requests, which can improve performance"""

    reject_unauthorized: Annotated[
        Optional[bool], pydantic.Field(alias="rejectUnauthorized")
    ] = True
    r"""Reject certificates that cannot be verified against a valid CA, such as self-signed certificates"""

    enable_assume_role: Annotated[
        Optional[bool], pydantic.Field(alias="enableAssumeRole")
    ] = False
    r"""Use Assume Role credentials to access SQS"""

    assume_role_arn: Annotated[Optional[str], pydantic.Field(alias="assumeRoleArn")] = (
        None
    )
    r"""Amazon Resource Name (ARN) of the role to assume"""

    assume_role_external_id: Annotated[
        Optional[str], pydantic.Field(alias="assumeRoleExternalId")
    ] = None
    r"""External ID to use when assuming role"""

    duration_seconds: Annotated[
        Optional[float], pydantic.Field(alias="durationSeconds")
    ] = 3600
    r"""Duration of the assumed role's session, in seconds. Minimum is 900 (15 minutes), default is 3600 (1 hour), and maximum is 43200 (12 hours)."""

    max_messages: Annotated[Optional[float], pydantic.Field(alias="maxMessages")] = 10
    r"""The maximum number of messages SQS should return in a poll request. Amazon SQS never returns more messages than this value (however, fewer messages might be returned). Valid values: 1 to 10."""

    visibility_timeout: Annotated[
        Optional[float], pydantic.Field(alias="visibilityTimeout")
    ] = 600
    r"""After messages are retrieved by a ReceiveMessage request, @{product} will hide them from subsequent retrieve requests for at least this duration. You can set this as high as 43200 sec. (12 hours)."""

    metadata: Optional[List[MetadatumSqs]] = None
    r"""Fields to add to events from this input"""

    poll_timeout: Annotated[Optional[float], pydantic.Field(alias="pollTimeout")] = 10
    r"""How long to wait for events before trying polling again. The lower the number the higher the AWS bill. The higher the number the longer it will take for the source to react to configuration changes and system restarts."""

    description: Optional[str] = None

    aws_api_key: Annotated[Optional[str], pydantic.Field(alias="awsApiKey")] = None

    aws_secret: Annotated[Optional[str], pydantic.Field(alias="awsSecret")] = None
    r"""Select or create a stored secret that references your access key and secret key"""

    num_receivers: Annotated[Optional[float], pydantic.Field(alias="numReceivers")] = 3
    r"""How many receiver processes to run. The higher the number, the better the throughput - at the expense of CPU overhead."""

    @property
    def additional_properties(self):
        return self.__pydantic_extra__

    @additional_properties.setter
    def additional_properties(self, value):
        self.__pydantic_extra__ = value  # pyright: ignore[reportIncompatibleVariableOverride]

    @field_serializer("queue_type")
    def serialize_queue_type(self, value):
        if isinstance(value, str):
            try:
                return models.InputQueueType(value)
            except ValueError:
                return value
        return value

    @field_serializer("aws_authentication_method")
    def serialize_aws_authentication_method(self, value):
        if isinstance(value, str):
            try:
                return models.InputAuthenticationMethodSqs(value)
            except ValueError:
                return value
        return value

    @field_serializer("signature_version")
    def serialize_signature_version(self, value):
        if isinstance(value, str):
            try:
                return models.InputSignatureVersionSqs(value)
            except ValueError:
                return value
        return value


class TypeModelDrivenTelemetry(str, Enum):
    MODEL_DRIVEN_TELEMETRY = "model_driven_telemetry"


class ConnectionModelDrivenTelemetryTypedDict(TypedDict):
    output: str
    pipeline: NotRequired[str]


class ConnectionModelDrivenTelemetry(BaseModel):
    output: str

    pipeline: Optional[str] = None


class ModeModelDrivenTelemetry(str, Enum, metaclass=utils.OpenEnumMeta):
    r"""With Smart mode, PQ will write events to the filesystem only when it detects backpressure from the processing engine. With Always On mode, PQ will always write events directly to the queue before forwarding them to the processing engine."""

    # Smart
    SMART = "smart"
    # Always On
    ALWAYS = "always"


class CompressionModelDrivenTelemetry(str, Enum, metaclass=utils.OpenEnumMeta):
    r"""Codec to use to compress the persisted data"""

    # None
    NONE = "none"
    # Gzip
    GZIP = "gzip"


class PqControlsModelDrivenTelemetryTypedDict(TypedDict):
    pass


class PqControlsModelDrivenTelemetry(BaseModel):
    pass


class PqModelDrivenTelemetryTypedDict(TypedDict):
    mode: NotRequired[ModeModelDrivenTelemetry]
    r"""With Smart mode, PQ will write events to the filesystem only when it detects backpressure from the processing engine. With Always On mode, PQ will always write events directly to the queue before forwarding them to the processing engine."""
    max_buffer_size: NotRequired[float]
    r"""The maximum number of events to hold in memory before writing the events to disk"""
    commit_frequency: NotRequired[float]
    r"""The number of events to send downstream before committing that Stream has read them"""
    max_file_size: NotRequired[str]
    r"""The maximum size to store in each queue file before closing and optionally compressing. Enter a numeral with units of KB, MB, etc."""
    max_size: NotRequired[str]
    r"""The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc."""
    path: NotRequired[str]
    r"""The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/inputs/<input-id>"""
    compress: NotRequired[CompressionModelDrivenTelemetry]
    r"""Codec to use to compress the persisted data"""
    pq_controls: NotRequired[PqControlsModelDrivenTelemetryTypedDict]


class PqModelDrivenTelemetry(BaseModel):
    mode: Annotated[
        Optional[ModeModelDrivenTelemetry], PlainValidator(validate_open_enum(False))
    ] = ModeModelDrivenTelemetry.ALWAYS
    r"""With Smart mode, PQ will write events to the filesystem only when it detects backpressure from the processing engine. With Always On mode, PQ will always write events directly to the queue before forwarding them to the processing engine."""

    max_buffer_size: Annotated[
        Optional[float], pydantic.Field(alias="maxBufferSize")
    ] = 1000
    r"""The maximum number of events to hold in memory before writing the events to disk"""

    commit_frequency: Annotated[
        Optional[float], pydantic.Field(alias="commitFrequency")
    ] = 42
    r"""The number of events to send downstream before committing that Stream has read them"""

    max_file_size: Annotated[Optional[str], pydantic.Field(alias="maxFileSize")] = (
        "1 MB"
    )
    r"""The maximum size to store in each queue file before closing and optionally compressing. Enter a numeral with units of KB, MB, etc."""

    max_size: Annotated[Optional[str], pydantic.Field(alias="maxSize")] = "5GB"
    r"""The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc."""

    path: Optional[str] = "$CRIBL_HOME/state/queues"
    r"""The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/inputs/<input-id>"""

    compress: Annotated[
        Optional[CompressionModelDrivenTelemetry],
        PlainValidator(validate_open_enum(False)),
    ] = CompressionModelDrivenTelemetry.NONE
    r"""Codec to use to compress the persisted data"""

    pq_controls: Annotated[
        Optional[PqControlsModelDrivenTelemetry], pydantic.Field(alias="pqControls")
    ] = None

    @field_serializer("mode")
    def serialize_mode(self, value):
        if isinstance(value, str):
            try:
                return models.ModeModelDrivenTelemetry(value)
            except ValueError:
                return value
        return value

    @field_serializer("compress")
    def serialize_compress(self, value):
        if isinstance(value, str):
            try:
                return models.CompressionModelDrivenTelemetry(value)
            except ValueError:
                return value
        return value


class MinimumTLSVersionModelDrivenTelemetry(str, Enum, metaclass=utils.OpenEnumMeta):
    TL_SV1 = "TLSv1"
    TL_SV1_1 = "TLSv1.1"
    TL_SV1_2 = "TLSv1.2"
    TL_SV1_3 = "TLSv1.3"


class MaximumTLSVersionModelDrivenTelemetry(str, Enum, metaclass=utils.OpenEnumMeta):
    TL_SV1 = "TLSv1"
    TL_SV1_1 = "TLSv1.1"
    TL_SV1_2 = "TLSv1.2"
    TL_SV1_3 = "TLSv1.3"


class TLSSettingsServerSideModelDrivenTelemetryTypedDict(TypedDict):
    disabled: NotRequired[bool]
    request_cert: NotRequired[bool]
    r"""Require clients to present their certificates. Used to perform client authentication using SSL certs."""
    reject_unauthorized: NotRequired[bool]
    r"""Reject certificates not authorized by a CA in the CA certificate path or by another trusted CA (such as the system's)"""
    common_name_regex: NotRequired[str]
    r"""Regex matching allowable common names in peer certificates' subject attribute"""
    certificate_name: NotRequired[str]
    r"""The name of the predefined certificate"""
    priv_key_path: NotRequired[str]
    r"""Path on server containing the private key to use. PEM format. Can reference $ENV_VARS."""
    passphrase: NotRequired[str]
    r"""Passphrase to use to decrypt private key"""
    cert_path: NotRequired[str]
    r"""Path on server containing certificates to use. PEM format. Can reference $ENV_VARS."""
    ca_path: NotRequired[str]
    r"""Path on server containing CA certificates to use. PEM format. Can reference $ENV_VARS."""
    min_version: NotRequired[MinimumTLSVersionModelDrivenTelemetry]
    max_version: NotRequired[MaximumTLSVersionModelDrivenTelemetry]


class TLSSettingsServerSideModelDrivenTelemetry(BaseModel):
    disabled: Optional[bool] = True

    request_cert: Annotated[Optional[bool], pydantic.Field(alias="requestCert")] = False
    r"""Require clients to present their certificates. Used to perform client authentication using SSL certs."""

    reject_unauthorized: Annotated[
        Optional[bool], pydantic.Field(alias="rejectUnauthorized")
    ] = True
    r"""Reject certificates not authorized by a CA in the CA certificate path or by another trusted CA (such as the system's)"""

    common_name_regex: Annotated[
        Optional[str], pydantic.Field(alias="commonNameRegex")
    ] = "/.*/"
    r"""Regex matching allowable common names in peer certificates' subject attribute"""

    certificate_name: Annotated[
        Optional[str], pydantic.Field(alias="certificateName")
    ] = None
    r"""The name of the predefined certificate"""

    priv_key_path: Annotated[Optional[str], pydantic.Field(alias="privKeyPath")] = None
    r"""Path on server containing the private key to use. PEM format. Can reference $ENV_VARS."""

    passphrase: Optional[str] = None
    r"""Passphrase to use to decrypt private key"""

    cert_path: Annotated[Optional[str], pydantic.Field(alias="certPath")] = None
    r"""Path on server containing certificates to use. PEM format. Can reference $ENV_VARS."""

    ca_path: Annotated[Optional[str], pydantic.Field(alias="caPath")] = None
    r"""Path on server containing CA certificates to use. PEM format. Can reference $ENV_VARS."""

    min_version: Annotated[
        Annotated[
            Optional[MinimumTLSVersionModelDrivenTelemetry],
            PlainValidator(validate_open_enum(False)),
        ],
        pydantic.Field(alias="minVersion"),
    ] = None

    max_version: Annotated[
        Annotated[
            Optional[MaximumTLSVersionModelDrivenTelemetry],
            PlainValidator(validate_open_enum(False)),
        ],
        pydantic.Field(alias="maxVersion"),
    ] = None

    @field_serializer("min_version")
    def serialize_min_version(self, value):
        if isinstance(value, str):
            try:
                return models.MinimumTLSVersionModelDrivenTelemetry(value)
            except ValueError:
                return value
        return value

    @field_serializer("max_version")
    def serialize_max_version(self, value):
        if isinstance(value, str):
            try:
                return models.MaximumTLSVersionModelDrivenTelemetry(value)
            except ValueError:
                return value
        return value


class MetadatumModelDrivenTelemetryTypedDict(TypedDict):
    name: str
    value: str
    r"""JavaScript expression to compute field's value, enclosed in quotes or backticks. (Can evaluate to a constant.)"""


class MetadatumModelDrivenTelemetry(BaseModel):
    name: str

    value: str
    r"""JavaScript expression to compute field's value, enclosed in quotes or backticks. (Can evaluate to a constant.)"""


class InputModelDrivenTelemetryTypedDict(TypedDict):
    type: TypeModelDrivenTelemetry
    id: NotRequired[str]
    r"""Unique ID for this input"""
    disabled: NotRequired[bool]
    pipeline: NotRequired[str]
    r"""Pipeline to process data from this Source before sending it through the Routes"""
    send_to_routes: NotRequired[bool]
    r"""Select whether to send data to Routes, or directly to Destinations."""
    environment: NotRequired[str]
    r"""Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere."""
    pq_enabled: NotRequired[bool]
    r"""Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers)."""
    streamtags: NotRequired[List[str]]
    r"""Tags for filtering and grouping in @{product}"""
    connections: NotRequired[List[ConnectionModelDrivenTelemetryTypedDict]]
    r"""Direct connections to Destinations, and optionally via a Pipeline or a Pack"""
    pq: NotRequired[PqModelDrivenTelemetryTypedDict]
    host: NotRequired[str]
    r"""Address to bind on. Defaults to 0.0.0.0 (all addresses)."""
    port: NotRequired[float]
    r"""Port to listen on"""
    tls: NotRequired[TLSSettingsServerSideModelDrivenTelemetryTypedDict]
    metadata: NotRequired[List[MetadatumModelDrivenTelemetryTypedDict]]
    r"""Fields to add to events from this input"""
    max_active_cxn: NotRequired[float]
    r"""Maximum number of active connections allowed per Worker Process. Use 0 for unlimited."""
    shutdown_timeout_ms: NotRequired[float]
    r"""Time in milliseconds to allow the server to shutdown gracefully before forcing shutdown. Defaults to 5000."""
    description: NotRequired[str]


class InputModelDrivenTelemetry(BaseModel):
    model_config = ConfigDict(
        populate_by_name=True, arbitrary_types_allowed=True, extra="allow"
    )
    __pydantic_extra__: Dict[str, Any] = pydantic.Field(init=False)

    type: TypeModelDrivenTelemetry

    id: Optional[str] = None
    r"""Unique ID for this input"""

    disabled: Optional[bool] = False

    pipeline: Optional[str] = None
    r"""Pipeline to process data from this Source before sending it through the Routes"""

    send_to_routes: Annotated[Optional[bool], pydantic.Field(alias="sendToRoutes")] = (
        True
    )
    r"""Select whether to send data to Routes, or directly to Destinations."""

    environment: Optional[str] = None
    r"""Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere."""

    pq_enabled: Annotated[Optional[bool], pydantic.Field(alias="pqEnabled")] = False
    r"""Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers)."""

    streamtags: Optional[List[str]] = None
    r"""Tags for filtering and grouping in @{product}"""

    connections: Optional[List[ConnectionModelDrivenTelemetry]] = None
    r"""Direct connections to Destinations, and optionally via a Pipeline or a Pack"""

    pq: Optional[PqModelDrivenTelemetry] = None

    host: Optional[str] = "0.0.0.0"
    r"""Address to bind on. Defaults to 0.0.0.0 (all addresses)."""

    port: Optional[float] = 57000
    r"""Port to listen on"""

    tls: Optional[TLSSettingsServerSideModelDrivenTelemetry] = None

    metadata: Optional[List[MetadatumModelDrivenTelemetry]] = None
    r"""Fields to add to events from this input"""

    max_active_cxn: Annotated[Optional[float], pydantic.Field(alias="maxActiveCxn")] = (
        1000
    )
    r"""Maximum number of active connections allowed per Worker Process. Use 0 for unlimited."""

    shutdown_timeout_ms: Annotated[
        Optional[float], pydantic.Field(alias="shutdownTimeoutMs")
    ] = 5000
    r"""Time in milliseconds to allow the server to shutdown gracefully before forcing shutdown. Defaults to 5000."""

    description: Optional[str] = None

    @property
    def additional_properties(self):
        return self.__pydantic_extra__

    @additional_properties.setter
    def additional_properties(self, value):
        self.__pydantic_extra__ = value  # pyright: ignore[reportIncompatibleVariableOverride]


class InputTypeOpenTelemetry(str, Enum):
    OPEN_TELEMETRY = "open_telemetry"


class ConnectionOpenTelemetryTypedDict(TypedDict):
    output: str
    pipeline: NotRequired[str]


class ConnectionOpenTelemetry(BaseModel):
    output: str

    pipeline: Optional[str] = None


class PqModeOpenTelemetry(str, Enum, metaclass=utils.OpenEnumMeta):
    r"""With Smart mode, PQ will write events to the filesystem only when it detects backpressure from the processing engine. With Always On mode, PQ will always write events directly to the queue before forwarding them to the processing engine."""

    # Smart
    SMART = "smart"
    # Always On
    ALWAYS = "always"


class PqCompressionOpenTelemetry(str, Enum, metaclass=utils.OpenEnumMeta):
    r"""Codec to use to compress the persisted data"""

    # None
    NONE = "none"
    # Gzip
    GZIP = "gzip"


class InputPqControlsOpenTelemetryTypedDict(TypedDict):
    pass


class InputPqControlsOpenTelemetry(BaseModel):
    pass


class PqOpenTelemetryTypedDict(TypedDict):
    mode: NotRequired[PqModeOpenTelemetry]
    r"""With Smart mode, PQ will write events to the filesystem only when it detects backpressure from the processing engine. With Always On mode, PQ will always write events directly to the queue before forwarding them to the processing engine."""
    max_buffer_size: NotRequired[float]
    r"""The maximum number of events to hold in memory before writing the events to disk"""
    commit_frequency: NotRequired[float]
    r"""The number of events to send downstream before committing that Stream has read them"""
    max_file_size: NotRequired[str]
    r"""The maximum size to store in each queue file before closing and optionally compressing. Enter a numeral with units of KB, MB, etc."""
    max_size: NotRequired[str]
    r"""The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc."""
    path: NotRequired[str]
    r"""The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/inputs/<input-id>"""
    compress: NotRequired[PqCompressionOpenTelemetry]
    r"""Codec to use to compress the persisted data"""
    pq_controls: NotRequired[InputPqControlsOpenTelemetryTypedDict]


class PqOpenTelemetry(BaseModel):
    mode: Annotated[
        Optional[PqModeOpenTelemetry], PlainValidator(validate_open_enum(False))
    ] = PqModeOpenTelemetry.ALWAYS
    r"""With Smart mode, PQ will write events to the filesystem only when it detects backpressure from the processing engine. With Always On mode, PQ will always write events directly to the queue before forwarding them to the processing engine."""

    max_buffer_size: Annotated[
        Optional[float], pydantic.Field(alias="maxBufferSize")
    ] = 1000
    r"""The maximum number of events to hold in memory before writing the events to disk"""

    commit_frequency: Annotated[
        Optional[float], pydantic.Field(alias="commitFrequency")
    ] = 42
    r"""The number of events to send downstream before committing that Stream has read them"""

    max_file_size: Annotated[Optional[str], pydantic.Field(alias="maxFileSize")] = (
        "1 MB"
    )
    r"""The maximum size to store in each queue file before closing and optionally compressing. Enter a numeral with units of KB, MB, etc."""

    max_size: Annotated[Optional[str], pydantic.Field(alias="maxSize")] = "5GB"
    r"""The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc."""

    path: Optional[str] = "$CRIBL_HOME/state/queues"
    r"""The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/inputs/<input-id>"""

    compress: Annotated[
        Optional[PqCompressionOpenTelemetry], PlainValidator(validate_open_enum(False))
    ] = PqCompressionOpenTelemetry.NONE
    r"""Codec to use to compress the persisted data"""

    pq_controls: Annotated[
        Optional[InputPqControlsOpenTelemetry], pydantic.Field(alias="pqControls")
    ] = None

    @field_serializer("mode")
    def serialize_mode(self, value):
        if isinstance(value, str):
            try:
                return models.PqModeOpenTelemetry(value)
            except ValueError:
                return value
        return value

    @field_serializer("compress")
    def serialize_compress(self, value):
        if isinstance(value, str):
            try:
                return models.PqCompressionOpenTelemetry(value)
            except ValueError:
                return value
        return value


class InputMinimumTLSVersionOpenTelemetry(str, Enum, metaclass=utils.OpenEnumMeta):
    TL_SV1 = "TLSv1"
    TL_SV1_1 = "TLSv1.1"
    TL_SV1_2 = "TLSv1.2"
    TL_SV1_3 = "TLSv1.3"


class InputMaximumTLSVersionOpenTelemetry(str, Enum, metaclass=utils.OpenEnumMeta):
    TL_SV1 = "TLSv1"
    TL_SV1_1 = "TLSv1.1"
    TL_SV1_2 = "TLSv1.2"
    TL_SV1_3 = "TLSv1.3"


class TLSSettingsServerSideOpenTelemetryTypedDict(TypedDict):
    disabled: NotRequired[bool]
    request_cert: NotRequired[bool]
    r"""Require clients to present their certificates. Used to perform client authentication using SSL certs."""
    reject_unauthorized: NotRequired[bool]
    r"""Reject certificates not authorized by a CA in the CA certificate path or by another trusted CA (such as the system's)"""
    common_name_regex: NotRequired[str]
    r"""Regex matching allowable common names in peer certificates' subject attribute"""
    certificate_name: NotRequired[str]
    r"""The name of the predefined certificate"""
    priv_key_path: NotRequired[str]
    r"""Path on server containing the private key to use. PEM format. Can reference $ENV_VARS."""
    passphrase: NotRequired[str]
    r"""Passphrase to use to decrypt private key"""
    cert_path: NotRequired[str]
    r"""Path on server containing certificates to use. PEM format. Can reference $ENV_VARS."""
    ca_path: NotRequired[str]
    r"""Path on server containing CA certificates to use. PEM format. Can reference $ENV_VARS."""
    min_version: NotRequired[InputMinimumTLSVersionOpenTelemetry]
    max_version: NotRequired[InputMaximumTLSVersionOpenTelemetry]


class TLSSettingsServerSideOpenTelemetry(BaseModel):
    disabled: Optional[bool] = True

    request_cert: Annotated[Optional[bool], pydantic.Field(alias="requestCert")] = False
    r"""Require clients to present their certificates. Used to perform client authentication using SSL certs."""

    reject_unauthorized: Annotated[
        Optional[bool], pydantic.Field(alias="rejectUnauthorized")
    ] = True
    r"""Reject certificates not authorized by a CA in the CA certificate path or by another trusted CA (such as the system's)"""

    common_name_regex: Annotated[
        Optional[str], pydantic.Field(alias="commonNameRegex")
    ] = "/.*/"
    r"""Regex matching allowable common names in peer certificates' subject attribute"""

    certificate_name: Annotated[
        Optional[str], pydantic.Field(alias="certificateName")
    ] = None
    r"""The name of the predefined certificate"""

    priv_key_path: Annotated[Optional[str], pydantic.Field(alias="privKeyPath")] = None
    r"""Path on server containing the private key to use. PEM format. Can reference $ENV_VARS."""

    passphrase: Optional[str] = None
    r"""Passphrase to use to decrypt private key"""

    cert_path: Annotated[Optional[str], pydantic.Field(alias="certPath")] = None
    r"""Path on server containing certificates to use. PEM format. Can reference $ENV_VARS."""

    ca_path: Annotated[Optional[str], pydantic.Field(alias="caPath")] = None
    r"""Path on server containing CA certificates to use. PEM format. Can reference $ENV_VARS."""

    min_version: Annotated[
        Annotated[
            Optional[InputMinimumTLSVersionOpenTelemetry],
            PlainValidator(validate_open_enum(False)),
        ],
        pydantic.Field(alias="minVersion"),
    ] = None

    max_version: Annotated[
        Annotated[
            Optional[InputMaximumTLSVersionOpenTelemetry],
            PlainValidator(validate_open_enum(False)),
        ],
        pydantic.Field(alias="maxVersion"),
    ] = None

    @field_serializer("min_version")
    def serialize_min_version(self, value):
        if isinstance(value, str):
            try:
                return models.InputMinimumTLSVersionOpenTelemetry(value)
            except ValueError:
                return value
        return value

    @field_serializer("max_version")
    def serialize_max_version(self, value):
        if isinstance(value, str):
            try:
                return models.InputMaximumTLSVersionOpenTelemetry(value)
            except ValueError:
                return value
        return value


class InputProtocolOpenTelemetry(str, Enum, metaclass=utils.OpenEnumMeta):
    r"""Select whether to leverage gRPC or HTTP for OpenTelemetry"""

    # gRPC
    GRPC = "grpc"
    # HTTP
    HTTP = "http"


class InputOTLPVersion(str, Enum, metaclass=utils.OpenEnumMeta):
    r"""The version of OTLP Protobuf definitions to use when interpreting received data"""

    # 0.10.0
    ZERO_DOT_10_DOT_0 = "0.10.0"
    # 1.3.1
    ONE_DOT_3_DOT_1 = "1.3.1"


class InputAuthenticationTypeOpenTelemetry(str, Enum, metaclass=utils.OpenEnumMeta):
    r"""OpenTelemetry authentication type"""

    NONE = "none"
    BASIC = "basic"
    CREDENTIALS_SECRET = "credentialsSecret"
    TOKEN = "token"
    TEXT_SECRET = "textSecret"
    OAUTH = "oauth"


class InputMetadatumOpenTelemetryTypedDict(TypedDict):
    name: str
    value: str
    r"""JavaScript expression to compute field's value, enclosed in quotes or backticks. (Can evaluate to a constant.)"""


class InputMetadatumOpenTelemetry(BaseModel):
    name: str

    value: str
    r"""JavaScript expression to compute field's value, enclosed in quotes or backticks. (Can evaluate to a constant.)"""


class InputOauthParamOpenTelemetryTypedDict(TypedDict):
    name: str
    r"""OAuth parameter name"""
    value: str
    r"""OAuth parameter value"""


class InputOauthParamOpenTelemetry(BaseModel):
    name: str
    r"""OAuth parameter name"""

    value: str
    r"""OAuth parameter value"""


class InputOauthHeaderOpenTelemetryTypedDict(TypedDict):
    name: str
    r"""OAuth header name"""
    value: str
    r"""OAuth header value"""


class InputOauthHeaderOpenTelemetry(BaseModel):
    name: str
    r"""OAuth header name"""

    value: str
    r"""OAuth header value"""


class InputOpenTelemetryTypedDict(TypedDict):
    type: InputTypeOpenTelemetry
    id: NotRequired[str]
    r"""Unique ID for this input"""
    disabled: NotRequired[bool]
    pipeline: NotRequired[str]
    r"""Pipeline to process data from this Source before sending it through the Routes"""
    send_to_routes: NotRequired[bool]
    r"""Select whether to send data to Routes, or directly to Destinations."""
    environment: NotRequired[str]
    r"""Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere."""
    pq_enabled: NotRequired[bool]
    r"""Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers)."""
    streamtags: NotRequired[List[str]]
    r"""Tags for filtering and grouping in @{product}"""
    connections: NotRequired[List[ConnectionOpenTelemetryTypedDict]]
    r"""Direct connections to Destinations, and optionally via a Pipeline or a Pack"""
    pq: NotRequired[PqOpenTelemetryTypedDict]
    host: NotRequired[str]
    r"""Address to bind on. Defaults to 0.0.0.0 (all addresses)."""
    port: NotRequired[float]
    r"""Port to listen on"""
    tls: NotRequired[TLSSettingsServerSideOpenTelemetryTypedDict]
    max_active_req: NotRequired[float]
    r"""Maximum number of active requests allowed per Worker Process. Set to 0 for unlimited. Caution: Increasing the limit above the default value, or setting it to unlimited, may degrade performance and reduce throughput."""
    max_requests_per_socket: NotRequired[int]
    r"""Maximum number of requests per socket before @{product} instructs the client to close the connection. Default is 0 (unlimited)."""
    enable_proxy_header: NotRequired[Any]
    capture_headers: NotRequired[Any]
    activity_log_sample_rate: NotRequired[Any]
    request_timeout: NotRequired[float]
    r"""How long to wait for an incoming request to complete before aborting it. Use 0 to disable."""
    socket_timeout: NotRequired[float]
    r"""How long @{product} should wait before assuming that an inactive socket has timed out. To wait forever, set to 0."""
    keep_alive_timeout: NotRequired[float]
    r"""After the last response is sent, @{product} will wait this long for additional data before closing the socket connection. Minimum 1 sec.; maximum 600 sec. (10 min.)."""
    enable_health_check: NotRequired[bool]
    r"""Enable to expose the /cribl_health endpoint, which returns 200 OK when this Source is healthy"""
    ip_allowlist_regex: NotRequired[str]
    r"""Messages from matched IP addresses will be processed, unless also matched by the denylist."""
    ip_denylist_regex: NotRequired[str]
    r"""Messages from matched IP addresses will be ignored. This takes precedence over the allowlist."""
    protocol: NotRequired[InputProtocolOpenTelemetry]
    r"""Select whether to leverage gRPC or HTTP for OpenTelemetry"""
    extract_spans: NotRequired[bool]
    r"""Enable to extract each incoming span to a separate event"""
    extract_metrics: NotRequired[bool]
    r"""Enable to extract each incoming Gauge or IntGauge metric to multiple events, one per data point"""
    otlp_version: NotRequired[InputOTLPVersion]
    r"""The version of OTLP Protobuf definitions to use when interpreting received data"""
    auth_type: NotRequired[InputAuthenticationTypeOpenTelemetry]
    r"""OpenTelemetry authentication type"""
    metadata: NotRequired[List[InputMetadatumOpenTelemetryTypedDict]]
    r"""Fields to add to events from this input"""
    max_active_cxn: NotRequired[float]
    r"""Maximum number of active connections allowed per Worker Process. Use 0 for unlimited."""
    description: NotRequired[str]
    username: NotRequired[str]
    password: NotRequired[str]
    token: NotRequired[str]
    r"""Bearer token to include in the authorization header"""
    credentials_secret: NotRequired[str]
    r"""Select or create a secret that references your credentials"""
    text_secret: NotRequired[str]
    r"""Select or create a stored text secret"""
    login_url: NotRequired[str]
    r"""URL for OAuth"""
    secret_param_name: NotRequired[str]
    r"""Secret parameter name to pass in request body"""
    secret: NotRequired[str]
    r"""Secret parameter value to pass in request body"""
    token_attribute_name: NotRequired[str]
    r"""Name of the auth token attribute in the OAuth response. Can be top-level (e.g., 'token'); or nested, using a period (e.g., 'data.token')."""
    auth_header_expr: NotRequired[str]
    r"""JavaScript expression to compute the Authorization header value to pass in requests. The value `${token}` is used to reference the token obtained from authentication, e.g.: `Bearer ${token}`."""
    token_timeout_secs: NotRequired[float]
    r"""How often the OAuth token should be refreshed."""
    oauth_params: NotRequired[List[InputOauthParamOpenTelemetryTypedDict]]
    r"""Additional parameters to send in the OAuth login request. @{product} will combine the secret with these parameters, and will send the URL-encoded result in a POST request to the endpoint specified in the 'Login URL'. We'll automatically add the content-type header 'application/x-www-form-urlencoded' when sending this request."""
    oauth_headers: NotRequired[List[InputOauthHeaderOpenTelemetryTypedDict]]
    r"""Additional headers to send in the OAuth login request. @{product} will automatically add the content-type header 'application/x-www-form-urlencoded' when sending this request."""
    extract_logs: NotRequired[bool]
    r"""Enable to extract each incoming log record to a separate event"""


class InputOpenTelemetry(BaseModel):
    model_config = ConfigDict(
        populate_by_name=True, arbitrary_types_allowed=True, extra="allow"
    )
    __pydantic_extra__: Dict[str, Any] = pydantic.Field(init=False)

    type: InputTypeOpenTelemetry

    id: Optional[str] = None
    r"""Unique ID for this input"""

    disabled: Optional[bool] = False

    pipeline: Optional[str] = None
    r"""Pipeline to process data from this Source before sending it through the Routes"""

    send_to_routes: Annotated[Optional[bool], pydantic.Field(alias="sendToRoutes")] = (
        True
    )
    r"""Select whether to send data to Routes, or directly to Destinations."""

    environment: Optional[str] = None
    r"""Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere."""

    pq_enabled: Annotated[Optional[bool], pydantic.Field(alias="pqEnabled")] = False
    r"""Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers)."""

    streamtags: Optional[List[str]] = None
    r"""Tags for filtering and grouping in @{product}"""

    connections: Optional[List[ConnectionOpenTelemetry]] = None
    r"""Direct connections to Destinations, and optionally via a Pipeline or a Pack"""

    pq: Optional[PqOpenTelemetry] = None

    host: Optional[str] = "0.0.0.0"
    r"""Address to bind on. Defaults to 0.0.0.0 (all addresses)."""

    port: Optional[float] = 4317
    r"""Port to listen on"""

    tls: Optional[TLSSettingsServerSideOpenTelemetry] = None

    max_active_req: Annotated[Optional[float], pydantic.Field(alias="maxActiveReq")] = (
        256
    )
    r"""Maximum number of active requests allowed per Worker Process. Set to 0 for unlimited. Caution: Increasing the limit above the default value, or setting it to unlimited, may degrade performance and reduce throughput."""

    max_requests_per_socket: Annotated[
        Optional[int], pydantic.Field(alias="maxRequestsPerSocket")
    ] = 0
    r"""Maximum number of requests per socket before @{product} instructs the client to close the connection. Default is 0 (unlimited)."""

    enable_proxy_header: Annotated[
        Optional[Any], pydantic.Field(alias="enableProxyHeader")
    ] = None

    capture_headers: Annotated[
        Optional[Any], pydantic.Field(alias="captureHeaders")
    ] = None

    activity_log_sample_rate: Annotated[
        Optional[Any], pydantic.Field(alias="activityLogSampleRate")
    ] = None

    request_timeout: Annotated[
        Optional[float], pydantic.Field(alias="requestTimeout")
    ] = 0
    r"""How long to wait for an incoming request to complete before aborting it. Use 0 to disable."""

    socket_timeout: Annotated[
        Optional[float], pydantic.Field(alias="socketTimeout")
    ] = 0
    r"""How long @{product} should wait before assuming that an inactive socket has timed out. To wait forever, set to 0."""

    keep_alive_timeout: Annotated[
        Optional[float], pydantic.Field(alias="keepAliveTimeout")
    ] = 15
    r"""After the last response is sent, @{product} will wait this long for additional data before closing the socket connection. Minimum 1 sec.; maximum 600 sec. (10 min.)."""

    enable_health_check: Annotated[
        Optional[bool], pydantic.Field(alias="enableHealthCheck")
    ] = False
    r"""Enable to expose the /cribl_health endpoint, which returns 200 OK when this Source is healthy"""

    ip_allowlist_regex: Annotated[
        Optional[str], pydantic.Field(alias="ipAllowlistRegex")
    ] = "/.*/"
    r"""Messages from matched IP addresses will be processed, unless also matched by the denylist."""

    ip_denylist_regex: Annotated[
        Optional[str], pydantic.Field(alias="ipDenylistRegex")
    ] = "/^$/"
    r"""Messages from matched IP addresses will be ignored. This takes precedence over the allowlist."""

    protocol: Annotated[
        Optional[InputProtocolOpenTelemetry], PlainValidator(validate_open_enum(False))
    ] = InputProtocolOpenTelemetry.GRPC
    r"""Select whether to leverage gRPC or HTTP for OpenTelemetry"""

    extract_spans: Annotated[Optional[bool], pydantic.Field(alias="extractSpans")] = (
        False
    )
    r"""Enable to extract each incoming span to a separate event"""

    extract_metrics: Annotated[
        Optional[bool], pydantic.Field(alias="extractMetrics")
    ] = False
    r"""Enable to extract each incoming Gauge or IntGauge metric to multiple events, one per data point"""

    otlp_version: Annotated[
        Annotated[
            Optional[InputOTLPVersion], PlainValidator(validate_open_enum(False))
        ],
        pydantic.Field(alias="otlpVersion"),
    ] = InputOTLPVersion.ZERO_DOT_10_DOT_0
    r"""The version of OTLP Protobuf definitions to use when interpreting received data"""

    auth_type: Annotated[
        Annotated[
            Optional[InputAuthenticationTypeOpenTelemetry],
            PlainValidator(validate_open_enum(False)),
        ],
        pydantic.Field(alias="authType"),
    ] = InputAuthenticationTypeOpenTelemetry.NONE
    r"""OpenTelemetry authentication type"""

    metadata: Optional[List[InputMetadatumOpenTelemetry]] = None
    r"""Fields to add to events from this input"""

    max_active_cxn: Annotated[Optional[float], pydantic.Field(alias="maxActiveCxn")] = (
        1000
    )
    r"""Maximum number of active connections allowed per Worker Process. Use 0 for unlimited."""

    description: Optional[str] = None

    username: Optional[str] = None

    password: Optional[str] = None

    token: Optional[str] = None
    r"""Bearer token to include in the authorization header"""

    credentials_secret: Annotated[
        Optional[str], pydantic.Field(alias="credentialsSecret")
    ] = None
    r"""Select or create a secret that references your credentials"""

    text_secret: Annotated[Optional[str], pydantic.Field(alias="textSecret")] = None
    r"""Select or create a stored text secret"""

    login_url: Annotated[Optional[str], pydantic.Field(alias="loginUrl")] = None
    r"""URL for OAuth"""

    secret_param_name: Annotated[
        Optional[str], pydantic.Field(alias="secretParamName")
    ] = None
    r"""Secret parameter name to pass in request body"""

    secret: Optional[str] = None
    r"""Secret parameter value to pass in request body"""

    token_attribute_name: Annotated[
        Optional[str], pydantic.Field(alias="tokenAttributeName")
    ] = None
    r"""Name of the auth token attribute in the OAuth response. Can be top-level (e.g., 'token'); or nested, using a period (e.g., 'data.token')."""

    auth_header_expr: Annotated[
        Optional[str], pydantic.Field(alias="authHeaderExpr")
    ] = "`Bearer ${token}`"
    r"""JavaScript expression to compute the Authorization header value to pass in requests. The value `${token}` is used to reference the token obtained from authentication, e.g.: `Bearer ${token}`."""

    token_timeout_secs: Annotated[
        Optional[float], pydantic.Field(alias="tokenTimeoutSecs")
    ] = 3600
    r"""How often the OAuth token should be refreshed."""

    oauth_params: Annotated[
        Optional[List[InputOauthParamOpenTelemetry]],
        pydantic.Field(alias="oauthParams"),
    ] = None
    r"""Additional parameters to send in the OAuth login request. @{product} will combine the secret with these parameters, and will send the URL-encoded result in a POST request to the endpoint specified in the 'Login URL'. We'll automatically add the content-type header 'application/x-www-form-urlencoded' when sending this request."""

    oauth_headers: Annotated[
        Optional[List[InputOauthHeaderOpenTelemetry]],
        pydantic.Field(alias="oauthHeaders"),
    ] = None
    r"""Additional headers to send in the OAuth login request. @{product} will automatically add the content-type header 'application/x-www-form-urlencoded' when sending this request."""

    extract_logs: Annotated[Optional[bool], pydantic.Field(alias="extractLogs")] = False
    r"""Enable to extract each incoming log record to a separate event"""

    @property
    def additional_properties(self):
        return self.__pydantic_extra__

    @additional_properties.setter
    def additional_properties(self, value):
        self.__pydantic_extra__ = value  # pyright: ignore[reportIncompatibleVariableOverride]

    @field_serializer("protocol")
    def serialize_protocol(self, value):
        if isinstance(value, str):
            try:
                return models.InputProtocolOpenTelemetry(value)
            except ValueError:
                return value
        return value

    @field_serializer("otlp_version")
    def serialize_otlp_version(self, value):
        if isinstance(value, str):
            try:
                return models.InputOTLPVersion(value)
            except ValueError:
                return value
        return value

    @field_serializer("auth_type")
    def serialize_auth_type(self, value):
        if isinstance(value, str):
            try:
                return models.InputAuthenticationTypeOpenTelemetry(value)
            except ValueError:
                return value
        return value


class InputTypeSnmp(str, Enum):
    SNMP = "snmp"


class ConnectionSnmpTypedDict(TypedDict):
    output: str
    pipeline: NotRequired[str]


class ConnectionSnmp(BaseModel):
    output: str

    pipeline: Optional[str] = None


class ModeSnmp(str, Enum, metaclass=utils.OpenEnumMeta):
    r"""With Smart mode, PQ will write events to the filesystem only when it detects backpressure from the processing engine. With Always On mode, PQ will always write events directly to the queue before forwarding them to the processing engine."""

    # Smart
    SMART = "smart"
    # Always On
    ALWAYS = "always"


class CompressionSnmp(str, Enum, metaclass=utils.OpenEnumMeta):
    r"""Codec to use to compress the persisted data"""

    # None
    NONE = "none"
    # Gzip
    GZIP = "gzip"


class PqControlsSnmpTypedDict(TypedDict):
    pass


class PqControlsSnmp(BaseModel):
    pass


class PqSnmpTypedDict(TypedDict):
    mode: NotRequired[ModeSnmp]
    r"""With Smart mode, PQ will write events to the filesystem only when it detects backpressure from the processing engine. With Always On mode, PQ will always write events directly to the queue before forwarding them to the processing engine."""
    max_buffer_size: NotRequired[float]
    r"""The maximum number of events to hold in memory before writing the events to disk"""
    commit_frequency: NotRequired[float]
    r"""The number of events to send downstream before committing that Stream has read them"""
    max_file_size: NotRequired[str]
    r"""The maximum size to store in each queue file before closing and optionally compressing. Enter a numeral with units of KB, MB, etc."""
    max_size: NotRequired[str]
    r"""The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc."""
    path: NotRequired[str]
    r"""The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/inputs/<input-id>"""
    compress: NotRequired[CompressionSnmp]
    r"""Codec to use to compress the persisted data"""
    pq_controls: NotRequired[PqControlsSnmpTypedDict]


class PqSnmp(BaseModel):
    mode: Annotated[Optional[ModeSnmp], PlainValidator(validate_open_enum(False))] = (
        ModeSnmp.ALWAYS
    )
    r"""With Smart mode, PQ will write events to the filesystem only when it detects backpressure from the processing engine. With Always On mode, PQ will always write events directly to the queue before forwarding them to the processing engine."""

    max_buffer_size: Annotated[
        Optional[float], pydantic.Field(alias="maxBufferSize")
    ] = 1000
    r"""The maximum number of events to hold in memory before writing the events to disk"""

    commit_frequency: Annotated[
        Optional[float], pydantic.Field(alias="commitFrequency")
    ] = 42
    r"""The number of events to send downstream before committing that Stream has read them"""

    max_file_size: Annotated[Optional[str], pydantic.Field(alias="maxFileSize")] = (
        "1 MB"
    )
    r"""The maximum size to store in each queue file before closing and optionally compressing. Enter a numeral with units of KB, MB, etc."""

    max_size: Annotated[Optional[str], pydantic.Field(alias="maxSize")] = "5GB"
    r"""The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc."""

    path: Optional[str] = "$CRIBL_HOME/state/queues"
    r"""The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/inputs/<input-id>"""

    compress: Annotated[
        Optional[CompressionSnmp], PlainValidator(validate_open_enum(False))
    ] = CompressionSnmp.NONE
    r"""Codec to use to compress the persisted data"""

    pq_controls: Annotated[
        Optional[PqControlsSnmp], pydantic.Field(alias="pqControls")
    ] = None

    @field_serializer("mode")
    def serialize_mode(self, value):
        if isinstance(value, str):
            try:
                return models.ModeSnmp(value)
            except ValueError:
                return value
        return value

    @field_serializer("compress")
    def serialize_compress(self, value):
        if isinstance(value, str):
            try:
                return models.CompressionSnmp(value)
            except ValueError:
                return value
        return value


class AuthenticationProtocol(str, Enum, metaclass=utils.OpenEnumMeta):
    # None
    NONE = "none"
    # MD5
    MD5 = "md5"
    # SHA1
    SHA = "sha"
    # SHA224
    SHA224 = "sha224"
    # SHA256
    SHA256 = "sha256"
    # SHA384
    SHA384 = "sha384"
    # SHA512
    SHA512 = "sha512"


class V3UserTypedDict(TypedDict):
    name: str
    auth_protocol: NotRequired[AuthenticationProtocol]
    auth_key: NotRequired[Any]
    priv_protocol: NotRequired[str]


class V3User(BaseModel):
    name: str

    auth_protocol: Annotated[
        Annotated[
            Optional[AuthenticationProtocol], PlainValidator(validate_open_enum(False))
        ],
        pydantic.Field(alias="authProtocol"),
    ] = AuthenticationProtocol.NONE

    auth_key: Annotated[Optional[Any], pydantic.Field(alias="authKey")] = None

    priv_protocol: Annotated[Optional[str], pydantic.Field(alias="privProtocol")] = (
        "none"
    )

    @field_serializer("auth_protocol")
    def serialize_auth_protocol(self, value):
        if isinstance(value, str):
            try:
                return models.AuthenticationProtocol(value)
            except ValueError:
                return value
        return value


class SNMPv3AuthenticationTypedDict(TypedDict):
    r"""Authentication parameters for SNMPv3 trap. Set the log level to debug if you are experiencing authentication or decryption issues."""

    v3_auth_enabled: NotRequired[bool]
    allow_unmatched_trap: NotRequired[bool]
    r"""Pass through traps that don't match any of the configured users. @{product} will not attempt to decrypt these traps."""
    v3_users: NotRequired[List[V3UserTypedDict]]
    r"""User credentials for receiving v3 traps"""


class SNMPv3Authentication(BaseModel):
    r"""Authentication parameters for SNMPv3 trap. Set the log level to debug if you are experiencing authentication or decryption issues."""

    v3_auth_enabled: Annotated[
        Optional[bool], pydantic.Field(alias="v3AuthEnabled")
    ] = False

    allow_unmatched_trap: Annotated[
        Optional[bool], pydantic.Field(alias="allowUnmatchedTrap")
    ] = False
    r"""Pass through traps that don't match any of the configured users. @{product} will not attempt to decrypt these traps."""

    v3_users: Annotated[Optional[List[V3User]], pydantic.Field(alias="v3Users")] = None
    r"""User credentials for receiving v3 traps"""


class MetadatumSnmpTypedDict(TypedDict):
    name: str
    value: str
    r"""JavaScript expression to compute field's value, enclosed in quotes or backticks. (Can evaluate to a constant.)"""


class MetadatumSnmp(BaseModel):
    name: str

    value: str
    r"""JavaScript expression to compute field's value, enclosed in quotes or backticks. (Can evaluate to a constant.)"""


class InputSnmpTypedDict(TypedDict):
    type: InputTypeSnmp
    id: NotRequired[str]
    r"""Unique ID for this input"""
    disabled: NotRequired[bool]
    pipeline: NotRequired[str]
    r"""Pipeline to process data from this Source before sending it through the Routes"""
    send_to_routes: NotRequired[bool]
    r"""Select whether to send data to Routes, or directly to Destinations."""
    environment: NotRequired[str]
    r"""Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere."""
    pq_enabled: NotRequired[bool]
    r"""Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers)."""
    streamtags: NotRequired[List[str]]
    r"""Tags for filtering and grouping in @{product}"""
    connections: NotRequired[List[ConnectionSnmpTypedDict]]
    r"""Direct connections to Destinations, and optionally via a Pipeline or a Pack"""
    pq: NotRequired[PqSnmpTypedDict]
    host: NotRequired[str]
    r"""Address to bind on. For IPv4 (all addresses), use the default '0.0.0.0'. For IPv6, enter '::' (all addresses) or specify an IP address."""
    port: NotRequired[float]
    r"""UDP port to receive SNMP traps on. Defaults to 162."""
    snmp_v3_auth: NotRequired[SNMPv3AuthenticationTypedDict]
    r"""Authentication parameters for SNMPv3 trap. Set the log level to debug if you are experiencing authentication or decryption issues."""
    max_buffer_size: NotRequired[float]
    r"""Maximum number of events to buffer when downstream is blocking."""
    ip_whitelist_regex: NotRequired[str]
    r"""Regex matching IP addresses that are allowed to send data"""
    metadata: NotRequired[List[MetadatumSnmpTypedDict]]
    r"""Fields to add to events from this input"""
    udp_socket_rx_buf_size: NotRequired[float]
    r"""Optionally, set the SO_RCVBUF socket option for the UDP socket. This value tells the operating system how many bytes can be buffered in the kernel before events are dropped. Leave blank to use the OS default. Caution: Increasing this value will affect OS memory utilization."""
    varbinds_with_types: NotRequired[bool]
    r"""If enabled, parses varbinds as an array of objects that include OID, value, and type"""
    best_effort_parsing: NotRequired[bool]
    r"""If enabled, the parser will attempt to parse varbind octet strings as UTF-8, first, otherwise will fallback to other methods"""
    description: NotRequired[str]


class InputSnmp(BaseModel):
    model_config = ConfigDict(
        populate_by_name=True, arbitrary_types_allowed=True, extra="allow"
    )
    __pydantic_extra__: Dict[str, Any] = pydantic.Field(init=False)

    type: InputTypeSnmp

    id: Optional[str] = None
    r"""Unique ID for this input"""

    disabled: Optional[bool] = False

    pipeline: Optional[str] = None
    r"""Pipeline to process data from this Source before sending it through the Routes"""

    send_to_routes: Annotated[Optional[bool], pydantic.Field(alias="sendToRoutes")] = (
        True
    )
    r"""Select whether to send data to Routes, or directly to Destinations."""

    environment: Optional[str] = None
    r"""Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere."""

    pq_enabled: Annotated[Optional[bool], pydantic.Field(alias="pqEnabled")] = False
    r"""Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers)."""

    streamtags: Optional[List[str]] = None
    r"""Tags for filtering and grouping in @{product}"""

    connections: Optional[List[ConnectionSnmp]] = None
    r"""Direct connections to Destinations, and optionally via a Pipeline or a Pack"""

    pq: Optional[PqSnmp] = None

    host: Optional[str] = "0.0.0.0"
    r"""Address to bind on. For IPv4 (all addresses), use the default '0.0.0.0'. For IPv6, enter '::' (all addresses) or specify an IP address."""

    port: Optional[float] = 162
    r"""UDP port to receive SNMP traps on. Defaults to 162."""

    snmp_v3_auth: Annotated[
        Optional[SNMPv3Authentication], pydantic.Field(alias="snmpV3Auth")
    ] = None
    r"""Authentication parameters for SNMPv3 trap. Set the log level to debug if you are experiencing authentication or decryption issues."""

    max_buffer_size: Annotated[
        Optional[float], pydantic.Field(alias="maxBufferSize")
    ] = 1000
    r"""Maximum number of events to buffer when downstream is blocking."""

    ip_whitelist_regex: Annotated[
        Optional[str], pydantic.Field(alias="ipWhitelistRegex")
    ] = "/.*/"
    r"""Regex matching IP addresses that are allowed to send data"""

    metadata: Optional[List[MetadatumSnmp]] = None
    r"""Fields to add to events from this input"""

    udp_socket_rx_buf_size: Annotated[
        Optional[float], pydantic.Field(alias="udpSocketRxBufSize")
    ] = None
    r"""Optionally, set the SO_RCVBUF socket option for the UDP socket. This value tells the operating system how many bytes can be buffered in the kernel before events are dropped. Leave blank to use the OS default. Caution: Increasing this value will affect OS memory utilization."""

    varbinds_with_types: Annotated[
        Optional[bool], pydantic.Field(alias="varbindsWithTypes")
    ] = False
    r"""If enabled, parses varbinds as an array of objects that include OID, value, and type"""

    best_effort_parsing: Annotated[
        Optional[bool], pydantic.Field(alias="bestEffortParsing")
    ] = False
    r"""If enabled, the parser will attempt to parse varbind octet strings as UTF-8, first, otherwise will fallback to other methods"""

    description: Optional[str] = None

    @property
    def additional_properties(self):
        return self.__pydantic_extra__

    @additional_properties.setter
    def additional_properties(self, value):
        self.__pydantic_extra__ = value  # pyright: ignore[reportIncompatibleVariableOverride]


class TypeS3Inventory(str, Enum):
    S3_INVENTORY = "s3_inventory"


class ConnectionS3InventoryTypedDict(TypedDict):
    output: str
    pipeline: NotRequired[str]


class ConnectionS3Inventory(BaseModel):
    output: str

    pipeline: Optional[str] = None


class ModeS3Inventory(str, Enum, metaclass=utils.OpenEnumMeta):
    r"""With Smart mode, PQ will write events to the filesystem only when it detects backpressure from the processing engine. With Always On mode, PQ will always write events directly to the queue before forwarding them to the processing engine."""

    # Smart
    SMART = "smart"
    # Always On
    ALWAYS = "always"


class CompressionS3Inventory(str, Enum, metaclass=utils.OpenEnumMeta):
    r"""Codec to use to compress the persisted data"""

    # None
    NONE = "none"
    # Gzip
    GZIP = "gzip"


class PqControlsS3InventoryTypedDict(TypedDict):
    pass


class PqControlsS3Inventory(BaseModel):
    pass


class PqS3InventoryTypedDict(TypedDict):
    mode: NotRequired[ModeS3Inventory]
    r"""With Smart mode, PQ will write events to the filesystem only when it detects backpressure from the processing engine. With Always On mode, PQ will always write events directly to the queue before forwarding them to the processing engine."""
    max_buffer_size: NotRequired[float]
    r"""The maximum number of events to hold in memory before writing the events to disk"""
    commit_frequency: NotRequired[float]
    r"""The number of events to send downstream before committing that Stream has read them"""
    max_file_size: NotRequired[str]
    r"""The maximum size to store in each queue file before closing and optionally compressing. Enter a numeral with units of KB, MB, etc."""
    max_size: NotRequired[str]
    r"""The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc."""
    path: NotRequired[str]
    r"""The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/inputs/<input-id>"""
    compress: NotRequired[CompressionS3Inventory]
    r"""Codec to use to compress the persisted data"""
    pq_controls: NotRequired[PqControlsS3InventoryTypedDict]


class PqS3Inventory(BaseModel):
    mode: Annotated[
        Optional[ModeS3Inventory], PlainValidator(validate_open_enum(False))
    ] = ModeS3Inventory.ALWAYS
    r"""With Smart mode, PQ will write events to the filesystem only when it detects backpressure from the processing engine. With Always On mode, PQ will always write events directly to the queue before forwarding them to the processing engine."""

    max_buffer_size: Annotated[
        Optional[float], pydantic.Field(alias="maxBufferSize")
    ] = 1000
    r"""The maximum number of events to hold in memory before writing the events to disk"""

    commit_frequency: Annotated[
        Optional[float], pydantic.Field(alias="commitFrequency")
    ] = 42
    r"""The number of events to send downstream before committing that Stream has read them"""

    max_file_size: Annotated[Optional[str], pydantic.Field(alias="maxFileSize")] = (
        "1 MB"
    )
    r"""The maximum size to store in each queue file before closing and optionally compressing. Enter a numeral with units of KB, MB, etc."""

    max_size: Annotated[Optional[str], pydantic.Field(alias="maxSize")] = "5GB"
    r"""The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc."""

    path: Optional[str] = "$CRIBL_HOME/state/queues"
    r"""The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/inputs/<input-id>"""

    compress: Annotated[
        Optional[CompressionS3Inventory], PlainValidator(validate_open_enum(False))
    ] = CompressionS3Inventory.NONE
    r"""Codec to use to compress the persisted data"""

    pq_controls: Annotated[
        Optional[PqControlsS3Inventory], pydantic.Field(alias="pqControls")
    ] = None

    @field_serializer("mode")
    def serialize_mode(self, value):
        if isinstance(value, str):
            try:
                return models.ModeS3Inventory(value)
            except ValueError:
                return value
        return value

    @field_serializer("compress")
    def serialize_compress(self, value):
        if isinstance(value, str):
            try:
                return models.CompressionS3Inventory(value)
            except ValueError:
                return value
        return value


class AuthenticationMethodS3Inventory(str, Enum, metaclass=utils.OpenEnumMeta):
    r"""AWS authentication method. Choose Auto to use IAM roles."""

    # Auto
    AUTO = "auto"
    # Manual
    MANUAL = "manual"
    # Secret Key pair
    SECRET = "secret"


class SignatureVersionS3Inventory(str, Enum, metaclass=utils.OpenEnumMeta):
    r"""Signature version to use for signing S3 requests"""

    V2 = "v2"
    V4 = "v4"


class PreprocessS3InventoryTypedDict(TypedDict):
    disabled: NotRequired[bool]
    command: NotRequired[str]
    r"""Command to feed the data through (via stdin) and process its output (stdout)"""
    args: NotRequired[List[str]]
    r"""Arguments to be added to the custom command"""


class PreprocessS3Inventory(BaseModel):
    disabled: Optional[bool] = True

    command: Optional[str] = None
    r"""Command to feed the data through (via stdin) and process its output (stdout)"""

    args: Optional[List[str]] = None
    r"""Arguments to be added to the custom command"""


class MetadatumS3InventoryTypedDict(TypedDict):
    name: str
    value: str
    r"""JavaScript expression to compute field's value, enclosed in quotes or backticks. (Can evaluate to a constant.)"""


class MetadatumS3Inventory(BaseModel):
    name: str

    value: str
    r"""JavaScript expression to compute field's value, enclosed in quotes or backticks. (Can evaluate to a constant.)"""


class CheckpointingS3InventoryTypedDict(TypedDict):
    enabled: NotRequired[bool]
    r"""Resume processing files after an interruption"""
    retries: NotRequired[float]
    r"""The number of times to retry processing when a processing error occurs. If Skip file on error is enabled, this setting is ignored."""


class CheckpointingS3Inventory(BaseModel):
    enabled: Optional[bool] = False
    r"""Resume processing files after an interruption"""

    retries: Optional[float] = 5
    r"""The number of times to retry processing when a processing error occurs. If Skip file on error is enabled, this setting is ignored."""


class TagAfterProcessingS3Inventory(str, Enum, metaclass=utils.OpenEnumMeta):
    FALSE = "false"
    TRUE = "true"


class InputS3InventoryTypedDict(TypedDict):
    type: TypeS3Inventory
    queue_name: str
    r"""The name, URL, or ARN of the SQS queue to read notifications from. When a non-AWS URL is specified, format must be: '{url}/myQueueName'. Example: 'https://host:port/myQueueName'. Value must be a JavaScript expression (which can evaluate to a constant value), enclosed in quotes or backticks. Can be evaluated only at init time. Example referencing a Global Variable: `https://host:port/myQueue-${C.vars.myVar}`."""
    id: NotRequired[str]
    r"""Unique ID for this input"""
    disabled: NotRequired[bool]
    pipeline: NotRequired[str]
    r"""Pipeline to process data from this Source before sending it through the Routes"""
    send_to_routes: NotRequired[bool]
    r"""Select whether to send data to Routes, or directly to Destinations."""
    environment: NotRequired[str]
    r"""Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere."""
    pq_enabled: NotRequired[bool]
    r"""Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers)."""
    streamtags: NotRequired[List[str]]
    r"""Tags for filtering and grouping in @{product}"""
    connections: NotRequired[List[ConnectionS3InventoryTypedDict]]
    r"""Direct connections to Destinations, and optionally via a Pipeline or a Pack"""
    pq: NotRequired[PqS3InventoryTypedDict]
    file_filter: NotRequired[str]
    r"""Regex matching file names to download and process. Defaults to: .*"""
    aws_account_id: NotRequired[str]
    r"""SQS queue owner's AWS account ID. Leave empty if SQS queue is in same AWS account."""
    aws_authentication_method: NotRequired[AuthenticationMethodS3Inventory]
    r"""AWS authentication method. Choose Auto to use IAM roles."""
    aws_secret_key: NotRequired[str]
    region: NotRequired[str]
    r"""AWS Region where the S3 bucket and SQS queue are located. Required, unless the Queue entry is a URL or ARN that includes a Region."""
    endpoint: NotRequired[str]
    r"""S3 service endpoint. If empty, defaults to the AWS Region-specific endpoint. Otherwise, it must point to S3-compatible endpoint."""
    signature_version: NotRequired[SignatureVersionS3Inventory]
    r"""Signature version to use for signing S3 requests"""
    reuse_connections: NotRequired[bool]
    r"""Reuse connections between requests, which can improve performance"""
    reject_unauthorized: NotRequired[bool]
    r"""Reject certificates that cannot be verified against a valid CA, such as self-signed certificates"""
    breaker_rulesets: NotRequired[List[str]]
    r"""A list of event-breaking rulesets that will be applied, in order, to the input data stream"""
    stale_channel_flush_ms: NotRequired[float]
    r"""How long (in milliseconds) the Event Breaker will wait for new data to be sent to a specific channel before flushing the data stream out, as is, to the Pipelines"""
    max_messages: NotRequired[float]
    r"""The maximum number of messages SQS should return in a poll request. Amazon SQS never returns more messages than this value (however, fewer messages might be returned). Valid values: 1 to 10."""
    visibility_timeout: NotRequired[float]
    r"""After messages are retrieved by a ReceiveMessage request, @{product} will hide them from subsequent retrieve requests for at least this duration. You can set this as high as 43200 sec. (12 hours)."""
    num_receivers: NotRequired[float]
    r"""How many receiver processes to run. The higher the number, the better the throughput - at the expense of CPU overhead."""
    socket_timeout: NotRequired[float]
    r"""Socket inactivity timeout (in seconds). Increase this value if timeouts occur due to backpressure."""
    skip_on_error: NotRequired[bool]
    r"""Skip files that trigger a processing error. Disabled by default, which allows retries after processing errors."""
    include_sqs_metadata: NotRequired[bool]
    r"""Attach SQS notification metadata to a __sqsMetadata field on each event"""
    enable_assume_role: NotRequired[bool]
    r"""Use Assume Role credentials to access Amazon S3"""
    assume_role_arn: NotRequired[str]
    r"""Amazon Resource Name (ARN) of the role to assume"""
    assume_role_external_id: NotRequired[str]
    r"""External ID to use when assuming role"""
    duration_seconds: NotRequired[float]
    r"""Duration of the assumed role's session, in seconds. Minimum is 900 (15 minutes), default is 3600 (1 hour), and maximum is 43200 (12 hours)."""
    enable_sqs_assume_role: NotRequired[bool]
    r"""Use Assume Role credentials when accessing Amazon SQS"""
    preprocess: NotRequired[PreprocessS3InventoryTypedDict]
    metadata: NotRequired[List[MetadatumS3InventoryTypedDict]]
    r"""Fields to add to events from this input"""
    parquet_chunk_size_mb: NotRequired[float]
    r"""Maximum file size for each Parquet chunk"""
    parquet_chunk_download_timeout: NotRequired[float]
    r"""The maximum time allowed for downloading a Parquet chunk. Processing will stop if a chunk cannot be downloaded within the time specified."""
    checkpointing: NotRequired[CheckpointingS3InventoryTypedDict]
    poll_timeout: NotRequired[float]
    r"""How long to wait for events before trying polling again. The lower the number the higher the AWS bill. The higher the number the longer it will take for the source to react to configuration changes and system restarts."""
    checksum_suffix: NotRequired[str]
    r"""Filename suffix of the manifest checksum file. If a filename matching this suffix is received        in the queue, the matching manifest file will be downloaded and validated against its value. Defaults to \"checksum\" """
    max_manifest_size_kb: NotRequired[int]
    r"""Maximum download size (KB) of each manifest or checksum file. Manifest files larger than this size will not be read.        Defaults to 4096."""
    validate_inventory_files: NotRequired[bool]
    r"""If set to Yes, each inventory file in the manifest will be validated against its checksum. Defaults to false"""
    description: NotRequired[str]
    aws_api_key: NotRequired[str]
    aws_secret: NotRequired[str]
    r"""Select or create a stored secret that references your access key and secret key"""
    tag_after_processing: NotRequired[TagAfterProcessingS3Inventory]
    processed_tag_key: NotRequired[str]
    r"""The key for the S3 object tag applied after processing. This field accepts an expression for dynamic generation."""
    processed_tag_value: NotRequired[str]
    r"""The value for the S3 object tag applied after processing. This field accepts an expression for dynamic generation."""


class InputS3Inventory(BaseModel):
    model_config = ConfigDict(
        populate_by_name=True, arbitrary_types_allowed=True, extra="allow"
    )
    __pydantic_extra__: Dict[str, Any] = pydantic.Field(init=False)

    type: TypeS3Inventory

    queue_name: Annotated[str, pydantic.Field(alias="queueName")]
    r"""The name, URL, or ARN of the SQS queue to read notifications from. When a non-AWS URL is specified, format must be: '{url}/myQueueName'. Example: 'https://host:port/myQueueName'. Value must be a JavaScript expression (which can evaluate to a constant value), enclosed in quotes or backticks. Can be evaluated only at init time. Example referencing a Global Variable: `https://host:port/myQueue-${C.vars.myVar}`."""

    id: Optional[str] = None
    r"""Unique ID for this input"""

    disabled: Optional[bool] = False

    pipeline: Optional[str] = None
    r"""Pipeline to process data from this Source before sending it through the Routes"""

    send_to_routes: Annotated[Optional[bool], pydantic.Field(alias="sendToRoutes")] = (
        True
    )
    r"""Select whether to send data to Routes, or directly to Destinations."""

    environment: Optional[str] = None
    r"""Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere."""

    pq_enabled: Annotated[Optional[bool], pydantic.Field(alias="pqEnabled")] = False
    r"""Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers)."""

    streamtags: Optional[List[str]] = None
    r"""Tags for filtering and grouping in @{product}"""

    connections: Optional[List[ConnectionS3Inventory]] = None
    r"""Direct connections to Destinations, and optionally via a Pipeline or a Pack"""

    pq: Optional[PqS3Inventory] = None

    file_filter: Annotated[Optional[str], pydantic.Field(alias="fileFilter")] = "/.*/"
    r"""Regex matching file names to download and process. Defaults to: .*"""

    aws_account_id: Annotated[Optional[str], pydantic.Field(alias="awsAccountId")] = (
        None
    )
    r"""SQS queue owner's AWS account ID. Leave empty if SQS queue is in same AWS account."""

    aws_authentication_method: Annotated[
        Annotated[
            Optional[AuthenticationMethodS3Inventory],
            PlainValidator(validate_open_enum(False)),
        ],
        pydantic.Field(alias="awsAuthenticationMethod"),
    ] = AuthenticationMethodS3Inventory.AUTO
    r"""AWS authentication method. Choose Auto to use IAM roles."""

    aws_secret_key: Annotated[Optional[str], pydantic.Field(alias="awsSecretKey")] = (
        None
    )

    region: Optional[str] = None
    r"""AWS Region where the S3 bucket and SQS queue are located. Required, unless the Queue entry is a URL or ARN that includes a Region."""

    endpoint: Optional[str] = None
    r"""S3 service endpoint. If empty, defaults to the AWS Region-specific endpoint. Otherwise, it must point to S3-compatible endpoint."""

    signature_version: Annotated[
        Annotated[
            Optional[SignatureVersionS3Inventory],
            PlainValidator(validate_open_enum(False)),
        ],
        pydantic.Field(alias="signatureVersion"),
    ] = SignatureVersionS3Inventory.V4
    r"""Signature version to use for signing S3 requests"""

    reuse_connections: Annotated[
        Optional[bool], pydantic.Field(alias="reuseConnections")
    ] = True
    r"""Reuse connections between requests, which can improve performance"""

    reject_unauthorized: Annotated[
        Optional[bool], pydantic.Field(alias="rejectUnauthorized")
    ] = True
    r"""Reject certificates that cannot be verified against a valid CA, such as self-signed certificates"""

    breaker_rulesets: Annotated[
        Optional[List[str]], pydantic.Field(alias="breakerRulesets")
    ] = None
    r"""A list of event-breaking rulesets that will be applied, in order, to the input data stream"""

    stale_channel_flush_ms: Annotated[
        Optional[float], pydantic.Field(alias="staleChannelFlushMs")
    ] = 10000
    r"""How long (in milliseconds) the Event Breaker will wait for new data to be sent to a specific channel before flushing the data stream out, as is, to the Pipelines"""

    max_messages: Annotated[Optional[float], pydantic.Field(alias="maxMessages")] = 1
    r"""The maximum number of messages SQS should return in a poll request. Amazon SQS never returns more messages than this value (however, fewer messages might be returned). Valid values: 1 to 10."""

    visibility_timeout: Annotated[
        Optional[float], pydantic.Field(alias="visibilityTimeout")
    ] = 600
    r"""After messages are retrieved by a ReceiveMessage request, @{product} will hide them from subsequent retrieve requests for at least this duration. You can set this as high as 43200 sec. (12 hours)."""

    num_receivers: Annotated[Optional[float], pydantic.Field(alias="numReceivers")] = 1
    r"""How many receiver processes to run. The higher the number, the better the throughput - at the expense of CPU overhead."""

    socket_timeout: Annotated[
        Optional[float], pydantic.Field(alias="socketTimeout")
    ] = 300
    r"""Socket inactivity timeout (in seconds). Increase this value if timeouts occur due to backpressure."""

    skip_on_error: Annotated[Optional[bool], pydantic.Field(alias="skipOnError")] = (
        False
    )
    r"""Skip files that trigger a processing error. Disabled by default, which allows retries after processing errors."""

    include_sqs_metadata: Annotated[
        Optional[bool], pydantic.Field(alias="includeSqsMetadata")
    ] = False
    r"""Attach SQS notification metadata to a __sqsMetadata field on each event"""

    enable_assume_role: Annotated[
        Optional[bool], pydantic.Field(alias="enableAssumeRole")
    ] = True
    r"""Use Assume Role credentials to access Amazon S3"""

    assume_role_arn: Annotated[Optional[str], pydantic.Field(alias="assumeRoleArn")] = (
        None
    )
    r"""Amazon Resource Name (ARN) of the role to assume"""

    assume_role_external_id: Annotated[
        Optional[str], pydantic.Field(alias="assumeRoleExternalId")
    ] = None
    r"""External ID to use when assuming role"""

    duration_seconds: Annotated[
        Optional[float], pydantic.Field(alias="durationSeconds")
    ] = 3600
    r"""Duration of the assumed role's session, in seconds. Minimum is 900 (15 minutes), default is 3600 (1 hour), and maximum is 43200 (12 hours)."""

    enable_sqs_assume_role: Annotated[
        Optional[bool], pydantic.Field(alias="enableSQSAssumeRole")
    ] = False
    r"""Use Assume Role credentials when accessing Amazon SQS"""

    preprocess: Optional[PreprocessS3Inventory] = None

    metadata: Optional[List[MetadatumS3Inventory]] = None
    r"""Fields to add to events from this input"""

    parquet_chunk_size_mb: Annotated[
        Optional[float], pydantic.Field(alias="parquetChunkSizeMB")
    ] = 5
    r"""Maximum file size for each Parquet chunk"""

    parquet_chunk_download_timeout: Annotated[
        Optional[float], pydantic.Field(alias="parquetChunkDownloadTimeout")
    ] = 600
    r"""The maximum time allowed for downloading a Parquet chunk. Processing will stop if a chunk cannot be downloaded within the time specified."""

    checkpointing: Optional[CheckpointingS3Inventory] = None

    poll_timeout: Annotated[Optional[float], pydantic.Field(alias="pollTimeout")] = 10
    r"""How long to wait for events before trying polling again. The lower the number the higher the AWS bill. The higher the number the longer it will take for the source to react to configuration changes and system restarts."""

    checksum_suffix: Annotated[
        Optional[str], pydantic.Field(alias="checksumSuffix")
    ] = "checksum"
    r"""Filename suffix of the manifest checksum file. If a filename matching this suffix is received        in the queue, the matching manifest file will be downloaded and validated against its value. Defaults to \"checksum\" """

    max_manifest_size_kb: Annotated[
        Optional[int], pydantic.Field(alias="maxManifestSizeKB")
    ] = 4096
    r"""Maximum download size (KB) of each manifest or checksum file. Manifest files larger than this size will not be read.        Defaults to 4096."""

    validate_inventory_files: Annotated[
        Optional[bool], pydantic.Field(alias="validateInventoryFiles")
    ] = False
    r"""If set to Yes, each inventory file in the manifest will be validated against its checksum. Defaults to false"""

    description: Optional[str] = None

    aws_api_key: Annotated[Optional[str], pydantic.Field(alias="awsApiKey")] = None

    aws_secret: Annotated[Optional[str], pydantic.Field(alias="awsSecret")] = None
    r"""Select or create a stored secret that references your access key and secret key"""

    tag_after_processing: Annotated[
        Annotated[
            Optional[TagAfterProcessingS3Inventory],
            PlainValidator(validate_open_enum(False)),
        ],
        pydantic.Field(alias="tagAfterProcessing"),
    ] = None

    processed_tag_key: Annotated[
        Optional[str], pydantic.Field(alias="processedTagKey")
    ] = None
    r"""The key for the S3 object tag applied after processing. This field accepts an expression for dynamic generation."""

    processed_tag_value: Annotated[
        Optional[str], pydantic.Field(alias="processedTagValue")
    ] = None
    r"""The value for the S3 object tag applied after processing. This field accepts an expression for dynamic generation."""

    @property
    def additional_properties(self):
        return self.__pydantic_extra__

    @additional_properties.setter
    def additional_properties(self, value):
        self.__pydantic_extra__ = value  # pyright: ignore[reportIncompatibleVariableOverride]

    @field_serializer("aws_authentication_method")
    def serialize_aws_authentication_method(self, value):
        if isinstance(value, str):
            try:
                return models.AuthenticationMethodS3Inventory(value)
            except ValueError:
                return value
        return value

    @field_serializer("signature_version")
    def serialize_signature_version(self, value):
        if isinstance(value, str):
            try:
                return models.SignatureVersionS3Inventory(value)
            except ValueError:
                return value
        return value

    @field_serializer("tag_after_processing")
    def serialize_tag_after_processing(self, value):
        if isinstance(value, str):
            try:
                return models.TagAfterProcessingS3Inventory(value)
            except ValueError:
                return value
        return value


class InputTypeS3(str, Enum):
    S3 = "s3"


class ConnectionS3TypedDict(TypedDict):
    output: str
    pipeline: NotRequired[str]


class ConnectionS3(BaseModel):
    output: str

    pipeline: Optional[str] = None


class ModeS3(str, Enum, metaclass=utils.OpenEnumMeta):
    r"""With Smart mode, PQ will write events to the filesystem only when it detects backpressure from the processing engine. With Always On mode, PQ will always write events directly to the queue before forwarding them to the processing engine."""

    # Smart
    SMART = "smart"
    # Always On
    ALWAYS = "always"


class PqCompressionS3(str, Enum, metaclass=utils.OpenEnumMeta):
    r"""Codec to use to compress the persisted data"""

    # None
    NONE = "none"
    # Gzip
    GZIP = "gzip"


class PqControlsS3TypedDict(TypedDict):
    pass


class PqControlsS3(BaseModel):
    pass


class PqS3TypedDict(TypedDict):
    mode: NotRequired[ModeS3]
    r"""With Smart mode, PQ will write events to the filesystem only when it detects backpressure from the processing engine. With Always On mode, PQ will always write events directly to the queue before forwarding them to the processing engine."""
    max_buffer_size: NotRequired[float]
    r"""The maximum number of events to hold in memory before writing the events to disk"""
    commit_frequency: NotRequired[float]
    r"""The number of events to send downstream before committing that Stream has read them"""
    max_file_size: NotRequired[str]
    r"""The maximum size to store in each queue file before closing and optionally compressing. Enter a numeral with units of KB, MB, etc."""
    max_size: NotRequired[str]
    r"""The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc."""
    path: NotRequired[str]
    r"""The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/inputs/<input-id>"""
    compress: NotRequired[PqCompressionS3]
    r"""Codec to use to compress the persisted data"""
    pq_controls: NotRequired[PqControlsS3TypedDict]


class PqS3(BaseModel):
    mode: Annotated[Optional[ModeS3], PlainValidator(validate_open_enum(False))] = (
        ModeS3.ALWAYS
    )
    r"""With Smart mode, PQ will write events to the filesystem only when it detects backpressure from the processing engine. With Always On mode, PQ will always write events directly to the queue before forwarding them to the processing engine."""

    max_buffer_size: Annotated[
        Optional[float], pydantic.Field(alias="maxBufferSize")
    ] = 1000
    r"""The maximum number of events to hold in memory before writing the events to disk"""

    commit_frequency: Annotated[
        Optional[float], pydantic.Field(alias="commitFrequency")
    ] = 42
    r"""The number of events to send downstream before committing that Stream has read them"""

    max_file_size: Annotated[Optional[str], pydantic.Field(alias="maxFileSize")] = (
        "1 MB"
    )
    r"""The maximum size to store in each queue file before closing and optionally compressing. Enter a numeral with units of KB, MB, etc."""

    max_size: Annotated[Optional[str], pydantic.Field(alias="maxSize")] = "5GB"
    r"""The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc."""

    path: Optional[str] = "$CRIBL_HOME/state/queues"
    r"""The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/inputs/<input-id>"""

    compress: Annotated[
        Optional[PqCompressionS3], PlainValidator(validate_open_enum(False))
    ] = PqCompressionS3.NONE
    r"""Codec to use to compress the persisted data"""

    pq_controls: Annotated[
        Optional[PqControlsS3], pydantic.Field(alias="pqControls")
    ] = None

    @field_serializer("mode")
    def serialize_mode(self, value):
        if isinstance(value, str):
            try:
                return models.ModeS3(value)
            except ValueError:
                return value
        return value

    @field_serializer("compress")
    def serialize_compress(self, value):
        if isinstance(value, str):
            try:
                return models.PqCompressionS3(value)
            except ValueError:
                return value
        return value


class InputAuthenticationMethodS3(str, Enum, metaclass=utils.OpenEnumMeta):
    r"""AWS authentication method. Choose Auto to use IAM roles."""

    # Auto
    AUTO = "auto"
    # Manual
    MANUAL = "manual"
    # Secret Key pair
    SECRET = "secret"


class InputSignatureVersionS3(str, Enum, metaclass=utils.OpenEnumMeta):
    r"""Signature version to use for signing S3 requests"""

    V2 = "v2"
    V4 = "v4"


class PreprocessS3TypedDict(TypedDict):
    disabled: NotRequired[bool]
    command: NotRequired[str]
    r"""Command to feed the data through (via stdin) and process its output (stdout)"""
    args: NotRequired[List[str]]
    r"""Arguments to be added to the custom command"""


class PreprocessS3(BaseModel):
    disabled: Optional[bool] = True

    command: Optional[str] = None
    r"""Command to feed the data through (via stdin) and process its output (stdout)"""

    args: Optional[List[str]] = None
    r"""Arguments to be added to the custom command"""


class MetadatumS3TypedDict(TypedDict):
    name: str
    value: str
    r"""JavaScript expression to compute field's value, enclosed in quotes or backticks. (Can evaluate to a constant.)"""


class MetadatumS3(BaseModel):
    name: str

    value: str
    r"""JavaScript expression to compute field's value, enclosed in quotes or backticks. (Can evaluate to a constant.)"""


class CheckpointingS3TypedDict(TypedDict):
    enabled: NotRequired[bool]
    r"""Resume processing files after an interruption"""
    retries: NotRequired[float]
    r"""The number of times to retry processing when a processing error occurs. If Skip file on error is enabled, this setting is ignored."""


class CheckpointingS3(BaseModel):
    enabled: Optional[bool] = False
    r"""Resume processing files after an interruption"""

    retries: Optional[float] = 5
    r"""The number of times to retry processing when a processing error occurs. If Skip file on error is enabled, this setting is ignored."""


class InputS3TypedDict(TypedDict):
    type: InputTypeS3
    queue_name: str
    r"""The name, URL, or ARN of the SQS queue to read notifications from. When a non-AWS URL is specified, format must be: '{url}/myQueueName'. Example: 'https://host:port/myQueueName'. Value must be a JavaScript expression (which can evaluate to a constant value), enclosed in quotes or backticks. Can be evaluated only at init time. Example referencing a Global Variable: `https://host:port/myQueue-${C.vars.myVar}`."""
    id: NotRequired[str]
    r"""Unique ID for this input"""
    disabled: NotRequired[bool]
    pipeline: NotRequired[str]
    r"""Pipeline to process data from this Source before sending it through the Routes"""
    send_to_routes: NotRequired[bool]
    r"""Select whether to send data to Routes, or directly to Destinations."""
    environment: NotRequired[str]
    r"""Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere."""
    pq_enabled: NotRequired[bool]
    r"""Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers)."""
    streamtags: NotRequired[List[str]]
    r"""Tags for filtering and grouping in @{product}"""
    connections: NotRequired[List[ConnectionS3TypedDict]]
    r"""Direct connections to Destinations, and optionally via a Pipeline or a Pack"""
    pq: NotRequired[PqS3TypedDict]
    file_filter: NotRequired[str]
    r"""Regex matching file names to download and process. Defaults to: .*"""
    aws_account_id: NotRequired[str]
    r"""SQS queue owner's AWS account ID. Leave empty if SQS queue is in same AWS account."""
    aws_authentication_method: NotRequired[InputAuthenticationMethodS3]
    r"""AWS authentication method. Choose Auto to use IAM roles."""
    aws_secret_key: NotRequired[str]
    region: NotRequired[str]
    r"""AWS Region where the S3 bucket and SQS queue are located. Required, unless the Queue entry is a URL or ARN that includes a Region."""
    endpoint: NotRequired[str]
    r"""S3 service endpoint. If empty, defaults to the AWS Region-specific endpoint. Otherwise, it must point to S3-compatible endpoint."""
    signature_version: NotRequired[InputSignatureVersionS3]
    r"""Signature version to use for signing S3 requests"""
    reuse_connections: NotRequired[bool]
    r"""Reuse connections between requests, which can improve performance"""
    reject_unauthorized: NotRequired[bool]
    r"""Reject certificates that cannot be verified against a valid CA, such as self-signed certificates"""
    breaker_rulesets: NotRequired[List[str]]
    r"""A list of event-breaking rulesets that will be applied, in order, to the input data stream"""
    stale_channel_flush_ms: NotRequired[float]
    r"""How long (in milliseconds) the Event Breaker will wait for new data to be sent to a specific channel before flushing the data stream out, as is, to the Pipelines"""
    max_messages: NotRequired[float]
    r"""The maximum number of messages SQS should return in a poll request. Amazon SQS never returns more messages than this value (however, fewer messages might be returned). Valid values: 1 to 10."""
    visibility_timeout: NotRequired[float]
    r"""After messages are retrieved by a ReceiveMessage request, @{product} will hide them from subsequent retrieve requests for at least this duration. You can set this as high as 43200 sec. (12 hours)."""
    num_receivers: NotRequired[float]
    r"""How many receiver processes to run. The higher the number, the better the throughput - at the expense of CPU overhead."""
    socket_timeout: NotRequired[float]
    r"""Socket inactivity timeout (in seconds). Increase this value if timeouts occur due to backpressure."""
    skip_on_error: NotRequired[bool]
    r"""Skip files that trigger a processing error. Disabled by default, which allows retries after processing errors."""
    include_sqs_metadata: NotRequired[bool]
    r"""Attach SQS notification metadata to a __sqsMetadata field on each event"""
    enable_assume_role: NotRequired[bool]
    r"""Use Assume Role credentials to access Amazon S3"""
    assume_role_arn: NotRequired[str]
    r"""Amazon Resource Name (ARN) of the role to assume"""
    assume_role_external_id: NotRequired[str]
    r"""External ID to use when assuming role"""
    duration_seconds: NotRequired[float]
    r"""Duration of the assumed role's session, in seconds. Minimum is 900 (15 minutes), default is 3600 (1 hour), and maximum is 43200 (12 hours)."""
    enable_sqs_assume_role: NotRequired[bool]
    r"""Use Assume Role credentials when accessing Amazon SQS"""
    preprocess: NotRequired[PreprocessS3TypedDict]
    metadata: NotRequired[List[MetadatumS3TypedDict]]
    r"""Fields to add to events from this input"""
    parquet_chunk_size_mb: NotRequired[float]
    r"""Maximum file size for each Parquet chunk"""
    parquet_chunk_download_timeout: NotRequired[float]
    r"""The maximum time allowed for downloading a Parquet chunk. Processing will stop if a chunk cannot be downloaded within the time specified."""
    checkpointing: NotRequired[CheckpointingS3TypedDict]
    poll_timeout: NotRequired[float]
    r"""How long to wait for events before trying polling again. The lower the number the higher the AWS bill. The higher the number the longer it will take for the source to react to configuration changes and system restarts."""
    encoding: NotRequired[str]
    r"""Character encoding to use when parsing ingested data. When not set, @{product} will default to UTF-8 but may incorrectly interpret multi-byte characters."""
    tag_after_processing: NotRequired[bool]
    r"""Add a tag to processed S3 objects. Requires s3:GetObjectTagging and s3:PutObjectTagging AWS permissions."""
    description: NotRequired[str]
    aws_api_key: NotRequired[str]
    aws_secret: NotRequired[str]
    r"""Select or create a stored secret that references your access key and secret key"""
    processed_tag_key: NotRequired[str]
    r"""The key for the S3 object tag applied after processing. This field accepts an expression for dynamic generation."""
    processed_tag_value: NotRequired[str]
    r"""The value for the S3 object tag applied after processing. This field accepts an expression for dynamic generation."""


class InputS3(BaseModel):
    model_config = ConfigDict(
        populate_by_name=True, arbitrary_types_allowed=True, extra="allow"
    )
    __pydantic_extra__: Dict[str, Any] = pydantic.Field(init=False)

    type: InputTypeS3

    queue_name: Annotated[str, pydantic.Field(alias="queueName")]
    r"""The name, URL, or ARN of the SQS queue to read notifications from. When a non-AWS URL is specified, format must be: '{url}/myQueueName'. Example: 'https://host:port/myQueueName'. Value must be a JavaScript expression (which can evaluate to a constant value), enclosed in quotes or backticks. Can be evaluated only at init time. Example referencing a Global Variable: `https://host:port/myQueue-${C.vars.myVar}`."""

    id: Optional[str] = None
    r"""Unique ID for this input"""

    disabled: Optional[bool] = False

    pipeline: Optional[str] = None
    r"""Pipeline to process data from this Source before sending it through the Routes"""

    send_to_routes: Annotated[Optional[bool], pydantic.Field(alias="sendToRoutes")] = (
        True
    )
    r"""Select whether to send data to Routes, or directly to Destinations."""

    environment: Optional[str] = None
    r"""Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere."""

    pq_enabled: Annotated[Optional[bool], pydantic.Field(alias="pqEnabled")] = False
    r"""Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers)."""

    streamtags: Optional[List[str]] = None
    r"""Tags for filtering and grouping in @{product}"""

    connections: Optional[List[ConnectionS3]] = None
    r"""Direct connections to Destinations, and optionally via a Pipeline or a Pack"""

    pq: Optional[PqS3] = None

    file_filter: Annotated[Optional[str], pydantic.Field(alias="fileFilter")] = "/.*/"
    r"""Regex matching file names to download and process. Defaults to: .*"""

    aws_account_id: Annotated[Optional[str], pydantic.Field(alias="awsAccountId")] = (
        None
    )
    r"""SQS queue owner's AWS account ID. Leave empty if SQS queue is in same AWS account."""

    aws_authentication_method: Annotated[
        Annotated[
            Optional[InputAuthenticationMethodS3],
            PlainValidator(validate_open_enum(False)),
        ],
        pydantic.Field(alias="awsAuthenticationMethod"),
    ] = InputAuthenticationMethodS3.AUTO
    r"""AWS authentication method. Choose Auto to use IAM roles."""

    aws_secret_key: Annotated[Optional[str], pydantic.Field(alias="awsSecretKey")] = (
        None
    )

    region: Optional[str] = None
    r"""AWS Region where the S3 bucket and SQS queue are located. Required, unless the Queue entry is a URL or ARN that includes a Region."""

    endpoint: Optional[str] = None
    r"""S3 service endpoint. If empty, defaults to the AWS Region-specific endpoint. Otherwise, it must point to S3-compatible endpoint."""

    signature_version: Annotated[
        Annotated[
            Optional[InputSignatureVersionS3], PlainValidator(validate_open_enum(False))
        ],
        pydantic.Field(alias="signatureVersion"),
    ] = InputSignatureVersionS3.V4
    r"""Signature version to use for signing S3 requests"""

    reuse_connections: Annotated[
        Optional[bool], pydantic.Field(alias="reuseConnections")
    ] = True
    r"""Reuse connections between requests, which can improve performance"""

    reject_unauthorized: Annotated[
        Optional[bool], pydantic.Field(alias="rejectUnauthorized")
    ] = True
    r"""Reject certificates that cannot be verified against a valid CA, such as self-signed certificates"""

    breaker_rulesets: Annotated[
        Optional[List[str]], pydantic.Field(alias="breakerRulesets")
    ] = None
    r"""A list of event-breaking rulesets that will be applied, in order, to the input data stream"""

    stale_channel_flush_ms: Annotated[
        Optional[float], pydantic.Field(alias="staleChannelFlushMs")
    ] = 10000
    r"""How long (in milliseconds) the Event Breaker will wait for new data to be sent to a specific channel before flushing the data stream out, as is, to the Pipelines"""

    max_messages: Annotated[Optional[float], pydantic.Field(alias="maxMessages")] = 1
    r"""The maximum number of messages SQS should return in a poll request. Amazon SQS never returns more messages than this value (however, fewer messages might be returned). Valid values: 1 to 10."""

    visibility_timeout: Annotated[
        Optional[float], pydantic.Field(alias="visibilityTimeout")
    ] = 600
    r"""After messages are retrieved by a ReceiveMessage request, @{product} will hide them from subsequent retrieve requests for at least this duration. You can set this as high as 43200 sec. (12 hours)."""

    num_receivers: Annotated[Optional[float], pydantic.Field(alias="numReceivers")] = 1
    r"""How many receiver processes to run. The higher the number, the better the throughput - at the expense of CPU overhead."""

    socket_timeout: Annotated[
        Optional[float], pydantic.Field(alias="socketTimeout")
    ] = 300
    r"""Socket inactivity timeout (in seconds). Increase this value if timeouts occur due to backpressure."""

    skip_on_error: Annotated[Optional[bool], pydantic.Field(alias="skipOnError")] = (
        False
    )
    r"""Skip files that trigger a processing error. Disabled by default, which allows retries after processing errors."""

    include_sqs_metadata: Annotated[
        Optional[bool], pydantic.Field(alias="includeSqsMetadata")
    ] = False
    r"""Attach SQS notification metadata to a __sqsMetadata field on each event"""

    enable_assume_role: Annotated[
        Optional[bool], pydantic.Field(alias="enableAssumeRole")
    ] = True
    r"""Use Assume Role credentials to access Amazon S3"""

    assume_role_arn: Annotated[Optional[str], pydantic.Field(alias="assumeRoleArn")] = (
        None
    )
    r"""Amazon Resource Name (ARN) of the role to assume"""

    assume_role_external_id: Annotated[
        Optional[str], pydantic.Field(alias="assumeRoleExternalId")
    ] = None
    r"""External ID to use when assuming role"""

    duration_seconds: Annotated[
        Optional[float], pydantic.Field(alias="durationSeconds")
    ] = 3600
    r"""Duration of the assumed role's session, in seconds. Minimum is 900 (15 minutes), default is 3600 (1 hour), and maximum is 43200 (12 hours)."""

    enable_sqs_assume_role: Annotated[
        Optional[bool], pydantic.Field(alias="enableSQSAssumeRole")
    ] = False
    r"""Use Assume Role credentials when accessing Amazon SQS"""

    preprocess: Optional[PreprocessS3] = None

    metadata: Optional[List[MetadatumS3]] = None
    r"""Fields to add to events from this input"""

    parquet_chunk_size_mb: Annotated[
        Optional[float], pydantic.Field(alias="parquetChunkSizeMB")
    ] = 5
    r"""Maximum file size for each Parquet chunk"""

    parquet_chunk_download_timeout: Annotated[
        Optional[float], pydantic.Field(alias="parquetChunkDownloadTimeout")
    ] = 600
    r"""The maximum time allowed for downloading a Parquet chunk. Processing will stop if a chunk cannot be downloaded within the time specified."""

    checkpointing: Optional[CheckpointingS3] = None

    poll_timeout: Annotated[Optional[float], pydantic.Field(alias="pollTimeout")] = 10
    r"""How long to wait for events before trying polling again. The lower the number the higher the AWS bill. The higher the number the longer it will take for the source to react to configuration changes and system restarts."""

    encoding: Optional[str] = None
    r"""Character encoding to use when parsing ingested data. When not set, @{product} will default to UTF-8 but may incorrectly interpret multi-byte characters."""

    tag_after_processing: Annotated[
        Optional[bool], pydantic.Field(alias="tagAfterProcessing")
    ] = False
    r"""Add a tag to processed S3 objects. Requires s3:GetObjectTagging and s3:PutObjectTagging AWS permissions."""

    description: Optional[str] = None

    aws_api_key: Annotated[Optional[str], pydantic.Field(alias="awsApiKey")] = None

    aws_secret: Annotated[Optional[str], pydantic.Field(alias="awsSecret")] = None
    r"""Select or create a stored secret that references your access key and secret key"""

    processed_tag_key: Annotated[
        Optional[str], pydantic.Field(alias="processedTagKey")
    ] = None
    r"""The key for the S3 object tag applied after processing. This field accepts an expression for dynamic generation."""

    processed_tag_value: Annotated[
        Optional[str], pydantic.Field(alias="processedTagValue")
    ] = None
    r"""The value for the S3 object tag applied after processing. This field accepts an expression for dynamic generation."""

    @property
    def additional_properties(self):
        return self.__pydantic_extra__

    @additional_properties.setter
    def additional_properties(self, value):
        self.__pydantic_extra__ = value  # pyright: ignore[reportIncompatibleVariableOverride]

    @field_serializer("aws_authentication_method")
    def serialize_aws_authentication_method(self, value):
        if isinstance(value, str):
            try:
                return models.InputAuthenticationMethodS3(value)
            except ValueError:
                return value
        return value

    @field_serializer("signature_version")
    def serialize_signature_version(self, value):
        if isinstance(value, str):
            try:
                return models.InputSignatureVersionS3(value)
            except ValueError:
                return value
        return value


class TypeMetrics(str, Enum):
    METRICS = "metrics"


class ConnectionMetricsTypedDict(TypedDict):
    output: str
    pipeline: NotRequired[str]


class ConnectionMetrics(BaseModel):
    output: str

    pipeline: Optional[str] = None


class ModeMetrics(str, Enum, metaclass=utils.OpenEnumMeta):
    r"""With Smart mode, PQ will write events to the filesystem only when it detects backpressure from the processing engine. With Always On mode, PQ will always write events directly to the queue before forwarding them to the processing engine."""

    # Smart
    SMART = "smart"
    # Always On
    ALWAYS = "always"


class CompressionMetrics(str, Enum, metaclass=utils.OpenEnumMeta):
    r"""Codec to use to compress the persisted data"""

    # None
    NONE = "none"
    # Gzip
    GZIP = "gzip"


class PqControlsMetricsTypedDict(TypedDict):
    pass


class PqControlsMetrics(BaseModel):
    pass


class PqMetricsTypedDict(TypedDict):
    mode: NotRequired[ModeMetrics]
    r"""With Smart mode, PQ will write events to the filesystem only when it detects backpressure from the processing engine. With Always On mode, PQ will always write events directly to the queue before forwarding them to the processing engine."""
    max_buffer_size: NotRequired[float]
    r"""The maximum number of events to hold in memory before writing the events to disk"""
    commit_frequency: NotRequired[float]
    r"""The number of events to send downstream before committing that Stream has read them"""
    max_file_size: NotRequired[str]
    r"""The maximum size to store in each queue file before closing and optionally compressing. Enter a numeral with units of KB, MB, etc."""
    max_size: NotRequired[str]
    r"""The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc."""
    path: NotRequired[str]
    r"""The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/inputs/<input-id>"""
    compress: NotRequired[CompressionMetrics]
    r"""Codec to use to compress the persisted data"""
    pq_controls: NotRequired[PqControlsMetricsTypedDict]


class PqMetrics(BaseModel):
    mode: Annotated[
        Optional[ModeMetrics], PlainValidator(validate_open_enum(False))
    ] = ModeMetrics.ALWAYS
    r"""With Smart mode, PQ will write events to the filesystem only when it detects backpressure from the processing engine. With Always On mode, PQ will always write events directly to the queue before forwarding them to the processing engine."""

    max_buffer_size: Annotated[
        Optional[float], pydantic.Field(alias="maxBufferSize")
    ] = 1000
    r"""The maximum number of events to hold in memory before writing the events to disk"""

    commit_frequency: Annotated[
        Optional[float], pydantic.Field(alias="commitFrequency")
    ] = 42
    r"""The number of events to send downstream before committing that Stream has read them"""

    max_file_size: Annotated[Optional[str], pydantic.Field(alias="maxFileSize")] = (
        "1 MB"
    )
    r"""The maximum size to store in each queue file before closing and optionally compressing. Enter a numeral with units of KB, MB, etc."""

    max_size: Annotated[Optional[str], pydantic.Field(alias="maxSize")] = "5GB"
    r"""The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc."""

    path: Optional[str] = "$CRIBL_HOME/state/queues"
    r"""The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/inputs/<input-id>"""

    compress: Annotated[
        Optional[CompressionMetrics], PlainValidator(validate_open_enum(False))
    ] = CompressionMetrics.NONE
    r"""Codec to use to compress the persisted data"""

    pq_controls: Annotated[
        Optional[PqControlsMetrics], pydantic.Field(alias="pqControls")
    ] = None

    @field_serializer("mode")
    def serialize_mode(self, value):
        if isinstance(value, str):
            try:
                return models.ModeMetrics(value)
            except ValueError:
                return value
        return value

    @field_serializer("compress")
    def serialize_compress(self, value):
        if isinstance(value, str):
            try:
                return models.CompressionMetrics(value)
            except ValueError:
                return value
        return value


class MinimumTLSVersionMetrics(str, Enum, metaclass=utils.OpenEnumMeta):
    TL_SV1 = "TLSv1"
    TL_SV1_1 = "TLSv1.1"
    TL_SV1_2 = "TLSv1.2"
    TL_SV1_3 = "TLSv1.3"


class MaximumTLSVersionMetrics(str, Enum, metaclass=utils.OpenEnumMeta):
    TL_SV1 = "TLSv1"
    TL_SV1_1 = "TLSv1.1"
    TL_SV1_2 = "TLSv1.2"
    TL_SV1_3 = "TLSv1.3"


class TLSSettingsServerSideMetricsTypedDict(TypedDict):
    disabled: NotRequired[bool]
    request_cert: NotRequired[bool]
    r"""Require clients to present their certificates. Used to perform client authentication using SSL certs."""
    reject_unauthorized: NotRequired[bool]
    r"""Reject certificates not authorized by a CA in the CA certificate path or by another trusted CA (such as the system's)"""
    common_name_regex: NotRequired[str]
    r"""Regex matching allowable common names in peer certificates' subject attribute"""
    certificate_name: NotRequired[str]
    r"""The name of the predefined certificate"""
    priv_key_path: NotRequired[str]
    r"""Path on server containing the private key to use. PEM format. Can reference $ENV_VARS."""
    passphrase: NotRequired[str]
    r"""Passphrase to use to decrypt private key"""
    cert_path: NotRequired[str]
    r"""Path on server containing certificates to use. PEM format. Can reference $ENV_VARS."""
    ca_path: NotRequired[str]
    r"""Path on server containing CA certificates to use. PEM format. Can reference $ENV_VARS."""
    min_version: NotRequired[MinimumTLSVersionMetrics]
    max_version: NotRequired[MaximumTLSVersionMetrics]


class TLSSettingsServerSideMetrics(BaseModel):
    disabled: Optional[bool] = True

    request_cert: Annotated[Optional[bool], pydantic.Field(alias="requestCert")] = False
    r"""Require clients to present their certificates. Used to perform client authentication using SSL certs."""

    reject_unauthorized: Annotated[
        Optional[bool], pydantic.Field(alias="rejectUnauthorized")
    ] = True
    r"""Reject certificates not authorized by a CA in the CA certificate path or by another trusted CA (such as the system's)"""

    common_name_regex: Annotated[
        Optional[str], pydantic.Field(alias="commonNameRegex")
    ] = "/.*/"
    r"""Regex matching allowable common names in peer certificates' subject attribute"""

    certificate_name: Annotated[
        Optional[str], pydantic.Field(alias="certificateName")
    ] = None
    r"""The name of the predefined certificate"""

    priv_key_path: Annotated[Optional[str], pydantic.Field(alias="privKeyPath")] = None
    r"""Path on server containing the private key to use. PEM format. Can reference $ENV_VARS."""

    passphrase: Optional[str] = None
    r"""Passphrase to use to decrypt private key"""

    cert_path: Annotated[Optional[str], pydantic.Field(alias="certPath")] = None
    r"""Path on server containing certificates to use. PEM format. Can reference $ENV_VARS."""

    ca_path: Annotated[Optional[str], pydantic.Field(alias="caPath")] = None
    r"""Path on server containing CA certificates to use. PEM format. Can reference $ENV_VARS."""

    min_version: Annotated[
        Annotated[
            Optional[MinimumTLSVersionMetrics],
            PlainValidator(validate_open_enum(False)),
        ],
        pydantic.Field(alias="minVersion"),
    ] = None

    max_version: Annotated[
        Annotated[
            Optional[MaximumTLSVersionMetrics],
            PlainValidator(validate_open_enum(False)),
        ],
        pydantic.Field(alias="maxVersion"),
    ] = None

    @field_serializer("min_version")
    def serialize_min_version(self, value):
        if isinstance(value, str):
            try:
                return models.MinimumTLSVersionMetrics(value)
            except ValueError:
                return value
        return value

    @field_serializer("max_version")
    def serialize_max_version(self, value):
        if isinstance(value, str):
            try:
                return models.MaximumTLSVersionMetrics(value)
            except ValueError:
                return value
        return value


class MetadatumMetricsTypedDict(TypedDict):
    name: str
    value: str
    r"""JavaScript expression to compute field's value, enclosed in quotes or backticks. (Can evaluate to a constant.)"""


class MetadatumMetrics(BaseModel):
    name: str

    value: str
    r"""JavaScript expression to compute field's value, enclosed in quotes or backticks. (Can evaluate to a constant.)"""


class InputMetricsTypedDict(TypedDict):
    type: TypeMetrics
    id: NotRequired[str]
    r"""Unique ID for this input"""
    disabled: NotRequired[bool]
    pipeline: NotRequired[str]
    r"""Pipeline to process data from this Source before sending it through the Routes"""
    send_to_routes: NotRequired[bool]
    r"""Select whether to send data to Routes, or directly to Destinations."""
    environment: NotRequired[str]
    r"""Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere."""
    pq_enabled: NotRequired[bool]
    r"""Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers)."""
    streamtags: NotRequired[List[str]]
    r"""Tags for filtering and grouping in @{product}"""
    connections: NotRequired[List[ConnectionMetricsTypedDict]]
    r"""Direct connections to Destinations, and optionally via a Pipeline or a Pack"""
    pq: NotRequired[PqMetricsTypedDict]
    host: NotRequired[str]
    r"""Address to bind on. For IPv4 (all addresses), use the default '0.0.0.0'. For IPv6, enter '::' (all addresses) or specify an IP address."""
    udp_port: NotRequired[float]
    r"""Enter UDP port number to listen on. Not required if listening on TCP."""
    tcp_port: NotRequired[float]
    r"""Enter TCP port number to listen on. Not required if listening on UDP."""
    max_buffer_size: NotRequired[float]
    r"""Maximum number of events to buffer when downstream is blocking. Only applies to UDP."""
    ip_whitelist_regex: NotRequired[str]
    r"""Regex matching IP addresses that are allowed to send data"""
    enable_proxy_header: NotRequired[bool]
    r"""Enable if the connection is proxied by a device that supports Proxy Protocol V1 or V2"""
    tls: NotRequired[TLSSettingsServerSideMetricsTypedDict]
    metadata: NotRequired[List[MetadatumMetricsTypedDict]]
    r"""Fields to add to events from this input"""
    udp_socket_rx_buf_size: NotRequired[float]
    r"""Optionally, set the SO_RCVBUF socket option for the UDP socket. This value tells the operating system how many bytes can be buffered in the kernel before events are dropped. Leave blank to use the OS default. Caution: Increasing this value will affect OS memory utilization."""
    description: NotRequired[str]


class InputMetrics(BaseModel):
    model_config = ConfigDict(
        populate_by_name=True, arbitrary_types_allowed=True, extra="allow"
    )
    __pydantic_extra__: Dict[str, Any] = pydantic.Field(init=False)

    type: TypeMetrics

    id: Optional[str] = None
    r"""Unique ID for this input"""

    disabled: Optional[bool] = False

    pipeline: Optional[str] = None
    r"""Pipeline to process data from this Source before sending it through the Routes"""

    send_to_routes: Annotated[Optional[bool], pydantic.Field(alias="sendToRoutes")] = (
        True
    )
    r"""Select whether to send data to Routes, or directly to Destinations."""

    environment: Optional[str] = None
    r"""Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere."""

    pq_enabled: Annotated[Optional[bool], pydantic.Field(alias="pqEnabled")] = False
    r"""Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers)."""

    streamtags: Optional[List[str]] = None
    r"""Tags for filtering and grouping in @{product}"""

    connections: Optional[List[ConnectionMetrics]] = None
    r"""Direct connections to Destinations, and optionally via a Pipeline or a Pack"""

    pq: Optional[PqMetrics] = None

    host: Optional[str] = "0.0.0.0"
    r"""Address to bind on. For IPv4 (all addresses), use the default '0.0.0.0'. For IPv6, enter '::' (all addresses) or specify an IP address."""

    udp_port: Annotated[Optional[float], pydantic.Field(alias="udpPort")] = None
    r"""Enter UDP port number to listen on. Not required if listening on TCP."""

    tcp_port: Annotated[Optional[float], pydantic.Field(alias="tcpPort")] = None
    r"""Enter TCP port number to listen on. Not required if listening on UDP."""

    max_buffer_size: Annotated[
        Optional[float], pydantic.Field(alias="maxBufferSize")
    ] = 1000
    r"""Maximum number of events to buffer when downstream is blocking. Only applies to UDP."""

    ip_whitelist_regex: Annotated[
        Optional[str], pydantic.Field(alias="ipWhitelistRegex")
    ] = "/.*/"
    r"""Regex matching IP addresses that are allowed to send data"""

    enable_proxy_header: Annotated[
        Optional[bool], pydantic.Field(alias="enableProxyHeader")
    ] = False
    r"""Enable if the connection is proxied by a device that supports Proxy Protocol V1 or V2"""

    tls: Optional[TLSSettingsServerSideMetrics] = None

    metadata: Optional[List[MetadatumMetrics]] = None
    r"""Fields to add to events from this input"""

    udp_socket_rx_buf_size: Annotated[
        Optional[float], pydantic.Field(alias="udpSocketRxBufSize")
    ] = None
    r"""Optionally, set the SO_RCVBUF socket option for the UDP socket. This value tells the operating system how many bytes can be buffered in the kernel before events are dropped. Leave blank to use the OS default. Caution: Increasing this value will affect OS memory utilization."""

    description: Optional[str] = None

    @property
    def additional_properties(self):
        return self.__pydantic_extra__

    @additional_properties.setter
    def additional_properties(self, value):
        self.__pydantic_extra__ = value  # pyright: ignore[reportIncompatibleVariableOverride]


class TypeCriblmetrics(str, Enum):
    CRIBLMETRICS = "criblmetrics"


class ConnectionCriblmetricsTypedDict(TypedDict):
    output: str
    pipeline: NotRequired[str]


class ConnectionCriblmetrics(BaseModel):
    output: str

    pipeline: Optional[str] = None


class ModeCriblmetrics(str, Enum, metaclass=utils.OpenEnumMeta):
    r"""With Smart mode, PQ will write events to the filesystem only when it detects backpressure from the processing engine. With Always On mode, PQ will always write events directly to the queue before forwarding them to the processing engine."""

    # Smart
    SMART = "smart"
    # Always On
    ALWAYS = "always"


class CompressionCriblmetrics(str, Enum, metaclass=utils.OpenEnumMeta):
    r"""Codec to use to compress the persisted data"""

    # None
    NONE = "none"
    # Gzip
    GZIP = "gzip"


class PqControlsCriblmetricsTypedDict(TypedDict):
    pass


class PqControlsCriblmetrics(BaseModel):
    pass


class PqCriblmetricsTypedDict(TypedDict):
    mode: NotRequired[ModeCriblmetrics]
    r"""With Smart mode, PQ will write events to the filesystem only when it detects backpressure from the processing engine. With Always On mode, PQ will always write events directly to the queue before forwarding them to the processing engine."""
    max_buffer_size: NotRequired[float]
    r"""The maximum number of events to hold in memory before writing the events to disk"""
    commit_frequency: NotRequired[float]
    r"""The number of events to send downstream before committing that Stream has read them"""
    max_file_size: NotRequired[str]
    r"""The maximum size to store in each queue file before closing and optionally compressing. Enter a numeral with units of KB, MB, etc."""
    max_size: NotRequired[str]
    r"""The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc."""
    path: NotRequired[str]
    r"""The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/inputs/<input-id>"""
    compress: NotRequired[CompressionCriblmetrics]
    r"""Codec to use to compress the persisted data"""
    pq_controls: NotRequired[PqControlsCriblmetricsTypedDict]


class PqCriblmetrics(BaseModel):
    mode: Annotated[
        Optional[ModeCriblmetrics], PlainValidator(validate_open_enum(False))
    ] = ModeCriblmetrics.ALWAYS
    r"""With Smart mode, PQ will write events to the filesystem only when it detects backpressure from the processing engine. With Always On mode, PQ will always write events directly to the queue before forwarding them to the processing engine."""

    max_buffer_size: Annotated[
        Optional[float], pydantic.Field(alias="maxBufferSize")
    ] = 1000
    r"""The maximum number of events to hold in memory before writing the events to disk"""

    commit_frequency: Annotated[
        Optional[float], pydantic.Field(alias="commitFrequency")
    ] = 42
    r"""The number of events to send downstream before committing that Stream has read them"""

    max_file_size: Annotated[Optional[str], pydantic.Field(alias="maxFileSize")] = (
        "1 MB"
    )
    r"""The maximum size to store in each queue file before closing and optionally compressing. Enter a numeral with units of KB, MB, etc."""

    max_size: Annotated[Optional[str], pydantic.Field(alias="maxSize")] = "5GB"
    r"""The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc."""

    path: Optional[str] = "$CRIBL_HOME/state/queues"
    r"""The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/inputs/<input-id>"""

    compress: Annotated[
        Optional[CompressionCriblmetrics], PlainValidator(validate_open_enum(False))
    ] = CompressionCriblmetrics.NONE
    r"""Codec to use to compress the persisted data"""

    pq_controls: Annotated[
        Optional[PqControlsCriblmetrics], pydantic.Field(alias="pqControls")
    ] = None

    @field_serializer("mode")
    def serialize_mode(self, value):
        if isinstance(value, str):
            try:
                return models.ModeCriblmetrics(value)
            except ValueError:
                return value
        return value

    @field_serializer("compress")
    def serialize_compress(self, value):
        if isinstance(value, str):
            try:
                return models.CompressionCriblmetrics(value)
            except ValueError:
                return value
        return value


class MetadatumCriblmetricsTypedDict(TypedDict):
    name: str
    value: str
    r"""JavaScript expression to compute field's value, enclosed in quotes or backticks. (Can evaluate to a constant.)"""


class MetadatumCriblmetrics(BaseModel):
    name: str

    value: str
    r"""JavaScript expression to compute field's value, enclosed in quotes or backticks. (Can evaluate to a constant.)"""


class InputCriblmetricsTypedDict(TypedDict):
    type: TypeCriblmetrics
    id: NotRequired[str]
    r"""Unique ID for this input"""
    disabled: NotRequired[bool]
    pipeline: NotRequired[str]
    r"""Pipeline to process data from this Source before sending it through the Routes"""
    send_to_routes: NotRequired[bool]
    r"""Select whether to send data to Routes, or directly to Destinations."""
    environment: NotRequired[str]
    r"""Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere."""
    pq_enabled: NotRequired[bool]
    r"""Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers)."""
    streamtags: NotRequired[List[str]]
    r"""Tags for filtering and grouping in @{product}"""
    connections: NotRequired[List[ConnectionCriblmetricsTypedDict]]
    r"""Direct connections to Destinations, and optionally via a Pipeline or a Pack"""
    pq: NotRequired[PqCriblmetricsTypedDict]
    prefix: NotRequired[str]
    r"""A prefix that is applied to the metrics provided by Cribl Stream"""
    full_fidelity: NotRequired[bool]
    r"""Include granular metrics. Disabling this will drop the following metrics events: `cribl.logstream.host.(in_bytes,in_events,out_bytes,out_events)`, `cribl.logstream.index.(in_bytes,in_events,out_bytes,out_events)`, `cribl.logstream.source.(in_bytes,in_events,out_bytes,out_events)`, `cribl.logstream.sourcetype.(in_bytes,in_events,out_bytes,out_events)`."""
    metadata: NotRequired[List[MetadatumCriblmetricsTypedDict]]
    r"""Fields to add to events from this input"""
    description: NotRequired[str]


class InputCriblmetrics(BaseModel):
    model_config = ConfigDict(
        populate_by_name=True, arbitrary_types_allowed=True, extra="allow"
    )
    __pydantic_extra__: Dict[str, Any] = pydantic.Field(init=False)

    type: TypeCriblmetrics

    id: Optional[str] = None
    r"""Unique ID for this input"""

    disabled: Optional[bool] = False

    pipeline: Optional[str] = None
    r"""Pipeline to process data from this Source before sending it through the Routes"""

    send_to_routes: Annotated[Optional[bool], pydantic.Field(alias="sendToRoutes")] = (
        True
    )
    r"""Select whether to send data to Routes, or directly to Destinations."""

    environment: Optional[str] = None
    r"""Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere."""

    pq_enabled: Annotated[Optional[bool], pydantic.Field(alias="pqEnabled")] = False
    r"""Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers)."""

    streamtags: Optional[List[str]] = None
    r"""Tags for filtering and grouping in @{product}"""

    connections: Optional[List[ConnectionCriblmetrics]] = None
    r"""Direct connections to Destinations, and optionally via a Pipeline or a Pack"""

    pq: Optional[PqCriblmetrics] = None

    prefix: Optional[str] = "cribl.logstream."
    r"""A prefix that is applied to the metrics provided by Cribl Stream"""

    full_fidelity: Annotated[Optional[bool], pydantic.Field(alias="fullFidelity")] = (
        True
    )
    r"""Include granular metrics. Disabling this will drop the following metrics events: `cribl.logstream.host.(in_bytes,in_events,out_bytes,out_events)`, `cribl.logstream.index.(in_bytes,in_events,out_bytes,out_events)`, `cribl.logstream.source.(in_bytes,in_events,out_bytes,out_events)`, `cribl.logstream.sourcetype.(in_bytes,in_events,out_bytes,out_events)`."""

    metadata: Optional[List[MetadatumCriblmetrics]] = None
    r"""Fields to add to events from this input"""

    description: Optional[str] = None

    @property
    def additional_properties(self):
        return self.__pydantic_extra__

    @additional_properties.setter
    def additional_properties(self, value):
        self.__pydantic_extra__ = value  # pyright: ignore[reportIncompatibleVariableOverride]


class InputTypeKinesis(str, Enum):
    KINESIS = "kinesis"


class ConnectionKinesisTypedDict(TypedDict):
    output: str
    pipeline: NotRequired[str]


class ConnectionKinesis(BaseModel):
    output: str

    pipeline: Optional[str] = None


class PqModeKinesis(str, Enum, metaclass=utils.OpenEnumMeta):
    r"""With Smart mode, PQ will write events to the filesystem only when it detects backpressure from the processing engine. With Always On mode, PQ will always write events directly to the queue before forwarding them to the processing engine."""

    # Smart
    SMART = "smart"
    # Always On
    ALWAYS = "always"


class PqCompressionKinesis(str, Enum, metaclass=utils.OpenEnumMeta):
    r"""Codec to use to compress the persisted data"""

    # None
    NONE = "none"
    # Gzip
    GZIP = "gzip"


class InputPqControlsKinesisTypedDict(TypedDict):
    pass


class InputPqControlsKinesis(BaseModel):
    pass


class PqKinesisTypedDict(TypedDict):
    mode: NotRequired[PqModeKinesis]
    r"""With Smart mode, PQ will write events to the filesystem only when it detects backpressure from the processing engine. With Always On mode, PQ will always write events directly to the queue before forwarding them to the processing engine."""
    max_buffer_size: NotRequired[float]
    r"""The maximum number of events to hold in memory before writing the events to disk"""
    commit_frequency: NotRequired[float]
    r"""The number of events to send downstream before committing that Stream has read them"""
    max_file_size: NotRequired[str]
    r"""The maximum size to store in each queue file before closing and optionally compressing. Enter a numeral with units of KB, MB, etc."""
    max_size: NotRequired[str]
    r"""The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc."""
    path: NotRequired[str]
    r"""The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/inputs/<input-id>"""
    compress: NotRequired[PqCompressionKinesis]
    r"""Codec to use to compress the persisted data"""
    pq_controls: NotRequired[InputPqControlsKinesisTypedDict]


class PqKinesis(BaseModel):
    mode: Annotated[
        Optional[PqModeKinesis], PlainValidator(validate_open_enum(False))
    ] = PqModeKinesis.ALWAYS
    r"""With Smart mode, PQ will write events to the filesystem only when it detects backpressure from the processing engine. With Always On mode, PQ will always write events directly to the queue before forwarding them to the processing engine."""

    max_buffer_size: Annotated[
        Optional[float], pydantic.Field(alias="maxBufferSize")
    ] = 1000
    r"""The maximum number of events to hold in memory before writing the events to disk"""

    commit_frequency: Annotated[
        Optional[float], pydantic.Field(alias="commitFrequency")
    ] = 42
    r"""The number of events to send downstream before committing that Stream has read them"""

    max_file_size: Annotated[Optional[str], pydantic.Field(alias="maxFileSize")] = (
        "1 MB"
    )
    r"""The maximum size to store in each queue file before closing and optionally compressing. Enter a numeral with units of KB, MB, etc."""

    max_size: Annotated[Optional[str], pydantic.Field(alias="maxSize")] = "5GB"
    r"""The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc."""

    path: Optional[str] = "$CRIBL_HOME/state/queues"
    r"""The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/inputs/<input-id>"""

    compress: Annotated[
        Optional[PqCompressionKinesis], PlainValidator(validate_open_enum(False))
    ] = PqCompressionKinesis.NONE
    r"""Codec to use to compress the persisted data"""

    pq_controls: Annotated[
        Optional[InputPqControlsKinesis], pydantic.Field(alias="pqControls")
    ] = None

    @field_serializer("mode")
    def serialize_mode(self, value):
        if isinstance(value, str):
            try:
                return models.PqModeKinesis(value)
            except ValueError:
                return value
        return value

    @field_serializer("compress")
    def serialize_compress(self, value):
        if isinstance(value, str):
            try:
                return models.PqCompressionKinesis(value)
            except ValueError:
                return value
        return value


class ShardIteratorStart(str, Enum, metaclass=utils.OpenEnumMeta):
    r"""Location at which to start reading a shard for the first time"""

    # Earliest record
    TRIM_HORIZON = "TRIM_HORIZON"
    # Latest record
    LATEST = "LATEST"


class InputRecordDataFormat(str, Enum, metaclass=utils.OpenEnumMeta):
    r"""Format of data inside the Kinesis Stream records. Gzip compression is automatically detected."""

    # Cribl
    CRIBL = "cribl"
    # Newline JSON
    NDJSON = "ndjson"
    # Cloudwatch Logs
    CLOUDWATCH = "cloudwatch"
    # Event per line
    LINE = "line"


class ShardLoadBalancing(str, Enum, metaclass=utils.OpenEnumMeta):
    r"""The load-balancing algorithm to use for spreading out shards across Workers and Worker Processes"""

    # Consistent Hashing
    CONSISTENT_HASHING = "ConsistentHashing"
    # Round Robin
    ROUND_ROBIN = "RoundRobin"


class InputAuthenticationMethodKinesis(str, Enum, metaclass=utils.OpenEnumMeta):
    r"""AWS authentication method. Choose Auto to use IAM roles."""

    # Auto
    AUTO = "auto"
    # Manual
    MANUAL = "manual"
    # Secret Key pair
    SECRET = "secret"


class InputSignatureVersionKinesis(str, Enum, metaclass=utils.OpenEnumMeta):
    r"""Signature version to use for signing Kinesis stream requests"""

    V2 = "v2"
    V4 = "v4"


class MetadatumKinesisTypedDict(TypedDict):
    name: str
    value: str
    r"""JavaScript expression to compute field's value, enclosed in quotes or backticks. (Can evaluate to a constant.)"""


class MetadatumKinesis(BaseModel):
    name: str

    value: str
    r"""JavaScript expression to compute field's value, enclosed in quotes or backticks. (Can evaluate to a constant.)"""


class InputKinesisTypedDict(TypedDict):
    type: InputTypeKinesis
    stream_name: str
    r"""Kinesis Data Stream to read data from"""
    region: str
    r"""Region where the Kinesis stream is located"""
    id: NotRequired[str]
    r"""Unique ID for this input"""
    disabled: NotRequired[bool]
    pipeline: NotRequired[str]
    r"""Pipeline to process data from this Source before sending it through the Routes"""
    send_to_routes: NotRequired[bool]
    r"""Select whether to send data to Routes, or directly to Destinations."""
    environment: NotRequired[str]
    r"""Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere."""
    pq_enabled: NotRequired[bool]
    r"""Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers)."""
    streamtags: NotRequired[List[str]]
    r"""Tags for filtering and grouping in @{product}"""
    connections: NotRequired[List[ConnectionKinesisTypedDict]]
    r"""Direct connections to Destinations, and optionally via a Pipeline or a Pack"""
    pq: NotRequired[PqKinesisTypedDict]
    service_interval: NotRequired[float]
    r"""Time interval in minutes between consecutive service calls"""
    shard_expr: NotRequired[str]
    r"""A JavaScript expression to be called with each shardId for the stream. If the expression evaluates to a truthy value, the shard will be processed."""
    shard_iterator_type: NotRequired[ShardIteratorStart]
    r"""Location at which to start reading a shard for the first time"""
    payload_format: NotRequired[InputRecordDataFormat]
    r"""Format of data inside the Kinesis Stream records. Gzip compression is automatically detected."""
    get_records_limit: NotRequired[float]
    r"""Maximum number of records per getRecords call"""
    get_records_limit_total: NotRequired[float]
    r"""Maximum number of records, across all shards, to pull down at once per Worker Process"""
    load_balancing_algorithm: NotRequired[ShardLoadBalancing]
    r"""The load-balancing algorithm to use for spreading out shards across Workers and Worker Processes"""
    aws_authentication_method: NotRequired[InputAuthenticationMethodKinesis]
    r"""AWS authentication method. Choose Auto to use IAM roles."""
    aws_secret_key: NotRequired[str]
    endpoint: NotRequired[str]
    r"""Kinesis stream service endpoint. If empty, defaults to the AWS Region-specific endpoint. Otherwise, it must point to Kinesis stream-compatible endpoint."""
    signature_version: NotRequired[InputSignatureVersionKinesis]
    r"""Signature version to use for signing Kinesis stream requests"""
    reuse_connections: NotRequired[bool]
    r"""Reuse connections between requests, which can improve performance"""
    reject_unauthorized: NotRequired[bool]
    r"""Reject certificates that cannot be verified against a valid CA, such as self-signed certificates"""
    enable_assume_role: NotRequired[bool]
    r"""Use Assume Role credentials to access Kinesis stream"""
    assume_role_arn: NotRequired[str]
    r"""Amazon Resource Name (ARN) of the role to assume"""
    assume_role_external_id: NotRequired[str]
    r"""External ID to use when assuming role"""
    duration_seconds: NotRequired[float]
    r"""Duration of the assumed role's session, in seconds. Minimum is 900 (15 minutes), default is 3600 (1 hour), and maximum is 43200 (12 hours)."""
    verify_kpl_check_sums: NotRequired[bool]
    r"""Verify Kinesis Producer Library (KPL) event checksums"""
    avoid_duplicates: NotRequired[bool]
    r"""When resuming streaming from a stored state, Stream will read the next available record, rather than rereading the last-read record. Enabling this setting can cause data loss after a Worker Node's unexpected shutdown or restart."""
    metadata: NotRequired[List[MetadatumKinesisTypedDict]]
    r"""Fields to add to events from this input"""
    description: NotRequired[str]
    aws_api_key: NotRequired[str]
    aws_secret: NotRequired[str]
    r"""Select or create a stored secret that references your access key and secret key"""


class InputKinesis(BaseModel):
    model_config = ConfigDict(
        populate_by_name=True, arbitrary_types_allowed=True, extra="allow"
    )
    __pydantic_extra__: Dict[str, Any] = pydantic.Field(init=False)

    type: InputTypeKinesis

    stream_name: Annotated[str, pydantic.Field(alias="streamName")]
    r"""Kinesis Data Stream to read data from"""

    region: str
    r"""Region where the Kinesis stream is located"""

    id: Optional[str] = None
    r"""Unique ID for this input"""

    disabled: Optional[bool] = False

    pipeline: Optional[str] = None
    r"""Pipeline to process data from this Source before sending it through the Routes"""

    send_to_routes: Annotated[Optional[bool], pydantic.Field(alias="sendToRoutes")] = (
        True
    )
    r"""Select whether to send data to Routes, or directly to Destinations."""

    environment: Optional[str] = None
    r"""Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere."""

    pq_enabled: Annotated[Optional[bool], pydantic.Field(alias="pqEnabled")] = False
    r"""Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers)."""

    streamtags: Optional[List[str]] = None
    r"""Tags for filtering and grouping in @{product}"""

    connections: Optional[List[ConnectionKinesis]] = None
    r"""Direct connections to Destinations, and optionally via a Pipeline or a Pack"""

    pq: Optional[PqKinesis] = None

    service_interval: Annotated[
        Optional[float], pydantic.Field(alias="serviceInterval")
    ] = 1
    r"""Time interval in minutes between consecutive service calls"""

    shard_expr: Annotated[Optional[str], pydantic.Field(alias="shardExpr")] = "true"
    r"""A JavaScript expression to be called with each shardId for the stream. If the expression evaluates to a truthy value, the shard will be processed."""

    shard_iterator_type: Annotated[
        Annotated[
            Optional[ShardIteratorStart], PlainValidator(validate_open_enum(False))
        ],
        pydantic.Field(alias="shardIteratorType"),
    ] = ShardIteratorStart.TRIM_HORIZON
    r"""Location at which to start reading a shard for the first time"""

    payload_format: Annotated[
        Annotated[
            Optional[InputRecordDataFormat], PlainValidator(validate_open_enum(False))
        ],
        pydantic.Field(alias="payloadFormat"),
    ] = InputRecordDataFormat.CRIBL
    r"""Format of data inside the Kinesis Stream records. Gzip compression is automatically detected."""

    get_records_limit: Annotated[
        Optional[float], pydantic.Field(alias="getRecordsLimit")
    ] = 5000
    r"""Maximum number of records per getRecords call"""

    get_records_limit_total: Annotated[
        Optional[float], pydantic.Field(alias="getRecordsLimitTotal")
    ] = 20000
    r"""Maximum number of records, across all shards, to pull down at once per Worker Process"""

    load_balancing_algorithm: Annotated[
        Annotated[
            Optional[ShardLoadBalancing], PlainValidator(validate_open_enum(False))
        ],
        pydantic.Field(alias="loadBalancingAlgorithm"),
    ] = ShardLoadBalancing.CONSISTENT_HASHING
    r"""The load-balancing algorithm to use for spreading out shards across Workers and Worker Processes"""

    aws_authentication_method: Annotated[
        Annotated[
            Optional[InputAuthenticationMethodKinesis],
            PlainValidator(validate_open_enum(False)),
        ],
        pydantic.Field(alias="awsAuthenticationMethod"),
    ] = InputAuthenticationMethodKinesis.AUTO
    r"""AWS authentication method. Choose Auto to use IAM roles."""

    aws_secret_key: Annotated[Optional[str], pydantic.Field(alias="awsSecretKey")] = (
        None
    )

    endpoint: Optional[str] = None
    r"""Kinesis stream service endpoint. If empty, defaults to the AWS Region-specific endpoint. Otherwise, it must point to Kinesis stream-compatible endpoint."""

    signature_version: Annotated[
        Annotated[
            Optional[InputSignatureVersionKinesis],
            PlainValidator(validate_open_enum(False)),
        ],
        pydantic.Field(alias="signatureVersion"),
    ] = InputSignatureVersionKinesis.V4
    r"""Signature version to use for signing Kinesis stream requests"""

    reuse_connections: Annotated[
        Optional[bool], pydantic.Field(alias="reuseConnections")
    ] = True
    r"""Reuse connections between requests, which can improve performance"""

    reject_unauthorized: Annotated[
        Optional[bool], pydantic.Field(alias="rejectUnauthorized")
    ] = True
    r"""Reject certificates that cannot be verified against a valid CA, such as self-signed certificates"""

    enable_assume_role: Annotated[
        Optional[bool], pydantic.Field(alias="enableAssumeRole")
    ] = False
    r"""Use Assume Role credentials to access Kinesis stream"""

    assume_role_arn: Annotated[Optional[str], pydantic.Field(alias="assumeRoleArn")] = (
        None
    )
    r"""Amazon Resource Name (ARN) of the role to assume"""

    assume_role_external_id: Annotated[
        Optional[str], pydantic.Field(alias="assumeRoleExternalId")
    ] = None
    r"""External ID to use when assuming role"""

    duration_seconds: Annotated[
        Optional[float], pydantic.Field(alias="durationSeconds")
    ] = 3600
    r"""Duration of the assumed role's session, in seconds. Minimum is 900 (15 minutes), default is 3600 (1 hour), and maximum is 43200 (12 hours)."""

    verify_kpl_check_sums: Annotated[
        Optional[bool], pydantic.Field(alias="verifyKPLCheckSums")
    ] = False
    r"""Verify Kinesis Producer Library (KPL) event checksums"""

    avoid_duplicates: Annotated[
        Optional[bool], pydantic.Field(alias="avoidDuplicates")
    ] = False
    r"""When resuming streaming from a stored state, Stream will read the next available record, rather than rereading the last-read record. Enabling this setting can cause data loss after a Worker Node's unexpected shutdown or restart."""

    metadata: Optional[List[MetadatumKinesis]] = None
    r"""Fields to add to events from this input"""

    description: Optional[str] = None

    aws_api_key: Annotated[Optional[str], pydantic.Field(alias="awsApiKey")] = None

    aws_secret: Annotated[Optional[str], pydantic.Field(alias="awsSecret")] = None
    r"""Select or create a stored secret that references your access key and secret key"""

    @property
    def additional_properties(self):
        return self.__pydantic_extra__

    @additional_properties.setter
    def additional_properties(self, value):
        self.__pydantic_extra__ = value  # pyright: ignore[reportIncompatibleVariableOverride]

    @field_serializer("shard_iterator_type")
    def serialize_shard_iterator_type(self, value):
        if isinstance(value, str):
            try:
                return models.ShardIteratorStart(value)
            except ValueError:
                return value
        return value

    @field_serializer("payload_format")
    def serialize_payload_format(self, value):
        if isinstance(value, str):
            try:
                return models.InputRecordDataFormat(value)
            except ValueError:
                return value
        return value

    @field_serializer("load_balancing_algorithm")
    def serialize_load_balancing_algorithm(self, value):
        if isinstance(value, str):
            try:
                return models.ShardLoadBalancing(value)
            except ValueError:
                return value
        return value

    @field_serializer("aws_authentication_method")
    def serialize_aws_authentication_method(self, value):
        if isinstance(value, str):
            try:
                return models.InputAuthenticationMethodKinesis(value)
            except ValueError:
                return value
        return value

    @field_serializer("signature_version")
    def serialize_signature_version(self, value):
        if isinstance(value, str):
            try:
                return models.InputSignatureVersionKinesis(value)
            except ValueError:
                return value
        return value


class TypeHTTPRaw(str, Enum):
    HTTP_RAW = "http_raw"


class ConnectionHTTPRawTypedDict(TypedDict):
    output: str
    pipeline: NotRequired[str]


class ConnectionHTTPRaw(BaseModel):
    output: str

    pipeline: Optional[str] = None


class ModeHTTPRaw(str, Enum, metaclass=utils.OpenEnumMeta):
    r"""With Smart mode, PQ will write events to the filesystem only when it detects backpressure from the processing engine. With Always On mode, PQ will always write events directly to the queue before forwarding them to the processing engine."""

    # Smart
    SMART = "smart"
    # Always On
    ALWAYS = "always"


class CompressionHTTPRaw(str, Enum, metaclass=utils.OpenEnumMeta):
    r"""Codec to use to compress the persisted data"""

    # None
    NONE = "none"
    # Gzip
    GZIP = "gzip"


class PqControlsHTTPRawTypedDict(TypedDict):
    pass


class PqControlsHTTPRaw(BaseModel):
    pass


class PqHTTPRawTypedDict(TypedDict):
    mode: NotRequired[ModeHTTPRaw]
    r"""With Smart mode, PQ will write events to the filesystem only when it detects backpressure from the processing engine. With Always On mode, PQ will always write events directly to the queue before forwarding them to the processing engine."""
    max_buffer_size: NotRequired[float]
    r"""The maximum number of events to hold in memory before writing the events to disk"""
    commit_frequency: NotRequired[float]
    r"""The number of events to send downstream before committing that Stream has read them"""
    max_file_size: NotRequired[str]
    r"""The maximum size to store in each queue file before closing and optionally compressing. Enter a numeral with units of KB, MB, etc."""
    max_size: NotRequired[str]
    r"""The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc."""
    path: NotRequired[str]
    r"""The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/inputs/<input-id>"""
    compress: NotRequired[CompressionHTTPRaw]
    r"""Codec to use to compress the persisted data"""
    pq_controls: NotRequired[PqControlsHTTPRawTypedDict]


class PqHTTPRaw(BaseModel):
    mode: Annotated[
        Optional[ModeHTTPRaw], PlainValidator(validate_open_enum(False))
    ] = ModeHTTPRaw.ALWAYS
    r"""With Smart mode, PQ will write events to the filesystem only when it detects backpressure from the processing engine. With Always On mode, PQ will always write events directly to the queue before forwarding them to the processing engine."""

    max_buffer_size: Annotated[
        Optional[float], pydantic.Field(alias="maxBufferSize")
    ] = 1000
    r"""The maximum number of events to hold in memory before writing the events to disk"""

    commit_frequency: Annotated[
        Optional[float], pydantic.Field(alias="commitFrequency")
    ] = 42
    r"""The number of events to send downstream before committing that Stream has read them"""

    max_file_size: Annotated[Optional[str], pydantic.Field(alias="maxFileSize")] = (
        "1 MB"
    )
    r"""The maximum size to store in each queue file before closing and optionally compressing. Enter a numeral with units of KB, MB, etc."""

    max_size: Annotated[Optional[str], pydantic.Field(alias="maxSize")] = "5GB"
    r"""The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc."""

    path: Optional[str] = "$CRIBL_HOME/state/queues"
    r"""The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/inputs/<input-id>"""

    compress: Annotated[
        Optional[CompressionHTTPRaw], PlainValidator(validate_open_enum(False))
    ] = CompressionHTTPRaw.NONE
    r"""Codec to use to compress the persisted data"""

    pq_controls: Annotated[
        Optional[PqControlsHTTPRaw], pydantic.Field(alias="pqControls")
    ] = None

    @field_serializer("mode")
    def serialize_mode(self, value):
        if isinstance(value, str):
            try:
                return models.ModeHTTPRaw(value)
            except ValueError:
                return value
        return value

    @field_serializer("compress")
    def serialize_compress(self, value):
        if isinstance(value, str):
            try:
                return models.CompressionHTTPRaw(value)
            except ValueError:
                return value
        return value


class MinimumTLSVersionHTTPRaw(str, Enum, metaclass=utils.OpenEnumMeta):
    TL_SV1 = "TLSv1"
    TL_SV1_1 = "TLSv1.1"
    TL_SV1_2 = "TLSv1.2"
    TL_SV1_3 = "TLSv1.3"


class MaximumTLSVersionHTTPRaw(str, Enum, metaclass=utils.OpenEnumMeta):
    TL_SV1 = "TLSv1"
    TL_SV1_1 = "TLSv1.1"
    TL_SV1_2 = "TLSv1.2"
    TL_SV1_3 = "TLSv1.3"


class TLSSettingsServerSideHTTPRawTypedDict(TypedDict):
    disabled: NotRequired[bool]
    request_cert: NotRequired[bool]
    r"""Require clients to present their certificates. Used to perform client authentication using SSL certs."""
    reject_unauthorized: NotRequired[bool]
    r"""Reject certificates not authorized by a CA in the CA certificate path or by another trusted CA (such as the system's)"""
    common_name_regex: NotRequired[str]
    r"""Regex matching allowable common names in peer certificates' subject attribute"""
    certificate_name: NotRequired[str]
    r"""The name of the predefined certificate"""
    priv_key_path: NotRequired[str]
    r"""Path on server containing the private key to use. PEM format. Can reference $ENV_VARS."""
    passphrase: NotRequired[str]
    r"""Passphrase to use to decrypt private key"""
    cert_path: NotRequired[str]
    r"""Path on server containing certificates to use. PEM format. Can reference $ENV_VARS."""
    ca_path: NotRequired[str]
    r"""Path on server containing CA certificates to use. PEM format. Can reference $ENV_VARS."""
    min_version: NotRequired[MinimumTLSVersionHTTPRaw]
    max_version: NotRequired[MaximumTLSVersionHTTPRaw]


class TLSSettingsServerSideHTTPRaw(BaseModel):
    disabled: Optional[bool] = True

    request_cert: Annotated[Optional[bool], pydantic.Field(alias="requestCert")] = False
    r"""Require clients to present their certificates. Used to perform client authentication using SSL certs."""

    reject_unauthorized: Annotated[
        Optional[bool], pydantic.Field(alias="rejectUnauthorized")
    ] = True
    r"""Reject certificates not authorized by a CA in the CA certificate path or by another trusted CA (such as the system's)"""

    common_name_regex: Annotated[
        Optional[str], pydantic.Field(alias="commonNameRegex")
    ] = "/.*/"
    r"""Regex matching allowable common names in peer certificates' subject attribute"""

    certificate_name: Annotated[
        Optional[str], pydantic.Field(alias="certificateName")
    ] = None
    r"""The name of the predefined certificate"""

    priv_key_path: Annotated[Optional[str], pydantic.Field(alias="privKeyPath")] = None
    r"""Path on server containing the private key to use. PEM format. Can reference $ENV_VARS."""

    passphrase: Optional[str] = None
    r"""Passphrase to use to decrypt private key"""

    cert_path: Annotated[Optional[str], pydantic.Field(alias="certPath")] = None
    r"""Path on server containing certificates to use. PEM format. Can reference $ENV_VARS."""

    ca_path: Annotated[Optional[str], pydantic.Field(alias="caPath")] = None
    r"""Path on server containing CA certificates to use. PEM format. Can reference $ENV_VARS."""

    min_version: Annotated[
        Annotated[
            Optional[MinimumTLSVersionHTTPRaw],
            PlainValidator(validate_open_enum(False)),
        ],
        pydantic.Field(alias="minVersion"),
    ] = None

    max_version: Annotated[
        Annotated[
            Optional[MaximumTLSVersionHTTPRaw],
            PlainValidator(validate_open_enum(False)),
        ],
        pydantic.Field(alias="maxVersion"),
    ] = None

    @field_serializer("min_version")
    def serialize_min_version(self, value):
        if isinstance(value, str):
            try:
                return models.MinimumTLSVersionHTTPRaw(value)
            except ValueError:
                return value
        return value

    @field_serializer("max_version")
    def serialize_max_version(self, value):
        if isinstance(value, str):
            try:
                return models.MaximumTLSVersionHTTPRaw(value)
            except ValueError:
                return value
        return value


class MetadatumHTTPRawTypedDict(TypedDict):
    name: str
    value: str
    r"""JavaScript expression to compute field's value, enclosed in quotes or backticks. (Can evaluate to a constant.)"""


class MetadatumHTTPRaw(BaseModel):
    name: str

    value: str
    r"""JavaScript expression to compute field's value, enclosed in quotes or backticks. (Can evaluate to a constant.)"""


class AuthTokensExtMetadatumHTTPRawTypedDict(TypedDict):
    name: str
    value: str
    r"""JavaScript expression to compute field's value, enclosed in quotes or backticks. (Can evaluate to a constant.)"""


class AuthTokensExtMetadatumHTTPRaw(BaseModel):
    name: str

    value: str
    r"""JavaScript expression to compute field's value, enclosed in quotes or backticks. (Can evaluate to a constant.)"""


class AuthTokensExtHTTPRawTypedDict(TypedDict):
    token: str
    r"""Shared secret to be provided by any client (Authorization: <token>)"""
    description: NotRequired[str]
    metadata: NotRequired[List[AuthTokensExtMetadatumHTTPRawTypedDict]]
    r"""Fields to add to events referencing this token"""


class AuthTokensExtHTTPRaw(BaseModel):
    token: str
    r"""Shared secret to be provided by any client (Authorization: <token>)"""

    description: Optional[str] = None

    metadata: Optional[List[AuthTokensExtMetadatumHTTPRaw]] = None
    r"""Fields to add to events referencing this token"""


class InputHTTPRawTypedDict(TypedDict):
    type: TypeHTTPRaw
    port: float
    r"""Port to listen on"""
    id: NotRequired[str]
    r"""Unique ID for this input"""
    disabled: NotRequired[bool]
    pipeline: NotRequired[str]
    r"""Pipeline to process data from this Source before sending it through the Routes"""
    send_to_routes: NotRequired[bool]
    r"""Select whether to send data to Routes, or directly to Destinations."""
    environment: NotRequired[str]
    r"""Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere."""
    pq_enabled: NotRequired[bool]
    r"""Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers)."""
    streamtags: NotRequired[List[str]]
    r"""Tags for filtering and grouping in @{product}"""
    connections: NotRequired[List[ConnectionHTTPRawTypedDict]]
    r"""Direct connections to Destinations, and optionally via a Pipeline or a Pack"""
    pq: NotRequired[PqHTTPRawTypedDict]
    host: NotRequired[str]
    r"""Address to bind on. Defaults to 0.0.0.0 (all addresses)."""
    auth_tokens: NotRequired[List[str]]
    r"""Shared secrets to be provided by any client (Authorization: <token>). If empty, unauthorized access is permitted."""
    tls: NotRequired[TLSSettingsServerSideHTTPRawTypedDict]
    max_active_req: NotRequired[float]
    r"""Maximum number of active requests allowed per Worker Process. Set to 0 for unlimited. Caution: Increasing the limit above the default value, or setting it to unlimited, may degrade performance and reduce throughput."""
    max_requests_per_socket: NotRequired[int]
    r"""Maximum number of requests per socket before @{product} instructs the client to close the connection. Default is 0 (unlimited)."""
    enable_proxy_header: NotRequired[bool]
    r"""Extract the client IP and port from PROXY protocol v1/v2. When enabled, the X-Forwarded-For header is ignored. Disable to use the X-Forwarded-For header for client IP extraction."""
    capture_headers: NotRequired[bool]
    r"""Add request headers to events, in the __headers field"""
    activity_log_sample_rate: NotRequired[float]
    r"""How often request activity is logged at the `info` level. A value of 1 would log every request, 10 every 10th request, etc."""
    request_timeout: NotRequired[float]
    r"""How long to wait for an incoming request to complete before aborting it. Use 0 to disable."""
    socket_timeout: NotRequired[float]
    r"""How long @{product} should wait before assuming that an inactive socket has timed out. To wait forever, set to 0."""
    keep_alive_timeout: NotRequired[float]
    r"""After the last response is sent, @{product} will wait this long for additional data before closing the socket connection. Minimum 1 second, maximum 600 seconds (10 minutes)."""
    enable_health_check: NotRequired[bool]
    r"""Expose the /cribl_health endpoint, which returns 200 OK when this Source is healthy"""
    ip_allowlist_regex: NotRequired[str]
    r"""Messages from matched IP addresses will be processed, unless also matched by the denylist"""
    ip_denylist_regex: NotRequired[str]
    r"""Messages from matched IP addresses will be ignored. This takes precedence over the allowlist."""
    breaker_rulesets: NotRequired[List[str]]
    r"""A list of event-breaking rulesets that will be applied, in order, to the input data stream"""
    stale_channel_flush_ms: NotRequired[float]
    r"""How long (in milliseconds) the Event Breaker will wait for new data to be sent to a specific channel before flushing the data stream out, as is, to the Pipelines"""
    metadata: NotRequired[List[MetadatumHTTPRawTypedDict]]
    r"""Fields to add to events from this input"""
    allowed_paths: NotRequired[List[str]]
    r"""List of URI paths accepted by this input, wildcards are supported, e.g /api/v*/hook. Defaults to allow all."""
    allowed_methods: NotRequired[List[str]]
    r"""List of HTTP methods accepted by this input. Wildcards are supported (such as P*, GET). Defaults to allow all."""
    auth_tokens_ext: NotRequired[List[AuthTokensExtHTTPRawTypedDict]]
    r"""Shared secrets to be provided by any client (Authorization: <token>). If empty, unauthorized access is permitted."""
    description: NotRequired[str]


class InputHTTPRaw(BaseModel):
    model_config = ConfigDict(
        populate_by_name=True, arbitrary_types_allowed=True, extra="allow"
    )
    __pydantic_extra__: Dict[str, Any] = pydantic.Field(init=False)

    type: TypeHTTPRaw

    port: float
    r"""Port to listen on"""

    id: Optional[str] = None
    r"""Unique ID for this input"""

    disabled: Optional[bool] = False

    pipeline: Optional[str] = None
    r"""Pipeline to process data from this Source before sending it through the Routes"""

    send_to_routes: Annotated[Optional[bool], pydantic.Field(alias="sendToRoutes")] = (
        True
    )
    r"""Select whether to send data to Routes, or directly to Destinations."""

    environment: Optional[str] = None
    r"""Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere."""

    pq_enabled: Annotated[Optional[bool], pydantic.Field(alias="pqEnabled")] = False
    r"""Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers)."""

    streamtags: Optional[List[str]] = None
    r"""Tags for filtering and grouping in @{product}"""

    connections: Optional[List[ConnectionHTTPRaw]] = None
    r"""Direct connections to Destinations, and optionally via a Pipeline or a Pack"""

    pq: Optional[PqHTTPRaw] = None

    host: Optional[str] = "0.0.0.0"
    r"""Address to bind on. Defaults to 0.0.0.0 (all addresses)."""

    auth_tokens: Annotated[Optional[List[str]], pydantic.Field(alias="authTokens")] = (
        None
    )
    r"""Shared secrets to be provided by any client (Authorization: <token>). If empty, unauthorized access is permitted."""

    tls: Optional[TLSSettingsServerSideHTTPRaw] = None

    max_active_req: Annotated[Optional[float], pydantic.Field(alias="maxActiveReq")] = (
        256
    )
    r"""Maximum number of active requests allowed per Worker Process. Set to 0 for unlimited. Caution: Increasing the limit above the default value, or setting it to unlimited, may degrade performance and reduce throughput."""

    max_requests_per_socket: Annotated[
        Optional[int], pydantic.Field(alias="maxRequestsPerSocket")
    ] = 0
    r"""Maximum number of requests per socket before @{product} instructs the client to close the connection. Default is 0 (unlimited)."""

    enable_proxy_header: Annotated[
        Optional[bool], pydantic.Field(alias="enableProxyHeader")
    ] = False
    r"""Extract the client IP and port from PROXY protocol v1/v2. When enabled, the X-Forwarded-For header is ignored. Disable to use the X-Forwarded-For header for client IP extraction."""

    capture_headers: Annotated[
        Optional[bool], pydantic.Field(alias="captureHeaders")
    ] = False
    r"""Add request headers to events, in the __headers field"""

    activity_log_sample_rate: Annotated[
        Optional[float], pydantic.Field(alias="activityLogSampleRate")
    ] = 100
    r"""How often request activity is logged at the `info` level. A value of 1 would log every request, 10 every 10th request, etc."""

    request_timeout: Annotated[
        Optional[float], pydantic.Field(alias="requestTimeout")
    ] = 0
    r"""How long to wait for an incoming request to complete before aborting it. Use 0 to disable."""

    socket_timeout: Annotated[
        Optional[float], pydantic.Field(alias="socketTimeout")
    ] = 0
    r"""How long @{product} should wait before assuming that an inactive socket has timed out. To wait forever, set to 0."""

    keep_alive_timeout: Annotated[
        Optional[float], pydantic.Field(alias="keepAliveTimeout")
    ] = 5
    r"""After the last response is sent, @{product} will wait this long for additional data before closing the socket connection. Minimum 1 second, maximum 600 seconds (10 minutes)."""

    enable_health_check: Annotated[
        Optional[bool], pydantic.Field(alias="enableHealthCheck")
    ] = False
    r"""Expose the /cribl_health endpoint, which returns 200 OK when this Source is healthy"""

    ip_allowlist_regex: Annotated[
        Optional[str], pydantic.Field(alias="ipAllowlistRegex")
    ] = "/.*/"
    r"""Messages from matched IP addresses will be processed, unless also matched by the denylist"""

    ip_denylist_regex: Annotated[
        Optional[str], pydantic.Field(alias="ipDenylistRegex")
    ] = "/^$/"
    r"""Messages from matched IP addresses will be ignored. This takes precedence over the allowlist."""

    breaker_rulesets: Annotated[
        Optional[List[str]], pydantic.Field(alias="breakerRulesets")
    ] = None
    r"""A list of event-breaking rulesets that will be applied, in order, to the input data stream"""

    stale_channel_flush_ms: Annotated[
        Optional[float], pydantic.Field(alias="staleChannelFlushMs")
    ] = 10000
    r"""How long (in milliseconds) the Event Breaker will wait for new data to be sent to a specific channel before flushing the data stream out, as is, to the Pipelines"""

    metadata: Optional[List[MetadatumHTTPRaw]] = None
    r"""Fields to add to events from this input"""

    allowed_paths: Annotated[
        Optional[List[str]], pydantic.Field(alias="allowedPaths")
    ] = None
    r"""List of URI paths accepted by this input, wildcards are supported, e.g /api/v*/hook. Defaults to allow all."""

    allowed_methods: Annotated[
        Optional[List[str]], pydantic.Field(alias="allowedMethods")
    ] = None
    r"""List of HTTP methods accepted by this input. Wildcards are supported (such as P*, GET). Defaults to allow all."""

    auth_tokens_ext: Annotated[
        Optional[List[AuthTokensExtHTTPRaw]], pydantic.Field(alias="authTokensExt")
    ] = None
    r"""Shared secrets to be provided by any client (Authorization: <token>). If empty, unauthorized access is permitted."""

    description: Optional[str] = None

    @property
    def additional_properties(self):
        return self.__pydantic_extra__

    @additional_properties.setter
    def additional_properties(self, value):
        self.__pydantic_extra__ = value  # pyright: ignore[reportIncompatibleVariableOverride]


class TypeDatagen(str, Enum):
    DATAGEN = "datagen"


class ConnectionDatagenTypedDict(TypedDict):
    output: str
    pipeline: NotRequired[str]


class ConnectionDatagen(BaseModel):
    output: str

    pipeline: Optional[str] = None


class ModeDatagen(str, Enum, metaclass=utils.OpenEnumMeta):
    r"""With Smart mode, PQ will write events to the filesystem only when it detects backpressure from the processing engine. With Always On mode, PQ will always write events directly to the queue before forwarding them to the processing engine."""

    # Smart
    SMART = "smart"
    # Always On
    ALWAYS = "always"


class CompressionDatagen(str, Enum, metaclass=utils.OpenEnumMeta):
    r"""Codec to use to compress the persisted data"""

    # None
    NONE = "none"
    # Gzip
    GZIP = "gzip"


class PqControlsDatagenTypedDict(TypedDict):
    pass


class PqControlsDatagen(BaseModel):
    pass


class PqDatagenTypedDict(TypedDict):
    mode: NotRequired[ModeDatagen]
    r"""With Smart mode, PQ will write events to the filesystem only when it detects backpressure from the processing engine. With Always On mode, PQ will always write events directly to the queue before forwarding them to the processing engine."""
    max_buffer_size: NotRequired[float]
    r"""The maximum number of events to hold in memory before writing the events to disk"""
    commit_frequency: NotRequired[float]
    r"""The number of events to send downstream before committing that Stream has read them"""
    max_file_size: NotRequired[str]
    r"""The maximum size to store in each queue file before closing and optionally compressing. Enter a numeral with units of KB, MB, etc."""
    max_size: NotRequired[str]
    r"""The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc."""
    path: NotRequired[str]
    r"""The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/inputs/<input-id>"""
    compress: NotRequired[CompressionDatagen]
    r"""Codec to use to compress the persisted data"""
    pq_controls: NotRequired[PqControlsDatagenTypedDict]


class PqDatagen(BaseModel):
    mode: Annotated[
        Optional[ModeDatagen], PlainValidator(validate_open_enum(False))
    ] = ModeDatagen.ALWAYS
    r"""With Smart mode, PQ will write events to the filesystem only when it detects backpressure from the processing engine. With Always On mode, PQ will always write events directly to the queue before forwarding them to the processing engine."""

    max_buffer_size: Annotated[
        Optional[float], pydantic.Field(alias="maxBufferSize")
    ] = 1000
    r"""The maximum number of events to hold in memory before writing the events to disk"""

    commit_frequency: Annotated[
        Optional[float], pydantic.Field(alias="commitFrequency")
    ] = 42
    r"""The number of events to send downstream before committing that Stream has read them"""

    max_file_size: Annotated[Optional[str], pydantic.Field(alias="maxFileSize")] = (
        "1 MB"
    )
    r"""The maximum size to store in each queue file before closing and optionally compressing. Enter a numeral with units of KB, MB, etc."""

    max_size: Annotated[Optional[str], pydantic.Field(alias="maxSize")] = "5GB"
    r"""The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc."""

    path: Optional[str] = "$CRIBL_HOME/state/queues"
    r"""The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/inputs/<input-id>"""

    compress: Annotated[
        Optional[CompressionDatagen], PlainValidator(validate_open_enum(False))
    ] = CompressionDatagen.NONE
    r"""Codec to use to compress the persisted data"""

    pq_controls: Annotated[
        Optional[PqControlsDatagen], pydantic.Field(alias="pqControls")
    ] = None

    @field_serializer("mode")
    def serialize_mode(self, value):
        if isinstance(value, str):
            try:
                return models.ModeDatagen(value)
            except ValueError:
                return value
        return value

    @field_serializer("compress")
    def serialize_compress(self, value):
        if isinstance(value, str):
            try:
                return models.CompressionDatagen(value)
            except ValueError:
                return value
        return value


class SampleTypedDict(TypedDict):
    sample: str
    events_per_sec: NotRequired[float]
    r"""Maximum number of events to generate per second per Worker Node. Defaults to 10."""


class Sample(BaseModel):
    sample: str

    events_per_sec: Annotated[Optional[float], pydantic.Field(alias="eventsPerSec")] = (
        10
    )
    r"""Maximum number of events to generate per second per Worker Node. Defaults to 10."""


class MetadatumDatagenTypedDict(TypedDict):
    name: str
    value: str
    r"""JavaScript expression to compute field's value, enclosed in quotes or backticks. (Can evaluate to a constant.)"""


class MetadatumDatagen(BaseModel):
    name: str

    value: str
    r"""JavaScript expression to compute field's value, enclosed in quotes or backticks. (Can evaluate to a constant.)"""


class InputDatagenTypedDict(TypedDict):
    type: TypeDatagen
    samples: List[SampleTypedDict]
    id: NotRequired[str]
    r"""Unique ID for this input"""
    disabled: NotRequired[bool]
    pipeline: NotRequired[str]
    r"""Pipeline to process data from this Source before sending it through the Routes"""
    send_to_routes: NotRequired[bool]
    r"""Select whether to send data to Routes, or directly to Destinations."""
    environment: NotRequired[str]
    r"""Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere."""
    pq_enabled: NotRequired[bool]
    r"""Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers)."""
    streamtags: NotRequired[List[str]]
    r"""Tags for filtering and grouping in @{product}"""
    connections: NotRequired[List[ConnectionDatagenTypedDict]]
    r"""Direct connections to Destinations, and optionally via a Pipeline or a Pack"""
    pq: NotRequired[PqDatagenTypedDict]
    metadata: NotRequired[List[MetadatumDatagenTypedDict]]
    r"""Fields to add to events from this input"""
    description: NotRequired[str]


class InputDatagen(BaseModel):
    model_config = ConfigDict(
        populate_by_name=True, arbitrary_types_allowed=True, extra="allow"
    )
    __pydantic_extra__: Dict[str, Any] = pydantic.Field(init=False)

    type: TypeDatagen

    samples: List[Sample]

    id: Optional[str] = None
    r"""Unique ID for this input"""

    disabled: Optional[bool] = False

    pipeline: Optional[str] = None
    r"""Pipeline to process data from this Source before sending it through the Routes"""

    send_to_routes: Annotated[Optional[bool], pydantic.Field(alias="sendToRoutes")] = (
        True
    )
    r"""Select whether to send data to Routes, or directly to Destinations."""

    environment: Optional[str] = None
    r"""Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere."""

    pq_enabled: Annotated[Optional[bool], pydantic.Field(alias="pqEnabled")] = False
    r"""Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers)."""

    streamtags: Optional[List[str]] = None
    r"""Tags for filtering and grouping in @{product}"""

    connections: Optional[List[ConnectionDatagen]] = None
    r"""Direct connections to Destinations, and optionally via a Pipeline or a Pack"""

    pq: Optional[PqDatagen] = None

    metadata: Optional[List[MetadatumDatagen]] = None
    r"""Fields to add to events from this input"""

    description: Optional[str] = None

    @property
    def additional_properties(self):
        return self.__pydantic_extra__

    @additional_properties.setter
    def additional_properties(self, value):
        self.__pydantic_extra__ = value  # pyright: ignore[reportIncompatibleVariableOverride]


class TypeDatadogAgent(str, Enum):
    DATADOG_AGENT = "datadog_agent"


class ConnectionDatadogAgentTypedDict(TypedDict):
    output: str
    pipeline: NotRequired[str]


class ConnectionDatadogAgent(BaseModel):
    output: str

    pipeline: Optional[str] = None


class ModeDatadogAgent(str, Enum, metaclass=utils.OpenEnumMeta):
    r"""With Smart mode, PQ will write events to the filesystem only when it detects backpressure from the processing engine. With Always On mode, PQ will always write events directly to the queue before forwarding them to the processing engine."""

    # Smart
    SMART = "smart"
    # Always On
    ALWAYS = "always"


class CompressionDatadogAgent(str, Enum, metaclass=utils.OpenEnumMeta):
    r"""Codec to use to compress the persisted data"""

    # None
    NONE = "none"
    # Gzip
    GZIP = "gzip"


class PqControlsDatadogAgentTypedDict(TypedDict):
    pass


class PqControlsDatadogAgent(BaseModel):
    pass


class PqDatadogAgentTypedDict(TypedDict):
    mode: NotRequired[ModeDatadogAgent]
    r"""With Smart mode, PQ will write events to the filesystem only when it detects backpressure from the processing engine. With Always On mode, PQ will always write events directly to the queue before forwarding them to the processing engine."""
    max_buffer_size: NotRequired[float]
    r"""The maximum number of events to hold in memory before writing the events to disk"""
    commit_frequency: NotRequired[float]
    r"""The number of events to send downstream before committing that Stream has read them"""
    max_file_size: NotRequired[str]
    r"""The maximum size to store in each queue file before closing and optionally compressing. Enter a numeral with units of KB, MB, etc."""
    max_size: NotRequired[str]
    r"""The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc."""
    path: NotRequired[str]
    r"""The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/inputs/<input-id>"""
    compress: NotRequired[CompressionDatadogAgent]
    r"""Codec to use to compress the persisted data"""
    pq_controls: NotRequired[PqControlsDatadogAgentTypedDict]


class PqDatadogAgent(BaseModel):
    mode: Annotated[
        Optional[ModeDatadogAgent], PlainValidator(validate_open_enum(False))
    ] = ModeDatadogAgent.ALWAYS
    r"""With Smart mode, PQ will write events to the filesystem only when it detects backpressure from the processing engine. With Always On mode, PQ will always write events directly to the queue before forwarding them to the processing engine."""

    max_buffer_size: Annotated[
        Optional[float], pydantic.Field(alias="maxBufferSize")
    ] = 1000
    r"""The maximum number of events to hold in memory before writing the events to disk"""

    commit_frequency: Annotated[
        Optional[float], pydantic.Field(alias="commitFrequency")
    ] = 42
    r"""The number of events to send downstream before committing that Stream has read them"""

    max_file_size: Annotated[Optional[str], pydantic.Field(alias="maxFileSize")] = (
        "1 MB"
    )
    r"""The maximum size to store in each queue file before closing and optionally compressing. Enter a numeral with units of KB, MB, etc."""

    max_size: Annotated[Optional[str], pydantic.Field(alias="maxSize")] = "5GB"
    r"""The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc."""

    path: Optional[str] = "$CRIBL_HOME/state/queues"
    r"""The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/inputs/<input-id>"""

    compress: Annotated[
        Optional[CompressionDatadogAgent], PlainValidator(validate_open_enum(False))
    ] = CompressionDatadogAgent.NONE
    r"""Codec to use to compress the persisted data"""

    pq_controls: Annotated[
        Optional[PqControlsDatadogAgent], pydantic.Field(alias="pqControls")
    ] = None

    @field_serializer("mode")
    def serialize_mode(self, value):
        if isinstance(value, str):
            try:
                return models.ModeDatadogAgent(value)
            except ValueError:
                return value
        return value

    @field_serializer("compress")
    def serialize_compress(self, value):
        if isinstance(value, str):
            try:
                return models.CompressionDatadogAgent(value)
            except ValueError:
                return value
        return value


class MinimumTLSVersionDatadogAgent(str, Enum, metaclass=utils.OpenEnumMeta):
    TL_SV1 = "TLSv1"
    TL_SV1_1 = "TLSv1.1"
    TL_SV1_2 = "TLSv1.2"
    TL_SV1_3 = "TLSv1.3"


class MaximumTLSVersionDatadogAgent(str, Enum, metaclass=utils.OpenEnumMeta):
    TL_SV1 = "TLSv1"
    TL_SV1_1 = "TLSv1.1"
    TL_SV1_2 = "TLSv1.2"
    TL_SV1_3 = "TLSv1.3"


class TLSSettingsServerSideDatadogAgentTypedDict(TypedDict):
    disabled: NotRequired[bool]
    request_cert: NotRequired[bool]
    r"""Require clients to present their certificates. Used to perform client authentication using SSL certs."""
    reject_unauthorized: NotRequired[bool]
    r"""Reject certificates not authorized by a CA in the CA certificate path or by another trusted CA (such as the system's)"""
    common_name_regex: NotRequired[str]
    r"""Regex matching allowable common names in peer certificates' subject attribute"""
    certificate_name: NotRequired[str]
    r"""The name of the predefined certificate"""
    priv_key_path: NotRequired[str]
    r"""Path on server containing the private key to use. PEM format. Can reference $ENV_VARS."""
    passphrase: NotRequired[str]
    r"""Passphrase to use to decrypt private key"""
    cert_path: NotRequired[str]
    r"""Path on server containing certificates to use. PEM format. Can reference $ENV_VARS."""
    ca_path: NotRequired[str]
    r"""Path on server containing CA certificates to use. PEM format. Can reference $ENV_VARS."""
    min_version: NotRequired[MinimumTLSVersionDatadogAgent]
    max_version: NotRequired[MaximumTLSVersionDatadogAgent]


class TLSSettingsServerSideDatadogAgent(BaseModel):
    disabled: Optional[bool] = True

    request_cert: Annotated[Optional[bool], pydantic.Field(alias="requestCert")] = False
    r"""Require clients to present their certificates. Used to perform client authentication using SSL certs."""

    reject_unauthorized: Annotated[
        Optional[bool], pydantic.Field(alias="rejectUnauthorized")
    ] = True
    r"""Reject certificates not authorized by a CA in the CA certificate path or by another trusted CA (such as the system's)"""

    common_name_regex: Annotated[
        Optional[str], pydantic.Field(alias="commonNameRegex")
    ] = "/.*/"
    r"""Regex matching allowable common names in peer certificates' subject attribute"""

    certificate_name: Annotated[
        Optional[str], pydantic.Field(alias="certificateName")
    ] = None
    r"""The name of the predefined certificate"""

    priv_key_path: Annotated[Optional[str], pydantic.Field(alias="privKeyPath")] = None
    r"""Path on server containing the private key to use. PEM format. Can reference $ENV_VARS."""

    passphrase: Optional[str] = None
    r"""Passphrase to use to decrypt private key"""

    cert_path: Annotated[Optional[str], pydantic.Field(alias="certPath")] = None
    r"""Path on server containing certificates to use. PEM format. Can reference $ENV_VARS."""

    ca_path: Annotated[Optional[str], pydantic.Field(alias="caPath")] = None
    r"""Path on server containing CA certificates to use. PEM format. Can reference $ENV_VARS."""

    min_version: Annotated[
        Annotated[
            Optional[MinimumTLSVersionDatadogAgent],
            PlainValidator(validate_open_enum(False)),
        ],
        pydantic.Field(alias="minVersion"),
    ] = None

    max_version: Annotated[
        Annotated[
            Optional[MaximumTLSVersionDatadogAgent],
            PlainValidator(validate_open_enum(False)),
        ],
        pydantic.Field(alias="maxVersion"),
    ] = None

    @field_serializer("min_version")
    def serialize_min_version(self, value):
        if isinstance(value, str):
            try:
                return models.MinimumTLSVersionDatadogAgent(value)
            except ValueError:
                return value
        return value

    @field_serializer("max_version")
    def serialize_max_version(self, value):
        if isinstance(value, str):
            try:
                return models.MaximumTLSVersionDatadogAgent(value)
            except ValueError:
                return value
        return value


class MetadatumDatadogAgentTypedDict(TypedDict):
    name: str
    value: str
    r"""JavaScript expression to compute field's value, enclosed in quotes or backticks. (Can evaluate to a constant.)"""


class MetadatumDatadogAgent(BaseModel):
    name: str

    value: str
    r"""JavaScript expression to compute field's value, enclosed in quotes or backticks. (Can evaluate to a constant.)"""


class ProxyModeDatadogAgentTypedDict(TypedDict):
    enabled: NotRequired[bool]
    r"""Toggle to Yes to send key validation requests from Datadog Agent to the Datadog API. If toggled to No (the default), Stream handles key validation requests by always responding that the key is valid."""
    reject_unauthorized: NotRequired[bool]
    r"""Whether to reject certificates that cannot be verified against a valid CA (e.g., self-signed certificates)."""


class ProxyModeDatadogAgent(BaseModel):
    enabled: Optional[bool] = False
    r"""Toggle to Yes to send key validation requests from Datadog Agent to the Datadog API. If toggled to No (the default), Stream handles key validation requests by always responding that the key is valid."""

    reject_unauthorized: Annotated[
        Optional[bool], pydantic.Field(alias="rejectUnauthorized")
    ] = True
    r"""Whether to reject certificates that cannot be verified against a valid CA (e.g., self-signed certificates)."""


class InputDatadogAgentTypedDict(TypedDict):
    type: TypeDatadogAgent
    port: float
    r"""Port to listen on"""
    id: NotRequired[str]
    r"""Unique ID for this input"""
    disabled: NotRequired[bool]
    pipeline: NotRequired[str]
    r"""Pipeline to process data from this Source before sending it through the Routes"""
    send_to_routes: NotRequired[bool]
    r"""Select whether to send data to Routes, or directly to Destinations."""
    environment: NotRequired[str]
    r"""Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere."""
    pq_enabled: NotRequired[bool]
    r"""Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers)."""
    streamtags: NotRequired[List[str]]
    r"""Tags for filtering and grouping in @{product}"""
    connections: NotRequired[List[ConnectionDatadogAgentTypedDict]]
    r"""Direct connections to Destinations, and optionally via a Pipeline or a Pack"""
    pq: NotRequired[PqDatadogAgentTypedDict]
    host: NotRequired[str]
    r"""Address to bind on. Defaults to 0.0.0.0 (all addresses)."""
    tls: NotRequired[TLSSettingsServerSideDatadogAgentTypedDict]
    max_active_req: NotRequired[float]
    r"""Maximum number of active requests allowed per Worker Process. Set to 0 for unlimited. Caution: Increasing the limit above the default value, or setting it to unlimited, may degrade performance and reduce throughput."""
    max_requests_per_socket: NotRequired[int]
    r"""Maximum number of requests per socket before @{product} instructs the client to close the connection. Default is 0 (unlimited)."""
    enable_proxy_header: NotRequired[bool]
    r"""Extract the client IP and port from PROXY protocol v1/v2. When enabled, the X-Forwarded-For header is ignored. Disable to use the X-Forwarded-For header for client IP extraction."""
    capture_headers: NotRequired[bool]
    r"""Add request headers to events, in the __headers field"""
    activity_log_sample_rate: NotRequired[float]
    r"""How often request activity is logged at the `info` level. A value of 1 would log every request, 10 every 10th request, etc."""
    request_timeout: NotRequired[float]
    r"""How long to wait for an incoming request to complete before aborting it. Use 0 to disable."""
    socket_timeout: NotRequired[float]
    r"""How long @{product} should wait before assuming that an inactive socket has timed out. To wait forever, set to 0."""
    keep_alive_timeout: NotRequired[float]
    r"""After the last response is sent, @{product} will wait this long for additional data before closing the socket connection. Minimum 1 second, maximum 600 seconds (10 minutes)."""
    enable_health_check: NotRequired[bool]
    r"""Expose the /cribl_health endpoint, which returns 200 OK when this Source is healthy"""
    ip_allowlist_regex: NotRequired[str]
    r"""Messages from matched IP addresses will be processed, unless also matched by the denylist"""
    ip_denylist_regex: NotRequired[str]
    r"""Messages from matched IP addresses will be ignored. This takes precedence over the allowlist."""
    extract_metrics: NotRequired[bool]
    r"""Toggle to Yes to extract each incoming metric to multiple events, one per data point. This works well when sending metrics to a statsd-type output. If sending metrics to DatadogHQ or any destination that accepts arbitrary JSON, leave toggled to No (the default)."""
    metadata: NotRequired[List[MetadatumDatadogAgentTypedDict]]
    r"""Fields to add to events from this input"""
    proxy_mode: NotRequired[ProxyModeDatadogAgentTypedDict]
    description: NotRequired[str]


class InputDatadogAgent(BaseModel):
    model_config = ConfigDict(
        populate_by_name=True, arbitrary_types_allowed=True, extra="allow"
    )
    __pydantic_extra__: Dict[str, Any] = pydantic.Field(init=False)

    type: TypeDatadogAgent

    port: float
    r"""Port to listen on"""

    id: Optional[str] = None
    r"""Unique ID for this input"""

    disabled: Optional[bool] = False

    pipeline: Optional[str] = None
    r"""Pipeline to process data from this Source before sending it through the Routes"""

    send_to_routes: Annotated[Optional[bool], pydantic.Field(alias="sendToRoutes")] = (
        True
    )
    r"""Select whether to send data to Routes, or directly to Destinations."""

    environment: Optional[str] = None
    r"""Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere."""

    pq_enabled: Annotated[Optional[bool], pydantic.Field(alias="pqEnabled")] = False
    r"""Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers)."""

    streamtags: Optional[List[str]] = None
    r"""Tags for filtering and grouping in @{product}"""

    connections: Optional[List[ConnectionDatadogAgent]] = None
    r"""Direct connections to Destinations, and optionally via a Pipeline or a Pack"""

    pq: Optional[PqDatadogAgent] = None

    host: Optional[str] = "0.0.0.0"
    r"""Address to bind on. Defaults to 0.0.0.0 (all addresses)."""

    tls: Optional[TLSSettingsServerSideDatadogAgent] = None

    max_active_req: Annotated[Optional[float], pydantic.Field(alias="maxActiveReq")] = (
        256
    )
    r"""Maximum number of active requests allowed per Worker Process. Set to 0 for unlimited. Caution: Increasing the limit above the default value, or setting it to unlimited, may degrade performance and reduce throughput."""

    max_requests_per_socket: Annotated[
        Optional[int], pydantic.Field(alias="maxRequestsPerSocket")
    ] = 0
    r"""Maximum number of requests per socket before @{product} instructs the client to close the connection. Default is 0 (unlimited)."""

    enable_proxy_header: Annotated[
        Optional[bool], pydantic.Field(alias="enableProxyHeader")
    ] = False
    r"""Extract the client IP and port from PROXY protocol v1/v2. When enabled, the X-Forwarded-For header is ignored. Disable to use the X-Forwarded-For header for client IP extraction."""

    capture_headers: Annotated[
        Optional[bool], pydantic.Field(alias="captureHeaders")
    ] = False
    r"""Add request headers to events, in the __headers field"""

    activity_log_sample_rate: Annotated[
        Optional[float], pydantic.Field(alias="activityLogSampleRate")
    ] = 100
    r"""How often request activity is logged at the `info` level. A value of 1 would log every request, 10 every 10th request, etc."""

    request_timeout: Annotated[
        Optional[float], pydantic.Field(alias="requestTimeout")
    ] = 0
    r"""How long to wait for an incoming request to complete before aborting it. Use 0 to disable."""

    socket_timeout: Annotated[
        Optional[float], pydantic.Field(alias="socketTimeout")
    ] = 0
    r"""How long @{product} should wait before assuming that an inactive socket has timed out. To wait forever, set to 0."""

    keep_alive_timeout: Annotated[
        Optional[float], pydantic.Field(alias="keepAliveTimeout")
    ] = 5
    r"""After the last response is sent, @{product} will wait this long for additional data before closing the socket connection. Minimum 1 second, maximum 600 seconds (10 minutes)."""

    enable_health_check: Annotated[
        Optional[bool], pydantic.Field(alias="enableHealthCheck")
    ] = False
    r"""Expose the /cribl_health endpoint, which returns 200 OK when this Source is healthy"""

    ip_allowlist_regex: Annotated[
        Optional[str], pydantic.Field(alias="ipAllowlistRegex")
    ] = "/.*/"
    r"""Messages from matched IP addresses will be processed, unless also matched by the denylist"""

    ip_denylist_regex: Annotated[
        Optional[str], pydantic.Field(alias="ipDenylistRegex")
    ] = "/^$/"
    r"""Messages from matched IP addresses will be ignored. This takes precedence over the allowlist."""

    extract_metrics: Annotated[
        Optional[bool], pydantic.Field(alias="extractMetrics")
    ] = False
    r"""Toggle to Yes to extract each incoming metric to multiple events, one per data point. This works well when sending metrics to a statsd-type output. If sending metrics to DatadogHQ or any destination that accepts arbitrary JSON, leave toggled to No (the default)."""

    metadata: Optional[List[MetadatumDatadogAgent]] = None
    r"""Fields to add to events from this input"""

    proxy_mode: Annotated[
        Optional[ProxyModeDatadogAgent], pydantic.Field(alias="proxyMode")
    ] = None

    description: Optional[str] = None

    @property
    def additional_properties(self):
        return self.__pydantic_extra__

    @additional_properties.setter
    def additional_properties(self, value):
        self.__pydantic_extra__ = value  # pyright: ignore[reportIncompatibleVariableOverride]


class TypeCrowdstrike(str, Enum):
    CROWDSTRIKE = "crowdstrike"


class ConnectionCrowdstrikeTypedDict(TypedDict):
    output: str
    pipeline: NotRequired[str]


class ConnectionCrowdstrike(BaseModel):
    output: str

    pipeline: Optional[str] = None


class ModeCrowdstrike(str, Enum, metaclass=utils.OpenEnumMeta):
    r"""With Smart mode, PQ will write events to the filesystem only when it detects backpressure from the processing engine. With Always On mode, PQ will always write events directly to the queue before forwarding them to the processing engine."""

    # Smart
    SMART = "smart"
    # Always On
    ALWAYS = "always"


class CompressionCrowdstrike(str, Enum, metaclass=utils.OpenEnumMeta):
    r"""Codec to use to compress the persisted data"""

    # None
    NONE = "none"
    # Gzip
    GZIP = "gzip"


class PqControlsCrowdstrikeTypedDict(TypedDict):
    pass


class PqControlsCrowdstrike(BaseModel):
    pass


class PqCrowdstrikeTypedDict(TypedDict):
    mode: NotRequired[ModeCrowdstrike]
    r"""With Smart mode, PQ will write events to the filesystem only when it detects backpressure from the processing engine. With Always On mode, PQ will always write events directly to the queue before forwarding them to the processing engine."""
    max_buffer_size: NotRequired[float]
    r"""The maximum number of events to hold in memory before writing the events to disk"""
    commit_frequency: NotRequired[float]
    r"""The number of events to send downstream before committing that Stream has read them"""
    max_file_size: NotRequired[str]
    r"""The maximum size to store in each queue file before closing and optionally compressing. Enter a numeral with units of KB, MB, etc."""
    max_size: NotRequired[str]
    r"""The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc."""
    path: NotRequired[str]
    r"""The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/inputs/<input-id>"""
    compress: NotRequired[CompressionCrowdstrike]
    r"""Codec to use to compress the persisted data"""
    pq_controls: NotRequired[PqControlsCrowdstrikeTypedDict]


class PqCrowdstrike(BaseModel):
    mode: Annotated[
        Optional[ModeCrowdstrike], PlainValidator(validate_open_enum(False))
    ] = ModeCrowdstrike.ALWAYS
    r"""With Smart mode, PQ will write events to the filesystem only when it detects backpressure from the processing engine. With Always On mode, PQ will always write events directly to the queue before forwarding them to the processing engine."""

    max_buffer_size: Annotated[
        Optional[float], pydantic.Field(alias="maxBufferSize")
    ] = 1000
    r"""The maximum number of events to hold in memory before writing the events to disk"""

    commit_frequency: Annotated[
        Optional[float], pydantic.Field(alias="commitFrequency")
    ] = 42
    r"""The number of events to send downstream before committing that Stream has read them"""

    max_file_size: Annotated[Optional[str], pydantic.Field(alias="maxFileSize")] = (
        "1 MB"
    )
    r"""The maximum size to store in each queue file before closing and optionally compressing. Enter a numeral with units of KB, MB, etc."""

    max_size: Annotated[Optional[str], pydantic.Field(alias="maxSize")] = "5GB"
    r"""The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc."""

    path: Optional[str] = "$CRIBL_HOME/state/queues"
    r"""The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/inputs/<input-id>"""

    compress: Annotated[
        Optional[CompressionCrowdstrike], PlainValidator(validate_open_enum(False))
    ] = CompressionCrowdstrike.NONE
    r"""Codec to use to compress the persisted data"""

    pq_controls: Annotated[
        Optional[PqControlsCrowdstrike], pydantic.Field(alias="pqControls")
    ] = None

    @field_serializer("mode")
    def serialize_mode(self, value):
        if isinstance(value, str):
            try:
                return models.ModeCrowdstrike(value)
            except ValueError:
                return value
        return value

    @field_serializer("compress")
    def serialize_compress(self, value):
        if isinstance(value, str):
            try:
                return models.CompressionCrowdstrike(value)
            except ValueError:
                return value
        return value


class AuthenticationMethodCrowdstrike(str, Enum, metaclass=utils.OpenEnumMeta):
    r"""AWS authentication method. Choose Auto to use IAM roles."""

    # Auto
    AUTO = "auto"
    # Manual
    MANUAL = "manual"
    # Secret Key pair
    SECRET = "secret"


class SignatureVersionCrowdstrike(str, Enum, metaclass=utils.OpenEnumMeta):
    r"""Signature version to use for signing S3 requests"""

    V2 = "v2"
    V4 = "v4"


class PreprocessCrowdstrikeTypedDict(TypedDict):
    disabled: NotRequired[bool]
    command: NotRequired[str]
    r"""Command to feed the data through (via stdin) and process its output (stdout)"""
    args: NotRequired[List[str]]
    r"""Arguments to be added to the custom command"""


class PreprocessCrowdstrike(BaseModel):
    disabled: Optional[bool] = True

    command: Optional[str] = None
    r"""Command to feed the data through (via stdin) and process its output (stdout)"""

    args: Optional[List[str]] = None
    r"""Arguments to be added to the custom command"""


class MetadatumCrowdstrikeTypedDict(TypedDict):
    name: str
    value: str
    r"""JavaScript expression to compute field's value, enclosed in quotes or backticks. (Can evaluate to a constant.)"""


class MetadatumCrowdstrike(BaseModel):
    name: str

    value: str
    r"""JavaScript expression to compute field's value, enclosed in quotes or backticks. (Can evaluate to a constant.)"""


class CheckpointingCrowdstrikeTypedDict(TypedDict):
    enabled: NotRequired[bool]
    r"""Resume processing files after an interruption"""
    retries: NotRequired[float]
    r"""The number of times to retry processing when a processing error occurs. If Skip file on error is enabled, this setting is ignored."""


class CheckpointingCrowdstrike(BaseModel):
    enabled: Optional[bool] = False
    r"""Resume processing files after an interruption"""

    retries: Optional[float] = 5
    r"""The number of times to retry processing when a processing error occurs. If Skip file on error is enabled, this setting is ignored."""


class TagAfterProcessingCrowdstrike(str, Enum, metaclass=utils.OpenEnumMeta):
    FALSE = "false"
    TRUE = "true"


class InputCrowdstrikeTypedDict(TypedDict):
    type: TypeCrowdstrike
    queue_name: str
    r"""The name, URL, or ARN of the SQS queue to read notifications from. When a non-AWS URL is specified, format must be: '{url}/myQueueName'. Example: 'https://host:port/myQueueName'. Value must be a JavaScript expression (which can evaluate to a constant value), enclosed in quotes or backticks. Can be evaluated only at init time. Example referencing a Global Variable: `https://host:port/myQueue-${C.vars.myVar}`."""
    id: NotRequired[str]
    r"""Unique ID for this input"""
    disabled: NotRequired[bool]
    pipeline: NotRequired[str]
    r"""Pipeline to process data from this Source before sending it through the Routes"""
    send_to_routes: NotRequired[bool]
    r"""Select whether to send data to Routes, or directly to Destinations."""
    environment: NotRequired[str]
    r"""Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere."""
    pq_enabled: NotRequired[bool]
    r"""Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers)."""
    streamtags: NotRequired[List[str]]
    r"""Tags for filtering and grouping in @{product}"""
    connections: NotRequired[List[ConnectionCrowdstrikeTypedDict]]
    r"""Direct connections to Destinations, and optionally via a Pipeline or a Pack"""
    pq: NotRequired[PqCrowdstrikeTypedDict]
    file_filter: NotRequired[str]
    r"""Regex matching file names to download and process. Defaults to: .*"""
    aws_account_id: NotRequired[str]
    r"""SQS queue owner's AWS account ID. Leave empty if SQS queue is in same AWS account."""
    aws_authentication_method: NotRequired[AuthenticationMethodCrowdstrike]
    r"""AWS authentication method. Choose Auto to use IAM roles."""
    aws_secret_key: NotRequired[str]
    region: NotRequired[str]
    r"""AWS Region where the S3 bucket and SQS queue are located. Required, unless the Queue entry is a URL or ARN that includes a Region."""
    endpoint: NotRequired[str]
    r"""S3 service endpoint. If empty, defaults to the AWS Region-specific endpoint. Otherwise, it must point to S3-compatible endpoint."""
    signature_version: NotRequired[SignatureVersionCrowdstrike]
    r"""Signature version to use for signing S3 requests"""
    reuse_connections: NotRequired[bool]
    r"""Reuse connections between requests, which can improve performance"""
    reject_unauthorized: NotRequired[bool]
    r"""Reject certificates that cannot be verified against a valid CA, such as self-signed certificates"""
    breaker_rulesets: NotRequired[List[str]]
    r"""A list of event-breaking rulesets that will be applied, in order, to the input data stream"""
    stale_channel_flush_ms: NotRequired[float]
    r"""How long (in milliseconds) the Event Breaker will wait for new data to be sent to a specific channel before flushing the data stream out, as is, to the Pipelines"""
    max_messages: NotRequired[float]
    r"""The maximum number of messages SQS should return in a poll request. Amazon SQS never returns more messages than this value (however, fewer messages might be returned). Valid values: 1 to 10."""
    visibility_timeout: NotRequired[float]
    r"""After messages are retrieved by a ReceiveMessage request, @{product} will hide them from subsequent retrieve requests for at least this duration. You can set this as high as 43200 sec. (12 hours)."""
    num_receivers: NotRequired[float]
    r"""How many receiver processes to run. The higher the number, the better the throughput - at the expense of CPU overhead."""
    socket_timeout: NotRequired[float]
    r"""Socket inactivity timeout (in seconds). Increase this value if timeouts occur due to backpressure."""
    skip_on_error: NotRequired[bool]
    r"""Skip files that trigger a processing error. Disabled by default, which allows retries after processing errors."""
    include_sqs_metadata: NotRequired[bool]
    r"""Attach SQS notification metadata to a __sqsMetadata field on each event"""
    enable_assume_role: NotRequired[bool]
    r"""Use Assume Role credentials to access Amazon S3"""
    assume_role_arn: NotRequired[str]
    r"""Amazon Resource Name (ARN) of the role to assume"""
    assume_role_external_id: NotRequired[str]
    r"""External ID to use when assuming role"""
    duration_seconds: NotRequired[float]
    r"""Duration of the assumed role's session, in seconds. Minimum is 900 (15 minutes), default is 3600 (1 hour), and maximum is 43200 (12 hours)."""
    enable_sqs_assume_role: NotRequired[bool]
    r"""Use Assume Role credentials when accessing Amazon SQS"""
    preprocess: NotRequired[PreprocessCrowdstrikeTypedDict]
    metadata: NotRequired[List[MetadatumCrowdstrikeTypedDict]]
    r"""Fields to add to events from this input"""
    checkpointing: NotRequired[CheckpointingCrowdstrikeTypedDict]
    poll_timeout: NotRequired[float]
    r"""How long to wait for events before trying polling again. The lower the number the higher the AWS bill. The higher the number the longer it will take for the source to react to configuration changes and system restarts."""
    encoding: NotRequired[str]
    r"""Character encoding to use when parsing ingested data. When not set, @{product} will default to UTF-8 but may incorrectly interpret multi-byte characters."""
    description: NotRequired[str]
    aws_api_key: NotRequired[str]
    aws_secret: NotRequired[str]
    r"""Select or create a stored secret that references your access key and secret key"""
    tag_after_processing: NotRequired[TagAfterProcessingCrowdstrike]
    processed_tag_key: NotRequired[str]
    r"""The key for the S3 object tag applied after processing. This field accepts an expression for dynamic generation."""
    processed_tag_value: NotRequired[str]
    r"""The value for the S3 object tag applied after processing. This field accepts an expression for dynamic generation."""


class InputCrowdstrike(BaseModel):
    model_config = ConfigDict(
        populate_by_name=True, arbitrary_types_allowed=True, extra="allow"
    )
    __pydantic_extra__: Dict[str, Any] = pydantic.Field(init=False)

    type: TypeCrowdstrike

    queue_name: Annotated[str, pydantic.Field(alias="queueName")]
    r"""The name, URL, or ARN of the SQS queue to read notifications from. When a non-AWS URL is specified, format must be: '{url}/myQueueName'. Example: 'https://host:port/myQueueName'. Value must be a JavaScript expression (which can evaluate to a constant value), enclosed in quotes or backticks. Can be evaluated only at init time. Example referencing a Global Variable: `https://host:port/myQueue-${C.vars.myVar}`."""

    id: Optional[str] = None
    r"""Unique ID for this input"""

    disabled: Optional[bool] = False

    pipeline: Optional[str] = None
    r"""Pipeline to process data from this Source before sending it through the Routes"""

    send_to_routes: Annotated[Optional[bool], pydantic.Field(alias="sendToRoutes")] = (
        True
    )
    r"""Select whether to send data to Routes, or directly to Destinations."""

    environment: Optional[str] = None
    r"""Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere."""

    pq_enabled: Annotated[Optional[bool], pydantic.Field(alias="pqEnabled")] = False
    r"""Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers)."""

    streamtags: Optional[List[str]] = None
    r"""Tags for filtering and grouping in @{product}"""

    connections: Optional[List[ConnectionCrowdstrike]] = None
    r"""Direct connections to Destinations, and optionally via a Pipeline or a Pack"""

    pq: Optional[PqCrowdstrike] = None

    file_filter: Annotated[Optional[str], pydantic.Field(alias="fileFilter")] = "/.*/"
    r"""Regex matching file names to download and process. Defaults to: .*"""

    aws_account_id: Annotated[Optional[str], pydantic.Field(alias="awsAccountId")] = (
        None
    )
    r"""SQS queue owner's AWS account ID. Leave empty if SQS queue is in same AWS account."""

    aws_authentication_method: Annotated[
        Annotated[
            Optional[AuthenticationMethodCrowdstrike],
            PlainValidator(validate_open_enum(False)),
        ],
        pydantic.Field(alias="awsAuthenticationMethod"),
    ] = AuthenticationMethodCrowdstrike.AUTO
    r"""AWS authentication method. Choose Auto to use IAM roles."""

    aws_secret_key: Annotated[Optional[str], pydantic.Field(alias="awsSecretKey")] = (
        None
    )

    region: Optional[str] = None
    r"""AWS Region where the S3 bucket and SQS queue are located. Required, unless the Queue entry is a URL or ARN that includes a Region."""

    endpoint: Optional[str] = None
    r"""S3 service endpoint. If empty, defaults to the AWS Region-specific endpoint. Otherwise, it must point to S3-compatible endpoint."""

    signature_version: Annotated[
        Annotated[
            Optional[SignatureVersionCrowdstrike],
            PlainValidator(validate_open_enum(False)),
        ],
        pydantic.Field(alias="signatureVersion"),
    ] = SignatureVersionCrowdstrike.V4
    r"""Signature version to use for signing S3 requests"""

    reuse_connections: Annotated[
        Optional[bool], pydantic.Field(alias="reuseConnections")
    ] = True
    r"""Reuse connections between requests, which can improve performance"""

    reject_unauthorized: Annotated[
        Optional[bool], pydantic.Field(alias="rejectUnauthorized")
    ] = True
    r"""Reject certificates that cannot be verified against a valid CA, such as self-signed certificates"""

    breaker_rulesets: Annotated[
        Optional[List[str]], pydantic.Field(alias="breakerRulesets")
    ] = None
    r"""A list of event-breaking rulesets that will be applied, in order, to the input data stream"""

    stale_channel_flush_ms: Annotated[
        Optional[float], pydantic.Field(alias="staleChannelFlushMs")
    ] = 10000
    r"""How long (in milliseconds) the Event Breaker will wait for new data to be sent to a specific channel before flushing the data stream out, as is, to the Pipelines"""

    max_messages: Annotated[Optional[float], pydantic.Field(alias="maxMessages")] = 1
    r"""The maximum number of messages SQS should return in a poll request. Amazon SQS never returns more messages than this value (however, fewer messages might be returned). Valid values: 1 to 10."""

    visibility_timeout: Annotated[
        Optional[float], pydantic.Field(alias="visibilityTimeout")
    ] = 21600
    r"""After messages are retrieved by a ReceiveMessage request, @{product} will hide them from subsequent retrieve requests for at least this duration. You can set this as high as 43200 sec. (12 hours)."""

    num_receivers: Annotated[Optional[float], pydantic.Field(alias="numReceivers")] = 1
    r"""How many receiver processes to run. The higher the number, the better the throughput - at the expense of CPU overhead."""

    socket_timeout: Annotated[
        Optional[float], pydantic.Field(alias="socketTimeout")
    ] = 300
    r"""Socket inactivity timeout (in seconds). Increase this value if timeouts occur due to backpressure."""

    skip_on_error: Annotated[Optional[bool], pydantic.Field(alias="skipOnError")] = (
        False
    )
    r"""Skip files that trigger a processing error. Disabled by default, which allows retries after processing errors."""

    include_sqs_metadata: Annotated[
        Optional[bool], pydantic.Field(alias="includeSqsMetadata")
    ] = False
    r"""Attach SQS notification metadata to a __sqsMetadata field on each event"""

    enable_assume_role: Annotated[
        Optional[bool], pydantic.Field(alias="enableAssumeRole")
    ] = True
    r"""Use Assume Role credentials to access Amazon S3"""

    assume_role_arn: Annotated[Optional[str], pydantic.Field(alias="assumeRoleArn")] = (
        None
    )
    r"""Amazon Resource Name (ARN) of the role to assume"""

    assume_role_external_id: Annotated[
        Optional[str], pydantic.Field(alias="assumeRoleExternalId")
    ] = None
    r"""External ID to use when assuming role"""

    duration_seconds: Annotated[
        Optional[float], pydantic.Field(alias="durationSeconds")
    ] = 3600
    r"""Duration of the assumed role's session, in seconds. Minimum is 900 (15 minutes), default is 3600 (1 hour), and maximum is 43200 (12 hours)."""

    enable_sqs_assume_role: Annotated[
        Optional[bool], pydantic.Field(alias="enableSQSAssumeRole")
    ] = False
    r"""Use Assume Role credentials when accessing Amazon SQS"""

    preprocess: Optional[PreprocessCrowdstrike] = None

    metadata: Optional[List[MetadatumCrowdstrike]] = None
    r"""Fields to add to events from this input"""

    checkpointing: Optional[CheckpointingCrowdstrike] = None

    poll_timeout: Annotated[Optional[float], pydantic.Field(alias="pollTimeout")] = 10
    r"""How long to wait for events before trying polling again. The lower the number the higher the AWS bill. The higher the number the longer it will take for the source to react to configuration changes and system restarts."""

    encoding: Optional[str] = None
    r"""Character encoding to use when parsing ingested data. When not set, @{product} will default to UTF-8 but may incorrectly interpret multi-byte characters."""

    description: Optional[str] = None

    aws_api_key: Annotated[Optional[str], pydantic.Field(alias="awsApiKey")] = None

    aws_secret: Annotated[Optional[str], pydantic.Field(alias="awsSecret")] = None
    r"""Select or create a stored secret that references your access key and secret key"""

    tag_after_processing: Annotated[
        Annotated[
            Optional[TagAfterProcessingCrowdstrike],
            PlainValidator(validate_open_enum(False)),
        ],
        pydantic.Field(alias="tagAfterProcessing"),
    ] = None

    processed_tag_key: Annotated[
        Optional[str], pydantic.Field(alias="processedTagKey")
    ] = None
    r"""The key for the S3 object tag applied after processing. This field accepts an expression for dynamic generation."""

    processed_tag_value: Annotated[
        Optional[str], pydantic.Field(alias="processedTagValue")
    ] = None
    r"""The value for the S3 object tag applied after processing. This field accepts an expression for dynamic generation."""

    @property
    def additional_properties(self):
        return self.__pydantic_extra__

    @additional_properties.setter
    def additional_properties(self, value):
        self.__pydantic_extra__ = value  # pyright: ignore[reportIncompatibleVariableOverride]

    @field_serializer("aws_authentication_method")
    def serialize_aws_authentication_method(self, value):
        if isinstance(value, str):
            try:
                return models.AuthenticationMethodCrowdstrike(value)
            except ValueError:
                return value
        return value

    @field_serializer("signature_version")
    def serialize_signature_version(self, value):
        if isinstance(value, str):
            try:
                return models.SignatureVersionCrowdstrike(value)
            except ValueError:
                return value
        return value

    @field_serializer("tag_after_processing")
    def serialize_tag_after_processing(self, value):
        if isinstance(value, str):
            try:
                return models.TagAfterProcessingCrowdstrike(value)
            except ValueError:
                return value
        return value


class TypeWindowsMetrics(str, Enum):
    WINDOWS_METRICS = "windows_metrics"


class ConnectionWindowsMetricsTypedDict(TypedDict):
    output: str
    pipeline: NotRequired[str]


class ConnectionWindowsMetrics(BaseModel):
    output: str

    pipeline: Optional[str] = None


class PqModeWindowsMetrics(str, Enum, metaclass=utils.OpenEnumMeta):
    r"""With Smart mode, PQ will write events to the filesystem only when it detects backpressure from the processing engine. With Always On mode, PQ will always write events directly to the queue before forwarding them to the processing engine."""

    # Smart
    SMART = "smart"
    # Always On
    ALWAYS = "always"


class CompressionWindowsMetrics(str, Enum, metaclass=utils.OpenEnumMeta):
    r"""Codec to use to compress the persisted data"""

    # None
    NONE = "none"
    # Gzip
    GZIP = "gzip"


class PqControlsWindowsMetricsTypedDict(TypedDict):
    pass


class PqControlsWindowsMetrics(BaseModel):
    pass


class PqWindowsMetricsTypedDict(TypedDict):
    mode: NotRequired[PqModeWindowsMetrics]
    r"""With Smart mode, PQ will write events to the filesystem only when it detects backpressure from the processing engine. With Always On mode, PQ will always write events directly to the queue before forwarding them to the processing engine."""
    max_buffer_size: NotRequired[float]
    r"""The maximum number of events to hold in memory before writing the events to disk"""
    commit_frequency: NotRequired[float]
    r"""The number of events to send downstream before committing that Stream has read them"""
    max_file_size: NotRequired[str]
    r"""The maximum size to store in each queue file before closing and optionally compressing. Enter a numeral with units of KB, MB, etc."""
    max_size: NotRequired[str]
    r"""The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc."""
    path: NotRequired[str]
    r"""The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/inputs/<input-id>"""
    compress: NotRequired[CompressionWindowsMetrics]
    r"""Codec to use to compress the persisted data"""
    pq_controls: NotRequired[PqControlsWindowsMetricsTypedDict]


class PqWindowsMetrics(BaseModel):
    mode: Annotated[
        Optional[PqModeWindowsMetrics], PlainValidator(validate_open_enum(False))
    ] = PqModeWindowsMetrics.ALWAYS
    r"""With Smart mode, PQ will write events to the filesystem only when it detects backpressure from the processing engine. With Always On mode, PQ will always write events directly to the queue before forwarding them to the processing engine."""

    max_buffer_size: Annotated[
        Optional[float], pydantic.Field(alias="maxBufferSize")
    ] = 1000
    r"""The maximum number of events to hold in memory before writing the events to disk"""

    commit_frequency: Annotated[
        Optional[float], pydantic.Field(alias="commitFrequency")
    ] = 42
    r"""The number of events to send downstream before committing that Stream has read them"""

    max_file_size: Annotated[Optional[str], pydantic.Field(alias="maxFileSize")] = (
        "1 MB"
    )
    r"""The maximum size to store in each queue file before closing and optionally compressing. Enter a numeral with units of KB, MB, etc."""

    max_size: Annotated[Optional[str], pydantic.Field(alias="maxSize")] = "5GB"
    r"""The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc."""

    path: Optional[str] = "$CRIBL_HOME/state/queues"
    r"""The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/inputs/<input-id>"""

    compress: Annotated[
        Optional[CompressionWindowsMetrics], PlainValidator(validate_open_enum(False))
    ] = CompressionWindowsMetrics.NONE
    r"""Codec to use to compress the persisted data"""

    pq_controls: Annotated[
        Optional[PqControlsWindowsMetrics], pydantic.Field(alias="pqControls")
    ] = None

    @field_serializer("mode")
    def serialize_mode(self, value):
        if isinstance(value, str):
            try:
                return models.PqModeWindowsMetrics(value)
            except ValueError:
                return value
        return value

    @field_serializer("compress")
    def serialize_compress(self, value):
        if isinstance(value, str):
            try:
                return models.CompressionWindowsMetrics(value)
            except ValueError:
                return value
        return value


class HostModeWindowsMetrics(str, Enum, metaclass=utils.OpenEnumMeta):
    r"""Select level of detail for host metrics"""

    # Basic
    BASIC = "basic"
    # All
    ALL = "all"
    # Custom
    CUSTOM = "custom"
    # Disabled
    DISABLED = "disabled"


class SystemModeWindowsMetrics(str, Enum, metaclass=utils.OpenEnumMeta):
    r"""Select the level of details for system metrics"""

    # Basic
    BASIC = "basic"
    # All
    ALL = "all"
    # Custom
    CUSTOM = "custom"
    # Disabled
    DISABLED = "disabled"


class SystemWindowsMetricsTypedDict(TypedDict):
    mode: NotRequired[SystemModeWindowsMetrics]
    r"""Select the level of details for system metrics"""
    detail: NotRequired[bool]
    r"""Generate metrics for all system information"""


class SystemWindowsMetrics(BaseModel):
    mode: Annotated[
        Optional[SystemModeWindowsMetrics], PlainValidator(validate_open_enum(False))
    ] = SystemModeWindowsMetrics.BASIC
    r"""Select the level of details for system metrics"""

    detail: Optional[bool] = False
    r"""Generate metrics for all system information"""

    @field_serializer("mode")
    def serialize_mode(self, value):
        if isinstance(value, str):
            try:
                return models.SystemModeWindowsMetrics(value)
            except ValueError:
                return value
        return value


class CPUModeWindowsMetrics(str, Enum, metaclass=utils.OpenEnumMeta):
    r"""Select the level of details for CPU metrics"""

    # Basic
    BASIC = "basic"
    # All
    ALL = "all"
    # Custom
    CUSTOM = "custom"
    # Disabled
    DISABLED = "disabled"


class CPUWindowsMetricsTypedDict(TypedDict):
    mode: NotRequired[CPUModeWindowsMetrics]
    r"""Select the level of details for CPU metrics"""
    per_cpu: NotRequired[bool]
    r"""Generate metrics for each CPU"""
    detail: NotRequired[bool]
    r"""Generate metrics for all CPU states"""
    time: NotRequired[bool]
    r"""Generate raw, monotonic CPU time counters"""


class CPUWindowsMetrics(BaseModel):
    mode: Annotated[
        Optional[CPUModeWindowsMetrics], PlainValidator(validate_open_enum(False))
    ] = CPUModeWindowsMetrics.BASIC
    r"""Select the level of details for CPU metrics"""

    per_cpu: Annotated[Optional[bool], pydantic.Field(alias="perCpu")] = False
    r"""Generate metrics for each CPU"""

    detail: Optional[bool] = False
    r"""Generate metrics for all CPU states"""

    time: Optional[bool] = False
    r"""Generate raw, monotonic CPU time counters"""

    @field_serializer("mode")
    def serialize_mode(self, value):
        if isinstance(value, str):
            try:
                return models.CPUModeWindowsMetrics(value)
            except ValueError:
                return value
        return value


class MemoryModeWindowsMetrics(str, Enum, metaclass=utils.OpenEnumMeta):
    r"""Select the level of details for memory metrics"""

    # Basic
    BASIC = "basic"
    # All
    ALL = "all"
    # Custom
    CUSTOM = "custom"
    # Disabled
    DISABLED = "disabled"


class MemoryWindowsMetricsTypedDict(TypedDict):
    mode: NotRequired[MemoryModeWindowsMetrics]
    r"""Select the level of details for memory metrics"""
    detail: NotRequired[bool]
    r"""Generate metrics for all memory states"""


class MemoryWindowsMetrics(BaseModel):
    mode: Annotated[
        Optional[MemoryModeWindowsMetrics], PlainValidator(validate_open_enum(False))
    ] = MemoryModeWindowsMetrics.BASIC
    r"""Select the level of details for memory metrics"""

    detail: Optional[bool] = False
    r"""Generate metrics for all memory states"""

    @field_serializer("mode")
    def serialize_mode(self, value):
        if isinstance(value, str):
            try:
                return models.MemoryModeWindowsMetrics(value)
            except ValueError:
                return value
        return value


class NetworkModeWindowsMetrics(str, Enum, metaclass=utils.OpenEnumMeta):
    r"""Select the level of details for network metrics"""

    # Basic
    BASIC = "basic"
    # All
    ALL = "all"
    # Custom
    CUSTOM = "custom"
    # Disabled
    DISABLED = "disabled"


class NetworkWindowsMetricsTypedDict(TypedDict):
    mode: NotRequired[NetworkModeWindowsMetrics]
    r"""Select the level of details for network metrics"""
    detail: NotRequired[bool]
    r"""Generate full network metrics"""
    protocols: NotRequired[bool]
    r"""Generate protocol metrics for ICMP, ICMPMsg, IP, TCP, UDP and UDPLite"""
    devices: NotRequired[List[str]]
    r"""Network interfaces to include/exclude. All interfaces are included if this list is empty."""
    per_interface: NotRequired[bool]
    r"""Generate separate metrics for each interface"""


class NetworkWindowsMetrics(BaseModel):
    mode: Annotated[
        Optional[NetworkModeWindowsMetrics], PlainValidator(validate_open_enum(False))
    ] = NetworkModeWindowsMetrics.BASIC
    r"""Select the level of details for network metrics"""

    detail: Optional[bool] = False
    r"""Generate full network metrics"""

    protocols: Optional[bool] = False
    r"""Generate protocol metrics for ICMP, ICMPMsg, IP, TCP, UDP and UDPLite"""

    devices: Optional[List[str]] = None
    r"""Network interfaces to include/exclude. All interfaces are included if this list is empty."""

    per_interface: Annotated[Optional[bool], pydantic.Field(alias="perInterface")] = (
        False
    )
    r"""Generate separate metrics for each interface"""

    @field_serializer("mode")
    def serialize_mode(self, value):
        if isinstance(value, str):
            try:
                return models.NetworkModeWindowsMetrics(value)
            except ValueError:
                return value
        return value


class DiskModeWindowsMetrics(str, Enum, metaclass=utils.OpenEnumMeta):
    r"""Select the level of details for disk metrics"""

    # Basic
    BASIC = "basic"
    # All
    ALL = "all"
    # Custom
    CUSTOM = "custom"
    # Disabled
    DISABLED = "disabled"


class DiskWindowsMetricsTypedDict(TypedDict):
    mode: NotRequired[DiskModeWindowsMetrics]
    r"""Select the level of details for disk metrics"""
    per_volume: NotRequired[bool]
    r"""Generate separate metrics for each volume"""
    detail: NotRequired[bool]
    r"""Generate full disk metrics"""
    volumes: NotRequired[List[str]]
    r"""Windows volumes to include/exclude. E.g.: C:, !E:, etc. Wildcards and ! (not) operators are supported. All volumes are included if this list is empty."""


class DiskWindowsMetrics(BaseModel):
    mode: Annotated[
        Optional[DiskModeWindowsMetrics], PlainValidator(validate_open_enum(False))
    ] = DiskModeWindowsMetrics.BASIC
    r"""Select the level of details for disk metrics"""

    per_volume: Annotated[Optional[bool], pydantic.Field(alias="perVolume")] = False
    r"""Generate separate metrics for each volume"""

    detail: Optional[bool] = False
    r"""Generate full disk metrics"""

    volumes: Optional[List[str]] = None
    r"""Windows volumes to include/exclude. E.g.: C:, !E:, etc. Wildcards and ! (not) operators are supported. All volumes are included if this list is empty."""

    @field_serializer("mode")
    def serialize_mode(self, value):
        if isinstance(value, str):
            try:
                return models.DiskModeWindowsMetrics(value)
            except ValueError:
                return value
        return value


class CustomWindowsMetricsTypedDict(TypedDict):
    system: NotRequired[SystemWindowsMetricsTypedDict]
    cpu: NotRequired[CPUWindowsMetricsTypedDict]
    memory: NotRequired[MemoryWindowsMetricsTypedDict]
    network: NotRequired[NetworkWindowsMetricsTypedDict]
    disk: NotRequired[DiskWindowsMetricsTypedDict]


class CustomWindowsMetrics(BaseModel):
    system: Optional[SystemWindowsMetrics] = None

    cpu: Optional[CPUWindowsMetrics] = None

    memory: Optional[MemoryWindowsMetrics] = None

    network: Optional[NetworkWindowsMetrics] = None

    disk: Optional[DiskWindowsMetrics] = None


class HostWindowsMetricsTypedDict(TypedDict):
    mode: NotRequired[HostModeWindowsMetrics]
    r"""Select level of detail for host metrics"""
    custom: NotRequired[CustomWindowsMetricsTypedDict]


class HostWindowsMetrics(BaseModel):
    mode: Annotated[
        Optional[HostModeWindowsMetrics], PlainValidator(validate_open_enum(False))
    ] = HostModeWindowsMetrics.BASIC
    r"""Select level of detail for host metrics"""

    custom: Optional[CustomWindowsMetrics] = None

    @field_serializer("mode")
    def serialize_mode(self, value):
        if isinstance(value, str):
            try:
                return models.HostModeWindowsMetrics(value)
            except ValueError:
                return value
        return value


class SetWindowsMetricsTypedDict(TypedDict):
    name: str
    filter_: str
    include_children: NotRequired[bool]


class SetWindowsMetrics(BaseModel):
    name: str

    filter_: Annotated[str, pydantic.Field(alias="filter")]

    include_children: Annotated[
        Optional[bool], pydantic.Field(alias="includeChildren")
    ] = False


class ProcessWindowsMetricsTypedDict(TypedDict):
    sets: NotRequired[List[SetWindowsMetricsTypedDict]]
    r"""Configure sets to collect process metrics"""


class ProcessWindowsMetrics(BaseModel):
    sets: Optional[List[SetWindowsMetrics]] = None
    r"""Configure sets to collect process metrics"""


class MetadatumWindowsMetricsTypedDict(TypedDict):
    name: str
    value: str
    r"""JavaScript expression to compute field's value, enclosed in quotes or backticks. (Can evaluate to a constant.)"""


class MetadatumWindowsMetrics(BaseModel):
    name: str

    value: str
    r"""JavaScript expression to compute field's value, enclosed in quotes or backticks. (Can evaluate to a constant.)"""


class DataCompressionFormatWindowsMetrics(str, Enum, metaclass=utils.OpenEnumMeta):
    NONE = "none"
    GZIP = "gzip"


class PersistenceWindowsMetricsTypedDict(TypedDict):
    enable: NotRequired[bool]
    r"""Spool metrics to disk for Cribl Edge and Search"""
    time_window: NotRequired[str]
    r"""Time span for each file bucket"""
    max_data_size: NotRequired[str]
    r"""Maximum disk space allowed to be consumed (examples: 420MB, 4GB). When limit is reached, older data will be deleted."""
    max_data_time: NotRequired[str]
    r"""Maximum amount of time to retain data (examples: 2h, 4d). When limit is reached, older data will be deleted."""
    compress: NotRequired[DataCompressionFormatWindowsMetrics]
    dest_path: NotRequired[str]
    r"""Path to use to write metrics. Defaults to $CRIBL_HOME/state/windows_metrics"""


class PersistenceWindowsMetrics(BaseModel):
    enable: Optional[bool] = False
    r"""Spool metrics to disk for Cribl Edge and Search"""

    time_window: Annotated[Optional[str], pydantic.Field(alias="timeWindow")] = "10m"
    r"""Time span for each file bucket"""

    max_data_size: Annotated[Optional[str], pydantic.Field(alias="maxDataSize")] = "1GB"
    r"""Maximum disk space allowed to be consumed (examples: 420MB, 4GB). When limit is reached, older data will be deleted."""

    max_data_time: Annotated[Optional[str], pydantic.Field(alias="maxDataTime")] = "24h"
    r"""Maximum amount of time to retain data (examples: 2h, 4d). When limit is reached, older data will be deleted."""

    compress: Annotated[
        Optional[DataCompressionFormatWindowsMetrics],
        PlainValidator(validate_open_enum(False)),
    ] = DataCompressionFormatWindowsMetrics.GZIP

    dest_path: Annotated[Optional[str], pydantic.Field(alias="destPath")] = (
        "$CRIBL_HOME/state/windows_metrics"
    )
    r"""Path to use to write metrics. Defaults to $CRIBL_HOME/state/windows_metrics"""

    @field_serializer("compress")
    def serialize_compress(self, value):
        if isinstance(value, str):
            try:
                return models.DataCompressionFormatWindowsMetrics(value)
            except ValueError:
                return value
        return value


class InputWindowsMetricsTypedDict(TypedDict):
    type: TypeWindowsMetrics
    id: NotRequired[str]
    r"""Unique ID for this input"""
    disabled: NotRequired[bool]
    pipeline: NotRequired[str]
    r"""Pipeline to process data from this Source before sending it through the Routes"""
    send_to_routes: NotRequired[bool]
    r"""Select whether to send data to Routes, or directly to Destinations."""
    environment: NotRequired[str]
    r"""Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere."""
    pq_enabled: NotRequired[bool]
    r"""Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers)."""
    streamtags: NotRequired[List[str]]
    r"""Tags for filtering and grouping in @{product}"""
    connections: NotRequired[List[ConnectionWindowsMetricsTypedDict]]
    r"""Direct connections to Destinations, and optionally via a Pipeline or a Pack"""
    pq: NotRequired[PqWindowsMetricsTypedDict]
    interval: NotRequired[float]
    r"""Time, in seconds, between consecutive metric collections. Default is 10 seconds."""
    host: NotRequired[HostWindowsMetricsTypedDict]
    process: NotRequired[ProcessWindowsMetricsTypedDict]
    metadata: NotRequired[List[MetadatumWindowsMetricsTypedDict]]
    r"""Fields to add to events from this input"""
    persistence: NotRequired[PersistenceWindowsMetricsTypedDict]
    disable_native_module: NotRequired[bool]
    r"""Enable to use built-in tools (PowerShell) to collect metrics instead of native API (default) [Learn more](https://docs.cribl.io/edge/sources-windows-metrics/#advanced-tab)"""
    description: NotRequired[str]


class InputWindowsMetrics(BaseModel):
    model_config = ConfigDict(
        populate_by_name=True, arbitrary_types_allowed=True, extra="allow"
    )
    __pydantic_extra__: Dict[str, Any] = pydantic.Field(init=False)

    type: TypeWindowsMetrics

    id: Optional[str] = None
    r"""Unique ID for this input"""

    disabled: Optional[bool] = False

    pipeline: Optional[str] = None
    r"""Pipeline to process data from this Source before sending it through the Routes"""

    send_to_routes: Annotated[Optional[bool], pydantic.Field(alias="sendToRoutes")] = (
        True
    )
    r"""Select whether to send data to Routes, or directly to Destinations."""

    environment: Optional[str] = None
    r"""Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere."""

    pq_enabled: Annotated[Optional[bool], pydantic.Field(alias="pqEnabled")] = False
    r"""Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers)."""

    streamtags: Optional[List[str]] = None
    r"""Tags for filtering and grouping in @{product}"""

    connections: Optional[List[ConnectionWindowsMetrics]] = None
    r"""Direct connections to Destinations, and optionally via a Pipeline or a Pack"""

    pq: Optional[PqWindowsMetrics] = None

    interval: Optional[float] = 10
    r"""Time, in seconds, between consecutive metric collections. Default is 10 seconds."""

    host: Optional[HostWindowsMetrics] = None

    process: Optional[ProcessWindowsMetrics] = None

    metadata: Optional[List[MetadatumWindowsMetrics]] = None
    r"""Fields to add to events from this input"""

    persistence: Optional[PersistenceWindowsMetrics] = None

    disable_native_module: Annotated[
        Optional[bool], pydantic.Field(alias="disableNativeModule")
    ] = False
    r"""Enable to use built-in tools (PowerShell) to collect metrics instead of native API (default) [Learn more](https://docs.cribl.io/edge/sources-windows-metrics/#advanced-tab)"""

    description: Optional[str] = None

    @property
    def additional_properties(self):
        return self.__pydantic_extra__

    @additional_properties.setter
    def additional_properties(self, value):
        self.__pydantic_extra__ = value  # pyright: ignore[reportIncompatibleVariableOverride]


class TypeKubeEvents(str, Enum):
    KUBE_EVENTS = "kube_events"


class ConnectionKubeEventsTypedDict(TypedDict):
    output: str
    pipeline: NotRequired[str]


class ConnectionKubeEvents(BaseModel):
    output: str

    pipeline: Optional[str] = None


class ModeKubeEvents(str, Enum, metaclass=utils.OpenEnumMeta):
    r"""With Smart mode, PQ will write events to the filesystem only when it detects backpressure from the processing engine. With Always On mode, PQ will always write events directly to the queue before forwarding them to the processing engine."""

    # Smart
    SMART = "smart"
    # Always On
    ALWAYS = "always"


class CompressionKubeEvents(str, Enum, metaclass=utils.OpenEnumMeta):
    r"""Codec to use to compress the persisted data"""

    # None
    NONE = "none"
    # Gzip
    GZIP = "gzip"


class PqControlsKubeEventsTypedDict(TypedDict):
    pass


class PqControlsKubeEvents(BaseModel):
    pass


class PqKubeEventsTypedDict(TypedDict):
    mode: NotRequired[ModeKubeEvents]
    r"""With Smart mode, PQ will write events to the filesystem only when it detects backpressure from the processing engine. With Always On mode, PQ will always write events directly to the queue before forwarding them to the processing engine."""
    max_buffer_size: NotRequired[float]
    r"""The maximum number of events to hold in memory before writing the events to disk"""
    commit_frequency: NotRequired[float]
    r"""The number of events to send downstream before committing that Stream has read them"""
    max_file_size: NotRequired[str]
    r"""The maximum size to store in each queue file before closing and optionally compressing. Enter a numeral with units of KB, MB, etc."""
    max_size: NotRequired[str]
    r"""The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc."""
    path: NotRequired[str]
    r"""The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/inputs/<input-id>"""
    compress: NotRequired[CompressionKubeEvents]
    r"""Codec to use to compress the persisted data"""
    pq_controls: NotRequired[PqControlsKubeEventsTypedDict]


class PqKubeEvents(BaseModel):
    mode: Annotated[
        Optional[ModeKubeEvents], PlainValidator(validate_open_enum(False))
    ] = ModeKubeEvents.ALWAYS
    r"""With Smart mode, PQ will write events to the filesystem only when it detects backpressure from the processing engine. With Always On mode, PQ will always write events directly to the queue before forwarding them to the processing engine."""

    max_buffer_size: Annotated[
        Optional[float], pydantic.Field(alias="maxBufferSize")
    ] = 1000
    r"""The maximum number of events to hold in memory before writing the events to disk"""

    commit_frequency: Annotated[
        Optional[float], pydantic.Field(alias="commitFrequency")
    ] = 42
    r"""The number of events to send downstream before committing that Stream has read them"""

    max_file_size: Annotated[Optional[str], pydantic.Field(alias="maxFileSize")] = (
        "1 MB"
    )
    r"""The maximum size to store in each queue file before closing and optionally compressing. Enter a numeral with units of KB, MB, etc."""

    max_size: Annotated[Optional[str], pydantic.Field(alias="maxSize")] = "5GB"
    r"""The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc."""

    path: Optional[str] = "$CRIBL_HOME/state/queues"
    r"""The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/inputs/<input-id>"""

    compress: Annotated[
        Optional[CompressionKubeEvents], PlainValidator(validate_open_enum(False))
    ] = CompressionKubeEvents.NONE
    r"""Codec to use to compress the persisted data"""

    pq_controls: Annotated[
        Optional[PqControlsKubeEvents], pydantic.Field(alias="pqControls")
    ] = None

    @field_serializer("mode")
    def serialize_mode(self, value):
        if isinstance(value, str):
            try:
                return models.ModeKubeEvents(value)
            except ValueError:
                return value
        return value

    @field_serializer("compress")
    def serialize_compress(self, value):
        if isinstance(value, str):
            try:
                return models.CompressionKubeEvents(value)
            except ValueError:
                return value
        return value


class RuleKubeEventsTypedDict(TypedDict):
    filter_: str
    r"""JavaScript expression applied to Kubernetes objects. Return 'true' to include it."""
    description: NotRequired[str]
    r"""Optional description of this rule's purpose"""


class RuleKubeEvents(BaseModel):
    filter_: Annotated[str, pydantic.Field(alias="filter")]
    r"""JavaScript expression applied to Kubernetes objects. Return 'true' to include it."""

    description: Optional[str] = None
    r"""Optional description of this rule's purpose"""


class MetadatumKubeEventsTypedDict(TypedDict):
    name: str
    value: str
    r"""JavaScript expression to compute field's value, enclosed in quotes or backticks. (Can evaluate to a constant.)"""


class MetadatumKubeEvents(BaseModel):
    name: str

    value: str
    r"""JavaScript expression to compute field's value, enclosed in quotes or backticks. (Can evaluate to a constant.)"""


class InputKubeEventsTypedDict(TypedDict):
    type: TypeKubeEvents
    id: NotRequired[str]
    r"""Unique ID for this input"""
    disabled: NotRequired[bool]
    pipeline: NotRequired[str]
    r"""Pipeline to process data from this Source before sending it through the Routes"""
    send_to_routes: NotRequired[bool]
    r"""Select whether to send data to Routes, or directly to Destinations."""
    environment: NotRequired[str]
    r"""Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere."""
    pq_enabled: NotRequired[bool]
    r"""Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers)."""
    streamtags: NotRequired[List[str]]
    r"""Tags for filtering and grouping in @{product}"""
    connections: NotRequired[List[ConnectionKubeEventsTypedDict]]
    r"""Direct connections to Destinations, and optionally via a Pipeline or a Pack"""
    pq: NotRequired[PqKubeEventsTypedDict]
    rules: NotRequired[List[RuleKubeEventsTypedDict]]
    r"""Filtering on event fields"""
    metadata: NotRequired[List[MetadatumKubeEventsTypedDict]]
    r"""Fields to add to events from this input"""
    description: NotRequired[str]


class InputKubeEvents(BaseModel):
    model_config = ConfigDict(
        populate_by_name=True, arbitrary_types_allowed=True, extra="allow"
    )
    __pydantic_extra__: Dict[str, Any] = pydantic.Field(init=False)

    type: TypeKubeEvents

    id: Optional[str] = None
    r"""Unique ID for this input"""

    disabled: Optional[bool] = False

    pipeline: Optional[str] = None
    r"""Pipeline to process data from this Source before sending it through the Routes"""

    send_to_routes: Annotated[Optional[bool], pydantic.Field(alias="sendToRoutes")] = (
        True
    )
    r"""Select whether to send data to Routes, or directly to Destinations."""

    environment: Optional[str] = None
    r"""Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere."""

    pq_enabled: Annotated[Optional[bool], pydantic.Field(alias="pqEnabled")] = False
    r"""Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers)."""

    streamtags: Optional[List[str]] = None
    r"""Tags for filtering and grouping in @{product}"""

    connections: Optional[List[ConnectionKubeEvents]] = None
    r"""Direct connections to Destinations, and optionally via a Pipeline or a Pack"""

    pq: Optional[PqKubeEvents] = None

    rules: Optional[List[RuleKubeEvents]] = None
    r"""Filtering on event fields"""

    metadata: Optional[List[MetadatumKubeEvents]] = None
    r"""Fields to add to events from this input"""

    description: Optional[str] = None

    @property
    def additional_properties(self):
        return self.__pydantic_extra__

    @additional_properties.setter
    def additional_properties(self, value):
        self.__pydantic_extra__ = value  # pyright: ignore[reportIncompatibleVariableOverride]


class TypeKubeLogs(str, Enum):
    KUBE_LOGS = "kube_logs"


class ConnectionKubeLogsTypedDict(TypedDict):
    output: str
    pipeline: NotRequired[str]


class ConnectionKubeLogs(BaseModel):
    output: str

    pipeline: Optional[str] = None


class ModeKubeLogs(str, Enum, metaclass=utils.OpenEnumMeta):
    r"""With Smart mode, PQ will write events to the filesystem only when it detects backpressure from the processing engine. With Always On mode, PQ will always write events directly to the queue before forwarding them to the processing engine."""

    # Smart
    SMART = "smart"
    # Always On
    ALWAYS = "always"


class PqCompressionKubeLogs(str, Enum, metaclass=utils.OpenEnumMeta):
    r"""Codec to use to compress the persisted data"""

    # None
    NONE = "none"
    # Gzip
    GZIP = "gzip"


class PqControlsKubeLogsTypedDict(TypedDict):
    pass


class PqControlsKubeLogs(BaseModel):
    pass


class PqKubeLogsTypedDict(TypedDict):
    mode: NotRequired[ModeKubeLogs]
    r"""With Smart mode, PQ will write events to the filesystem only when it detects backpressure from the processing engine. With Always On mode, PQ will always write events directly to the queue before forwarding them to the processing engine."""
    max_buffer_size: NotRequired[float]
    r"""The maximum number of events to hold in memory before writing the events to disk"""
    commit_frequency: NotRequired[float]
    r"""The number of events to send downstream before committing that Stream has read them"""
    max_file_size: NotRequired[str]
    r"""The maximum size to store in each queue file before closing and optionally compressing. Enter a numeral with units of KB, MB, etc."""
    max_size: NotRequired[str]
    r"""The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc."""
    path: NotRequired[str]
    r"""The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/inputs/<input-id>"""
    compress: NotRequired[PqCompressionKubeLogs]
    r"""Codec to use to compress the persisted data"""
    pq_controls: NotRequired[PqControlsKubeLogsTypedDict]


class PqKubeLogs(BaseModel):
    mode: Annotated[
        Optional[ModeKubeLogs], PlainValidator(validate_open_enum(False))
    ] = ModeKubeLogs.ALWAYS
    r"""With Smart mode, PQ will write events to the filesystem only when it detects backpressure from the processing engine. With Always On mode, PQ will always write events directly to the queue before forwarding them to the processing engine."""

    max_buffer_size: Annotated[
        Optional[float], pydantic.Field(alias="maxBufferSize")
    ] = 1000
    r"""The maximum number of events to hold in memory before writing the events to disk"""

    commit_frequency: Annotated[
        Optional[float], pydantic.Field(alias="commitFrequency")
    ] = 42
    r"""The number of events to send downstream before committing that Stream has read them"""

    max_file_size: Annotated[Optional[str], pydantic.Field(alias="maxFileSize")] = (
        "1 MB"
    )
    r"""The maximum size to store in each queue file before closing and optionally compressing. Enter a numeral with units of KB, MB, etc."""

    max_size: Annotated[Optional[str], pydantic.Field(alias="maxSize")] = "5GB"
    r"""The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc."""

    path: Optional[str] = "$CRIBL_HOME/state/queues"
    r"""The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/inputs/<input-id>"""

    compress: Annotated[
        Optional[PqCompressionKubeLogs], PlainValidator(validate_open_enum(False))
    ] = PqCompressionKubeLogs.NONE
    r"""Codec to use to compress the persisted data"""

    pq_controls: Annotated[
        Optional[PqControlsKubeLogs], pydantic.Field(alias="pqControls")
    ] = None

    @field_serializer("mode")
    def serialize_mode(self, value):
        if isinstance(value, str):
            try:
                return models.ModeKubeLogs(value)
            except ValueError:
                return value
        return value

    @field_serializer("compress")
    def serialize_compress(self, value):
        if isinstance(value, str):
            try:
                return models.PqCompressionKubeLogs(value)
            except ValueError:
                return value
        return value


class RuleKubeLogsTypedDict(TypedDict):
    filter_: str
    r"""JavaScript expression applied to Pod objects. Return 'true' to include it."""
    description: NotRequired[str]
    r"""Optional description of this rule's purpose"""


class RuleKubeLogs(BaseModel):
    filter_: Annotated[str, pydantic.Field(alias="filter")]
    r"""JavaScript expression applied to Pod objects. Return 'true' to include it."""

    description: Optional[str] = None
    r"""Optional description of this rule's purpose"""


class MetadatumKubeLogsTypedDict(TypedDict):
    name: str
    value: str
    r"""JavaScript expression to compute field's value, enclosed in quotes or backticks. (Can evaluate to a constant.)"""


class MetadatumKubeLogs(BaseModel):
    name: str

    value: str
    r"""JavaScript expression to compute field's value, enclosed in quotes or backticks. (Can evaluate to a constant.)"""


class PersistenceCompressionKubeLogs(str, Enum, metaclass=utils.OpenEnumMeta):
    r"""Data compression format. Default is gzip."""

    NONE = "none"
    GZIP = "gzip"


class DiskSpoolingKubeLogsTypedDict(TypedDict):
    enable: NotRequired[bool]
    r"""Spool events on disk for Cribl Edge and Search. Default is disabled."""
    time_window: NotRequired[str]
    r"""Time period for grouping spooled events. Default is 10m."""
    max_data_size: NotRequired[str]
    r"""Maximum disk space that can be consumed before older buckets are deleted. Examples: 420MB, 4GB. Default is 1GB."""
    max_data_time: NotRequired[str]
    r"""Maximum amount of time to retain data before older buckets are deleted. Examples: 2h, 4d. Default is 24h."""
    compress: NotRequired[PersistenceCompressionKubeLogs]
    r"""Data compression format. Default is gzip."""


class DiskSpoolingKubeLogs(BaseModel):
    enable: Optional[bool] = False
    r"""Spool events on disk for Cribl Edge and Search. Default is disabled."""

    time_window: Annotated[Optional[str], pydantic.Field(alias="timeWindow")] = "10m"
    r"""Time period for grouping spooled events. Default is 10m."""

    max_data_size: Annotated[Optional[str], pydantic.Field(alias="maxDataSize")] = "1GB"
    r"""Maximum disk space that can be consumed before older buckets are deleted. Examples: 420MB, 4GB. Default is 1GB."""

    max_data_time: Annotated[Optional[str], pydantic.Field(alias="maxDataTime")] = "24h"
    r"""Maximum amount of time to retain data before older buckets are deleted. Examples: 2h, 4d. Default is 24h."""

    compress: Annotated[
        Optional[PersistenceCompressionKubeLogs],
        PlainValidator(validate_open_enum(False)),
    ] = PersistenceCompressionKubeLogs.GZIP
    r"""Data compression format. Default is gzip."""

    @field_serializer("compress")
    def serialize_compress(self, value):
        if isinstance(value, str):
            try:
                return models.PersistenceCompressionKubeLogs(value)
            except ValueError:
                return value
        return value


class InputKubeLogsTypedDict(TypedDict):
    type: TypeKubeLogs
    id: NotRequired[str]
    r"""Unique ID for this input"""
    disabled: NotRequired[bool]
    pipeline: NotRequired[str]
    r"""Pipeline to process data from this Source before sending it through the Routes"""
    send_to_routes: NotRequired[bool]
    r"""Select whether to send data to Routes, or directly to Destinations."""
    environment: NotRequired[str]
    r"""Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere."""
    pq_enabled: NotRequired[bool]
    r"""Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers)."""
    streamtags: NotRequired[List[str]]
    r"""Tags for filtering and grouping in @{product}"""
    connections: NotRequired[List[ConnectionKubeLogsTypedDict]]
    r"""Direct connections to Destinations, and optionally via a Pipeline or a Pack"""
    pq: NotRequired[PqKubeLogsTypedDict]
    interval: NotRequired[float]
    r"""Time, in seconds, between checks for new containers. Default is 15 secs."""
    rules: NotRequired[List[RuleKubeLogsTypedDict]]
    r"""Add rules to decide which Pods to collect logs from. Logs are collected if no rules are given or if all the rules' expressions evaluate to true."""
    timestamps: NotRequired[bool]
    r"""For use when containers do not emit a timestamp, prefix each line of output with a timestamp. If you enable this setting, you can use the Kubernetes Logs Event Breaker and the kubernetes_logs Pre-processing Pipeline to remove them from the events after the timestamps are extracted."""
    metadata: NotRequired[List[MetadatumKubeLogsTypedDict]]
    r"""Fields to add to events from this input"""
    persistence: NotRequired[DiskSpoolingKubeLogsTypedDict]
    breaker_rulesets: NotRequired[List[str]]
    r"""A list of event-breaking rulesets that will be applied, in order, to the input data stream"""
    stale_channel_flush_ms: NotRequired[float]
    r"""How long (in milliseconds) the Event Breaker will wait for new data to be sent to a specific channel before flushing the data stream out, as is, to the Pipelines"""
    enable_load_balancing: NotRequired[bool]
    r"""Load balance traffic across all Worker Processes"""
    description: NotRequired[str]


class InputKubeLogs(BaseModel):
    model_config = ConfigDict(
        populate_by_name=True, arbitrary_types_allowed=True, extra="allow"
    )
    __pydantic_extra__: Dict[str, Any] = pydantic.Field(init=False)

    type: TypeKubeLogs

    id: Optional[str] = None
    r"""Unique ID for this input"""

    disabled: Optional[bool] = False

    pipeline: Optional[str] = None
    r"""Pipeline to process data from this Source before sending it through the Routes"""

    send_to_routes: Annotated[Optional[bool], pydantic.Field(alias="sendToRoutes")] = (
        True
    )
    r"""Select whether to send data to Routes, or directly to Destinations."""

    environment: Optional[str] = None
    r"""Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere."""

    pq_enabled: Annotated[Optional[bool], pydantic.Field(alias="pqEnabled")] = False
    r"""Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers)."""

    streamtags: Optional[List[str]] = None
    r"""Tags for filtering and grouping in @{product}"""

    connections: Optional[List[ConnectionKubeLogs]] = None
    r"""Direct connections to Destinations, and optionally via a Pipeline or a Pack"""

    pq: Optional[PqKubeLogs] = None

    interval: Optional[float] = 15
    r"""Time, in seconds, between checks for new containers. Default is 15 secs."""

    rules: Optional[List[RuleKubeLogs]] = None
    r"""Add rules to decide which Pods to collect logs from. Logs are collected if no rules are given or if all the rules' expressions evaluate to true."""

    timestamps: Optional[bool] = False
    r"""For use when containers do not emit a timestamp, prefix each line of output with a timestamp. If you enable this setting, you can use the Kubernetes Logs Event Breaker and the kubernetes_logs Pre-processing Pipeline to remove them from the events after the timestamps are extracted."""

    metadata: Optional[List[MetadatumKubeLogs]] = None
    r"""Fields to add to events from this input"""

    persistence: Optional[DiskSpoolingKubeLogs] = None

    breaker_rulesets: Annotated[
        Optional[List[str]], pydantic.Field(alias="breakerRulesets")
    ] = None
    r"""A list of event-breaking rulesets that will be applied, in order, to the input data stream"""

    stale_channel_flush_ms: Annotated[
        Optional[float], pydantic.Field(alias="staleChannelFlushMs")
    ] = 10000
    r"""How long (in milliseconds) the Event Breaker will wait for new data to be sent to a specific channel before flushing the data stream out, as is, to the Pipelines"""

    enable_load_balancing: Annotated[
        Optional[bool], pydantic.Field(alias="enableLoadBalancing")
    ] = False
    r"""Load balance traffic across all Worker Processes"""

    description: Optional[str] = None

    @property
    def additional_properties(self):
        return self.__pydantic_extra__

    @additional_properties.setter
    def additional_properties(self, value):
        self.__pydantic_extra__ = value  # pyright: ignore[reportIncompatibleVariableOverride]


class TypeKubeMetrics(str, Enum):
    KUBE_METRICS = "kube_metrics"


class ConnectionKubeMetricsTypedDict(TypedDict):
    output: str
    pipeline: NotRequired[str]


class ConnectionKubeMetrics(BaseModel):
    output: str

    pipeline: Optional[str] = None


class ModeKubeMetrics(str, Enum, metaclass=utils.OpenEnumMeta):
    r"""With Smart mode, PQ will write events to the filesystem only when it detects backpressure from the processing engine. With Always On mode, PQ will always write events directly to the queue before forwarding them to the processing engine."""

    # Smart
    SMART = "smart"
    # Always On
    ALWAYS = "always"


class CompressionKubeMetrics(str, Enum, metaclass=utils.OpenEnumMeta):
    r"""Codec to use to compress the persisted data"""

    # None
    NONE = "none"
    # Gzip
    GZIP = "gzip"


class PqControlsKubeMetricsTypedDict(TypedDict):
    pass


class PqControlsKubeMetrics(BaseModel):
    pass


class PqKubeMetricsTypedDict(TypedDict):
    mode: NotRequired[ModeKubeMetrics]
    r"""With Smart mode, PQ will write events to the filesystem only when it detects backpressure from the processing engine. With Always On mode, PQ will always write events directly to the queue before forwarding them to the processing engine."""
    max_buffer_size: NotRequired[float]
    r"""The maximum number of events to hold in memory before writing the events to disk"""
    commit_frequency: NotRequired[float]
    r"""The number of events to send downstream before committing that Stream has read them"""
    max_file_size: NotRequired[str]
    r"""The maximum size to store in each queue file before closing and optionally compressing. Enter a numeral with units of KB, MB, etc."""
    max_size: NotRequired[str]
    r"""The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc."""
    path: NotRequired[str]
    r"""The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/inputs/<input-id>"""
    compress: NotRequired[CompressionKubeMetrics]
    r"""Codec to use to compress the persisted data"""
    pq_controls: NotRequired[PqControlsKubeMetricsTypedDict]


class PqKubeMetrics(BaseModel):
    mode: Annotated[
        Optional[ModeKubeMetrics], PlainValidator(validate_open_enum(False))
    ] = ModeKubeMetrics.ALWAYS
    r"""With Smart mode, PQ will write events to the filesystem only when it detects backpressure from the processing engine. With Always On mode, PQ will always write events directly to the queue before forwarding them to the processing engine."""

    max_buffer_size: Annotated[
        Optional[float], pydantic.Field(alias="maxBufferSize")
    ] = 1000
    r"""The maximum number of events to hold in memory before writing the events to disk"""

    commit_frequency: Annotated[
        Optional[float], pydantic.Field(alias="commitFrequency")
    ] = 42
    r"""The number of events to send downstream before committing that Stream has read them"""

    max_file_size: Annotated[Optional[str], pydantic.Field(alias="maxFileSize")] = (
        "1 MB"
    )
    r"""The maximum size to store in each queue file before closing and optionally compressing. Enter a numeral with units of KB, MB, etc."""

    max_size: Annotated[Optional[str], pydantic.Field(alias="maxSize")] = "5GB"
    r"""The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc."""

    path: Optional[str] = "$CRIBL_HOME/state/queues"
    r"""The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/inputs/<input-id>"""

    compress: Annotated[
        Optional[CompressionKubeMetrics], PlainValidator(validate_open_enum(False))
    ] = CompressionKubeMetrics.NONE
    r"""Codec to use to compress the persisted data"""

    pq_controls: Annotated[
        Optional[PqControlsKubeMetrics], pydantic.Field(alias="pqControls")
    ] = None

    @field_serializer("mode")
    def serialize_mode(self, value):
        if isinstance(value, str):
            try:
                return models.ModeKubeMetrics(value)
            except ValueError:
                return value
        return value

    @field_serializer("compress")
    def serialize_compress(self, value):
        if isinstance(value, str):
            try:
                return models.CompressionKubeMetrics(value)
            except ValueError:
                return value
        return value


class RuleKubeMetricsTypedDict(TypedDict):
    filter_: str
    r"""JavaScript expression applied to Kubernetes objects. Return 'true' to include it."""
    description: NotRequired[str]
    r"""Optional description of this rule's purpose"""


class RuleKubeMetrics(BaseModel):
    filter_: Annotated[str, pydantic.Field(alias="filter")]
    r"""JavaScript expression applied to Kubernetes objects. Return 'true' to include it."""

    description: Optional[str] = None
    r"""Optional description of this rule's purpose"""


class MetadatumKubeMetricsTypedDict(TypedDict):
    name: str
    value: str
    r"""JavaScript expression to compute field's value, enclosed in quotes or backticks. (Can evaluate to a constant.)"""


class MetadatumKubeMetrics(BaseModel):
    name: str

    value: str
    r"""JavaScript expression to compute field's value, enclosed in quotes or backticks. (Can evaluate to a constant.)"""


class DataCompressionFormatKubeMetrics(str, Enum, metaclass=utils.OpenEnumMeta):
    NONE = "none"
    GZIP = "gzip"


class PersistenceKubeMetricsTypedDict(TypedDict):
    enable: NotRequired[bool]
    r"""Spool metrics on disk for Cribl Search"""
    time_window: NotRequired[str]
    r"""Time span for each file bucket"""
    max_data_size: NotRequired[str]
    r"""Maximum disk space allowed to be consumed (examples: 420MB, 4GB). When limit is reached, older data will be deleted."""
    max_data_time: NotRequired[str]
    r"""Maximum amount of time to retain data (examples: 2h, 4d). When limit is reached, older data will be deleted."""
    compress: NotRequired[DataCompressionFormatKubeMetrics]
    dest_path: NotRequired[str]
    r"""Path to use to write metrics. Defaults to $CRIBL_HOME/state/<id>"""


class PersistenceKubeMetrics(BaseModel):
    enable: Optional[bool] = False
    r"""Spool metrics on disk for Cribl Search"""

    time_window: Annotated[Optional[str], pydantic.Field(alias="timeWindow")] = "10m"
    r"""Time span for each file bucket"""

    max_data_size: Annotated[Optional[str], pydantic.Field(alias="maxDataSize")] = "1GB"
    r"""Maximum disk space allowed to be consumed (examples: 420MB, 4GB). When limit is reached, older data will be deleted."""

    max_data_time: Annotated[Optional[str], pydantic.Field(alias="maxDataTime")] = "24h"
    r"""Maximum amount of time to retain data (examples: 2h, 4d). When limit is reached, older data will be deleted."""

    compress: Annotated[
        Optional[DataCompressionFormatKubeMetrics],
        PlainValidator(validate_open_enum(False)),
    ] = DataCompressionFormatKubeMetrics.GZIP

    dest_path: Annotated[Optional[str], pydantic.Field(alias="destPath")] = (
        "$CRIBL_HOME/state/kube_metrics"
    )
    r"""Path to use to write metrics. Defaults to $CRIBL_HOME/state/<id>"""

    @field_serializer("compress")
    def serialize_compress(self, value):
        if isinstance(value, str):
            try:
                return models.DataCompressionFormatKubeMetrics(value)
            except ValueError:
                return value
        return value


class InputKubeMetricsTypedDict(TypedDict):
    type: TypeKubeMetrics
    id: NotRequired[str]
    r"""Unique ID for this input"""
    disabled: NotRequired[bool]
    pipeline: NotRequired[str]
    r"""Pipeline to process data from this Source before sending it through the Routes"""
    send_to_routes: NotRequired[bool]
    r"""Select whether to send data to Routes, or directly to Destinations."""
    environment: NotRequired[str]
    r"""Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere."""
    pq_enabled: NotRequired[bool]
    r"""Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers)."""
    streamtags: NotRequired[List[str]]
    r"""Tags for filtering and grouping in @{product}"""
    connections: NotRequired[List[ConnectionKubeMetricsTypedDict]]
    r"""Direct connections to Destinations, and optionally via a Pipeline or a Pack"""
    pq: NotRequired[PqKubeMetricsTypedDict]
    interval: NotRequired[float]
    r"""Time, in seconds, between consecutive metrics collections. Default is 15 secs."""
    rules: NotRequired[List[RuleKubeMetricsTypedDict]]
    r"""Add rules to decide which Kubernetes objects to generate metrics for. Events are generated if no rules are given or of all the rules' expressions evaluate to true."""
    metadata: NotRequired[List[MetadatumKubeMetricsTypedDict]]
    r"""Fields to add to events from this input"""
    persistence: NotRequired[PersistenceKubeMetricsTypedDict]
    description: NotRequired[str]


class InputKubeMetrics(BaseModel):
    model_config = ConfigDict(
        populate_by_name=True, arbitrary_types_allowed=True, extra="allow"
    )
    __pydantic_extra__: Dict[str, Any] = pydantic.Field(init=False)

    type: TypeKubeMetrics

    id: Optional[str] = None
    r"""Unique ID for this input"""

    disabled: Optional[bool] = False

    pipeline: Optional[str] = None
    r"""Pipeline to process data from this Source before sending it through the Routes"""

    send_to_routes: Annotated[Optional[bool], pydantic.Field(alias="sendToRoutes")] = (
        True
    )
    r"""Select whether to send data to Routes, or directly to Destinations."""

    environment: Optional[str] = None
    r"""Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere."""

    pq_enabled: Annotated[Optional[bool], pydantic.Field(alias="pqEnabled")] = False
    r"""Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers)."""

    streamtags: Optional[List[str]] = None
    r"""Tags for filtering and grouping in @{product}"""

    connections: Optional[List[ConnectionKubeMetrics]] = None
    r"""Direct connections to Destinations, and optionally via a Pipeline or a Pack"""

    pq: Optional[PqKubeMetrics] = None

    interval: Optional[float] = 15
    r"""Time, in seconds, between consecutive metrics collections. Default is 15 secs."""

    rules: Optional[List[RuleKubeMetrics]] = None
    r"""Add rules to decide which Kubernetes objects to generate metrics for. Events are generated if no rules are given or of all the rules' expressions evaluate to true."""

    metadata: Optional[List[MetadatumKubeMetrics]] = None
    r"""Fields to add to events from this input"""

    persistence: Optional[PersistenceKubeMetrics] = None

    description: Optional[str] = None

    @property
    def additional_properties(self):
        return self.__pydantic_extra__

    @additional_properties.setter
    def additional_properties(self, value):
        self.__pydantic_extra__ = value  # pyright: ignore[reportIncompatibleVariableOverride]


class TypeSystemState(str, Enum):
    SYSTEM_STATE = "system_state"


class ConnectionSystemStateTypedDict(TypedDict):
    output: str
    pipeline: NotRequired[str]


class ConnectionSystemState(BaseModel):
    output: str

    pipeline: Optional[str] = None


class ModeSystemState(str, Enum, metaclass=utils.OpenEnumMeta):
    r"""With Smart mode, PQ will write events to the filesystem only when it detects backpressure from the processing engine. With Always On mode, PQ will always write events directly to the queue before forwarding them to the processing engine."""

    # Smart
    SMART = "smart"
    # Always On
    ALWAYS = "always"


class CompressionSystemState(str, Enum, metaclass=utils.OpenEnumMeta):
    r"""Codec to use to compress the persisted data"""

    # None
    NONE = "none"
    # Gzip
    GZIP = "gzip"


class PqControlsSystemStateTypedDict(TypedDict):
    pass


class PqControlsSystemState(BaseModel):
    pass


class PqSystemStateTypedDict(TypedDict):
    mode: NotRequired[ModeSystemState]
    r"""With Smart mode, PQ will write events to the filesystem only when it detects backpressure from the processing engine. With Always On mode, PQ will always write events directly to the queue before forwarding them to the processing engine."""
    max_buffer_size: NotRequired[float]
    r"""The maximum number of events to hold in memory before writing the events to disk"""
    commit_frequency: NotRequired[float]
    r"""The number of events to send downstream before committing that Stream has read them"""
    max_file_size: NotRequired[str]
    r"""The maximum size to store in each queue file before closing and optionally compressing. Enter a numeral with units of KB, MB, etc."""
    max_size: NotRequired[str]
    r"""The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc."""
    path: NotRequired[str]
    r"""The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/inputs/<input-id>"""
    compress: NotRequired[CompressionSystemState]
    r"""Codec to use to compress the persisted data"""
    pq_controls: NotRequired[PqControlsSystemStateTypedDict]


class PqSystemState(BaseModel):
    mode: Annotated[
        Optional[ModeSystemState], PlainValidator(validate_open_enum(False))
    ] = ModeSystemState.ALWAYS
    r"""With Smart mode, PQ will write events to the filesystem only when it detects backpressure from the processing engine. With Always On mode, PQ will always write events directly to the queue before forwarding them to the processing engine."""

    max_buffer_size: Annotated[
        Optional[float], pydantic.Field(alias="maxBufferSize")
    ] = 1000
    r"""The maximum number of events to hold in memory before writing the events to disk"""

    commit_frequency: Annotated[
        Optional[float], pydantic.Field(alias="commitFrequency")
    ] = 42
    r"""The number of events to send downstream before committing that Stream has read them"""

    max_file_size: Annotated[Optional[str], pydantic.Field(alias="maxFileSize")] = (
        "1 MB"
    )
    r"""The maximum size to store in each queue file before closing and optionally compressing. Enter a numeral with units of KB, MB, etc."""

    max_size: Annotated[Optional[str], pydantic.Field(alias="maxSize")] = "5GB"
    r"""The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc."""

    path: Optional[str] = "$CRIBL_HOME/state/queues"
    r"""The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/inputs/<input-id>"""

    compress: Annotated[
        Optional[CompressionSystemState], PlainValidator(validate_open_enum(False))
    ] = CompressionSystemState.NONE
    r"""Codec to use to compress the persisted data"""

    pq_controls: Annotated[
        Optional[PqControlsSystemState], pydantic.Field(alias="pqControls")
    ] = None

    @field_serializer("mode")
    def serialize_mode(self, value):
        if isinstance(value, str):
            try:
                return models.ModeSystemState(value)
            except ValueError:
                return value
        return value

    @field_serializer("compress")
    def serialize_compress(self, value):
        if isinstance(value, str):
            try:
                return models.CompressionSystemState(value)
            except ValueError:
                return value
        return value


class MetadatumSystemStateTypedDict(TypedDict):
    name: str
    value: str
    r"""JavaScript expression to compute field's value, enclosed in quotes or backticks. (Can evaluate to a constant.)"""


class MetadatumSystemState(BaseModel):
    name: str

    value: str
    r"""JavaScript expression to compute field's value, enclosed in quotes or backticks. (Can evaluate to a constant.)"""


class HostsFileTypedDict(TypedDict):
    r"""Creates events based on entries collected from the hosts file"""

    enable: NotRequired[bool]


class HostsFile(BaseModel):
    r"""Creates events based on entries collected from the hosts file"""

    enable: Optional[bool] = True


class InterfacesTypedDict(TypedDict):
    r"""Creates events for each of the host’s network interfaces"""

    enable: NotRequired[bool]


class Interfaces(BaseModel):
    r"""Creates events for each of the host’s network interfaces"""

    enable: Optional[bool] = True


class DisksAndFileSystemsTypedDict(TypedDict):
    r"""Creates events for physical disks, partitions, and file systems"""

    enable: NotRequired[bool]


class DisksAndFileSystems(BaseModel):
    r"""Creates events for physical disks, partitions, and file systems"""

    enable: Optional[bool] = True


class HostInfoTypedDict(TypedDict):
    r"""Creates events based on the host system’s current state"""

    enable: NotRequired[bool]


class HostInfo(BaseModel):
    r"""Creates events based on the host system’s current state"""

    enable: Optional[bool] = True


class InputRoutesTypedDict(TypedDict):
    r"""Creates events based on entries collected from the host’s network routes"""

    enable: NotRequired[bool]


class InputRoutes(BaseModel):
    r"""Creates events based on entries collected from the host’s network routes"""

    enable: Optional[bool] = True


class DNSTypedDict(TypedDict):
    r"""Creates events for DNS resolvers and search entries"""

    enable: NotRequired[bool]


class DNS(BaseModel):
    r"""Creates events for DNS resolvers and search entries"""

    enable: Optional[bool] = True


class UsersAndGroupsTypedDict(TypedDict):
    r"""Creates events for local users and groups"""

    enable: NotRequired[bool]


class UsersAndGroups(BaseModel):
    r"""Creates events for local users and groups"""

    enable: Optional[bool] = True


class FirewallTypedDict(TypedDict):
    r"""Creates events for Firewall rules entries"""

    enable: NotRequired[bool]


class Firewall(BaseModel):
    r"""Creates events for Firewall rules entries"""

    enable: Optional[bool] = True


class ServicesTypedDict(TypedDict):
    r"""Creates events from the list of services"""

    enable: NotRequired[bool]


class Services(BaseModel):
    r"""Creates events from the list of services"""

    enable: Optional[bool] = True


class ListeningPortsTypedDict(TypedDict):
    r"""Creates events from list of listening ports"""

    enable: NotRequired[bool]


class ListeningPorts(BaseModel):
    r"""Creates events from list of listening ports"""

    enable: Optional[bool] = True


class LoggedInUsersTypedDict(TypedDict):
    r"""Creates events from list of logged-in users"""

    enable: NotRequired[bool]


class LoggedInUsers(BaseModel):
    r"""Creates events from list of logged-in users"""

    enable: Optional[bool] = True


class CollectorsTypedDict(TypedDict):
    hostsfile: NotRequired[HostsFileTypedDict]
    r"""Creates events based on entries collected from the hosts file"""
    interfaces: NotRequired[InterfacesTypedDict]
    r"""Creates events for each of the host’s network interfaces"""
    disk: NotRequired[DisksAndFileSystemsTypedDict]
    r"""Creates events for physical disks, partitions, and file systems"""
    metadata: NotRequired[HostInfoTypedDict]
    r"""Creates events based on the host system’s current state"""
    routes: NotRequired[InputRoutesTypedDict]
    r"""Creates events based on entries collected from the host’s network routes"""
    dns: NotRequired[DNSTypedDict]
    r"""Creates events for DNS resolvers and search entries"""
    user: NotRequired[UsersAndGroupsTypedDict]
    r"""Creates events for local users and groups"""
    firewall: NotRequired[FirewallTypedDict]
    r"""Creates events for Firewall rules entries"""
    services: NotRequired[ServicesTypedDict]
    r"""Creates events from the list of services"""
    ports: NotRequired[ListeningPortsTypedDict]
    r"""Creates events from list of listening ports"""
    login_users: NotRequired[LoggedInUsersTypedDict]
    r"""Creates events from list of logged-in users"""


class Collectors(BaseModel):
    hostsfile: Optional[HostsFile] = None
    r"""Creates events based on entries collected from the hosts file"""

    interfaces: Optional[Interfaces] = None
    r"""Creates events for each of the host’s network interfaces"""

    disk: Optional[DisksAndFileSystems] = None
    r"""Creates events for physical disks, partitions, and file systems"""

    metadata: Optional[HostInfo] = None
    r"""Creates events based on the host system’s current state"""

    routes: Optional[InputRoutes] = None
    r"""Creates events based on entries collected from the host’s network routes"""

    dns: Optional[DNS] = None
    r"""Creates events for DNS resolvers and search entries"""

    user: Optional[UsersAndGroups] = None
    r"""Creates events for local users and groups"""

    firewall: Optional[Firewall] = None
    r"""Creates events for Firewall rules entries"""

    services: Optional[Services] = None
    r"""Creates events from the list of services"""

    ports: Optional[ListeningPorts] = None
    r"""Creates events from list of listening ports"""

    login_users: Annotated[
        Optional[LoggedInUsers], pydantic.Field(alias="loginUsers")
    ] = None
    r"""Creates events from list of logged-in users"""


class DataCompressionFormatSystemState(str, Enum, metaclass=utils.OpenEnumMeta):
    NONE = "none"
    GZIP = "gzip"


class PersistenceSystemStateTypedDict(TypedDict):
    enable: NotRequired[bool]
    r"""Spool metrics to disk for Cribl Edge and Search"""
    time_window: NotRequired[str]
    r"""Time span for each file bucket"""
    max_data_size: NotRequired[str]
    r"""Maximum disk space allowed to be consumed (examples: 420MB, 4GB). When limit is reached, older data will be deleted."""
    max_data_time: NotRequired[str]
    r"""Maximum amount of time to retain data (examples: 2h, 4d). When limit is reached, older data will be deleted."""
    compress: NotRequired[DataCompressionFormatSystemState]
    dest_path: NotRequired[str]
    r"""Path to use to write metrics. Defaults to $CRIBL_HOME/state/system_state"""


class PersistenceSystemState(BaseModel):
    enable: Optional[bool] = False
    r"""Spool metrics to disk for Cribl Edge and Search"""

    time_window: Annotated[Optional[str], pydantic.Field(alias="timeWindow")] = "10m"
    r"""Time span for each file bucket"""

    max_data_size: Annotated[Optional[str], pydantic.Field(alias="maxDataSize")] = "1GB"
    r"""Maximum disk space allowed to be consumed (examples: 420MB, 4GB). When limit is reached, older data will be deleted."""

    max_data_time: Annotated[Optional[str], pydantic.Field(alias="maxDataTime")] = "24h"
    r"""Maximum amount of time to retain data (examples: 2h, 4d). When limit is reached, older data will be deleted."""

    compress: Annotated[
        Optional[DataCompressionFormatSystemState],
        PlainValidator(validate_open_enum(False)),
    ] = DataCompressionFormatSystemState.NONE

    dest_path: Annotated[Optional[str], pydantic.Field(alias="destPath")] = (
        "$CRIBL_HOME/state/system_state"
    )
    r"""Path to use to write metrics. Defaults to $CRIBL_HOME/state/system_state"""

    @field_serializer("compress")
    def serialize_compress(self, value):
        if isinstance(value, str):
            try:
                return models.DataCompressionFormatSystemState(value)
            except ValueError:
                return value
        return value


class InputSystemStateTypedDict(TypedDict):
    type: TypeSystemState
    id: NotRequired[str]
    r"""Unique ID for this input"""
    disabled: NotRequired[bool]
    pipeline: NotRequired[str]
    r"""Pipeline to process data from this Source before sending it through the Routes"""
    send_to_routes: NotRequired[bool]
    r"""Select whether to send data to Routes, or directly to Destinations."""
    environment: NotRequired[str]
    r"""Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere."""
    pq_enabled: NotRequired[bool]
    r"""Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers)."""
    streamtags: NotRequired[List[str]]
    r"""Tags for filtering and grouping in @{product}"""
    connections: NotRequired[List[ConnectionSystemStateTypedDict]]
    r"""Direct connections to Destinations, and optionally via a Pipeline or a Pack"""
    pq: NotRequired[PqSystemStateTypedDict]
    interval: NotRequired[float]
    r"""Time, in seconds, between consecutive state collections. Default is 300 seconds (5 minutes)."""
    metadata: NotRequired[List[MetadatumSystemStateTypedDict]]
    r"""Fields to add to events from this input"""
    collectors: NotRequired[CollectorsTypedDict]
    persistence: NotRequired[PersistenceSystemStateTypedDict]
    disable_native_module: NotRequired[bool]
    r"""Enable to use built-in tools (PowerShell) to collect events instead of native API (default) [Learn more](https://docs.cribl.io/edge/sources-system-state/#advanced-tab)"""
    description: NotRequired[str]


class InputSystemState(BaseModel):
    model_config = ConfigDict(
        populate_by_name=True, arbitrary_types_allowed=True, extra="allow"
    )
    __pydantic_extra__: Dict[str, Any] = pydantic.Field(init=False)

    type: TypeSystemState

    id: Optional[str] = None
    r"""Unique ID for this input"""

    disabled: Optional[bool] = False

    pipeline: Optional[str] = None
    r"""Pipeline to process data from this Source before sending it through the Routes"""

    send_to_routes: Annotated[Optional[bool], pydantic.Field(alias="sendToRoutes")] = (
        True
    )
    r"""Select whether to send data to Routes, or directly to Destinations."""

    environment: Optional[str] = None
    r"""Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere."""

    pq_enabled: Annotated[Optional[bool], pydantic.Field(alias="pqEnabled")] = False
    r"""Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers)."""

    streamtags: Optional[List[str]] = None
    r"""Tags for filtering and grouping in @{product}"""

    connections: Optional[List[ConnectionSystemState]] = None
    r"""Direct connections to Destinations, and optionally via a Pipeline or a Pack"""

    pq: Optional[PqSystemState] = None

    interval: Optional[float] = 300
    r"""Time, in seconds, between consecutive state collections. Default is 300 seconds (5 minutes)."""

    metadata: Optional[List[MetadatumSystemState]] = None
    r"""Fields to add to events from this input"""

    collectors: Optional[Collectors] = None

    persistence: Optional[PersistenceSystemState] = None

    disable_native_module: Annotated[
        Optional[bool], pydantic.Field(alias="disableNativeModule")
    ] = False
    r"""Enable to use built-in tools (PowerShell) to collect events instead of native API (default) [Learn more](https://docs.cribl.io/edge/sources-system-state/#advanced-tab)"""

    description: Optional[str] = None

    @property
    def additional_properties(self):
        return self.__pydantic_extra__

    @additional_properties.setter
    def additional_properties(self, value):
        self.__pydantic_extra__ = value  # pyright: ignore[reportIncompatibleVariableOverride]


class TypeSystemMetrics(str, Enum):
    SYSTEM_METRICS = "system_metrics"


class ConnectionSystemMetricsTypedDict(TypedDict):
    output: str
    pipeline: NotRequired[str]


class ConnectionSystemMetrics(BaseModel):
    output: str

    pipeline: Optional[str] = None


class PqModeSystemMetrics(str, Enum, metaclass=utils.OpenEnumMeta):
    r"""With Smart mode, PQ will write events to the filesystem only when it detects backpressure from the processing engine. With Always On mode, PQ will always write events directly to the queue before forwarding them to the processing engine."""

    # Smart
    SMART = "smart"
    # Always On
    ALWAYS = "always"


class CompressionSystemMetrics(str, Enum, metaclass=utils.OpenEnumMeta):
    r"""Codec to use to compress the persisted data"""

    # None
    NONE = "none"
    # Gzip
    GZIP = "gzip"


class PqControlsSystemMetricsTypedDict(TypedDict):
    pass


class PqControlsSystemMetrics(BaseModel):
    pass


class PqSystemMetricsTypedDict(TypedDict):
    mode: NotRequired[PqModeSystemMetrics]
    r"""With Smart mode, PQ will write events to the filesystem only when it detects backpressure from the processing engine. With Always On mode, PQ will always write events directly to the queue before forwarding them to the processing engine."""
    max_buffer_size: NotRequired[float]
    r"""The maximum number of events to hold in memory before writing the events to disk"""
    commit_frequency: NotRequired[float]
    r"""The number of events to send downstream before committing that Stream has read them"""
    max_file_size: NotRequired[str]
    r"""The maximum size to store in each queue file before closing and optionally compressing. Enter a numeral with units of KB, MB, etc."""
    max_size: NotRequired[str]
    r"""The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc."""
    path: NotRequired[str]
    r"""The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/inputs/<input-id>"""
    compress: NotRequired[CompressionSystemMetrics]
    r"""Codec to use to compress the persisted data"""
    pq_controls: NotRequired[PqControlsSystemMetricsTypedDict]


class PqSystemMetrics(BaseModel):
    mode: Annotated[
        Optional[PqModeSystemMetrics], PlainValidator(validate_open_enum(False))
    ] = PqModeSystemMetrics.ALWAYS
    r"""With Smart mode, PQ will write events to the filesystem only when it detects backpressure from the processing engine. With Always On mode, PQ will always write events directly to the queue before forwarding them to the processing engine."""

    max_buffer_size: Annotated[
        Optional[float], pydantic.Field(alias="maxBufferSize")
    ] = 1000
    r"""The maximum number of events to hold in memory before writing the events to disk"""

    commit_frequency: Annotated[
        Optional[float], pydantic.Field(alias="commitFrequency")
    ] = 42
    r"""The number of events to send downstream before committing that Stream has read them"""

    max_file_size: Annotated[Optional[str], pydantic.Field(alias="maxFileSize")] = (
        "1 MB"
    )
    r"""The maximum size to store in each queue file before closing and optionally compressing. Enter a numeral with units of KB, MB, etc."""

    max_size: Annotated[Optional[str], pydantic.Field(alias="maxSize")] = "5GB"
    r"""The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc."""

    path: Optional[str] = "$CRIBL_HOME/state/queues"
    r"""The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/inputs/<input-id>"""

    compress: Annotated[
        Optional[CompressionSystemMetrics], PlainValidator(validate_open_enum(False))
    ] = CompressionSystemMetrics.NONE
    r"""Codec to use to compress the persisted data"""

    pq_controls: Annotated[
        Optional[PqControlsSystemMetrics], pydantic.Field(alias="pqControls")
    ] = None

    @field_serializer("mode")
    def serialize_mode(self, value):
        if isinstance(value, str):
            try:
                return models.PqModeSystemMetrics(value)
            except ValueError:
                return value
        return value

    @field_serializer("compress")
    def serialize_compress(self, value):
        if isinstance(value, str):
            try:
                return models.CompressionSystemMetrics(value)
            except ValueError:
                return value
        return value


class HostModeSystemMetrics(str, Enum, metaclass=utils.OpenEnumMeta):
    r"""Select level of detail for host metrics"""

    # Basic
    BASIC = "basic"
    # All
    ALL = "all"
    # Custom
    CUSTOM = "custom"
    # Disabled
    DISABLED = "disabled"


class SystemModeSystemMetrics(str, Enum, metaclass=utils.OpenEnumMeta):
    r"""Select the level of detail for system metrics"""

    # Basic
    BASIC = "basic"
    # All
    ALL = "all"
    # Custom
    CUSTOM = "custom"
    # Disabled
    DISABLED = "disabled"


class SystemSystemMetricsTypedDict(TypedDict):
    mode: NotRequired[SystemModeSystemMetrics]
    r"""Select the level of detail for system metrics"""
    processes: NotRequired[bool]
    r"""Generate metrics for the numbers of processes in various states"""


class SystemSystemMetrics(BaseModel):
    mode: Annotated[
        Optional[SystemModeSystemMetrics], PlainValidator(validate_open_enum(False))
    ] = SystemModeSystemMetrics.BASIC
    r"""Select the level of detail for system metrics"""

    processes: Optional[bool] = False
    r"""Generate metrics for the numbers of processes in various states"""

    @field_serializer("mode")
    def serialize_mode(self, value):
        if isinstance(value, str):
            try:
                return models.SystemModeSystemMetrics(value)
            except ValueError:
                return value
        return value


class CPUModeSystemMetrics(str, Enum, metaclass=utils.OpenEnumMeta):
    r"""Select the level of detail for CPU metrics"""

    # Basic
    BASIC = "basic"
    # All
    ALL = "all"
    # Custom
    CUSTOM = "custom"
    # Disabled
    DISABLED = "disabled"


class CPUSystemMetricsTypedDict(TypedDict):
    mode: NotRequired[CPUModeSystemMetrics]
    r"""Select the level of detail for CPU metrics"""
    per_cpu: NotRequired[bool]
    r"""Generate metrics for each CPU"""
    detail: NotRequired[bool]
    r"""Generate metrics for all CPU states"""
    time: NotRequired[bool]
    r"""Generate raw, monotonic CPU time counters"""


class CPUSystemMetrics(BaseModel):
    mode: Annotated[
        Optional[CPUModeSystemMetrics], PlainValidator(validate_open_enum(False))
    ] = CPUModeSystemMetrics.BASIC
    r"""Select the level of detail for CPU metrics"""

    per_cpu: Annotated[Optional[bool], pydantic.Field(alias="perCpu")] = False
    r"""Generate metrics for each CPU"""

    detail: Optional[bool] = False
    r"""Generate metrics for all CPU states"""

    time: Optional[bool] = False
    r"""Generate raw, monotonic CPU time counters"""

    @field_serializer("mode")
    def serialize_mode(self, value):
        if isinstance(value, str):
            try:
                return models.CPUModeSystemMetrics(value)
            except ValueError:
                return value
        return value


class MemoryModeSystemMetrics(str, Enum, metaclass=utils.OpenEnumMeta):
    r"""Select the level of detail for memory metrics"""

    # Basic
    BASIC = "basic"
    # All
    ALL = "all"
    # Custom
    CUSTOM = "custom"
    # Disabled
    DISABLED = "disabled"


class MemorySystemMetricsTypedDict(TypedDict):
    mode: NotRequired[MemoryModeSystemMetrics]
    r"""Select the level of detail for memory metrics"""
    detail: NotRequired[bool]
    r"""Generate metrics for all memory states"""


class MemorySystemMetrics(BaseModel):
    mode: Annotated[
        Optional[MemoryModeSystemMetrics], PlainValidator(validate_open_enum(False))
    ] = MemoryModeSystemMetrics.BASIC
    r"""Select the level of detail for memory metrics"""

    detail: Optional[bool] = False
    r"""Generate metrics for all memory states"""

    @field_serializer("mode")
    def serialize_mode(self, value):
        if isinstance(value, str):
            try:
                return models.MemoryModeSystemMetrics(value)
            except ValueError:
                return value
        return value


class NetworkModeSystemMetrics(str, Enum, metaclass=utils.OpenEnumMeta):
    r"""Select the level of detail for network metrics"""

    # Basic
    BASIC = "basic"
    # All
    ALL = "all"
    # Custom
    CUSTOM = "custom"
    # Disabled
    DISABLED = "disabled"


class NetworkSystemMetricsTypedDict(TypedDict):
    mode: NotRequired[NetworkModeSystemMetrics]
    r"""Select the level of detail for network metrics"""
    detail: NotRequired[bool]
    r"""Generate full network metrics"""
    protocols: NotRequired[bool]
    r"""Generate protocol metrics for ICMP, ICMPMsg, IP, TCP, UDP and UDPLite"""
    devices: NotRequired[List[str]]
    r"""Network interfaces to include/exclude. Examples: eth0, !lo. All interfaces are included if this list is empty."""
    per_interface: NotRequired[bool]
    r"""Generate separate metrics for each interface"""


class NetworkSystemMetrics(BaseModel):
    mode: Annotated[
        Optional[NetworkModeSystemMetrics], PlainValidator(validate_open_enum(False))
    ] = NetworkModeSystemMetrics.BASIC
    r"""Select the level of detail for network metrics"""

    detail: Optional[bool] = False
    r"""Generate full network metrics"""

    protocols: Optional[bool] = False
    r"""Generate protocol metrics for ICMP, ICMPMsg, IP, TCP, UDP and UDPLite"""

    devices: Optional[List[str]] = None
    r"""Network interfaces to include/exclude. Examples: eth0, !lo. All interfaces are included if this list is empty."""

    per_interface: Annotated[Optional[bool], pydantic.Field(alias="perInterface")] = (
        False
    )
    r"""Generate separate metrics for each interface"""

    @field_serializer("mode")
    def serialize_mode(self, value):
        if isinstance(value, str):
            try:
                return models.NetworkModeSystemMetrics(value)
            except ValueError:
                return value
        return value


class DiskModeSystemMetrics(str, Enum, metaclass=utils.OpenEnumMeta):
    r"""Select the level of detail for disk metrics"""

    # Basic
    BASIC = "basic"
    # All
    ALL = "all"
    # Custom
    CUSTOM = "custom"
    # Disabled
    DISABLED = "disabled"


class DiskSystemMetricsTypedDict(TypedDict):
    mode: NotRequired[DiskModeSystemMetrics]
    r"""Select the level of detail for disk metrics"""
    detail: NotRequired[bool]
    r"""Generate full disk metrics"""
    inodes: NotRequired[bool]
    r"""Generate filesystem inode metrics"""
    devices: NotRequired[List[str]]
    r"""Block devices to include/exclude. Examples: sda*, !loop*. Wildcards and ! (not) operators are supported. All devices are included if this list is empty."""
    mountpoints: NotRequired[List[str]]
    r"""Filesystem mountpoints to include/exclude. Examples: /, /home, !/proc*, !/tmp. Wildcards and ! (not) operators are supported. All mountpoints are included if this list is empty."""
    fstypes: NotRequired[List[str]]
    r"""Filesystem types to include/exclude. Examples: ext4, !*tmpfs, !squashfs. Wildcards and ! (not) operators are supported. All types are included if this list is empty."""
    per_device: NotRequired[bool]
    r"""Generate separate metrics for each device"""


class DiskSystemMetrics(BaseModel):
    mode: Annotated[
        Optional[DiskModeSystemMetrics], PlainValidator(validate_open_enum(False))
    ] = DiskModeSystemMetrics.BASIC
    r"""Select the level of detail for disk metrics"""

    detail: Optional[bool] = False
    r"""Generate full disk metrics"""

    inodes: Optional[bool] = False
    r"""Generate filesystem inode metrics"""

    devices: Optional[List[str]] = None
    r"""Block devices to include/exclude. Examples: sda*, !loop*. Wildcards and ! (not) operators are supported. All devices are included if this list is empty."""

    mountpoints: Optional[List[str]] = None
    r"""Filesystem mountpoints to include/exclude. Examples: /, /home, !/proc*, !/tmp. Wildcards and ! (not) operators are supported. All mountpoints are included if this list is empty."""

    fstypes: Optional[List[str]] = None
    r"""Filesystem types to include/exclude. Examples: ext4, !*tmpfs, !squashfs. Wildcards and ! (not) operators are supported. All types are included if this list is empty."""

    per_device: Annotated[Optional[bool], pydantic.Field(alias="perDevice")] = False
    r"""Generate separate metrics for each device"""

    @field_serializer("mode")
    def serialize_mode(self, value):
        if isinstance(value, str):
            try:
                return models.DiskModeSystemMetrics(value)
            except ValueError:
                return value
        return value


class CustomSystemMetricsTypedDict(TypedDict):
    system: NotRequired[SystemSystemMetricsTypedDict]
    cpu: NotRequired[CPUSystemMetricsTypedDict]
    memory: NotRequired[MemorySystemMetricsTypedDict]
    network: NotRequired[NetworkSystemMetricsTypedDict]
    disk: NotRequired[DiskSystemMetricsTypedDict]


class CustomSystemMetrics(BaseModel):
    system: Optional[SystemSystemMetrics] = None

    cpu: Optional[CPUSystemMetrics] = None

    memory: Optional[MemorySystemMetrics] = None

    network: Optional[NetworkSystemMetrics] = None

    disk: Optional[DiskSystemMetrics] = None


class HostSystemMetricsTypedDict(TypedDict):
    mode: NotRequired[HostModeSystemMetrics]
    r"""Select level of detail for host metrics"""
    custom: NotRequired[CustomSystemMetricsTypedDict]


class HostSystemMetrics(BaseModel):
    mode: Annotated[
        Optional[HostModeSystemMetrics], PlainValidator(validate_open_enum(False))
    ] = HostModeSystemMetrics.BASIC
    r"""Select level of detail for host metrics"""

    custom: Optional[CustomSystemMetrics] = None

    @field_serializer("mode")
    def serialize_mode(self, value):
        if isinstance(value, str):
            try:
                return models.HostModeSystemMetrics(value)
            except ValueError:
                return value
        return value


class SetSystemMetricsTypedDict(TypedDict):
    name: str
    filter_: str
    include_children: NotRequired[bool]


class SetSystemMetrics(BaseModel):
    name: str

    filter_: Annotated[str, pydantic.Field(alias="filter")]

    include_children: Annotated[
        Optional[bool], pydantic.Field(alias="includeChildren")
    ] = False


class ProcessSystemMetricsTypedDict(TypedDict):
    sets: NotRequired[List[SetSystemMetricsTypedDict]]
    r"""Configure sets to collect process metrics"""


class ProcessSystemMetrics(BaseModel):
    sets: Optional[List[SetSystemMetrics]] = None
    r"""Configure sets to collect process metrics"""


class ContainerMode(str, Enum, metaclass=utils.OpenEnumMeta):
    r"""Select the level of detail for container metrics"""

    # Basic
    BASIC = "basic"
    # All
    ALL = "all"
    # Custom
    CUSTOM = "custom"
    # Disabled
    DISABLED = "disabled"


class ContainerFilterTypedDict(TypedDict):
    expr: str


class ContainerFilter(BaseModel):
    expr: str


class ContainerTypedDict(TypedDict):
    mode: NotRequired[ContainerMode]
    r"""Select the level of detail for container metrics"""
    docker_socket: NotRequired[List[str]]
    r"""Full paths for Docker's UNIX-domain socket"""
    docker_timeout: NotRequired[float]
    r"""Timeout, in seconds, for the Docker API"""
    filters: NotRequired[List[ContainerFilterTypedDict]]
    r"""Containers matching any of these will be included. All are included if no filters are added."""
    all_containers: NotRequired[bool]
    r"""Include stopped and paused containers"""
    per_device: NotRequired[bool]
    r"""Generate separate metrics for each device"""
    detail: NotRequired[bool]
    r"""Generate full container metrics"""


class Container(BaseModel):
    mode: Annotated[
        Optional[ContainerMode], PlainValidator(validate_open_enum(False))
    ] = ContainerMode.BASIC
    r"""Select the level of detail for container metrics"""

    docker_socket: Annotated[
        Optional[List[str]], pydantic.Field(alias="dockerSocket")
    ] = None
    r"""Full paths for Docker's UNIX-domain socket"""

    docker_timeout: Annotated[
        Optional[float], pydantic.Field(alias="dockerTimeout")
    ] = 5
    r"""Timeout, in seconds, for the Docker API"""

    filters: Optional[List[ContainerFilter]] = None
    r"""Containers matching any of these will be included. All are included if no filters are added."""

    all_containers: Annotated[Optional[bool], pydantic.Field(alias="allContainers")] = (
        False
    )
    r"""Include stopped and paused containers"""

    per_device: Annotated[Optional[bool], pydantic.Field(alias="perDevice")] = False
    r"""Generate separate metrics for each device"""

    detail: Optional[bool] = False
    r"""Generate full container metrics"""

    @field_serializer("mode")
    def serialize_mode(self, value):
        if isinstance(value, str):
            try:
                return models.ContainerMode(value)
            except ValueError:
                return value
        return value


class MetadatumSystemMetricsTypedDict(TypedDict):
    name: str
    value: str
    r"""JavaScript expression to compute field's value, enclosed in quotes or backticks. (Can evaluate to a constant.)"""


class MetadatumSystemMetrics(BaseModel):
    name: str

    value: str
    r"""JavaScript expression to compute field's value, enclosed in quotes or backticks. (Can evaluate to a constant.)"""


class DataCompressionFormatSystemMetrics(str, Enum, metaclass=utils.OpenEnumMeta):
    NONE = "none"
    GZIP = "gzip"


class PersistenceSystemMetricsTypedDict(TypedDict):
    enable: NotRequired[bool]
    r"""Spool metrics to disk for Cribl Edge and Search"""
    time_window: NotRequired[str]
    r"""Time span for each file bucket"""
    max_data_size: NotRequired[str]
    r"""Maximum disk space allowed to be consumed (examples: 420MB, 4GB). When limit is reached, older data will be deleted."""
    max_data_time: NotRequired[str]
    r"""Maximum amount of time to retain data (examples: 2h, 4d). When limit is reached, older data will be deleted."""
    compress: NotRequired[DataCompressionFormatSystemMetrics]
    dest_path: NotRequired[str]
    r"""Path to use to write metrics. Defaults to $CRIBL_HOME/state/system_metrics"""


class PersistenceSystemMetrics(BaseModel):
    enable: Optional[bool] = False
    r"""Spool metrics to disk for Cribl Edge and Search"""

    time_window: Annotated[Optional[str], pydantic.Field(alias="timeWindow")] = "10m"
    r"""Time span for each file bucket"""

    max_data_size: Annotated[Optional[str], pydantic.Field(alias="maxDataSize")] = "1GB"
    r"""Maximum disk space allowed to be consumed (examples: 420MB, 4GB). When limit is reached, older data will be deleted."""

    max_data_time: Annotated[Optional[str], pydantic.Field(alias="maxDataTime")] = "24h"
    r"""Maximum amount of time to retain data (examples: 2h, 4d). When limit is reached, older data will be deleted."""

    compress: Annotated[
        Optional[DataCompressionFormatSystemMetrics],
        PlainValidator(validate_open_enum(False)),
    ] = DataCompressionFormatSystemMetrics.GZIP

    dest_path: Annotated[Optional[str], pydantic.Field(alias="destPath")] = (
        "$CRIBL_HOME/state/system_metrics"
    )
    r"""Path to use to write metrics. Defaults to $CRIBL_HOME/state/system_metrics"""

    @field_serializer("compress")
    def serialize_compress(self, value):
        if isinstance(value, str):
            try:
                return models.DataCompressionFormatSystemMetrics(value)
            except ValueError:
                return value
        return value


class InputSystemMetricsTypedDict(TypedDict):
    type: TypeSystemMetrics
    id: NotRequired[str]
    r"""Unique ID for this input"""
    disabled: NotRequired[bool]
    pipeline: NotRequired[str]
    r"""Pipeline to process data from this Source before sending it through the Routes"""
    send_to_routes: NotRequired[bool]
    r"""Select whether to send data to Routes, or directly to Destinations."""
    environment: NotRequired[str]
    r"""Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere."""
    pq_enabled: NotRequired[bool]
    r"""Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers)."""
    streamtags: NotRequired[List[str]]
    r"""Tags for filtering and grouping in @{product}"""
    connections: NotRequired[List[ConnectionSystemMetricsTypedDict]]
    r"""Direct connections to Destinations, and optionally via a Pipeline or a Pack"""
    pq: NotRequired[PqSystemMetricsTypedDict]
    interval: NotRequired[float]
    r"""Time, in seconds, between consecutive metric collections. Default is 10 seconds."""
    host: NotRequired[HostSystemMetricsTypedDict]
    process: NotRequired[ProcessSystemMetricsTypedDict]
    container: NotRequired[ContainerTypedDict]
    metadata: NotRequired[List[MetadatumSystemMetricsTypedDict]]
    r"""Fields to add to events from this input"""
    persistence: NotRequired[PersistenceSystemMetricsTypedDict]
    description: NotRequired[str]


class InputSystemMetrics(BaseModel):
    model_config = ConfigDict(
        populate_by_name=True, arbitrary_types_allowed=True, extra="allow"
    )
    __pydantic_extra__: Dict[str, Any] = pydantic.Field(init=False)

    type: TypeSystemMetrics

    id: Optional[str] = None
    r"""Unique ID for this input"""

    disabled: Optional[bool] = False

    pipeline: Optional[str] = None
    r"""Pipeline to process data from this Source before sending it through the Routes"""

    send_to_routes: Annotated[Optional[bool], pydantic.Field(alias="sendToRoutes")] = (
        True
    )
    r"""Select whether to send data to Routes, or directly to Destinations."""

    environment: Optional[str] = None
    r"""Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere."""

    pq_enabled: Annotated[Optional[bool], pydantic.Field(alias="pqEnabled")] = False
    r"""Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers)."""

    streamtags: Optional[List[str]] = None
    r"""Tags for filtering and grouping in @{product}"""

    connections: Optional[List[ConnectionSystemMetrics]] = None
    r"""Direct connections to Destinations, and optionally via a Pipeline or a Pack"""

    pq: Optional[PqSystemMetrics] = None

    interval: Optional[float] = 10
    r"""Time, in seconds, between consecutive metric collections. Default is 10 seconds."""

    host: Optional[HostSystemMetrics] = None

    process: Optional[ProcessSystemMetrics] = None

    container: Optional[Container] = None

    metadata: Optional[List[MetadatumSystemMetrics]] = None
    r"""Fields to add to events from this input"""

    persistence: Optional[PersistenceSystemMetrics] = None

    description: Optional[str] = None

    @property
    def additional_properties(self):
        return self.__pydantic_extra__

    @additional_properties.setter
    def additional_properties(self, value):
        self.__pydantic_extra__ = value  # pyright: ignore[reportIncompatibleVariableOverride]


class InputTypeTcpjson(str, Enum):
    TCPJSON = "tcpjson"


class ConnectionTcpjsonTypedDict(TypedDict):
    output: str
    pipeline: NotRequired[str]


class ConnectionTcpjson(BaseModel):
    output: str

    pipeline: Optional[str] = None


class PqModeTcpjson(str, Enum, metaclass=utils.OpenEnumMeta):
    r"""With Smart mode, PQ will write events to the filesystem only when it detects backpressure from the processing engine. With Always On mode, PQ will always write events directly to the queue before forwarding them to the processing engine."""

    # Smart
    SMART = "smart"
    # Always On
    ALWAYS = "always"


class PqCompressionTcpjson(str, Enum, metaclass=utils.OpenEnumMeta):
    r"""Codec to use to compress the persisted data"""

    # None
    NONE = "none"
    # Gzip
    GZIP = "gzip"


class InputPqControlsTcpjsonTypedDict(TypedDict):
    pass


class InputPqControlsTcpjson(BaseModel):
    pass


class PqTcpjsonTypedDict(TypedDict):
    mode: NotRequired[PqModeTcpjson]
    r"""With Smart mode, PQ will write events to the filesystem only when it detects backpressure from the processing engine. With Always On mode, PQ will always write events directly to the queue before forwarding them to the processing engine."""
    max_buffer_size: NotRequired[float]
    r"""The maximum number of events to hold in memory before writing the events to disk"""
    commit_frequency: NotRequired[float]
    r"""The number of events to send downstream before committing that Stream has read them"""
    max_file_size: NotRequired[str]
    r"""The maximum size to store in each queue file before closing and optionally compressing. Enter a numeral with units of KB, MB, etc."""
    max_size: NotRequired[str]
    r"""The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc."""
    path: NotRequired[str]
    r"""The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/inputs/<input-id>"""
    compress: NotRequired[PqCompressionTcpjson]
    r"""Codec to use to compress the persisted data"""
    pq_controls: NotRequired[InputPqControlsTcpjsonTypedDict]


class PqTcpjson(BaseModel):
    mode: Annotated[
        Optional[PqModeTcpjson], PlainValidator(validate_open_enum(False))
    ] = PqModeTcpjson.ALWAYS
    r"""With Smart mode, PQ will write events to the filesystem only when it detects backpressure from the processing engine. With Always On mode, PQ will always write events directly to the queue before forwarding them to the processing engine."""

    max_buffer_size: Annotated[
        Optional[float], pydantic.Field(alias="maxBufferSize")
    ] = 1000
    r"""The maximum number of events to hold in memory before writing the events to disk"""

    commit_frequency: Annotated[
        Optional[float], pydantic.Field(alias="commitFrequency")
    ] = 42
    r"""The number of events to send downstream before committing that Stream has read them"""

    max_file_size: Annotated[Optional[str], pydantic.Field(alias="maxFileSize")] = (
        "1 MB"
    )
    r"""The maximum size to store in each queue file before closing and optionally compressing. Enter a numeral with units of KB, MB, etc."""

    max_size: Annotated[Optional[str], pydantic.Field(alias="maxSize")] = "5GB"
    r"""The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc."""

    path: Optional[str] = "$CRIBL_HOME/state/queues"
    r"""The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/inputs/<input-id>"""

    compress: Annotated[
        Optional[PqCompressionTcpjson], PlainValidator(validate_open_enum(False))
    ] = PqCompressionTcpjson.NONE
    r"""Codec to use to compress the persisted data"""

    pq_controls: Annotated[
        Optional[InputPqControlsTcpjson], pydantic.Field(alias="pqControls")
    ] = None

    @field_serializer("mode")
    def serialize_mode(self, value):
        if isinstance(value, str):
            try:
                return models.PqModeTcpjson(value)
            except ValueError:
                return value
        return value

    @field_serializer("compress")
    def serialize_compress(self, value):
        if isinstance(value, str):
            try:
                return models.PqCompressionTcpjson(value)
            except ValueError:
                return value
        return value


class InputMinimumTLSVersionTcpjson(str, Enum, metaclass=utils.OpenEnumMeta):
    TL_SV1 = "TLSv1"
    TL_SV1_1 = "TLSv1.1"
    TL_SV1_2 = "TLSv1.2"
    TL_SV1_3 = "TLSv1.3"


class InputMaximumTLSVersionTcpjson(str, Enum, metaclass=utils.OpenEnumMeta):
    TL_SV1 = "TLSv1"
    TL_SV1_1 = "TLSv1.1"
    TL_SV1_2 = "TLSv1.2"
    TL_SV1_3 = "TLSv1.3"


class TLSSettingsServerSideTcpjsonTypedDict(TypedDict):
    disabled: NotRequired[bool]
    request_cert: NotRequired[bool]
    r"""Require clients to present their certificates. Used to perform client authentication using SSL certs."""
    reject_unauthorized: NotRequired[bool]
    r"""Reject certificates not authorized by a CA in the CA certificate path or by another trusted CA (such as the system's)"""
    common_name_regex: NotRequired[str]
    r"""Regex matching allowable common names in peer certificates' subject attribute"""
    certificate_name: NotRequired[str]
    r"""The name of the predefined certificate"""
    priv_key_path: NotRequired[str]
    r"""Path on server containing the private key to use. PEM format. Can reference $ENV_VARS."""
    passphrase: NotRequired[str]
    r"""Passphrase to use to decrypt private key"""
    cert_path: NotRequired[str]
    r"""Path on server containing certificates to use. PEM format. Can reference $ENV_VARS."""
    ca_path: NotRequired[str]
    r"""Path on server containing CA certificates to use. PEM format. Can reference $ENV_VARS."""
    min_version: NotRequired[InputMinimumTLSVersionTcpjson]
    max_version: NotRequired[InputMaximumTLSVersionTcpjson]


class TLSSettingsServerSideTcpjson(BaseModel):
    disabled: Optional[bool] = True

    request_cert: Annotated[Optional[bool], pydantic.Field(alias="requestCert")] = False
    r"""Require clients to present their certificates. Used to perform client authentication using SSL certs."""

    reject_unauthorized: Annotated[
        Optional[bool], pydantic.Field(alias="rejectUnauthorized")
    ] = True
    r"""Reject certificates not authorized by a CA in the CA certificate path or by another trusted CA (such as the system's)"""

    common_name_regex: Annotated[
        Optional[str], pydantic.Field(alias="commonNameRegex")
    ] = "/.*/"
    r"""Regex matching allowable common names in peer certificates' subject attribute"""

    certificate_name: Annotated[
        Optional[str], pydantic.Field(alias="certificateName")
    ] = None
    r"""The name of the predefined certificate"""

    priv_key_path: Annotated[Optional[str], pydantic.Field(alias="privKeyPath")] = None
    r"""Path on server containing the private key to use. PEM format. Can reference $ENV_VARS."""

    passphrase: Optional[str] = None
    r"""Passphrase to use to decrypt private key"""

    cert_path: Annotated[Optional[str], pydantic.Field(alias="certPath")] = None
    r"""Path on server containing certificates to use. PEM format. Can reference $ENV_VARS."""

    ca_path: Annotated[Optional[str], pydantic.Field(alias="caPath")] = None
    r"""Path on server containing CA certificates to use. PEM format. Can reference $ENV_VARS."""

    min_version: Annotated[
        Annotated[
            Optional[InputMinimumTLSVersionTcpjson],
            PlainValidator(validate_open_enum(False)),
        ],
        pydantic.Field(alias="minVersion"),
    ] = None

    max_version: Annotated[
        Annotated[
            Optional[InputMaximumTLSVersionTcpjson],
            PlainValidator(validate_open_enum(False)),
        ],
        pydantic.Field(alias="maxVersion"),
    ] = None

    @field_serializer("min_version")
    def serialize_min_version(self, value):
        if isinstance(value, str):
            try:
                return models.InputMinimumTLSVersionTcpjson(value)
            except ValueError:
                return value
        return value

    @field_serializer("max_version")
    def serialize_max_version(self, value):
        if isinstance(value, str):
            try:
                return models.InputMaximumTLSVersionTcpjson(value)
            except ValueError:
                return value
        return value


class MetadatumTcpjsonTypedDict(TypedDict):
    name: str
    value: str
    r"""JavaScript expression to compute field's value, enclosed in quotes or backticks. (Can evaluate to a constant.)"""


class MetadatumTcpjson(BaseModel):
    name: str

    value: str
    r"""JavaScript expression to compute field's value, enclosed in quotes or backticks. (Can evaluate to a constant.)"""


class InputAuthenticationMethodTcpjson(str, Enum, metaclass=utils.OpenEnumMeta):
    r"""Select Manual to enter an auth token directly, or select Secret to use a text secret to authenticate"""

    MANUAL = "manual"
    SECRET = "secret"


class InputTcpjsonTypedDict(TypedDict):
    type: InputTypeTcpjson
    port: float
    r"""Port to listen on"""
    id: NotRequired[str]
    r"""Unique ID for this input"""
    disabled: NotRequired[bool]
    pipeline: NotRequired[str]
    r"""Pipeline to process data from this Source before sending it through the Routes"""
    send_to_routes: NotRequired[bool]
    r"""Select whether to send data to Routes, or directly to Destinations."""
    environment: NotRequired[str]
    r"""Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere."""
    pq_enabled: NotRequired[bool]
    r"""Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers)."""
    streamtags: NotRequired[List[str]]
    r"""Tags for filtering and grouping in @{product}"""
    connections: NotRequired[List[ConnectionTcpjsonTypedDict]]
    r"""Direct connections to Destinations, and optionally via a Pipeline or a Pack"""
    pq: NotRequired[PqTcpjsonTypedDict]
    host: NotRequired[str]
    r"""Address to bind on. Defaults to 0.0.0.0 (all addresses)."""
    tls: NotRequired[TLSSettingsServerSideTcpjsonTypedDict]
    ip_whitelist_regex: NotRequired[str]
    r"""Regex matching IP addresses that are allowed to establish a connection"""
    max_active_cxn: NotRequired[float]
    r"""Maximum number of active connections allowed per Worker Process. Use 0 for unlimited."""
    socket_idle_timeout: NotRequired[float]
    r"""How long @{product} should wait before assuming that an inactive socket has timed out. After this time, the connection will be closed. Leave at 0 for no inactive socket monitoring."""
    socket_ending_max_wait: NotRequired[float]
    r"""How long the server will wait after initiating a closure for a client to close its end of the connection. If the client doesn't close the connection within this time, the server will forcefully terminate the socket to prevent resource leaks and ensure efficient connection cleanup and system stability. Leave at 0 for no inactive socket monitoring."""
    socket_max_lifespan: NotRequired[float]
    r"""The maximum duration a socket can remain open, even if active. This helps manage resources and mitigate issues caused by TCP pinning. Set to 0 to disable."""
    enable_proxy_header: NotRequired[bool]
    r"""Enable if the connection is proxied by a device that supports proxy protocol v1 or v2"""
    metadata: NotRequired[List[MetadatumTcpjsonTypedDict]]
    r"""Fields to add to events from this input"""
    enable_load_balancing: NotRequired[bool]
    r"""Load balance traffic across all Worker Processes"""
    auth_type: NotRequired[InputAuthenticationMethodTcpjson]
    r"""Select Manual to enter an auth token directly, or select Secret to use a text secret to authenticate"""
    description: NotRequired[str]
    auth_token: NotRequired[str]
    r"""Shared secret to be provided by any client (in authToken header field). If empty, unauthorized access is permitted."""
    text_secret: NotRequired[str]
    r"""Select or create a stored text secret"""


class InputTcpjson(BaseModel):
    model_config = ConfigDict(
        populate_by_name=True, arbitrary_types_allowed=True, extra="allow"
    )
    __pydantic_extra__: Dict[str, Any] = pydantic.Field(init=False)

    type: InputTypeTcpjson

    port: float
    r"""Port to listen on"""

    id: Optional[str] = None
    r"""Unique ID for this input"""

    disabled: Optional[bool] = False

    pipeline: Optional[str] = None
    r"""Pipeline to process data from this Source before sending it through the Routes"""

    send_to_routes: Annotated[Optional[bool], pydantic.Field(alias="sendToRoutes")] = (
        True
    )
    r"""Select whether to send data to Routes, or directly to Destinations."""

    environment: Optional[str] = None
    r"""Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere."""

    pq_enabled: Annotated[Optional[bool], pydantic.Field(alias="pqEnabled")] = False
    r"""Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers)."""

    streamtags: Optional[List[str]] = None
    r"""Tags for filtering and grouping in @{product}"""

    connections: Optional[List[ConnectionTcpjson]] = None
    r"""Direct connections to Destinations, and optionally via a Pipeline or a Pack"""

    pq: Optional[PqTcpjson] = None

    host: Optional[str] = "0.0.0.0"
    r"""Address to bind on. Defaults to 0.0.0.0 (all addresses)."""

    tls: Optional[TLSSettingsServerSideTcpjson] = None

    ip_whitelist_regex: Annotated[
        Optional[str], pydantic.Field(alias="ipWhitelistRegex")
    ] = "/.*/"
    r"""Regex matching IP addresses that are allowed to establish a connection"""

    max_active_cxn: Annotated[Optional[float], pydantic.Field(alias="maxActiveCxn")] = (
        1000
    )
    r"""Maximum number of active connections allowed per Worker Process. Use 0 for unlimited."""

    socket_idle_timeout: Annotated[
        Optional[float], pydantic.Field(alias="socketIdleTimeout")
    ] = 0
    r"""How long @{product} should wait before assuming that an inactive socket has timed out. After this time, the connection will be closed. Leave at 0 for no inactive socket monitoring."""

    socket_ending_max_wait: Annotated[
        Optional[float], pydantic.Field(alias="socketEndingMaxWait")
    ] = 30
    r"""How long the server will wait after initiating a closure for a client to close its end of the connection. If the client doesn't close the connection within this time, the server will forcefully terminate the socket to prevent resource leaks and ensure efficient connection cleanup and system stability. Leave at 0 for no inactive socket monitoring."""

    socket_max_lifespan: Annotated[
        Optional[float], pydantic.Field(alias="socketMaxLifespan")
    ] = 0
    r"""The maximum duration a socket can remain open, even if active. This helps manage resources and mitigate issues caused by TCP pinning. Set to 0 to disable."""

    enable_proxy_header: Annotated[
        Optional[bool], pydantic.Field(alias="enableProxyHeader")
    ] = False
    r"""Enable if the connection is proxied by a device that supports proxy protocol v1 or v2"""

    metadata: Optional[List[MetadatumTcpjson]] = None
    r"""Fields to add to events from this input"""

    enable_load_balancing: Annotated[
        Optional[bool], pydantic.Field(alias="enableLoadBalancing")
    ] = False
    r"""Load balance traffic across all Worker Processes"""

    auth_type: Annotated[
        Annotated[
            Optional[InputAuthenticationMethodTcpjson],
            PlainValidator(validate_open_enum(False)),
        ],
        pydantic.Field(alias="authType"),
    ] = InputAuthenticationMethodTcpjson.MANUAL
    r"""Select Manual to enter an auth token directly, or select Secret to use a text secret to authenticate"""

    description: Optional[str] = None

    auth_token: Annotated[Optional[str], pydantic.Field(alias="authToken")] = ""
    r"""Shared secret to be provided by any client (in authToken header field). If empty, unauthorized access is permitted."""

    text_secret: Annotated[Optional[str], pydantic.Field(alias="textSecret")] = None
    r"""Select or create a stored text secret"""

    @property
    def additional_properties(self):
        return self.__pydantic_extra__

    @additional_properties.setter
    def additional_properties(self, value):
        self.__pydantic_extra__ = value  # pyright: ignore[reportIncompatibleVariableOverride]

    @field_serializer("auth_type")
    def serialize_auth_type(self, value):
        if isinstance(value, str):
            try:
                return models.InputAuthenticationMethodTcpjson(value)
            except ValueError:
                return value
        return value


class TypeCriblLakeHTTP(str, Enum):
    CRIBL_LAKE_HTTP = "cribl_lake_http"


class ConnectionCriblLakeHTTPTypedDict(TypedDict):
    output: str
    pipeline: NotRequired[str]


class ConnectionCriblLakeHTTP(BaseModel):
    output: str

    pipeline: Optional[str] = None


class ModeCriblLakeHTTP(str, Enum, metaclass=utils.OpenEnumMeta):
    r"""With Smart mode, PQ will write events to the filesystem only when it detects backpressure from the processing engine. With Always On mode, PQ will always write events directly to the queue before forwarding them to the processing engine."""

    # Smart
    SMART = "smart"
    # Always On
    ALWAYS = "always"


class CompressionCriblLakeHTTP(str, Enum, metaclass=utils.OpenEnumMeta):
    r"""Codec to use to compress the persisted data"""

    # None
    NONE = "none"
    # Gzip
    GZIP = "gzip"


class PqControlsCriblLakeHTTPTypedDict(TypedDict):
    pass


class PqControlsCriblLakeHTTP(BaseModel):
    pass


class PqCriblLakeHTTPTypedDict(TypedDict):
    mode: NotRequired[ModeCriblLakeHTTP]
    r"""With Smart mode, PQ will write events to the filesystem only when it detects backpressure from the processing engine. With Always On mode, PQ will always write events directly to the queue before forwarding them to the processing engine."""
    max_buffer_size: NotRequired[float]
    r"""The maximum number of events to hold in memory before writing the events to disk"""
    commit_frequency: NotRequired[float]
    r"""The number of events to send downstream before committing that Stream has read them"""
    max_file_size: NotRequired[str]
    r"""The maximum size to store in each queue file before closing and optionally compressing. Enter a numeral with units of KB, MB, etc."""
    max_size: NotRequired[str]
    r"""The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc."""
    path: NotRequired[str]
    r"""The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/inputs/<input-id>"""
    compress: NotRequired[CompressionCriblLakeHTTP]
    r"""Codec to use to compress the persisted data"""
    pq_controls: NotRequired[PqControlsCriblLakeHTTPTypedDict]


class PqCriblLakeHTTP(BaseModel):
    mode: Annotated[
        Optional[ModeCriblLakeHTTP], PlainValidator(validate_open_enum(False))
    ] = ModeCriblLakeHTTP.ALWAYS
    r"""With Smart mode, PQ will write events to the filesystem only when it detects backpressure from the processing engine. With Always On mode, PQ will always write events directly to the queue before forwarding them to the processing engine."""

    max_buffer_size: Annotated[
        Optional[float], pydantic.Field(alias="maxBufferSize")
    ] = 1000
    r"""The maximum number of events to hold in memory before writing the events to disk"""

    commit_frequency: Annotated[
        Optional[float], pydantic.Field(alias="commitFrequency")
    ] = 42
    r"""The number of events to send downstream before committing that Stream has read them"""

    max_file_size: Annotated[Optional[str], pydantic.Field(alias="maxFileSize")] = (
        "1 MB"
    )
    r"""The maximum size to store in each queue file before closing and optionally compressing. Enter a numeral with units of KB, MB, etc."""

    max_size: Annotated[Optional[str], pydantic.Field(alias="maxSize")] = "5GB"
    r"""The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc."""

    path: Optional[str] = "$CRIBL_HOME/state/queues"
    r"""The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/inputs/<input-id>"""

    compress: Annotated[
        Optional[CompressionCriblLakeHTTP], PlainValidator(validate_open_enum(False))
    ] = CompressionCriblLakeHTTP.NONE
    r"""Codec to use to compress the persisted data"""

    pq_controls: Annotated[
        Optional[PqControlsCriblLakeHTTP], pydantic.Field(alias="pqControls")
    ] = None

    @field_serializer("mode")
    def serialize_mode(self, value):
        if isinstance(value, str):
            try:
                return models.ModeCriblLakeHTTP(value)
            except ValueError:
                return value
        return value

    @field_serializer("compress")
    def serialize_compress(self, value):
        if isinstance(value, str):
            try:
                return models.CompressionCriblLakeHTTP(value)
            except ValueError:
                return value
        return value


class MinimumTLSVersionCriblLakeHTTP(str, Enum, metaclass=utils.OpenEnumMeta):
    TL_SV1 = "TLSv1"
    TL_SV1_1 = "TLSv1.1"
    TL_SV1_2 = "TLSv1.2"
    TL_SV1_3 = "TLSv1.3"


class MaximumTLSVersionCriblLakeHTTP(str, Enum, metaclass=utils.OpenEnumMeta):
    TL_SV1 = "TLSv1"
    TL_SV1_1 = "TLSv1.1"
    TL_SV1_2 = "TLSv1.2"
    TL_SV1_3 = "TLSv1.3"


class TLSSettingsServerSideCriblLakeHTTPTypedDict(TypedDict):
    disabled: NotRequired[bool]
    request_cert: NotRequired[bool]
    r"""Require clients to present their certificates. Used to perform client authentication using SSL certs."""
    reject_unauthorized: NotRequired[bool]
    r"""Reject certificates not authorized by a CA in the CA certificate path or by another trusted CA (such as the system's)"""
    common_name_regex: NotRequired[str]
    r"""Regex matching allowable common names in peer certificates' subject attribute"""
    certificate_name: NotRequired[str]
    r"""The name of the predefined certificate"""
    priv_key_path: NotRequired[str]
    r"""Path on server containing the private key to use. PEM format. Can reference $ENV_VARS."""
    passphrase: NotRequired[str]
    r"""Passphrase to use to decrypt private key"""
    cert_path: NotRequired[str]
    r"""Path on server containing certificates to use. PEM format. Can reference $ENV_VARS."""
    ca_path: NotRequired[str]
    r"""Path on server containing CA certificates to use. PEM format. Can reference $ENV_VARS."""
    min_version: NotRequired[MinimumTLSVersionCriblLakeHTTP]
    max_version: NotRequired[MaximumTLSVersionCriblLakeHTTP]


class TLSSettingsServerSideCriblLakeHTTP(BaseModel):
    disabled: Optional[bool] = True

    request_cert: Annotated[Optional[bool], pydantic.Field(alias="requestCert")] = False
    r"""Require clients to present their certificates. Used to perform client authentication using SSL certs."""

    reject_unauthorized: Annotated[
        Optional[bool], pydantic.Field(alias="rejectUnauthorized")
    ] = True
    r"""Reject certificates not authorized by a CA in the CA certificate path or by another trusted CA (such as the system's)"""

    common_name_regex: Annotated[
        Optional[str], pydantic.Field(alias="commonNameRegex")
    ] = "/.*/"
    r"""Regex matching allowable common names in peer certificates' subject attribute"""

    certificate_name: Annotated[
        Optional[str], pydantic.Field(alias="certificateName")
    ] = None
    r"""The name of the predefined certificate"""

    priv_key_path: Annotated[Optional[str], pydantic.Field(alias="privKeyPath")] = None
    r"""Path on server containing the private key to use. PEM format. Can reference $ENV_VARS."""

    passphrase: Optional[str] = None
    r"""Passphrase to use to decrypt private key"""

    cert_path: Annotated[Optional[str], pydantic.Field(alias="certPath")] = None
    r"""Path on server containing certificates to use. PEM format. Can reference $ENV_VARS."""

    ca_path: Annotated[Optional[str], pydantic.Field(alias="caPath")] = None
    r"""Path on server containing CA certificates to use. PEM format. Can reference $ENV_VARS."""

    min_version: Annotated[
        Annotated[
            Optional[MinimumTLSVersionCriblLakeHTTP],
            PlainValidator(validate_open_enum(False)),
        ],
        pydantic.Field(alias="minVersion"),
    ] = None

    max_version: Annotated[
        Annotated[
            Optional[MaximumTLSVersionCriblLakeHTTP],
            PlainValidator(validate_open_enum(False)),
        ],
        pydantic.Field(alias="maxVersion"),
    ] = None

    @field_serializer("min_version")
    def serialize_min_version(self, value):
        if isinstance(value, str):
            try:
                return models.MinimumTLSVersionCriblLakeHTTP(value)
            except ValueError:
                return value
        return value

    @field_serializer("max_version")
    def serialize_max_version(self, value):
        if isinstance(value, str):
            try:
                return models.MaximumTLSVersionCriblLakeHTTP(value)
            except ValueError:
                return value
        return value


class MetadatumCriblLakeHTTPTypedDict(TypedDict):
    name: str
    value: str
    r"""JavaScript expression to compute field's value, enclosed in quotes or backticks. (Can evaluate to a constant.)"""


class MetadatumCriblLakeHTTP(BaseModel):
    name: str

    value: str
    r"""JavaScript expression to compute field's value, enclosed in quotes or backticks. (Can evaluate to a constant.)"""


class AuthTokensExtMetadatumCriblLakeHTTPTypedDict(TypedDict):
    name: str
    value: str
    r"""JavaScript expression to compute field's value, enclosed in quotes or backticks. (Can evaluate to a constant.)"""


class AuthTokensExtMetadatumCriblLakeHTTP(BaseModel):
    name: str

    value: str
    r"""JavaScript expression to compute field's value, enclosed in quotes or backticks. (Can evaluate to a constant.)"""


class SplunkHecMetadataTypedDict(TypedDict):
    enabled: NotRequired[bool]


class SplunkHecMetadata(BaseModel):
    enabled: Optional[bool] = None


class ElasticsearchMetadataTypedDict(TypedDict):
    enabled: NotRequired[bool]


class ElasticsearchMetadata(BaseModel):
    enabled: Optional[bool] = None


class AuthTokensExtCriblLakeHTTPTypedDict(TypedDict):
    token: str
    description: NotRequired[str]
    metadata: NotRequired[List[AuthTokensExtMetadatumCriblLakeHTTPTypedDict]]
    r"""Fields to add to events referencing this token"""
    splunk_hec_metadata: NotRequired[SplunkHecMetadataTypedDict]
    elasticsearch_metadata: NotRequired[ElasticsearchMetadataTypedDict]


class AuthTokensExtCriblLakeHTTP(BaseModel):
    token: str

    description: Optional[str] = None

    metadata: Optional[List[AuthTokensExtMetadatumCriblLakeHTTP]] = None
    r"""Fields to add to events referencing this token"""

    splunk_hec_metadata: Annotated[
        Optional[SplunkHecMetadata], pydantic.Field(alias="splunkHecMetadata")
    ] = None

    elasticsearch_metadata: Annotated[
        Optional[ElasticsearchMetadata], pydantic.Field(alias="elasticsearchMetadata")
    ] = None


class InputCriblLakeHTTPTypedDict(TypedDict):
    type: TypeCriblLakeHTTP
    port: float
    r"""Port to listen on"""
    id: NotRequired[str]
    r"""Unique ID for this input"""
    disabled: NotRequired[bool]
    pipeline: NotRequired[str]
    r"""Pipeline to process data from this Source before sending it through the Routes"""
    send_to_routes: NotRequired[bool]
    r"""Select whether to send data to Routes, or directly to Destinations."""
    environment: NotRequired[str]
    r"""Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere."""
    pq_enabled: NotRequired[bool]
    r"""Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers)."""
    streamtags: NotRequired[List[str]]
    r"""Tags for filtering and grouping in @{product}"""
    connections: NotRequired[List[ConnectionCriblLakeHTTPTypedDict]]
    r"""Direct connections to Destinations, and optionally via a Pipeline or a Pack"""
    pq: NotRequired[PqCriblLakeHTTPTypedDict]
    host: NotRequired[str]
    r"""Address to bind on. Defaults to 0.0.0.0 (all addresses)."""
    auth_tokens: NotRequired[List[str]]
    r"""Shared secrets to be provided by any client (Authorization: <token>). If empty, unauthorized access is permitted."""
    tls: NotRequired[TLSSettingsServerSideCriblLakeHTTPTypedDict]
    max_active_req: NotRequired[float]
    r"""Maximum number of active requests allowed per Worker Process. Set to 0 for unlimited. Caution: Increasing the limit above the default value, or setting it to unlimited, may degrade performance and reduce throughput."""
    max_requests_per_socket: NotRequired[int]
    r"""Maximum number of requests per socket before @{product} instructs the client to close the connection. Default is 0 (unlimited)."""
    enable_proxy_header: NotRequired[bool]
    r"""Extract the client IP and port from PROXY protocol v1/v2. When enabled, the X-Forwarded-For header is ignored. Disable to use the X-Forwarded-For header for client IP extraction."""
    capture_headers: NotRequired[bool]
    r"""Add request headers to events, in the __headers field"""
    activity_log_sample_rate: NotRequired[float]
    r"""How often request activity is logged at the `info` level. A value of 1 would log every request, 10 every 10th request, etc."""
    request_timeout: NotRequired[float]
    r"""How long to wait for an incoming request to complete before aborting it. Use 0 to disable."""
    socket_timeout: NotRequired[float]
    r"""How long @{product} should wait before assuming that an inactive socket has timed out. To wait forever, set to 0."""
    keep_alive_timeout: NotRequired[float]
    r"""After the last response is sent, @{product} will wait this long for additional data before closing the socket connection. Minimum 1 second, maximum 600 seconds (10 minutes)."""
    enable_health_check: NotRequired[bool]
    r"""Expose the /cribl_health endpoint, which returns 200 OK when this Source is healthy"""
    ip_allowlist_regex: NotRequired[str]
    r"""Messages from matched IP addresses will be processed, unless also matched by the denylist"""
    ip_denylist_regex: NotRequired[str]
    r"""Messages from matched IP addresses will be ignored. This takes precedence over the allowlist."""
    cribl_api: NotRequired[str]
    r"""Absolute path on which to listen for the Cribl HTTP API requests. Only _bulk (default /cribl/_bulk) is available. Use empty string to disable."""
    elastic_api: NotRequired[str]
    r"""Absolute path on which to listen for the Elasticsearch API requests. Only _bulk (default /elastic/_bulk) is available. Use empty string to disable."""
    splunk_hec_api: NotRequired[str]
    r"""Absolute path on which listen for the Splunk HTTP Event Collector API requests. Use empty string to disable."""
    splunk_hec_acks: NotRequired[bool]
    metadata: NotRequired[List[MetadatumCriblLakeHTTPTypedDict]]
    r"""Fields to add to events from this input"""
    auth_tokens_ext: NotRequired[List[AuthTokensExtCriblLakeHTTPTypedDict]]
    description: NotRequired[str]


class InputCriblLakeHTTP(BaseModel):
    model_config = ConfigDict(
        populate_by_name=True, arbitrary_types_allowed=True, extra="allow"
    )
    __pydantic_extra__: Dict[str, Any] = pydantic.Field(init=False)

    type: TypeCriblLakeHTTP

    port: float
    r"""Port to listen on"""

    id: Optional[str] = None
    r"""Unique ID for this input"""

    disabled: Optional[bool] = False

    pipeline: Optional[str] = None
    r"""Pipeline to process data from this Source before sending it through the Routes"""

    send_to_routes: Annotated[Optional[bool], pydantic.Field(alias="sendToRoutes")] = (
        True
    )
    r"""Select whether to send data to Routes, or directly to Destinations."""

    environment: Optional[str] = None
    r"""Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere."""

    pq_enabled: Annotated[Optional[bool], pydantic.Field(alias="pqEnabled")] = False
    r"""Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers)."""

    streamtags: Optional[List[str]] = None
    r"""Tags for filtering and grouping in @{product}"""

    connections: Optional[List[ConnectionCriblLakeHTTP]] = None
    r"""Direct connections to Destinations, and optionally via a Pipeline or a Pack"""

    pq: Optional[PqCriblLakeHTTP] = None

    host: Optional[str] = "0.0.0.0"
    r"""Address to bind on. Defaults to 0.0.0.0 (all addresses)."""

    auth_tokens: Annotated[Optional[List[str]], pydantic.Field(alias="authTokens")] = (
        None
    )
    r"""Shared secrets to be provided by any client (Authorization: <token>). If empty, unauthorized access is permitted."""

    tls: Optional[TLSSettingsServerSideCriblLakeHTTP] = None

    max_active_req: Annotated[Optional[float], pydantic.Field(alias="maxActiveReq")] = (
        256
    )
    r"""Maximum number of active requests allowed per Worker Process. Set to 0 for unlimited. Caution: Increasing the limit above the default value, or setting it to unlimited, may degrade performance and reduce throughput."""

    max_requests_per_socket: Annotated[
        Optional[int], pydantic.Field(alias="maxRequestsPerSocket")
    ] = 0
    r"""Maximum number of requests per socket before @{product} instructs the client to close the connection. Default is 0 (unlimited)."""

    enable_proxy_header: Annotated[
        Optional[bool], pydantic.Field(alias="enableProxyHeader")
    ] = False
    r"""Extract the client IP and port from PROXY protocol v1/v2. When enabled, the X-Forwarded-For header is ignored. Disable to use the X-Forwarded-For header for client IP extraction."""

    capture_headers: Annotated[
        Optional[bool], pydantic.Field(alias="captureHeaders")
    ] = False
    r"""Add request headers to events, in the __headers field"""

    activity_log_sample_rate: Annotated[
        Optional[float], pydantic.Field(alias="activityLogSampleRate")
    ] = 100
    r"""How often request activity is logged at the `info` level. A value of 1 would log every request, 10 every 10th request, etc."""

    request_timeout: Annotated[
        Optional[float], pydantic.Field(alias="requestTimeout")
    ] = 0
    r"""How long to wait for an incoming request to complete before aborting it. Use 0 to disable."""

    socket_timeout: Annotated[
        Optional[float], pydantic.Field(alias="socketTimeout")
    ] = 0
    r"""How long @{product} should wait before assuming that an inactive socket has timed out. To wait forever, set to 0."""

    keep_alive_timeout: Annotated[
        Optional[float], pydantic.Field(alias="keepAliveTimeout")
    ] = 5
    r"""After the last response is sent, @{product} will wait this long for additional data before closing the socket connection. Minimum 1 second, maximum 600 seconds (10 minutes)."""

    enable_health_check: Annotated[
        Optional[bool], pydantic.Field(alias="enableHealthCheck")
    ] = False
    r"""Expose the /cribl_health endpoint, which returns 200 OK when this Source is healthy"""

    ip_allowlist_regex: Annotated[
        Optional[str], pydantic.Field(alias="ipAllowlistRegex")
    ] = "/.*/"
    r"""Messages from matched IP addresses will be processed, unless also matched by the denylist"""

    ip_denylist_regex: Annotated[
        Optional[str], pydantic.Field(alias="ipDenylistRegex")
    ] = "/^$/"
    r"""Messages from matched IP addresses will be ignored. This takes precedence over the allowlist."""

    cribl_api: Annotated[Optional[str], pydantic.Field(alias="criblAPI")] = "/cribl"
    r"""Absolute path on which to listen for the Cribl HTTP API requests. Only _bulk (default /cribl/_bulk) is available. Use empty string to disable."""

    elastic_api: Annotated[Optional[str], pydantic.Field(alias="elasticAPI")] = (
        "/elastic"
    )
    r"""Absolute path on which to listen for the Elasticsearch API requests. Only _bulk (default /elastic/_bulk) is available. Use empty string to disable."""

    splunk_hec_api: Annotated[Optional[str], pydantic.Field(alias="splunkHecAPI")] = (
        "/services/collector"
    )
    r"""Absolute path on which listen for the Splunk HTTP Event Collector API requests. Use empty string to disable."""

    splunk_hec_acks: Annotated[
        Optional[bool], pydantic.Field(alias="splunkHecAcks")
    ] = False

    metadata: Optional[List[MetadatumCriblLakeHTTP]] = None
    r"""Fields to add to events from this input"""

    auth_tokens_ext: Annotated[
        Optional[List[AuthTokensExtCriblLakeHTTP]],
        pydantic.Field(alias="authTokensExt"),
    ] = None

    description: Optional[str] = None

    @property
    def additional_properties(self):
        return self.__pydantic_extra__

    @additional_properties.setter
    def additional_properties(self, value):
        self.__pydantic_extra__ = value  # pyright: ignore[reportIncompatibleVariableOverride]


class InputTypeCriblHTTP(str, Enum):
    CRIBL_HTTP = "cribl_http"


class ConnectionCriblHTTPTypedDict(TypedDict):
    output: str
    pipeline: NotRequired[str]


class ConnectionCriblHTTP(BaseModel):
    output: str

    pipeline: Optional[str] = None


class PqModeCriblHTTP(str, Enum, metaclass=utils.OpenEnumMeta):
    r"""With Smart mode, PQ will write events to the filesystem only when it detects backpressure from the processing engine. With Always On mode, PQ will always write events directly to the queue before forwarding them to the processing engine."""

    # Smart
    SMART = "smart"
    # Always On
    ALWAYS = "always"


class PqCompressionCriblHTTP(str, Enum, metaclass=utils.OpenEnumMeta):
    r"""Codec to use to compress the persisted data"""

    # None
    NONE = "none"
    # Gzip
    GZIP = "gzip"


class InputPqControlsCriblHTTPTypedDict(TypedDict):
    pass


class InputPqControlsCriblHTTP(BaseModel):
    pass


class PqCriblHTTPTypedDict(TypedDict):
    mode: NotRequired[PqModeCriblHTTP]
    r"""With Smart mode, PQ will write events to the filesystem only when it detects backpressure from the processing engine. With Always On mode, PQ will always write events directly to the queue before forwarding them to the processing engine."""
    max_buffer_size: NotRequired[float]
    r"""The maximum number of events to hold in memory before writing the events to disk"""
    commit_frequency: NotRequired[float]
    r"""The number of events to send downstream before committing that Stream has read them"""
    max_file_size: NotRequired[str]
    r"""The maximum size to store in each queue file before closing and optionally compressing. Enter a numeral with units of KB, MB, etc."""
    max_size: NotRequired[str]
    r"""The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc."""
    path: NotRequired[str]
    r"""The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/inputs/<input-id>"""
    compress: NotRequired[PqCompressionCriblHTTP]
    r"""Codec to use to compress the persisted data"""
    pq_controls: NotRequired[InputPqControlsCriblHTTPTypedDict]


class PqCriblHTTP(BaseModel):
    mode: Annotated[
        Optional[PqModeCriblHTTP], PlainValidator(validate_open_enum(False))
    ] = PqModeCriblHTTP.ALWAYS
    r"""With Smart mode, PQ will write events to the filesystem only when it detects backpressure from the processing engine. With Always On mode, PQ will always write events directly to the queue before forwarding them to the processing engine."""

    max_buffer_size: Annotated[
        Optional[float], pydantic.Field(alias="maxBufferSize")
    ] = 1000
    r"""The maximum number of events to hold in memory before writing the events to disk"""

    commit_frequency: Annotated[
        Optional[float], pydantic.Field(alias="commitFrequency")
    ] = 42
    r"""The number of events to send downstream before committing that Stream has read them"""

    max_file_size: Annotated[Optional[str], pydantic.Field(alias="maxFileSize")] = (
        "1 MB"
    )
    r"""The maximum size to store in each queue file before closing and optionally compressing. Enter a numeral with units of KB, MB, etc."""

    max_size: Annotated[Optional[str], pydantic.Field(alias="maxSize")] = "5GB"
    r"""The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc."""

    path: Optional[str] = "$CRIBL_HOME/state/queues"
    r"""The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/inputs/<input-id>"""

    compress: Annotated[
        Optional[PqCompressionCriblHTTP], PlainValidator(validate_open_enum(False))
    ] = PqCompressionCriblHTTP.NONE
    r"""Codec to use to compress the persisted data"""

    pq_controls: Annotated[
        Optional[InputPqControlsCriblHTTP], pydantic.Field(alias="pqControls")
    ] = None

    @field_serializer("mode")
    def serialize_mode(self, value):
        if isinstance(value, str):
            try:
                return models.PqModeCriblHTTP(value)
            except ValueError:
                return value
        return value

    @field_serializer("compress")
    def serialize_compress(self, value):
        if isinstance(value, str):
            try:
                return models.PqCompressionCriblHTTP(value)
            except ValueError:
                return value
        return value


class InputAuthTokenCriblHTTPTypedDict(TypedDict):
    token_secret: str
    r"""Select or create a stored text secret"""
    enabled: NotRequired[bool]
    description: NotRequired[str]
    r"""Optional token description"""


class InputAuthTokenCriblHTTP(BaseModel):
    token_secret: Annotated[str, pydantic.Field(alias="tokenSecret")]
    r"""Select or create a stored text secret"""

    enabled: Optional[bool] = True

    description: Optional[str] = None
    r"""Optional token description"""


class InputMinimumTLSVersionCriblHTTP(str, Enum, metaclass=utils.OpenEnumMeta):
    TL_SV1 = "TLSv1"
    TL_SV1_1 = "TLSv1.1"
    TL_SV1_2 = "TLSv1.2"
    TL_SV1_3 = "TLSv1.3"


class InputMaximumTLSVersionCriblHTTP(str, Enum, metaclass=utils.OpenEnumMeta):
    TL_SV1 = "TLSv1"
    TL_SV1_1 = "TLSv1.1"
    TL_SV1_2 = "TLSv1.2"
    TL_SV1_3 = "TLSv1.3"


class TLSSettingsServerSideCriblHTTPTypedDict(TypedDict):
    disabled: NotRequired[bool]
    request_cert: NotRequired[bool]
    r"""Require clients to present their certificates. Used to perform client authentication using SSL certs."""
    reject_unauthorized: NotRequired[bool]
    r"""Reject certificates not authorized by a CA in the CA certificate path or by another trusted CA (such as the system's)"""
    common_name_regex: NotRequired[str]
    r"""Regex matching allowable common names in peer certificates' subject attribute"""
    certificate_name: NotRequired[str]
    r"""The name of the predefined certificate"""
    priv_key_path: NotRequired[str]
    r"""Path on server containing the private key to use. PEM format. Can reference $ENV_VARS."""
    passphrase: NotRequired[str]
    r"""Passphrase to use to decrypt private key"""
    cert_path: NotRequired[str]
    r"""Path on server containing certificates to use. PEM format. Can reference $ENV_VARS."""
    ca_path: NotRequired[str]
    r"""Path on server containing CA certificates to use. PEM format. Can reference $ENV_VARS."""
    min_version: NotRequired[InputMinimumTLSVersionCriblHTTP]
    max_version: NotRequired[InputMaximumTLSVersionCriblHTTP]


class TLSSettingsServerSideCriblHTTP(BaseModel):
    disabled: Optional[bool] = True

    request_cert: Annotated[Optional[bool], pydantic.Field(alias="requestCert")] = False
    r"""Require clients to present their certificates. Used to perform client authentication using SSL certs."""

    reject_unauthorized: Annotated[
        Optional[bool], pydantic.Field(alias="rejectUnauthorized")
    ] = True
    r"""Reject certificates not authorized by a CA in the CA certificate path or by another trusted CA (such as the system's)"""

    common_name_regex: Annotated[
        Optional[str], pydantic.Field(alias="commonNameRegex")
    ] = "/.*/"
    r"""Regex matching allowable common names in peer certificates' subject attribute"""

    certificate_name: Annotated[
        Optional[str], pydantic.Field(alias="certificateName")
    ] = None
    r"""The name of the predefined certificate"""

    priv_key_path: Annotated[Optional[str], pydantic.Field(alias="privKeyPath")] = None
    r"""Path on server containing the private key to use. PEM format. Can reference $ENV_VARS."""

    passphrase: Optional[str] = None
    r"""Passphrase to use to decrypt private key"""

    cert_path: Annotated[Optional[str], pydantic.Field(alias="certPath")] = None
    r"""Path on server containing certificates to use. PEM format. Can reference $ENV_VARS."""

    ca_path: Annotated[Optional[str], pydantic.Field(alias="caPath")] = None
    r"""Path on server containing CA certificates to use. PEM format. Can reference $ENV_VARS."""

    min_version: Annotated[
        Annotated[
            Optional[InputMinimumTLSVersionCriblHTTP],
            PlainValidator(validate_open_enum(False)),
        ],
        pydantic.Field(alias="minVersion"),
    ] = None

    max_version: Annotated[
        Annotated[
            Optional[InputMaximumTLSVersionCriblHTTP],
            PlainValidator(validate_open_enum(False)),
        ],
        pydantic.Field(alias="maxVersion"),
    ] = None

    @field_serializer("min_version")
    def serialize_min_version(self, value):
        if isinstance(value, str):
            try:
                return models.InputMinimumTLSVersionCriblHTTP(value)
            except ValueError:
                return value
        return value

    @field_serializer("max_version")
    def serialize_max_version(self, value):
        if isinstance(value, str):
            try:
                return models.InputMaximumTLSVersionCriblHTTP(value)
            except ValueError:
                return value
        return value


class MetadatumCriblHTTPTypedDict(TypedDict):
    name: str
    value: str
    r"""JavaScript expression to compute field's value, enclosed in quotes or backticks. (Can evaluate to a constant.)"""


class MetadatumCriblHTTP(BaseModel):
    name: str

    value: str
    r"""JavaScript expression to compute field's value, enclosed in quotes or backticks. (Can evaluate to a constant.)"""


class InputCriblHTTPTypedDict(TypedDict):
    type: InputTypeCriblHTTP
    port: float
    r"""Port to listen on"""
    id: NotRequired[str]
    r"""Unique ID for this input"""
    disabled: NotRequired[bool]
    pipeline: NotRequired[str]
    r"""Pipeline to process data from this Source before sending it through the Routes"""
    send_to_routes: NotRequired[bool]
    r"""Select whether to send data to Routes, or directly to Destinations."""
    environment: NotRequired[str]
    r"""Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere."""
    pq_enabled: NotRequired[bool]
    r"""Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers)."""
    streamtags: NotRequired[List[str]]
    r"""Tags for filtering and grouping in @{product}"""
    connections: NotRequired[List[ConnectionCriblHTTPTypedDict]]
    r"""Direct connections to Destinations, and optionally via a Pipeline or a Pack"""
    pq: NotRequired[PqCriblHTTPTypedDict]
    host: NotRequired[str]
    r"""Address to bind on. Defaults to 0.0.0.0 (all addresses)."""
    auth_tokens: NotRequired[List[InputAuthTokenCriblHTTPTypedDict]]
    r"""Shared secrets to be used by connected environments to authorize connections. These tokens should be installed in Cribl HTTP destinations in connected environments."""
    tls: NotRequired[TLSSettingsServerSideCriblHTTPTypedDict]
    max_active_req: NotRequired[float]
    r"""Maximum number of active requests allowed per Worker Process. Set to 0 for unlimited. Caution: Increasing the limit above the default value, or setting it to unlimited, may degrade performance and reduce throughput."""
    max_requests_per_socket: NotRequired[int]
    r"""Maximum number of requests per socket before @{product} instructs the client to close the connection. Default is 0 (unlimited)."""
    enable_proxy_header: NotRequired[bool]
    r"""Extract the client IP and port from PROXY protocol v1/v2. When enabled, the X-Forwarded-For header is ignored. Disable to use the X-Forwarded-For header for client IP extraction."""
    capture_headers: NotRequired[bool]
    r"""Add request headers to events, in the __headers field"""
    activity_log_sample_rate: NotRequired[float]
    r"""How often request activity is logged at the `info` level. A value of 1 would log every request, 10 every 10th request, etc."""
    request_timeout: NotRequired[float]
    r"""How long to wait for an incoming request to complete before aborting it. Use 0 to disable."""
    socket_timeout: NotRequired[float]
    r"""How long @{product} should wait before assuming that an inactive socket has timed out. To wait forever, set to 0."""
    keep_alive_timeout: NotRequired[float]
    r"""After the last response is sent, @{product} will wait this long for additional data before closing the socket connection. Minimum 1 second, maximum 600 seconds (10 minutes)."""
    enable_health_check: NotRequired[bool]
    r"""Expose the /cribl_health endpoint, which returns 200 OK when this Source is healthy"""
    ip_allowlist_regex: NotRequired[str]
    r"""Messages from matched IP addresses will be processed, unless also matched by the denylist"""
    ip_denylist_regex: NotRequired[str]
    r"""Messages from matched IP addresses will be ignored. This takes precedence over the allowlist."""
    metadata: NotRequired[List[MetadatumCriblHTTPTypedDict]]
    r"""Fields to add to events from this input"""
    description: NotRequired[str]


class InputCriblHTTP(BaseModel):
    model_config = ConfigDict(
        populate_by_name=True, arbitrary_types_allowed=True, extra="allow"
    )
    __pydantic_extra__: Dict[str, Any] = pydantic.Field(init=False)

    type: InputTypeCriblHTTP

    port: float
    r"""Port to listen on"""

    id: Optional[str] = None
    r"""Unique ID for this input"""

    disabled: Optional[bool] = False

    pipeline: Optional[str] = None
    r"""Pipeline to process data from this Source before sending it through the Routes"""

    send_to_routes: Annotated[Optional[bool], pydantic.Field(alias="sendToRoutes")] = (
        True
    )
    r"""Select whether to send data to Routes, or directly to Destinations."""

    environment: Optional[str] = None
    r"""Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere."""

    pq_enabled: Annotated[Optional[bool], pydantic.Field(alias="pqEnabled")] = False
    r"""Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers)."""

    streamtags: Optional[List[str]] = None
    r"""Tags for filtering and grouping in @{product}"""

    connections: Optional[List[ConnectionCriblHTTP]] = None
    r"""Direct connections to Destinations, and optionally via a Pipeline or a Pack"""

    pq: Optional[PqCriblHTTP] = None

    host: Optional[str] = "0.0.0.0"
    r"""Address to bind on. Defaults to 0.0.0.0 (all addresses)."""

    auth_tokens: Annotated[
        Optional[List[InputAuthTokenCriblHTTP]], pydantic.Field(alias="authTokens")
    ] = None
    r"""Shared secrets to be used by connected environments to authorize connections. These tokens should be installed in Cribl HTTP destinations in connected environments."""

    tls: Optional[TLSSettingsServerSideCriblHTTP] = None

    max_active_req: Annotated[Optional[float], pydantic.Field(alias="maxActiveReq")] = (
        256
    )
    r"""Maximum number of active requests allowed per Worker Process. Set to 0 for unlimited. Caution: Increasing the limit above the default value, or setting it to unlimited, may degrade performance and reduce throughput."""

    max_requests_per_socket: Annotated[
        Optional[int], pydantic.Field(alias="maxRequestsPerSocket")
    ] = 0
    r"""Maximum number of requests per socket before @{product} instructs the client to close the connection. Default is 0 (unlimited)."""

    enable_proxy_header: Annotated[
        Optional[bool], pydantic.Field(alias="enableProxyHeader")
    ] = False
    r"""Extract the client IP and port from PROXY protocol v1/v2. When enabled, the X-Forwarded-For header is ignored. Disable to use the X-Forwarded-For header for client IP extraction."""

    capture_headers: Annotated[
        Optional[bool], pydantic.Field(alias="captureHeaders")
    ] = False
    r"""Add request headers to events, in the __headers field"""

    activity_log_sample_rate: Annotated[
        Optional[float], pydantic.Field(alias="activityLogSampleRate")
    ] = 100
    r"""How often request activity is logged at the `info` level. A value of 1 would log every request, 10 every 10th request, etc."""

    request_timeout: Annotated[
        Optional[float], pydantic.Field(alias="requestTimeout")
    ] = 0
    r"""How long to wait for an incoming request to complete before aborting it. Use 0 to disable."""

    socket_timeout: Annotated[
        Optional[float], pydantic.Field(alias="socketTimeout")
    ] = 0
    r"""How long @{product} should wait before assuming that an inactive socket has timed out. To wait forever, set to 0."""

    keep_alive_timeout: Annotated[
        Optional[float], pydantic.Field(alias="keepAliveTimeout")
    ] = 5
    r"""After the last response is sent, @{product} will wait this long for additional data before closing the socket connection. Minimum 1 second, maximum 600 seconds (10 minutes)."""

    enable_health_check: Annotated[
        Optional[bool], pydantic.Field(alias="enableHealthCheck")
    ] = False
    r"""Expose the /cribl_health endpoint, which returns 200 OK when this Source is healthy"""

    ip_allowlist_regex: Annotated[
        Optional[str], pydantic.Field(alias="ipAllowlistRegex")
    ] = "/.*/"
    r"""Messages from matched IP addresses will be processed, unless also matched by the denylist"""

    ip_denylist_regex: Annotated[
        Optional[str], pydantic.Field(alias="ipDenylistRegex")
    ] = "/^$/"
    r"""Messages from matched IP addresses will be ignored. This takes precedence over the allowlist."""

    metadata: Optional[List[MetadatumCriblHTTP]] = None
    r"""Fields to add to events from this input"""

    description: Optional[str] = None

    @property
    def additional_properties(self):
        return self.__pydantic_extra__

    @additional_properties.setter
    def additional_properties(self, value):
        self.__pydantic_extra__ = value  # pyright: ignore[reportIncompatibleVariableOverride]


class InputTypeCriblTCP(str, Enum):
    CRIBL_TCP = "cribl_tcp"


class ConnectionCriblTCPTypedDict(TypedDict):
    output: str
    pipeline: NotRequired[str]


class ConnectionCriblTCP(BaseModel):
    output: str

    pipeline: Optional[str] = None


class PqModeCriblTCP(str, Enum, metaclass=utils.OpenEnumMeta):
    r"""With Smart mode, PQ will write events to the filesystem only when it detects backpressure from the processing engine. With Always On mode, PQ will always write events directly to the queue before forwarding them to the processing engine."""

    # Smart
    SMART = "smart"
    # Always On
    ALWAYS = "always"


class PqCompressionCriblTCP(str, Enum, metaclass=utils.OpenEnumMeta):
    r"""Codec to use to compress the persisted data"""

    # None
    NONE = "none"
    # Gzip
    GZIP = "gzip"


class InputPqControlsCriblTCPTypedDict(TypedDict):
    pass


class InputPqControlsCriblTCP(BaseModel):
    pass


class PqCriblTCPTypedDict(TypedDict):
    mode: NotRequired[PqModeCriblTCP]
    r"""With Smart mode, PQ will write events to the filesystem only when it detects backpressure from the processing engine. With Always On mode, PQ will always write events directly to the queue before forwarding them to the processing engine."""
    max_buffer_size: NotRequired[float]
    r"""The maximum number of events to hold in memory before writing the events to disk"""
    commit_frequency: NotRequired[float]
    r"""The number of events to send downstream before committing that Stream has read them"""
    max_file_size: NotRequired[str]
    r"""The maximum size to store in each queue file before closing and optionally compressing. Enter a numeral with units of KB, MB, etc."""
    max_size: NotRequired[str]
    r"""The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc."""
    path: NotRequired[str]
    r"""The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/inputs/<input-id>"""
    compress: NotRequired[PqCompressionCriblTCP]
    r"""Codec to use to compress the persisted data"""
    pq_controls: NotRequired[InputPqControlsCriblTCPTypedDict]


class PqCriblTCP(BaseModel):
    mode: Annotated[
        Optional[PqModeCriblTCP], PlainValidator(validate_open_enum(False))
    ] = PqModeCriblTCP.ALWAYS
    r"""With Smart mode, PQ will write events to the filesystem only when it detects backpressure from the processing engine. With Always On mode, PQ will always write events directly to the queue before forwarding them to the processing engine."""

    max_buffer_size: Annotated[
        Optional[float], pydantic.Field(alias="maxBufferSize")
    ] = 1000
    r"""The maximum number of events to hold in memory before writing the events to disk"""

    commit_frequency: Annotated[
        Optional[float], pydantic.Field(alias="commitFrequency")
    ] = 42
    r"""The number of events to send downstream before committing that Stream has read them"""

    max_file_size: Annotated[Optional[str], pydantic.Field(alias="maxFileSize")] = (
        "1 MB"
    )
    r"""The maximum size to store in each queue file before closing and optionally compressing. Enter a numeral with units of KB, MB, etc."""

    max_size: Annotated[Optional[str], pydantic.Field(alias="maxSize")] = "5GB"
    r"""The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc."""

    path: Optional[str] = "$CRIBL_HOME/state/queues"
    r"""The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/inputs/<input-id>"""

    compress: Annotated[
        Optional[PqCompressionCriblTCP], PlainValidator(validate_open_enum(False))
    ] = PqCompressionCriblTCP.NONE
    r"""Codec to use to compress the persisted data"""

    pq_controls: Annotated[
        Optional[InputPqControlsCriblTCP], pydantic.Field(alias="pqControls")
    ] = None

    @field_serializer("mode")
    def serialize_mode(self, value):
        if isinstance(value, str):
            try:
                return models.PqModeCriblTCP(value)
            except ValueError:
                return value
        return value

    @field_serializer("compress")
    def serialize_compress(self, value):
        if isinstance(value, str):
            try:
                return models.PqCompressionCriblTCP(value)
            except ValueError:
                return value
        return value


class InputMinimumTLSVersionCriblTCP(str, Enum, metaclass=utils.OpenEnumMeta):
    TL_SV1 = "TLSv1"
    TL_SV1_1 = "TLSv1.1"
    TL_SV1_2 = "TLSv1.2"
    TL_SV1_3 = "TLSv1.3"


class InputMaximumTLSVersionCriblTCP(str, Enum, metaclass=utils.OpenEnumMeta):
    TL_SV1 = "TLSv1"
    TL_SV1_1 = "TLSv1.1"
    TL_SV1_2 = "TLSv1.2"
    TL_SV1_3 = "TLSv1.3"


class TLSSettingsServerSideCriblTCPTypedDict(TypedDict):
    disabled: NotRequired[bool]
    request_cert: NotRequired[bool]
    r"""Require clients to present their certificates. Used to perform client authentication using SSL certs."""
    reject_unauthorized: NotRequired[bool]
    r"""Reject certificates not authorized by a CA in the CA certificate path or by another trusted CA (such as the system's)"""
    common_name_regex: NotRequired[str]
    r"""Regex matching allowable common names in peer certificates' subject attribute"""
    certificate_name: NotRequired[str]
    r"""The name of the predefined certificate"""
    priv_key_path: NotRequired[str]
    r"""Path on server containing the private key to use. PEM format. Can reference $ENV_VARS."""
    passphrase: NotRequired[str]
    r"""Passphrase to use to decrypt private key"""
    cert_path: NotRequired[str]
    r"""Path on server containing certificates to use. PEM format. Can reference $ENV_VARS."""
    ca_path: NotRequired[str]
    r"""Path on server containing CA certificates to use. PEM format. Can reference $ENV_VARS."""
    min_version: NotRequired[InputMinimumTLSVersionCriblTCP]
    max_version: NotRequired[InputMaximumTLSVersionCriblTCP]


class TLSSettingsServerSideCriblTCP(BaseModel):
    disabled: Optional[bool] = True

    request_cert: Annotated[Optional[bool], pydantic.Field(alias="requestCert")] = False
    r"""Require clients to present their certificates. Used to perform client authentication using SSL certs."""

    reject_unauthorized: Annotated[
        Optional[bool], pydantic.Field(alias="rejectUnauthorized")
    ] = True
    r"""Reject certificates not authorized by a CA in the CA certificate path or by another trusted CA (such as the system's)"""

    common_name_regex: Annotated[
        Optional[str], pydantic.Field(alias="commonNameRegex")
    ] = "/.*/"
    r"""Regex matching allowable common names in peer certificates' subject attribute"""

    certificate_name: Annotated[
        Optional[str], pydantic.Field(alias="certificateName")
    ] = None
    r"""The name of the predefined certificate"""

    priv_key_path: Annotated[Optional[str], pydantic.Field(alias="privKeyPath")] = None
    r"""Path on server containing the private key to use. PEM format. Can reference $ENV_VARS."""

    passphrase: Optional[str] = None
    r"""Passphrase to use to decrypt private key"""

    cert_path: Annotated[Optional[str], pydantic.Field(alias="certPath")] = None
    r"""Path on server containing certificates to use. PEM format. Can reference $ENV_VARS."""

    ca_path: Annotated[Optional[str], pydantic.Field(alias="caPath")] = None
    r"""Path on server containing CA certificates to use. PEM format. Can reference $ENV_VARS."""

    min_version: Annotated[
        Annotated[
            Optional[InputMinimumTLSVersionCriblTCP],
            PlainValidator(validate_open_enum(False)),
        ],
        pydantic.Field(alias="minVersion"),
    ] = None

    max_version: Annotated[
        Annotated[
            Optional[InputMaximumTLSVersionCriblTCP],
            PlainValidator(validate_open_enum(False)),
        ],
        pydantic.Field(alias="maxVersion"),
    ] = None

    @field_serializer("min_version")
    def serialize_min_version(self, value):
        if isinstance(value, str):
            try:
                return models.InputMinimumTLSVersionCriblTCP(value)
            except ValueError:
                return value
        return value

    @field_serializer("max_version")
    def serialize_max_version(self, value):
        if isinstance(value, str):
            try:
                return models.InputMaximumTLSVersionCriblTCP(value)
            except ValueError:
                return value
        return value


class MetadatumCriblTCPTypedDict(TypedDict):
    name: str
    value: str
    r"""JavaScript expression to compute field's value, enclosed in quotes or backticks. (Can evaluate to a constant.)"""


class MetadatumCriblTCP(BaseModel):
    name: str

    value: str
    r"""JavaScript expression to compute field's value, enclosed in quotes or backticks. (Can evaluate to a constant.)"""


class InputAuthTokenCriblTCPTypedDict(TypedDict):
    token_secret: str
    r"""Select or create a stored text secret"""
    enabled: NotRequired[bool]
    description: NotRequired[str]
    r"""Optional token description"""


class InputAuthTokenCriblTCP(BaseModel):
    token_secret: Annotated[str, pydantic.Field(alias="tokenSecret")]
    r"""Select or create a stored text secret"""

    enabled: Optional[bool] = True

    description: Optional[str] = None
    r"""Optional token description"""


class InputCriblTCPTypedDict(TypedDict):
    type: InputTypeCriblTCP
    port: float
    r"""Port to listen on"""
    id: NotRequired[str]
    r"""Unique ID for this input"""
    disabled: NotRequired[bool]
    pipeline: NotRequired[str]
    r"""Pipeline to process data from this Source before sending it through the Routes"""
    send_to_routes: NotRequired[bool]
    r"""Select whether to send data to Routes, or directly to Destinations."""
    environment: NotRequired[str]
    r"""Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere."""
    pq_enabled: NotRequired[bool]
    r"""Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers)."""
    streamtags: NotRequired[List[str]]
    r"""Tags for filtering and grouping in @{product}"""
    connections: NotRequired[List[ConnectionCriblTCPTypedDict]]
    r"""Direct connections to Destinations, and optionally via a Pipeline or a Pack"""
    pq: NotRequired[PqCriblTCPTypedDict]
    host: NotRequired[str]
    r"""Address to bind on. Defaults to 0.0.0.0 (all addresses)."""
    tls: NotRequired[TLSSettingsServerSideCriblTCPTypedDict]
    max_active_cxn: NotRequired[float]
    r"""Maximum number of active connections allowed per Worker Process. Use 0 for unlimited."""
    socket_idle_timeout: NotRequired[float]
    r"""How long @{product} should wait before assuming that an inactive socket has timed out. After this time, the connection will be closed. Leave at 0 for no inactive socket monitoring."""
    socket_ending_max_wait: NotRequired[float]
    r"""How long the server will wait after initiating a closure for a client to close its end of the connection. If the client doesn't close the connection within this time, the server will forcefully terminate the socket to prevent resource leaks and ensure efficient connection cleanup and system stability. Leave at 0 for no inactive socket monitoring."""
    socket_max_lifespan: NotRequired[float]
    r"""The maximum duration a socket can remain open, even if active. This helps manage resources and mitigate issues caused by TCP pinning. Set to 0 to disable."""
    enable_proxy_header: NotRequired[bool]
    r"""Enable if the connection is proxied by a device that supports proxy protocol v1 or v2"""
    metadata: NotRequired[List[MetadatumCriblTCPTypedDict]]
    r"""Fields to add to events from this input"""
    enable_load_balancing: NotRequired[bool]
    r"""Load balance traffic across all Worker Processes"""
    auth_tokens: NotRequired[List[InputAuthTokenCriblTCPTypedDict]]
    r"""Shared secrets to be used by connected environments to authorize connections. These tokens should be installed in Cribl TCP destinations in connected environments."""
    description: NotRequired[str]


class InputCriblTCP(BaseModel):
    model_config = ConfigDict(
        populate_by_name=True, arbitrary_types_allowed=True, extra="allow"
    )
    __pydantic_extra__: Dict[str, Any] = pydantic.Field(init=False)

    type: InputTypeCriblTCP

    port: float
    r"""Port to listen on"""

    id: Optional[str] = None
    r"""Unique ID for this input"""

    disabled: Optional[bool] = False

    pipeline: Optional[str] = None
    r"""Pipeline to process data from this Source before sending it through the Routes"""

    send_to_routes: Annotated[Optional[bool], pydantic.Field(alias="sendToRoutes")] = (
        True
    )
    r"""Select whether to send data to Routes, or directly to Destinations."""

    environment: Optional[str] = None
    r"""Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere."""

    pq_enabled: Annotated[Optional[bool], pydantic.Field(alias="pqEnabled")] = False
    r"""Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers)."""

    streamtags: Optional[List[str]] = None
    r"""Tags for filtering and grouping in @{product}"""

    connections: Optional[List[ConnectionCriblTCP]] = None
    r"""Direct connections to Destinations, and optionally via a Pipeline or a Pack"""

    pq: Optional[PqCriblTCP] = None

    host: Optional[str] = "0.0.0.0"
    r"""Address to bind on. Defaults to 0.0.0.0 (all addresses)."""

    tls: Optional[TLSSettingsServerSideCriblTCP] = None

    max_active_cxn: Annotated[Optional[float], pydantic.Field(alias="maxActiveCxn")] = (
        1000
    )
    r"""Maximum number of active connections allowed per Worker Process. Use 0 for unlimited."""

    socket_idle_timeout: Annotated[
        Optional[float], pydantic.Field(alias="socketIdleTimeout")
    ] = 0
    r"""How long @{product} should wait before assuming that an inactive socket has timed out. After this time, the connection will be closed. Leave at 0 for no inactive socket monitoring."""

    socket_ending_max_wait: Annotated[
        Optional[float], pydantic.Field(alias="socketEndingMaxWait")
    ] = 30
    r"""How long the server will wait after initiating a closure for a client to close its end of the connection. If the client doesn't close the connection within this time, the server will forcefully terminate the socket to prevent resource leaks and ensure efficient connection cleanup and system stability. Leave at 0 for no inactive socket monitoring."""

    socket_max_lifespan: Annotated[
        Optional[float], pydantic.Field(alias="socketMaxLifespan")
    ] = 0
    r"""The maximum duration a socket can remain open, even if active. This helps manage resources and mitigate issues caused by TCP pinning. Set to 0 to disable."""

    enable_proxy_header: Annotated[
        Optional[bool], pydantic.Field(alias="enableProxyHeader")
    ] = False
    r"""Enable if the connection is proxied by a device that supports proxy protocol v1 or v2"""

    metadata: Optional[List[MetadatumCriblTCP]] = None
    r"""Fields to add to events from this input"""

    enable_load_balancing: Annotated[
        Optional[bool], pydantic.Field(alias="enableLoadBalancing")
    ] = False
    r"""Load balance traffic across all Worker Processes"""

    auth_tokens: Annotated[
        Optional[List[InputAuthTokenCriblTCP]], pydantic.Field(alias="authTokens")
    ] = None
    r"""Shared secrets to be used by connected environments to authorize connections. These tokens should be installed in Cribl TCP destinations in connected environments."""

    description: Optional[str] = None

    @property
    def additional_properties(self):
        return self.__pydantic_extra__

    @additional_properties.setter
    def additional_properties(self, value):
        self.__pydantic_extra__ = value  # pyright: ignore[reportIncompatibleVariableOverride]


class TypeCribl(str, Enum):
    CRIBL = "cribl"


class ConnectionCriblTypedDict(TypedDict):
    output: str
    pipeline: NotRequired[str]


class ConnectionCribl(BaseModel):
    output: str

    pipeline: Optional[str] = None


class ModeCribl(str, Enum, metaclass=utils.OpenEnumMeta):
    r"""With Smart mode, PQ will write events to the filesystem only when it detects backpressure from the processing engine. With Always On mode, PQ will always write events directly to the queue before forwarding them to the processing engine."""

    # Smart
    SMART = "smart"
    # Always On
    ALWAYS = "always"


class CompressionCribl(str, Enum, metaclass=utils.OpenEnumMeta):
    r"""Codec to use to compress the persisted data"""

    # None
    NONE = "none"
    # Gzip
    GZIP = "gzip"


class PqControlsCriblTypedDict(TypedDict):
    pass


class PqControlsCribl(BaseModel):
    pass


class PqCriblTypedDict(TypedDict):
    mode: NotRequired[ModeCribl]
    r"""With Smart mode, PQ will write events to the filesystem only when it detects backpressure from the processing engine. With Always On mode, PQ will always write events directly to the queue before forwarding them to the processing engine."""
    max_buffer_size: NotRequired[float]
    r"""The maximum number of events to hold in memory before writing the events to disk"""
    commit_frequency: NotRequired[float]
    r"""The number of events to send downstream before committing that Stream has read them"""
    max_file_size: NotRequired[str]
    r"""The maximum size to store in each queue file before closing and optionally compressing. Enter a numeral with units of KB, MB, etc."""
    max_size: NotRequired[str]
    r"""The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc."""
    path: NotRequired[str]
    r"""The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/inputs/<input-id>"""
    compress: NotRequired[CompressionCribl]
    r"""Codec to use to compress the persisted data"""
    pq_controls: NotRequired[PqControlsCriblTypedDict]


class PqCribl(BaseModel):
    mode: Annotated[Optional[ModeCribl], PlainValidator(validate_open_enum(False))] = (
        ModeCribl.ALWAYS
    )
    r"""With Smart mode, PQ will write events to the filesystem only when it detects backpressure from the processing engine. With Always On mode, PQ will always write events directly to the queue before forwarding them to the processing engine."""

    max_buffer_size: Annotated[
        Optional[float], pydantic.Field(alias="maxBufferSize")
    ] = 1000
    r"""The maximum number of events to hold in memory before writing the events to disk"""

    commit_frequency: Annotated[
        Optional[float], pydantic.Field(alias="commitFrequency")
    ] = 42
    r"""The number of events to send downstream before committing that Stream has read them"""

    max_file_size: Annotated[Optional[str], pydantic.Field(alias="maxFileSize")] = (
        "1 MB"
    )
    r"""The maximum size to store in each queue file before closing and optionally compressing. Enter a numeral with units of KB, MB, etc."""

    max_size: Annotated[Optional[str], pydantic.Field(alias="maxSize")] = "5GB"
    r"""The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc."""

    path: Optional[str] = "$CRIBL_HOME/state/queues"
    r"""The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/inputs/<input-id>"""

    compress: Annotated[
        Optional[CompressionCribl], PlainValidator(validate_open_enum(False))
    ] = CompressionCribl.NONE
    r"""Codec to use to compress the persisted data"""

    pq_controls: Annotated[
        Optional[PqControlsCribl], pydantic.Field(alias="pqControls")
    ] = None

    @field_serializer("mode")
    def serialize_mode(self, value):
        if isinstance(value, str):
            try:
                return models.ModeCribl(value)
            except ValueError:
                return value
        return value

    @field_serializer("compress")
    def serialize_compress(self, value):
        if isinstance(value, str):
            try:
                return models.CompressionCribl(value)
            except ValueError:
                return value
        return value


class MetadatumCriblTypedDict(TypedDict):
    name: str
    value: str
    r"""JavaScript expression to compute field's value, enclosed in quotes or backticks. (Can evaluate to a constant.)"""


class MetadatumCribl(BaseModel):
    name: str

    value: str
    r"""JavaScript expression to compute field's value, enclosed in quotes or backticks. (Can evaluate to a constant.)"""


class InputCriblTypedDict(TypedDict):
    type: TypeCribl
    id: NotRequired[str]
    r"""Unique ID for this input"""
    disabled: NotRequired[bool]
    pipeline: NotRequired[str]
    r"""Pipeline to process data from this Source before sending it through the Routes"""
    send_to_routes: NotRequired[bool]
    r"""Select whether to send data to Routes, or directly to Destinations."""
    environment: NotRequired[str]
    r"""Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere."""
    pq_enabled: NotRequired[bool]
    r"""Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers)."""
    streamtags: NotRequired[List[str]]
    r"""Tags for filtering and grouping in @{product}"""
    connections: NotRequired[List[ConnectionCriblTypedDict]]
    r"""Direct connections to Destinations, and optionally via a Pipeline or a Pack"""
    pq: NotRequired[PqCriblTypedDict]
    filter_: NotRequired[str]
    metadata: NotRequired[List[MetadatumCriblTypedDict]]
    r"""Fields to add to events from this input"""
    description: NotRequired[str]


class InputCribl(BaseModel):
    model_config = ConfigDict(
        populate_by_name=True, arbitrary_types_allowed=True, extra="allow"
    )
    __pydantic_extra__: Dict[str, Any] = pydantic.Field(init=False)

    type: TypeCribl

    id: Optional[str] = None
    r"""Unique ID for this input"""

    disabled: Optional[bool] = False

    pipeline: Optional[str] = None
    r"""Pipeline to process data from this Source before sending it through the Routes"""

    send_to_routes: Annotated[Optional[bool], pydantic.Field(alias="sendToRoutes")] = (
        True
    )
    r"""Select whether to send data to Routes, or directly to Destinations."""

    environment: Optional[str] = None
    r"""Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere."""

    pq_enabled: Annotated[Optional[bool], pydantic.Field(alias="pqEnabled")] = False
    r"""Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers)."""

    streamtags: Optional[List[str]] = None
    r"""Tags for filtering and grouping in @{product}"""

    connections: Optional[List[ConnectionCribl]] = None
    r"""Direct connections to Destinations, and optionally via a Pipeline or a Pack"""

    pq: Optional[PqCribl] = None

    filter_: Annotated[Optional[str], pydantic.Field(alias="filter")] = None

    metadata: Optional[List[MetadatumCribl]] = None
    r"""Fields to add to events from this input"""

    description: Optional[str] = None

    @property
    def additional_properties(self):
        return self.__pydantic_extra__

    @additional_properties.setter
    def additional_properties(self, value):
        self.__pydantic_extra__ = value  # pyright: ignore[reportIncompatibleVariableOverride]


class InputTypeGooglePubsub(str, Enum):
    GOOGLE_PUBSUB = "google_pubsub"


class ConnectionGooglePubsubTypedDict(TypedDict):
    output: str
    pipeline: NotRequired[str]


class ConnectionGooglePubsub(BaseModel):
    output: str

    pipeline: Optional[str] = None


class PqModeGooglePubsub(str, Enum, metaclass=utils.OpenEnumMeta):
    r"""With Smart mode, PQ will write events to the filesystem only when it detects backpressure from the processing engine. With Always On mode, PQ will always write events directly to the queue before forwarding them to the processing engine."""

    # Smart
    SMART = "smart"
    # Always On
    ALWAYS = "always"


class PqCompressionGooglePubsub(str, Enum, metaclass=utils.OpenEnumMeta):
    r"""Codec to use to compress the persisted data"""

    # None
    NONE = "none"
    # Gzip
    GZIP = "gzip"


class InputPqControlsGooglePubsubTypedDict(TypedDict):
    pass


class InputPqControlsGooglePubsub(BaseModel):
    pass


class PqGooglePubsubTypedDict(TypedDict):
    mode: NotRequired[PqModeGooglePubsub]
    r"""With Smart mode, PQ will write events to the filesystem only when it detects backpressure from the processing engine. With Always On mode, PQ will always write events directly to the queue before forwarding them to the processing engine."""
    max_buffer_size: NotRequired[float]
    r"""The maximum number of events to hold in memory before writing the events to disk"""
    commit_frequency: NotRequired[float]
    r"""The number of events to send downstream before committing that Stream has read them"""
    max_file_size: NotRequired[str]
    r"""The maximum size to store in each queue file before closing and optionally compressing. Enter a numeral with units of KB, MB, etc."""
    max_size: NotRequired[str]
    r"""The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc."""
    path: NotRequired[str]
    r"""The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/inputs/<input-id>"""
    compress: NotRequired[PqCompressionGooglePubsub]
    r"""Codec to use to compress the persisted data"""
    pq_controls: NotRequired[InputPqControlsGooglePubsubTypedDict]


class PqGooglePubsub(BaseModel):
    mode: Annotated[
        Optional[PqModeGooglePubsub], PlainValidator(validate_open_enum(False))
    ] = PqModeGooglePubsub.ALWAYS
    r"""With Smart mode, PQ will write events to the filesystem only when it detects backpressure from the processing engine. With Always On mode, PQ will always write events directly to the queue before forwarding them to the processing engine."""

    max_buffer_size: Annotated[
        Optional[float], pydantic.Field(alias="maxBufferSize")
    ] = 1000
    r"""The maximum number of events to hold in memory before writing the events to disk"""

    commit_frequency: Annotated[
        Optional[float], pydantic.Field(alias="commitFrequency")
    ] = 42
    r"""The number of events to send downstream before committing that Stream has read them"""

    max_file_size: Annotated[Optional[str], pydantic.Field(alias="maxFileSize")] = (
        "1 MB"
    )
    r"""The maximum size to store in each queue file before closing and optionally compressing. Enter a numeral with units of KB, MB, etc."""

    max_size: Annotated[Optional[str], pydantic.Field(alias="maxSize")] = "5GB"
    r"""The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc."""

    path: Optional[str] = "$CRIBL_HOME/state/queues"
    r"""The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/inputs/<input-id>"""

    compress: Annotated[
        Optional[PqCompressionGooglePubsub], PlainValidator(validate_open_enum(False))
    ] = PqCompressionGooglePubsub.NONE
    r"""Codec to use to compress the persisted data"""

    pq_controls: Annotated[
        Optional[InputPqControlsGooglePubsub], pydantic.Field(alias="pqControls")
    ] = None

    @field_serializer("mode")
    def serialize_mode(self, value):
        if isinstance(value, str):
            try:
                return models.PqModeGooglePubsub(value)
            except ValueError:
                return value
        return value

    @field_serializer("compress")
    def serialize_compress(self, value):
        if isinstance(value, str):
            try:
                return models.PqCompressionGooglePubsub(value)
            except ValueError:
                return value
        return value


class InputGoogleAuthenticationMethod(str, Enum, metaclass=utils.OpenEnumMeta):
    r"""Choose Auto to use Google Application Default Credentials (ADC), Manual to enter Google service account credentials directly, or Secret to select or create a stored secret that references Google service account credentials."""

    # Auto
    AUTO = "auto"
    # Manual
    MANUAL = "manual"
    # Secret
    SECRET = "secret"


class MetadatumGooglePubsubTypedDict(TypedDict):
    name: str
    value: str
    r"""JavaScript expression to compute field's value, enclosed in quotes or backticks. (Can evaluate to a constant.)"""


class MetadatumGooglePubsub(BaseModel):
    name: str

    value: str
    r"""JavaScript expression to compute field's value, enclosed in quotes or backticks. (Can evaluate to a constant.)"""


class InputGooglePubsubTypedDict(TypedDict):
    type: InputTypeGooglePubsub
    subscription_name: str
    r"""ID of the subscription to use when receiving events. When Monitor subscription is enabled, the fully qualified subscription name must be entered. Example: projects/myProject/subscriptions/mySubscription"""
    id: NotRequired[str]
    r"""Unique ID for this input"""
    disabled: NotRequired[bool]
    pipeline: NotRequired[str]
    r"""Pipeline to process data from this Source before sending it through the Routes"""
    send_to_routes: NotRequired[bool]
    r"""Select whether to send data to Routes, or directly to Destinations."""
    environment: NotRequired[str]
    r"""Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere."""
    pq_enabled: NotRequired[bool]
    r"""Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers)."""
    streamtags: NotRequired[List[str]]
    r"""Tags for filtering and grouping in @{product}"""
    connections: NotRequired[List[ConnectionGooglePubsubTypedDict]]
    r"""Direct connections to Destinations, and optionally via a Pipeline or a Pack"""
    pq: NotRequired[PqGooglePubsubTypedDict]
    topic_name: NotRequired[str]
    r"""ID of the topic to receive events from. When Monitor subscription is enabled, any value may be entered."""
    monitor_subscription: NotRequired[bool]
    r"""Use when the subscription is not created by this Source and topic is not known"""
    create_topic: NotRequired[bool]
    r"""Create topic if it does not exist"""
    create_subscription: NotRequired[bool]
    r"""Create subscription if it does not exist"""
    region: NotRequired[str]
    r"""Region to retrieve messages from. Select 'default' to allow Google to auto-select the nearest region. When using ordered delivery, the selected region must be allowed by message storage policy."""
    google_auth_method: NotRequired[InputGoogleAuthenticationMethod]
    r"""Choose Auto to use Google Application Default Credentials (ADC), Manual to enter Google service account credentials directly, or Secret to select or create a stored secret that references Google service account credentials."""
    service_account_credentials: NotRequired[str]
    r"""Contents of service account credentials (JSON keys) file downloaded from Google Cloud. To upload a file, click the upload button at this field's upper right."""
    secret: NotRequired[str]
    r"""Select or create a stored text secret"""
    max_backlog: NotRequired[float]
    r"""If Destination exerts backpressure, this setting limits how many inbound events Stream will queue for processing before it stops retrieving events"""
    concurrency: NotRequired[float]
    r"""How many streams to pull messages from at one time. Doubling the value doubles the number of messages this Source pulls from the topic (if available), while consuming more CPU and memory. Defaults to 5."""
    request_timeout: NotRequired[float]
    r"""Pull request timeout, in milliseconds"""
    metadata: NotRequired[List[MetadatumGooglePubsubTypedDict]]
    r"""Fields to add to events from this input"""
    description: NotRequired[str]
    ordered_delivery: NotRequired[bool]
    r"""Receive events in the order they were added to the queue. The process sending events must have ordering enabled."""


class InputGooglePubsub(BaseModel):
    model_config = ConfigDict(
        populate_by_name=True, arbitrary_types_allowed=True, extra="allow"
    )
    __pydantic_extra__: Dict[str, Any] = pydantic.Field(init=False)

    type: InputTypeGooglePubsub

    subscription_name: Annotated[str, pydantic.Field(alias="subscriptionName")]
    r"""ID of the subscription to use when receiving events. When Monitor subscription is enabled, the fully qualified subscription name must be entered. Example: projects/myProject/subscriptions/mySubscription"""

    id: Optional[str] = None
    r"""Unique ID for this input"""

    disabled: Optional[bool] = False

    pipeline: Optional[str] = None
    r"""Pipeline to process data from this Source before sending it through the Routes"""

    send_to_routes: Annotated[Optional[bool], pydantic.Field(alias="sendToRoutes")] = (
        True
    )
    r"""Select whether to send data to Routes, or directly to Destinations."""

    environment: Optional[str] = None
    r"""Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere."""

    pq_enabled: Annotated[Optional[bool], pydantic.Field(alias="pqEnabled")] = False
    r"""Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers)."""

    streamtags: Optional[List[str]] = None
    r"""Tags for filtering and grouping in @{product}"""

    connections: Optional[List[ConnectionGooglePubsub]] = None
    r"""Direct connections to Destinations, and optionally via a Pipeline or a Pack"""

    pq: Optional[PqGooglePubsub] = None

    topic_name: Annotated[Optional[str], pydantic.Field(alias="topicName")] = "cribl"
    r"""ID of the topic to receive events from. When Monitor subscription is enabled, any value may be entered."""

    monitor_subscription: Annotated[
        Optional[bool], pydantic.Field(alias="monitorSubscription")
    ] = False
    r"""Use when the subscription is not created by this Source and topic is not known"""

    create_topic: Annotated[Optional[bool], pydantic.Field(alias="createTopic")] = False
    r"""Create topic if it does not exist"""

    create_subscription: Annotated[
        Optional[bool], pydantic.Field(alias="createSubscription")
    ] = True
    r"""Create subscription if it does not exist"""

    region: Optional[str] = None
    r"""Region to retrieve messages from. Select 'default' to allow Google to auto-select the nearest region. When using ordered delivery, the selected region must be allowed by message storage policy."""

    google_auth_method: Annotated[
        Annotated[
            Optional[InputGoogleAuthenticationMethod],
            PlainValidator(validate_open_enum(False)),
        ],
        pydantic.Field(alias="googleAuthMethod"),
    ] = InputGoogleAuthenticationMethod.MANUAL
    r"""Choose Auto to use Google Application Default Credentials (ADC), Manual to enter Google service account credentials directly, or Secret to select or create a stored secret that references Google service account credentials."""

    service_account_credentials: Annotated[
        Optional[str], pydantic.Field(alias="serviceAccountCredentials")
    ] = None
    r"""Contents of service account credentials (JSON keys) file downloaded from Google Cloud. To upload a file, click the upload button at this field's upper right."""

    secret: Optional[str] = None
    r"""Select or create a stored text secret"""

    max_backlog: Annotated[Optional[float], pydantic.Field(alias="maxBacklog")] = 1000
    r"""If Destination exerts backpressure, this setting limits how many inbound events Stream will queue for processing before it stops retrieving events"""

    concurrency: Optional[float] = 5
    r"""How many streams to pull messages from at one time. Doubling the value doubles the number of messages this Source pulls from the topic (if available), while consuming more CPU and memory. Defaults to 5."""

    request_timeout: Annotated[
        Optional[float], pydantic.Field(alias="requestTimeout")
    ] = 60000
    r"""Pull request timeout, in milliseconds"""

    metadata: Optional[List[MetadatumGooglePubsub]] = None
    r"""Fields to add to events from this input"""

    description: Optional[str] = None

    ordered_delivery: Annotated[
        Optional[bool], pydantic.Field(alias="orderedDelivery")
    ] = False
    r"""Receive events in the order they were added to the queue. The process sending events must have ordering enabled."""

    @property
    def additional_properties(self):
        return self.__pydantic_extra__

    @additional_properties.setter
    def additional_properties(self, value):
        self.__pydantic_extra__ = value  # pyright: ignore[reportIncompatibleVariableOverride]

    @field_serializer("google_auth_method")
    def serialize_google_auth_method(self, value):
        if isinstance(value, str):
            try:
                return models.InputGoogleAuthenticationMethod(value)
            except ValueError:
                return value
        return value


class TypeFirehose(str, Enum):
    FIREHOSE = "firehose"


class ConnectionFirehoseTypedDict(TypedDict):
    output: str
    pipeline: NotRequired[str]


class ConnectionFirehose(BaseModel):
    output: str

    pipeline: Optional[str] = None


class ModeFirehose(str, Enum, metaclass=utils.OpenEnumMeta):
    r"""With Smart mode, PQ will write events to the filesystem only when it detects backpressure from the processing engine. With Always On mode, PQ will always write events directly to the queue before forwarding them to the processing engine."""

    # Smart
    SMART = "smart"
    # Always On
    ALWAYS = "always"


class CompressionFirehose(str, Enum, metaclass=utils.OpenEnumMeta):
    r"""Codec to use to compress the persisted data"""

    # None
    NONE = "none"
    # Gzip
    GZIP = "gzip"


class PqControlsFirehoseTypedDict(TypedDict):
    pass


class PqControlsFirehose(BaseModel):
    pass


class PqFirehoseTypedDict(TypedDict):
    mode: NotRequired[ModeFirehose]
    r"""With Smart mode, PQ will write events to the filesystem only when it detects backpressure from the processing engine. With Always On mode, PQ will always write events directly to the queue before forwarding them to the processing engine."""
    max_buffer_size: NotRequired[float]
    r"""The maximum number of events to hold in memory before writing the events to disk"""
    commit_frequency: NotRequired[float]
    r"""The number of events to send downstream before committing that Stream has read them"""
    max_file_size: NotRequired[str]
    r"""The maximum size to store in each queue file before closing and optionally compressing. Enter a numeral with units of KB, MB, etc."""
    max_size: NotRequired[str]
    r"""The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc."""
    path: NotRequired[str]
    r"""The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/inputs/<input-id>"""
    compress: NotRequired[CompressionFirehose]
    r"""Codec to use to compress the persisted data"""
    pq_controls: NotRequired[PqControlsFirehoseTypedDict]


class PqFirehose(BaseModel):
    mode: Annotated[
        Optional[ModeFirehose], PlainValidator(validate_open_enum(False))
    ] = ModeFirehose.ALWAYS
    r"""With Smart mode, PQ will write events to the filesystem only when it detects backpressure from the processing engine. With Always On mode, PQ will always write events directly to the queue before forwarding them to the processing engine."""

    max_buffer_size: Annotated[
        Optional[float], pydantic.Field(alias="maxBufferSize")
    ] = 1000
    r"""The maximum number of events to hold in memory before writing the events to disk"""

    commit_frequency: Annotated[
        Optional[float], pydantic.Field(alias="commitFrequency")
    ] = 42
    r"""The number of events to send downstream before committing that Stream has read them"""

    max_file_size: Annotated[Optional[str], pydantic.Field(alias="maxFileSize")] = (
        "1 MB"
    )
    r"""The maximum size to store in each queue file before closing and optionally compressing. Enter a numeral with units of KB, MB, etc."""

    max_size: Annotated[Optional[str], pydantic.Field(alias="maxSize")] = "5GB"
    r"""The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc."""

    path: Optional[str] = "$CRIBL_HOME/state/queues"
    r"""The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/inputs/<input-id>"""

    compress: Annotated[
        Optional[CompressionFirehose], PlainValidator(validate_open_enum(False))
    ] = CompressionFirehose.NONE
    r"""Codec to use to compress the persisted data"""

    pq_controls: Annotated[
        Optional[PqControlsFirehose], pydantic.Field(alias="pqControls")
    ] = None

    @field_serializer("mode")
    def serialize_mode(self, value):
        if isinstance(value, str):
            try:
                return models.ModeFirehose(value)
            except ValueError:
                return value
        return value

    @field_serializer("compress")
    def serialize_compress(self, value):
        if isinstance(value, str):
            try:
                return models.CompressionFirehose(value)
            except ValueError:
                return value
        return value


class MinimumTLSVersionFirehose(str, Enum, metaclass=utils.OpenEnumMeta):
    TL_SV1 = "TLSv1"
    TL_SV1_1 = "TLSv1.1"
    TL_SV1_2 = "TLSv1.2"
    TL_SV1_3 = "TLSv1.3"


class MaximumTLSVersionFirehose(str, Enum, metaclass=utils.OpenEnumMeta):
    TL_SV1 = "TLSv1"
    TL_SV1_1 = "TLSv1.1"
    TL_SV1_2 = "TLSv1.2"
    TL_SV1_3 = "TLSv1.3"


class TLSSettingsServerSideFirehoseTypedDict(TypedDict):
    disabled: NotRequired[bool]
    request_cert: NotRequired[bool]
    r"""Require clients to present their certificates. Used to perform client authentication using SSL certs."""
    reject_unauthorized: NotRequired[bool]
    r"""Reject certificates not authorized by a CA in the CA certificate path or by another trusted CA (such as the system's)"""
    common_name_regex: NotRequired[str]
    r"""Regex matching allowable common names in peer certificates' subject attribute"""
    certificate_name: NotRequired[str]
    r"""The name of the predefined certificate"""
    priv_key_path: NotRequired[str]
    r"""Path on server containing the private key to use. PEM format. Can reference $ENV_VARS."""
    passphrase: NotRequired[str]
    r"""Passphrase to use to decrypt private key"""
    cert_path: NotRequired[str]
    r"""Path on server containing certificates to use. PEM format. Can reference $ENV_VARS."""
    ca_path: NotRequired[str]
    r"""Path on server containing CA certificates to use. PEM format. Can reference $ENV_VARS."""
    min_version: NotRequired[MinimumTLSVersionFirehose]
    max_version: NotRequired[MaximumTLSVersionFirehose]


class TLSSettingsServerSideFirehose(BaseModel):
    disabled: Optional[bool] = True

    request_cert: Annotated[Optional[bool], pydantic.Field(alias="requestCert")] = False
    r"""Require clients to present their certificates. Used to perform client authentication using SSL certs."""

    reject_unauthorized: Annotated[
        Optional[bool], pydantic.Field(alias="rejectUnauthorized")
    ] = True
    r"""Reject certificates not authorized by a CA in the CA certificate path or by another trusted CA (such as the system's)"""

    common_name_regex: Annotated[
        Optional[str], pydantic.Field(alias="commonNameRegex")
    ] = "/.*/"
    r"""Regex matching allowable common names in peer certificates' subject attribute"""

    certificate_name: Annotated[
        Optional[str], pydantic.Field(alias="certificateName")
    ] = None
    r"""The name of the predefined certificate"""

    priv_key_path: Annotated[Optional[str], pydantic.Field(alias="privKeyPath")] = None
    r"""Path on server containing the private key to use. PEM format. Can reference $ENV_VARS."""

    passphrase: Optional[str] = None
    r"""Passphrase to use to decrypt private key"""

    cert_path: Annotated[Optional[str], pydantic.Field(alias="certPath")] = None
    r"""Path on server containing certificates to use. PEM format. Can reference $ENV_VARS."""

    ca_path: Annotated[Optional[str], pydantic.Field(alias="caPath")] = None
    r"""Path on server containing CA certificates to use. PEM format. Can reference $ENV_VARS."""

    min_version: Annotated[
        Annotated[
            Optional[MinimumTLSVersionFirehose],
            PlainValidator(validate_open_enum(False)),
        ],
        pydantic.Field(alias="minVersion"),
    ] = None

    max_version: Annotated[
        Annotated[
            Optional[MaximumTLSVersionFirehose],
            PlainValidator(validate_open_enum(False)),
        ],
        pydantic.Field(alias="maxVersion"),
    ] = None

    @field_serializer("min_version")
    def serialize_min_version(self, value):
        if isinstance(value, str):
            try:
                return models.MinimumTLSVersionFirehose(value)
            except ValueError:
                return value
        return value

    @field_serializer("max_version")
    def serialize_max_version(self, value):
        if isinstance(value, str):
            try:
                return models.MaximumTLSVersionFirehose(value)
            except ValueError:
                return value
        return value


class MetadatumFirehoseTypedDict(TypedDict):
    name: str
    value: str
    r"""JavaScript expression to compute field's value, enclosed in quotes or backticks. (Can evaluate to a constant.)"""


class MetadatumFirehose(BaseModel):
    name: str

    value: str
    r"""JavaScript expression to compute field's value, enclosed in quotes or backticks. (Can evaluate to a constant.)"""


class InputFirehoseTypedDict(TypedDict):
    type: TypeFirehose
    port: float
    r"""Port to listen on"""
    id: NotRequired[str]
    r"""Unique ID for this input"""
    disabled: NotRequired[bool]
    pipeline: NotRequired[str]
    r"""Pipeline to process data from this Source before sending it through the Routes"""
    send_to_routes: NotRequired[bool]
    r"""Select whether to send data to Routes, or directly to Destinations."""
    environment: NotRequired[str]
    r"""Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere."""
    pq_enabled: NotRequired[bool]
    r"""Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers)."""
    streamtags: NotRequired[List[str]]
    r"""Tags for filtering and grouping in @{product}"""
    connections: NotRequired[List[ConnectionFirehoseTypedDict]]
    r"""Direct connections to Destinations, and optionally via a Pipeline or a Pack"""
    pq: NotRequired[PqFirehoseTypedDict]
    host: NotRequired[str]
    r"""Address to bind on. Defaults to 0.0.0.0 (all addresses)."""
    auth_tokens: NotRequired[List[str]]
    r"""Shared secrets to be provided by any client (Authorization: <token>). If empty, unauthorized access is permitted."""
    tls: NotRequired[TLSSettingsServerSideFirehoseTypedDict]
    max_active_req: NotRequired[float]
    r"""Maximum number of active requests allowed per Worker Process. Set to 0 for unlimited. Caution: Increasing the limit above the default value, or setting it to unlimited, may degrade performance and reduce throughput."""
    max_requests_per_socket: NotRequired[int]
    r"""Maximum number of requests per socket before @{product} instructs the client to close the connection. Default is 0 (unlimited)."""
    enable_proxy_header: NotRequired[bool]
    r"""Extract the client IP and port from PROXY protocol v1/v2. When enabled, the X-Forwarded-For header is ignored. Disable to use the X-Forwarded-For header for client IP extraction."""
    capture_headers: NotRequired[bool]
    r"""Add request headers to events, in the __headers field"""
    activity_log_sample_rate: NotRequired[float]
    r"""How often request activity is logged at the `info` level. A value of 1 would log every request, 10 every 10th request, etc."""
    request_timeout: NotRequired[float]
    r"""How long to wait for an incoming request to complete before aborting it. Use 0 to disable."""
    socket_timeout: NotRequired[float]
    r"""How long @{product} should wait before assuming that an inactive socket has timed out. To wait forever, set to 0."""
    keep_alive_timeout: NotRequired[float]
    r"""After the last response is sent, @{product} will wait this long for additional data before closing the socket connection. Minimum 1 second, maximum 600 seconds (10 minutes)."""
    enable_health_check: NotRequired[bool]
    r"""Expose the /cribl_health endpoint, which returns 200 OK when this Source is healthy"""
    ip_allowlist_regex: NotRequired[str]
    r"""Messages from matched IP addresses will be processed, unless also matched by the denylist"""
    ip_denylist_regex: NotRequired[str]
    r"""Messages from matched IP addresses will be ignored. This takes precedence over the allowlist."""
    metadata: NotRequired[List[MetadatumFirehoseTypedDict]]
    r"""Fields to add to events from this input"""
    description: NotRequired[str]


class InputFirehose(BaseModel):
    model_config = ConfigDict(
        populate_by_name=True, arbitrary_types_allowed=True, extra="allow"
    )
    __pydantic_extra__: Dict[str, Any] = pydantic.Field(init=False)

    type: TypeFirehose

    port: float
    r"""Port to listen on"""

    id: Optional[str] = None
    r"""Unique ID for this input"""

    disabled: Optional[bool] = False

    pipeline: Optional[str] = None
    r"""Pipeline to process data from this Source before sending it through the Routes"""

    send_to_routes: Annotated[Optional[bool], pydantic.Field(alias="sendToRoutes")] = (
        True
    )
    r"""Select whether to send data to Routes, or directly to Destinations."""

    environment: Optional[str] = None
    r"""Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere."""

    pq_enabled: Annotated[Optional[bool], pydantic.Field(alias="pqEnabled")] = False
    r"""Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers)."""

    streamtags: Optional[List[str]] = None
    r"""Tags for filtering and grouping in @{product}"""

    connections: Optional[List[ConnectionFirehose]] = None
    r"""Direct connections to Destinations, and optionally via a Pipeline or a Pack"""

    pq: Optional[PqFirehose] = None

    host: Optional[str] = "0.0.0.0"
    r"""Address to bind on. Defaults to 0.0.0.0 (all addresses)."""

    auth_tokens: Annotated[Optional[List[str]], pydantic.Field(alias="authTokens")] = (
        None
    )
    r"""Shared secrets to be provided by any client (Authorization: <token>). If empty, unauthorized access is permitted."""

    tls: Optional[TLSSettingsServerSideFirehose] = None

    max_active_req: Annotated[Optional[float], pydantic.Field(alias="maxActiveReq")] = (
        256
    )
    r"""Maximum number of active requests allowed per Worker Process. Set to 0 for unlimited. Caution: Increasing the limit above the default value, or setting it to unlimited, may degrade performance and reduce throughput."""

    max_requests_per_socket: Annotated[
        Optional[int], pydantic.Field(alias="maxRequestsPerSocket")
    ] = 0
    r"""Maximum number of requests per socket before @{product} instructs the client to close the connection. Default is 0 (unlimited)."""

    enable_proxy_header: Annotated[
        Optional[bool], pydantic.Field(alias="enableProxyHeader")
    ] = False
    r"""Extract the client IP and port from PROXY protocol v1/v2. When enabled, the X-Forwarded-For header is ignored. Disable to use the X-Forwarded-For header for client IP extraction."""

    capture_headers: Annotated[
        Optional[bool], pydantic.Field(alias="captureHeaders")
    ] = False
    r"""Add request headers to events, in the __headers field"""

    activity_log_sample_rate: Annotated[
        Optional[float], pydantic.Field(alias="activityLogSampleRate")
    ] = 100
    r"""How often request activity is logged at the `info` level. A value of 1 would log every request, 10 every 10th request, etc."""

    request_timeout: Annotated[
        Optional[float], pydantic.Field(alias="requestTimeout")
    ] = 0
    r"""How long to wait for an incoming request to complete before aborting it. Use 0 to disable."""

    socket_timeout: Annotated[
        Optional[float], pydantic.Field(alias="socketTimeout")
    ] = 0
    r"""How long @{product} should wait before assuming that an inactive socket has timed out. To wait forever, set to 0."""

    keep_alive_timeout: Annotated[
        Optional[float], pydantic.Field(alias="keepAliveTimeout")
    ] = 5
    r"""After the last response is sent, @{product} will wait this long for additional data before closing the socket connection. Minimum 1 second, maximum 600 seconds (10 minutes)."""

    enable_health_check: Annotated[
        Optional[bool], pydantic.Field(alias="enableHealthCheck")
    ] = False
    r"""Expose the /cribl_health endpoint, which returns 200 OK when this Source is healthy"""

    ip_allowlist_regex: Annotated[
        Optional[str], pydantic.Field(alias="ipAllowlistRegex")
    ] = "/.*/"
    r"""Messages from matched IP addresses will be processed, unless also matched by the denylist"""

    ip_denylist_regex: Annotated[
        Optional[str], pydantic.Field(alias="ipDenylistRegex")
    ] = "/^$/"
    r"""Messages from matched IP addresses will be ignored. This takes precedence over the allowlist."""

    metadata: Optional[List[MetadatumFirehose]] = None
    r"""Fields to add to events from this input"""

    description: Optional[str] = None

    @property
    def additional_properties(self):
        return self.__pydantic_extra__

    @additional_properties.setter
    def additional_properties(self, value):
        self.__pydantic_extra__ = value  # pyright: ignore[reportIncompatibleVariableOverride]


class InputExecType(str, Enum):
    EXEC = "exec"


class InputExecConnectionTypedDict(TypedDict):
    output: str
    pipeline: NotRequired[str]


class InputExecConnection(BaseModel):
    output: str

    pipeline: Optional[str] = None


class InputExecMode(str, Enum, metaclass=utils.OpenEnumMeta):
    r"""With Smart mode, PQ will write events to the filesystem only when it detects backpressure from the processing engine. With Always On mode, PQ will always write events directly to the queue before forwarding them to the processing engine."""

    # Smart
    SMART = "smart"
    # Always On
    ALWAYS = "always"


class InputExecCompression(str, Enum, metaclass=utils.OpenEnumMeta):
    r"""Codec to use to compress the persisted data"""

    # None
    NONE = "none"
    # Gzip
    GZIP = "gzip"


class InputExecPqControlsTypedDict(TypedDict):
    pass


class InputExecPqControls(BaseModel):
    pass


class InputExecPqTypedDict(TypedDict):
    mode: NotRequired[InputExecMode]
    r"""With Smart mode, PQ will write events to the filesystem only when it detects backpressure from the processing engine. With Always On mode, PQ will always write events directly to the queue before forwarding them to the processing engine."""
    max_buffer_size: NotRequired[float]
    r"""The maximum number of events to hold in memory before writing the events to disk"""
    commit_frequency: NotRequired[float]
    r"""The number of events to send downstream before committing that Stream has read them"""
    max_file_size: NotRequired[str]
    r"""The maximum size to store in each queue file before closing and optionally compressing. Enter a numeral with units of KB, MB, etc."""
    max_size: NotRequired[str]
    r"""The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc."""
    path: NotRequired[str]
    r"""The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/inputs/<input-id>"""
    compress: NotRequired[InputExecCompression]
    r"""Codec to use to compress the persisted data"""
    pq_controls: NotRequired[InputExecPqControlsTypedDict]


class InputExecPq(BaseModel):
    mode: Annotated[
        Optional[InputExecMode], PlainValidator(validate_open_enum(False))
    ] = InputExecMode.ALWAYS
    r"""With Smart mode, PQ will write events to the filesystem only when it detects backpressure from the processing engine. With Always On mode, PQ will always write events directly to the queue before forwarding them to the processing engine."""

    max_buffer_size: Annotated[
        Optional[float], pydantic.Field(alias="maxBufferSize")
    ] = 1000
    r"""The maximum number of events to hold in memory before writing the events to disk"""

    commit_frequency: Annotated[
        Optional[float], pydantic.Field(alias="commitFrequency")
    ] = 42
    r"""The number of events to send downstream before committing that Stream has read them"""

    max_file_size: Annotated[Optional[str], pydantic.Field(alias="maxFileSize")] = (
        "1 MB"
    )
    r"""The maximum size to store in each queue file before closing and optionally compressing. Enter a numeral with units of KB, MB, etc."""

    max_size: Annotated[Optional[str], pydantic.Field(alias="maxSize")] = "5GB"
    r"""The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc."""

    path: Optional[str] = "$CRIBL_HOME/state/queues"
    r"""The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/inputs/<input-id>"""

    compress: Annotated[
        Optional[InputExecCompression], PlainValidator(validate_open_enum(False))
    ] = InputExecCompression.NONE
    r"""Codec to use to compress the persisted data"""

    pq_controls: Annotated[
        Optional[InputExecPqControls], pydantic.Field(alias="pqControls")
    ] = None

    @field_serializer("mode")
    def serialize_mode(self, value):
        if isinstance(value, str):
            try:
                return models.InputExecMode(value)
            except ValueError:
                return value
        return value

    @field_serializer("compress")
    def serialize_compress(self, value):
        if isinstance(value, str):
            try:
                return models.InputExecCompression(value)
            except ValueError:
                return value
        return value


class ScheduleType(str, Enum, metaclass=utils.OpenEnumMeta):
    r"""Select a schedule type; either an interval (in seconds) or a cron-style schedule."""

    INTERVAL = "interval"
    CRON_SCHEDULE = "cronSchedule"


class InputExecMetadatumTypedDict(TypedDict):
    name: str
    value: str
    r"""JavaScript expression to compute field's value, enclosed in quotes or backticks. (Can evaluate to a constant.)"""


class InputExecMetadatum(BaseModel):
    name: str

    value: str
    r"""JavaScript expression to compute field's value, enclosed in quotes or backticks. (Can evaluate to a constant.)"""


class InputExecTypedDict(TypedDict):
    type: InputExecType
    command: str
    r"""Command to execute; supports Bourne shell (or CMD on Windows) syntax"""
    id: NotRequired[str]
    r"""Unique ID for this input"""
    disabled: NotRequired[bool]
    pipeline: NotRequired[str]
    r"""Pipeline to process data from this Source before sending it through the Routes"""
    send_to_routes: NotRequired[bool]
    r"""Select whether to send data to Routes, or directly to Destinations."""
    environment: NotRequired[str]
    r"""Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere."""
    pq_enabled: NotRequired[bool]
    r"""Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers)."""
    streamtags: NotRequired[List[str]]
    r"""Tags for filtering and grouping in @{product}"""
    connections: NotRequired[List[InputExecConnectionTypedDict]]
    r"""Direct connections to Destinations, and optionally via a Pipeline or a Pack"""
    pq: NotRequired[InputExecPqTypedDict]
    retries: NotRequired[float]
    r"""Maximum number of retry attempts in the event that the command fails"""
    schedule_type: NotRequired[ScheduleType]
    r"""Select a schedule type; either an interval (in seconds) or a cron-style schedule."""
    breaker_rulesets: NotRequired[List[str]]
    r"""A list of event-breaking rulesets that will be applied, in order, to the input data stream"""
    stale_channel_flush_ms: NotRequired[float]
    r"""How long (in milliseconds) the Event Breaker will wait for new data to be sent to a specific channel before flushing the data stream out, as is, to the Pipelines"""
    metadata: NotRequired[List[InputExecMetadatumTypedDict]]
    r"""Fields to add to events from this input"""
    description: NotRequired[str]
    interval: NotRequired[float]
    r"""Interval between command executions in seconds."""
    cron_schedule: NotRequired[str]
    r"""Cron schedule to execute the command on."""


class InputExec(BaseModel):
    model_config = ConfigDict(
        populate_by_name=True, arbitrary_types_allowed=True, extra="allow"
    )
    __pydantic_extra__: Dict[str, Any] = pydantic.Field(init=False)

    type: InputExecType

    command: str
    r"""Command to execute; supports Bourne shell (or CMD on Windows) syntax"""

    id: Optional[str] = None
    r"""Unique ID for this input"""

    disabled: Optional[bool] = False

    pipeline: Optional[str] = None
    r"""Pipeline to process data from this Source before sending it through the Routes"""

    send_to_routes: Annotated[Optional[bool], pydantic.Field(alias="sendToRoutes")] = (
        True
    )
    r"""Select whether to send data to Routes, or directly to Destinations."""

    environment: Optional[str] = None
    r"""Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere."""

    pq_enabled: Annotated[Optional[bool], pydantic.Field(alias="pqEnabled")] = False
    r"""Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers)."""

    streamtags: Optional[List[str]] = None
    r"""Tags for filtering and grouping in @{product}"""

    connections: Optional[List[InputExecConnection]] = None
    r"""Direct connections to Destinations, and optionally via a Pipeline or a Pack"""

    pq: Optional[InputExecPq] = None

    retries: Optional[float] = 10
    r"""Maximum number of retry attempts in the event that the command fails"""

    schedule_type: Annotated[
        Annotated[Optional[ScheduleType], PlainValidator(validate_open_enum(False))],
        pydantic.Field(alias="scheduleType"),
    ] = ScheduleType.INTERVAL
    r"""Select a schedule type; either an interval (in seconds) or a cron-style schedule."""

    breaker_rulesets: Annotated[
        Optional[List[str]], pydantic.Field(alias="breakerRulesets")
    ] = None
    r"""A list of event-breaking rulesets that will be applied, in order, to the input data stream"""

    stale_channel_flush_ms: Annotated[
        Optional[float], pydantic.Field(alias="staleChannelFlushMs")
    ] = 10000
    r"""How long (in milliseconds) the Event Breaker will wait for new data to be sent to a specific channel before flushing the data stream out, as is, to the Pipelines"""

    metadata: Optional[List[InputExecMetadatum]] = None
    r"""Fields to add to events from this input"""

    description: Optional[str] = None

    interval: Optional[float] = 60
    r"""Interval between command executions in seconds."""

    cron_schedule: Annotated[Optional[str], pydantic.Field(alias="cronSchedule")] = (
        "* * * * *"
    )
    r"""Cron schedule to execute the command on."""

    @property
    def additional_properties(self):
        return self.__pydantic_extra__

    @additional_properties.setter
    def additional_properties(self, value):
        self.__pydantic_extra__ = value  # pyright: ignore[reportIncompatibleVariableOverride]

    @field_serializer("schedule_type")
    def serialize_schedule_type(self, value):
        if isinstance(value, str):
            try:
                return models.ScheduleType(value)
            except ValueError:
                return value
        return value


class TypeEventhub(str, Enum):
    EVENTHUB = "eventhub"


class ConnectionEventhubTypedDict(TypedDict):
    output: str
    pipeline: NotRequired[str]


class ConnectionEventhub(BaseModel):
    output: str

    pipeline: Optional[str] = None


class ModeEventhub(str, Enum, metaclass=utils.OpenEnumMeta):
    r"""With Smart mode, PQ will write events to the filesystem only when it detects backpressure from the processing engine. With Always On mode, PQ will always write events directly to the queue before forwarding them to the processing engine."""

    # Smart
    SMART = "smart"
    # Always On
    ALWAYS = "always"


class CompressionEventhub(str, Enum, metaclass=utils.OpenEnumMeta):
    r"""Codec to use to compress the persisted data"""

    # None
    NONE = "none"
    # Gzip
    GZIP = "gzip"


class PqControlsEventhubTypedDict(TypedDict):
    pass


class PqControlsEventhub(BaseModel):
    pass


class PqEventhubTypedDict(TypedDict):
    mode: NotRequired[ModeEventhub]
    r"""With Smart mode, PQ will write events to the filesystem only when it detects backpressure from the processing engine. With Always On mode, PQ will always write events directly to the queue before forwarding them to the processing engine."""
    max_buffer_size: NotRequired[float]
    r"""The maximum number of events to hold in memory before writing the events to disk"""
    commit_frequency: NotRequired[float]
    r"""The number of events to send downstream before committing that Stream has read them"""
    max_file_size: NotRequired[str]
    r"""The maximum size to store in each queue file before closing and optionally compressing. Enter a numeral with units of KB, MB, etc."""
    max_size: NotRequired[str]
    r"""The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc."""
    path: NotRequired[str]
    r"""The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/inputs/<input-id>"""
    compress: NotRequired[CompressionEventhub]
    r"""Codec to use to compress the persisted data"""
    pq_controls: NotRequired[PqControlsEventhubTypedDict]


class PqEventhub(BaseModel):
    mode: Annotated[
        Optional[ModeEventhub], PlainValidator(validate_open_enum(False))
    ] = ModeEventhub.ALWAYS
    r"""With Smart mode, PQ will write events to the filesystem only when it detects backpressure from the processing engine. With Always On mode, PQ will always write events directly to the queue before forwarding them to the processing engine."""

    max_buffer_size: Annotated[
        Optional[float], pydantic.Field(alias="maxBufferSize")
    ] = 1000
    r"""The maximum number of events to hold in memory before writing the events to disk"""

    commit_frequency: Annotated[
        Optional[float], pydantic.Field(alias="commitFrequency")
    ] = 42
    r"""The number of events to send downstream before committing that Stream has read them"""

    max_file_size: Annotated[Optional[str], pydantic.Field(alias="maxFileSize")] = (
        "1 MB"
    )
    r"""The maximum size to store in each queue file before closing and optionally compressing. Enter a numeral with units of KB, MB, etc."""

    max_size: Annotated[Optional[str], pydantic.Field(alias="maxSize")] = "5GB"
    r"""The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc."""

    path: Optional[str] = "$CRIBL_HOME/state/queues"
    r"""The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/inputs/<input-id>"""

    compress: Annotated[
        Optional[CompressionEventhub], PlainValidator(validate_open_enum(False))
    ] = CompressionEventhub.NONE
    r"""Codec to use to compress the persisted data"""

    pq_controls: Annotated[
        Optional[PqControlsEventhub], pydantic.Field(alias="pqControls")
    ] = None

    @field_serializer("mode")
    def serialize_mode(self, value):
        if isinstance(value, str):
            try:
                return models.ModeEventhub(value)
            except ValueError:
                return value
        return value

    @field_serializer("compress")
    def serialize_compress(self, value):
        if isinstance(value, str):
            try:
                return models.CompressionEventhub(value)
            except ValueError:
                return value
        return value


class AuthTypeAuthenticationMethodEventhub(str, Enum, metaclass=utils.OpenEnumMeta):
    r"""Enter password directly, or select a stored secret"""

    MANUAL = "manual"
    SECRET = "secret"


class SASLMechanismEventhub(str, Enum, metaclass=utils.OpenEnumMeta):
    # PLAIN
    PLAIN = "plain"
    # OAUTHBEARER
    OAUTHBEARER = "oauthbearer"


class ClientSecretAuthTypeAuthenticationMethodEventhub(
    str, Enum, metaclass=utils.OpenEnumMeta
):
    MANUAL = "manual"
    SECRET = "secret"
    CERTIFICATE = "certificate"


class InputMicrosoftEntraIDAuthenticationEndpoint(
    str, Enum, metaclass=utils.OpenEnumMeta
):
    r"""Endpoint used to acquire authentication tokens from Azure"""

    HTTPS_LOGIN_MICROSOFTONLINE_COM = "https://login.microsoftonline.com"
    HTTPS_LOGIN_MICROSOFTONLINE_US = "https://login.microsoftonline.us"
    HTTPS_LOGIN_PARTNER_MICROSOFTONLINE_CN = "https://login.partner.microsoftonline.cn"


class AuthenticationEventhubTypedDict(TypedDict):
    r"""Authentication parameters to use when connecting to brokers. Using TLS is highly recommended."""

    disabled: NotRequired[bool]
    auth_type: NotRequired[AuthTypeAuthenticationMethodEventhub]
    r"""Enter password directly, or select a stored secret"""
    password: NotRequired[str]
    r"""Connection-string primary key, or connection-string secondary key, from the Event Hubs workspace"""
    text_secret: NotRequired[str]
    r"""Select or create a stored text secret"""
    mechanism: NotRequired[SASLMechanismEventhub]
    username: NotRequired[str]
    r"""The username for authentication. For Event Hubs, this should always be $ConnectionString."""
    client_secret_auth_type: NotRequired[
        ClientSecretAuthTypeAuthenticationMethodEventhub
    ]
    client_secret: NotRequired[str]
    r"""client_secret to pass in the OAuth request parameter"""
    client_text_secret: NotRequired[str]
    r"""Select or create a stored text secret"""
    certificate_name: NotRequired[str]
    r"""Select or create a stored certificate"""
    cert_path: NotRequired[str]
    priv_key_path: NotRequired[str]
    passphrase: NotRequired[str]
    oauth_endpoint: NotRequired[InputMicrosoftEntraIDAuthenticationEndpoint]
    r"""Endpoint used to acquire authentication tokens from Azure"""
    client_id: NotRequired[str]
    r"""client_id to pass in the OAuth request parameter"""
    tenant_id: NotRequired[str]
    r"""Directory ID (tenant identifier) in Azure Active Directory"""
    scope: NotRequired[str]
    r"""Scope to pass in the OAuth request parameter"""


class AuthenticationEventhub(BaseModel):
    r"""Authentication parameters to use when connecting to brokers. Using TLS is highly recommended."""

    disabled: Optional[bool] = False

    auth_type: Annotated[
        Annotated[
            Optional[AuthTypeAuthenticationMethodEventhub],
            PlainValidator(validate_open_enum(False)),
        ],
        pydantic.Field(alias="authType"),
    ] = AuthTypeAuthenticationMethodEventhub.MANUAL
    r"""Enter password directly, or select a stored secret"""

    password: Optional[str] = None
    r"""Connection-string primary key, or connection-string secondary key, from the Event Hubs workspace"""

    text_secret: Annotated[Optional[str], pydantic.Field(alias="textSecret")] = None
    r"""Select or create a stored text secret"""

    mechanism: Annotated[
        Optional[SASLMechanismEventhub], PlainValidator(validate_open_enum(False))
    ] = SASLMechanismEventhub.PLAIN

    username: Optional[str] = "$ConnectionString"
    r"""The username for authentication. For Event Hubs, this should always be $ConnectionString."""

    client_secret_auth_type: Annotated[
        Annotated[
            Optional[ClientSecretAuthTypeAuthenticationMethodEventhub],
            PlainValidator(validate_open_enum(False)),
        ],
        pydantic.Field(alias="clientSecretAuthType"),
    ] = ClientSecretAuthTypeAuthenticationMethodEventhub.MANUAL

    client_secret: Annotated[Optional[str], pydantic.Field(alias="clientSecret")] = None
    r"""client_secret to pass in the OAuth request parameter"""

    client_text_secret: Annotated[
        Optional[str], pydantic.Field(alias="clientTextSecret")
    ] = None
    r"""Select or create a stored text secret"""

    certificate_name: Annotated[
        Optional[str], pydantic.Field(alias="certificateName")
    ] = None
    r"""Select or create a stored certificate"""

    cert_path: Annotated[Optional[str], pydantic.Field(alias="certPath")] = None

    priv_key_path: Annotated[Optional[str], pydantic.Field(alias="privKeyPath")] = None

    passphrase: Optional[str] = None

    oauth_endpoint: Annotated[
        Annotated[
            Optional[InputMicrosoftEntraIDAuthenticationEndpoint],
            PlainValidator(validate_open_enum(False)),
        ],
        pydantic.Field(alias="oauthEndpoint"),
    ] = InputMicrosoftEntraIDAuthenticationEndpoint.HTTPS_LOGIN_MICROSOFTONLINE_COM
    r"""Endpoint used to acquire authentication tokens from Azure"""

    client_id: Annotated[Optional[str], pydantic.Field(alias="clientId")] = None
    r"""client_id to pass in the OAuth request parameter"""

    tenant_id: Annotated[Optional[str], pydantic.Field(alias="tenantId")] = None
    r"""Directory ID (tenant identifier) in Azure Active Directory"""

    scope: Optional[str] = None
    r"""Scope to pass in the OAuth request parameter"""

    @field_serializer("auth_type")
    def serialize_auth_type(self, value):
        if isinstance(value, str):
            try:
                return models.AuthTypeAuthenticationMethodEventhub(value)
            except ValueError:
                return value
        return value

    @field_serializer("mechanism")
    def serialize_mechanism(self, value):
        if isinstance(value, str):
            try:
                return models.SASLMechanismEventhub(value)
            except ValueError:
                return value
        return value

    @field_serializer("client_secret_auth_type")
    def serialize_client_secret_auth_type(self, value):
        if isinstance(value, str):
            try:
                return models.ClientSecretAuthTypeAuthenticationMethodEventhub(value)
            except ValueError:
                return value
        return value

    @field_serializer("oauth_endpoint")
    def serialize_oauth_endpoint(self, value):
        if isinstance(value, str):
            try:
                return models.InputMicrosoftEntraIDAuthenticationEndpoint(value)
            except ValueError:
                return value
        return value


class TLSSettingsClientSideEventhubTypedDict(TypedDict):
    disabled: NotRequired[bool]
    reject_unauthorized: NotRequired[bool]
    r"""Reject certificates that are not authorized by a CA in the CA certificate path, or by another trusted CA (such as the system's)"""


class TLSSettingsClientSideEventhub(BaseModel):
    disabled: Optional[bool] = False

    reject_unauthorized: Annotated[
        Optional[bool], pydantic.Field(alias="rejectUnauthorized")
    ] = True
    r"""Reject certificates that are not authorized by a CA in the CA certificate path, or by another trusted CA (such as the system's)"""


class MetadatumEventhubTypedDict(TypedDict):
    name: str
    value: str
    r"""JavaScript expression to compute field's value, enclosed in quotes or backticks. (Can evaluate to a constant.)"""


class MetadatumEventhub(BaseModel):
    name: str

    value: str
    r"""JavaScript expression to compute field's value, enclosed in quotes or backticks. (Can evaluate to a constant.)"""


class InputEventhubTypedDict(TypedDict):
    type: TypeEventhub
    brokers: List[str]
    r"""List of Event Hubs Kafka brokers to connect to (example: yourdomain.servicebus.windows.net:9093). The hostname can be found in the host portion of the primary or secondary connection string in Shared Access Policies."""
    topics: List[str]
    r"""The name of the Event Hub (Kafka topic) to subscribe to. Warning: To optimize performance, Cribl suggests subscribing each Event Hubs Source to only a single topic."""
    id: NotRequired[str]
    r"""Unique ID for this input"""
    disabled: NotRequired[bool]
    pipeline: NotRequired[str]
    r"""Pipeline to process data from this Source before sending it through the Routes"""
    send_to_routes: NotRequired[bool]
    r"""Select whether to send data to Routes, or directly to Destinations."""
    environment: NotRequired[str]
    r"""Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere."""
    pq_enabled: NotRequired[bool]
    r"""Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers)."""
    streamtags: NotRequired[List[str]]
    r"""Tags for filtering and grouping in @{product}"""
    connections: NotRequired[List[ConnectionEventhubTypedDict]]
    r"""Direct connections to Destinations, and optionally via a Pipeline or a Pack"""
    pq: NotRequired[PqEventhubTypedDict]
    group_id: NotRequired[str]
    r"""The consumer group this instance belongs to. Default is 'Cribl'."""
    from_beginning: NotRequired[bool]
    r"""Start reading from earliest available data; relevant only during initial subscription"""
    connection_timeout: NotRequired[float]
    r"""Maximum time to wait for a connection to complete successfully"""
    request_timeout: NotRequired[float]
    r"""Maximum time to wait for Kafka to respond to a request"""
    max_retries: NotRequired[float]
    r"""If messages are failing, you can set the maximum number of retries as high as 100 to prevent loss of data"""
    max_back_off: NotRequired[float]
    r"""The maximum wait time for a retry, in milliseconds. Default (and minimum) is 30,000 ms (30 seconds); maximum is 180,000 ms (180 seconds)."""
    initial_backoff: NotRequired[float]
    r"""Initial value used to calculate the retry, in milliseconds. Maximum is 600,000 ms (10 minutes)."""
    backoff_rate: NotRequired[float]
    r"""Set the backoff multiplier (2-20) to control the retry frequency for failed messages. For faster retries, use a lower multiplier. For slower retries with more delay between attempts, use a higher multiplier. The multiplier is used in an exponential backoff formula; see the Kafka [documentation](https://kafka.js.org/docs/retry-detailed) for details."""
    authentication_timeout: NotRequired[float]
    r"""Maximum time to wait for Kafka to respond to an authentication request"""
    reauthentication_threshold: NotRequired[float]
    r"""Specifies a time window during which @{product} can reauthenticate if needed. Creates the window measuring backward from the moment when credentials are set to expire."""
    sasl: NotRequired[AuthenticationEventhubTypedDict]
    r"""Authentication parameters to use when connecting to brokers. Using TLS is highly recommended."""
    tls: NotRequired[TLSSettingsClientSideEventhubTypedDict]
    session_timeout: NotRequired[float]
    r"""Timeout (session.timeout.ms in Kafka domain) used to detect client failures when using Kafka's group-management facilities.
    If the client sends no heartbeats to the broker before the timeout expires, the broker will remove the client from the group and initiate a rebalance.
    Value must be lower than rebalanceTimeout.
    See details [here](https://github.com/Azure/azure-event-hubs-for-kafka/blob/master/CONFIGURATION.md).
    """
    rebalance_timeout: NotRequired[float]
    r"""Maximum allowed time (rebalance.timeout.ms in Kafka domain) for each worker to join the group after a rebalance begins.
    If the timeout is exceeded, the coordinator broker will remove the worker from the group.
    See [Recommended configurations](https://github.com/Azure/azure-event-hubs-for-kafka/blob/master/CONFIGURATION.md).
    """
    heartbeat_interval: NotRequired[float]
    r"""Expected time (heartbeat.interval.ms in Kafka domain) between heartbeats to the consumer coordinator when using Kafka's group-management facilities.
    Value must be lower than sessionTimeout and typically should not exceed 1/3 of the sessionTimeout value.
    See [Recommended configurations](https://github.com/Azure/azure-event-hubs-for-kafka/blob/master/CONFIGURATION.md).
    """
    auto_commit_interval: NotRequired[float]
    r"""How often to commit offsets. If both this and Offset commit threshold are set, @{product} commits offsets when either condition is met. If both are empty, @{product} commits offsets after each batch."""
    auto_commit_threshold: NotRequired[float]
    r"""How many events are needed to trigger an offset commit. If both this and Offset commit interval are set, @{product} commits offsets when either condition is met. If both are empty, @{product} commits offsets after each batch."""
    max_bytes_per_partition: NotRequired[float]
    r"""Maximum amount of data that Kafka will return per partition, per fetch request. Must equal or exceed the maximum message size (maxBytesPerPartition) that Kafka is configured to allow. Otherwise, @{product} can get stuck trying to retrieve messages. Defaults to 1048576 (1 MB)."""
    max_bytes: NotRequired[float]
    r"""Maximum number of bytes that Kafka will return per fetch request. Defaults to 10485760 (10 MB)."""
    max_socket_errors: NotRequired[float]
    r"""Maximum number of network errors before the consumer re-creates a socket"""
    minimize_duplicates: NotRequired[bool]
    r"""Minimize duplicate events by starting only one consumer for each topic partition"""
    metadata: NotRequired[List[MetadatumEventhubTypedDict]]
    r"""Fields to add to events from this input"""
    description: NotRequired[str]


class InputEventhub(BaseModel):
    model_config = ConfigDict(
        populate_by_name=True, arbitrary_types_allowed=True, extra="allow"
    )
    __pydantic_extra__: Dict[str, Any] = pydantic.Field(init=False)

    type: TypeEventhub

    brokers: List[str]
    r"""List of Event Hubs Kafka brokers to connect to (example: yourdomain.servicebus.windows.net:9093). The hostname can be found in the host portion of the primary or secondary connection string in Shared Access Policies."""

    topics: List[str]
    r"""The name of the Event Hub (Kafka topic) to subscribe to. Warning: To optimize performance, Cribl suggests subscribing each Event Hubs Source to only a single topic."""

    id: Optional[str] = None
    r"""Unique ID for this input"""

    disabled: Optional[bool] = False

    pipeline: Optional[str] = None
    r"""Pipeline to process data from this Source before sending it through the Routes"""

    send_to_routes: Annotated[Optional[bool], pydantic.Field(alias="sendToRoutes")] = (
        True
    )
    r"""Select whether to send data to Routes, or directly to Destinations."""

    environment: Optional[str] = None
    r"""Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere."""

    pq_enabled: Annotated[Optional[bool], pydantic.Field(alias="pqEnabled")] = False
    r"""Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers)."""

    streamtags: Optional[List[str]] = None
    r"""Tags for filtering and grouping in @{product}"""

    connections: Optional[List[ConnectionEventhub]] = None
    r"""Direct connections to Destinations, and optionally via a Pipeline or a Pack"""

    pq: Optional[PqEventhub] = None

    group_id: Annotated[Optional[str], pydantic.Field(alias="groupId")] = "Cribl"
    r"""The consumer group this instance belongs to. Default is 'Cribl'."""

    from_beginning: Annotated[Optional[bool], pydantic.Field(alias="fromBeginning")] = (
        True
    )
    r"""Start reading from earliest available data; relevant only during initial subscription"""

    connection_timeout: Annotated[
        Optional[float], pydantic.Field(alias="connectionTimeout")
    ] = 10000
    r"""Maximum time to wait for a connection to complete successfully"""

    request_timeout: Annotated[
        Optional[float], pydantic.Field(alias="requestTimeout")
    ] = 60000
    r"""Maximum time to wait for Kafka to respond to a request"""

    max_retries: Annotated[Optional[float], pydantic.Field(alias="maxRetries")] = 5
    r"""If messages are failing, you can set the maximum number of retries as high as 100 to prevent loss of data"""

    max_back_off: Annotated[Optional[float], pydantic.Field(alias="maxBackOff")] = 30000
    r"""The maximum wait time for a retry, in milliseconds. Default (and minimum) is 30,000 ms (30 seconds); maximum is 180,000 ms (180 seconds)."""

    initial_backoff: Annotated[
        Optional[float], pydantic.Field(alias="initialBackoff")
    ] = 300
    r"""Initial value used to calculate the retry, in milliseconds. Maximum is 600,000 ms (10 minutes)."""

    backoff_rate: Annotated[Optional[float], pydantic.Field(alias="backoffRate")] = 2
    r"""Set the backoff multiplier (2-20) to control the retry frequency for failed messages. For faster retries, use a lower multiplier. For slower retries with more delay between attempts, use a higher multiplier. The multiplier is used in an exponential backoff formula; see the Kafka [documentation](https://kafka.js.org/docs/retry-detailed) for details."""

    authentication_timeout: Annotated[
        Optional[float], pydantic.Field(alias="authenticationTimeout")
    ] = 10000
    r"""Maximum time to wait for Kafka to respond to an authentication request"""

    reauthentication_threshold: Annotated[
        Optional[float], pydantic.Field(alias="reauthenticationThreshold")
    ] = 10000
    r"""Specifies a time window during which @{product} can reauthenticate if needed. Creates the window measuring backward from the moment when credentials are set to expire."""

    sasl: Optional[AuthenticationEventhub] = None
    r"""Authentication parameters to use when connecting to brokers. Using TLS is highly recommended."""

    tls: Optional[TLSSettingsClientSideEventhub] = None

    session_timeout: Annotated[
        Optional[float], pydantic.Field(alias="sessionTimeout")
    ] = 30000
    r"""Timeout (session.timeout.ms in Kafka domain) used to detect client failures when using Kafka's group-management facilities.
    If the client sends no heartbeats to the broker before the timeout expires, the broker will remove the client from the group and initiate a rebalance.
    Value must be lower than rebalanceTimeout.
    See details [here](https://github.com/Azure/azure-event-hubs-for-kafka/blob/master/CONFIGURATION.md).
    """

    rebalance_timeout: Annotated[
        Optional[float], pydantic.Field(alias="rebalanceTimeout")
    ] = 60000
    r"""Maximum allowed time (rebalance.timeout.ms in Kafka domain) for each worker to join the group after a rebalance begins.
    If the timeout is exceeded, the coordinator broker will remove the worker from the group.
    See [Recommended configurations](https://github.com/Azure/azure-event-hubs-for-kafka/blob/master/CONFIGURATION.md).
    """

    heartbeat_interval: Annotated[
        Optional[float], pydantic.Field(alias="heartbeatInterval")
    ] = 3000
    r"""Expected time (heartbeat.interval.ms in Kafka domain) between heartbeats to the consumer coordinator when using Kafka's group-management facilities.
    Value must be lower than sessionTimeout and typically should not exceed 1/3 of the sessionTimeout value.
    See [Recommended configurations](https://github.com/Azure/azure-event-hubs-for-kafka/blob/master/CONFIGURATION.md).
    """

    auto_commit_interval: Annotated[
        Optional[float], pydantic.Field(alias="autoCommitInterval")
    ] = None
    r"""How often to commit offsets. If both this and Offset commit threshold are set, @{product} commits offsets when either condition is met. If both are empty, @{product} commits offsets after each batch."""

    auto_commit_threshold: Annotated[
        Optional[float], pydantic.Field(alias="autoCommitThreshold")
    ] = None
    r"""How many events are needed to trigger an offset commit. If both this and Offset commit interval are set, @{product} commits offsets when either condition is met. If both are empty, @{product} commits offsets after each batch."""

    max_bytes_per_partition: Annotated[
        Optional[float], pydantic.Field(alias="maxBytesPerPartition")
    ] = 1048576
    r"""Maximum amount of data that Kafka will return per partition, per fetch request. Must equal or exceed the maximum message size (maxBytesPerPartition) that Kafka is configured to allow. Otherwise, @{product} can get stuck trying to retrieve messages. Defaults to 1048576 (1 MB)."""

    max_bytes: Annotated[Optional[float], pydantic.Field(alias="maxBytes")] = 10485760
    r"""Maximum number of bytes that Kafka will return per fetch request. Defaults to 10485760 (10 MB)."""

    max_socket_errors: Annotated[
        Optional[float], pydantic.Field(alias="maxSocketErrors")
    ] = 0
    r"""Maximum number of network errors before the consumer re-creates a socket"""

    minimize_duplicates: Annotated[
        Optional[bool], pydantic.Field(alias="minimizeDuplicates")
    ] = False
    r"""Minimize duplicate events by starting only one consumer for each topic partition"""

    metadata: Optional[List[MetadatumEventhub]] = None
    r"""Fields to add to events from this input"""

    description: Optional[str] = None

    @property
    def additional_properties(self):
        return self.__pydantic_extra__

    @additional_properties.setter
    def additional_properties(self, value):
        self.__pydantic_extra__ = value  # pyright: ignore[reportIncompatibleVariableOverride]


class TypeOffice365MsgTrace(str, Enum):
    OFFICE365_MSG_TRACE = "office365_msg_trace"


class ConnectionOffice365MsgTraceTypedDict(TypedDict):
    output: str
    pipeline: NotRequired[str]


class ConnectionOffice365MsgTrace(BaseModel):
    output: str

    pipeline: Optional[str] = None


class ModeOffice365MsgTrace(str, Enum, metaclass=utils.OpenEnumMeta):
    r"""With Smart mode, PQ will write events to the filesystem only when it detects backpressure from the processing engine. With Always On mode, PQ will always write events directly to the queue before forwarding them to the processing engine."""

    # Smart
    SMART = "smart"
    # Always On
    ALWAYS = "always"


class CompressionOffice365MsgTrace(str, Enum, metaclass=utils.OpenEnumMeta):
    r"""Codec to use to compress the persisted data"""

    # None
    NONE = "none"
    # Gzip
    GZIP = "gzip"


class PqControlsOffice365MsgTraceTypedDict(TypedDict):
    pass


class PqControlsOffice365MsgTrace(BaseModel):
    pass


class PqOffice365MsgTraceTypedDict(TypedDict):
    mode: NotRequired[ModeOffice365MsgTrace]
    r"""With Smart mode, PQ will write events to the filesystem only when it detects backpressure from the processing engine. With Always On mode, PQ will always write events directly to the queue before forwarding them to the processing engine."""
    max_buffer_size: NotRequired[float]
    r"""The maximum number of events to hold in memory before writing the events to disk"""
    commit_frequency: NotRequired[float]
    r"""The number of events to send downstream before committing that Stream has read them"""
    max_file_size: NotRequired[str]
    r"""The maximum size to store in each queue file before closing and optionally compressing. Enter a numeral with units of KB, MB, etc."""
    max_size: NotRequired[str]
    r"""The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc."""
    path: NotRequired[str]
    r"""The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/inputs/<input-id>"""
    compress: NotRequired[CompressionOffice365MsgTrace]
    r"""Codec to use to compress the persisted data"""
    pq_controls: NotRequired[PqControlsOffice365MsgTraceTypedDict]


class PqOffice365MsgTrace(BaseModel):
    mode: Annotated[
        Optional[ModeOffice365MsgTrace], PlainValidator(validate_open_enum(False))
    ] = ModeOffice365MsgTrace.ALWAYS
    r"""With Smart mode, PQ will write events to the filesystem only when it detects backpressure from the processing engine. With Always On mode, PQ will always write events directly to the queue before forwarding them to the processing engine."""

    max_buffer_size: Annotated[
        Optional[float], pydantic.Field(alias="maxBufferSize")
    ] = 1000
    r"""The maximum number of events to hold in memory before writing the events to disk"""

    commit_frequency: Annotated[
        Optional[float], pydantic.Field(alias="commitFrequency")
    ] = 42
    r"""The number of events to send downstream before committing that Stream has read them"""

    max_file_size: Annotated[Optional[str], pydantic.Field(alias="maxFileSize")] = (
        "1 MB"
    )
    r"""The maximum size to store in each queue file before closing and optionally compressing. Enter a numeral with units of KB, MB, etc."""

    max_size: Annotated[Optional[str], pydantic.Field(alias="maxSize")] = "5GB"
    r"""The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc."""

    path: Optional[str] = "$CRIBL_HOME/state/queues"
    r"""The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/inputs/<input-id>"""

    compress: Annotated[
        Optional[CompressionOffice365MsgTrace],
        PlainValidator(validate_open_enum(False)),
    ] = CompressionOffice365MsgTrace.NONE
    r"""Codec to use to compress the persisted data"""

    pq_controls: Annotated[
        Optional[PqControlsOffice365MsgTrace], pydantic.Field(alias="pqControls")
    ] = None

    @field_serializer("mode")
    def serialize_mode(self, value):
        if isinstance(value, str):
            try:
                return models.ModeOffice365MsgTrace(value)
            except ValueError:
                return value
        return value

    @field_serializer("compress")
    def serialize_compress(self, value):
        if isinstance(value, str):
            try:
                return models.CompressionOffice365MsgTrace(value)
            except ValueError:
                return value
        return value


class AuthenticationMethodOffice365MsgTrace(str, Enum, metaclass=utils.OpenEnumMeta):
    r"""Select authentication method."""

    MANUAL = "manual"
    SECRET = "secret"
    OAUTH = "oauth"
    OAUTH_SECRET = "oauthSecret"
    OAUTH_CERT = "oauthCert"


class LogLevelOffice365MsgTrace(str, Enum, metaclass=utils.OpenEnumMeta):
    r"""Log Level (verbosity) for collection runtime behavior."""

    ERROR = "error"
    WARN = "warn"
    INFO = "info"
    DEBUG = "debug"
    SILLY = "silly"


class MetadatumOffice365MsgTraceTypedDict(TypedDict):
    name: str
    value: str
    r"""JavaScript expression to compute field's value, enclosed in quotes or backticks. (Can evaluate to a constant.)"""


class MetadatumOffice365MsgTrace(BaseModel):
    name: str

    value: str
    r"""JavaScript expression to compute field's value, enclosed in quotes or backticks. (Can evaluate to a constant.)"""


class RetryTypeOffice365MsgTrace(str, Enum, metaclass=utils.OpenEnumMeta):
    r"""The algorithm to use when performing HTTP retries"""

    # Disabled
    NONE = "none"
    # Backoff
    BACKOFF = "backoff"
    # Static
    STATIC = "static"


class RetryRulesOffice365MsgTraceTypedDict(TypedDict):
    type: NotRequired[RetryTypeOffice365MsgTrace]
    r"""The algorithm to use when performing HTTP retries"""
    interval: NotRequired[float]
    r"""Time interval between failed request and first retry (kickoff). Maximum allowed value is 20,000 ms (1/3 minute)."""
    limit: NotRequired[float]
    r"""The maximum number of times to retry a failed HTTP request"""
    multiplier: NotRequired[float]
    r"""Base for exponential backoff, e.g., base 2 means that retries will occur after 2, then 4, then 8 seconds, and so on"""
    codes: NotRequired[List[float]]
    r"""List of http codes that trigger a retry. Leave empty to use the default list of 429, 500, and 503."""
    enable_header: NotRequired[bool]
    r"""Honor any Retry-After header that specifies a delay (in seconds) or a timestamp after which to retry the request. The delay is limited to 20 seconds, even if the Retry-After header specifies a longer delay. When disabled, all Retry-After headers are ignored."""
    retry_connect_timeout: NotRequired[bool]
    r"""Make a single retry attempt when a connection timeout (ETIMEDOUT) error occurs"""
    retry_connect_reset: NotRequired[bool]
    r"""Retry request when a connection reset (ECONNRESET) error occurs"""


class RetryRulesOffice365MsgTrace(BaseModel):
    type: Annotated[
        Optional[RetryTypeOffice365MsgTrace], PlainValidator(validate_open_enum(False))
    ] = RetryTypeOffice365MsgTrace.BACKOFF
    r"""The algorithm to use when performing HTTP retries"""

    interval: Optional[float] = 1000
    r"""Time interval between failed request and first retry (kickoff). Maximum allowed value is 20,000 ms (1/3 minute)."""

    limit: Optional[float] = 5
    r"""The maximum number of times to retry a failed HTTP request"""

    multiplier: Optional[float] = 2
    r"""Base for exponential backoff, e.g., base 2 means that retries will occur after 2, then 4, then 8 seconds, and so on"""

    codes: Optional[List[float]] = None
    r"""List of http codes that trigger a retry. Leave empty to use the default list of 429, 500, and 503."""

    enable_header: Annotated[Optional[bool], pydantic.Field(alias="enableHeader")] = (
        True
    )
    r"""Honor any Retry-After header that specifies a delay (in seconds) or a timestamp after which to retry the request. The delay is limited to 20 seconds, even if the Retry-After header specifies a longer delay. When disabled, all Retry-After headers are ignored."""

    retry_connect_timeout: Annotated[
        Optional[bool], pydantic.Field(alias="retryConnectTimeout")
    ] = False
    r"""Make a single retry attempt when a connection timeout (ETIMEDOUT) error occurs"""

    retry_connect_reset: Annotated[
        Optional[bool], pydantic.Field(alias="retryConnectReset")
    ] = False
    r"""Retry request when a connection reset (ECONNRESET) error occurs"""

    @field_serializer("type")
    def serialize_type(self, value):
        if isinstance(value, str):
            try:
                return models.RetryTypeOffice365MsgTrace(value)
            except ValueError:
                return value
        return value


class SubscriptionPlanOffice365MsgTrace(str, Enum, metaclass=utils.OpenEnumMeta):
    r"""Office 365 subscription plan for your organization, typically Office 365 Enterprise"""

    # Office 365 Enterprise
    ENTERPRISE_GCC = "enterprise_gcc"
    # Office 365 GCC
    GCC = "gcc"
    # Office 365 GCC High
    GCC_HIGH = "gcc_high"
    # Office 365 DoD
    DOD = "dod"


class CertOptionsTypedDict(TypedDict):
    priv_key_path: str
    r"""Path to the private key to use. Key should be in PEM format. Can reference $ENV_VARS."""
    cert_path: str
    r"""Path to the certificate to use. Certificate should be in PEM format. Can reference $ENV_VARS."""
    certificate_name: NotRequired[str]
    r"""The name of the predefined certificate."""
    passphrase: NotRequired[str]
    r"""Passphrase to use to decrypt the private key."""


class CertOptions(BaseModel):
    priv_key_path: Annotated[str, pydantic.Field(alias="privKeyPath")]
    r"""Path to the private key to use. Key should be in PEM format. Can reference $ENV_VARS."""

    cert_path: Annotated[str, pydantic.Field(alias="certPath")]
    r"""Path to the certificate to use. Certificate should be in PEM format. Can reference $ENV_VARS."""

    certificate_name: Annotated[
        Optional[str], pydantic.Field(alias="certificateName")
    ] = None
    r"""The name of the predefined certificate."""

    passphrase: Optional[str] = None
    r"""Passphrase to use to decrypt the private key."""


class InputOffice365MsgTraceTypedDict(TypedDict):
    type: TypeOffice365MsgTrace
    id: NotRequired[str]
    r"""Unique ID for this input"""
    disabled: NotRequired[bool]
    pipeline: NotRequired[str]
    r"""Pipeline to process data from this Source before sending it through the Routes"""
    send_to_routes: NotRequired[bool]
    r"""Select whether to send data to Routes, or directly to Destinations."""
    environment: NotRequired[str]
    r"""Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere."""
    pq_enabled: NotRequired[bool]
    r"""Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers)."""
    streamtags: NotRequired[List[str]]
    r"""Tags for filtering and grouping in @{product}"""
    connections: NotRequired[List[ConnectionOffice365MsgTraceTypedDict]]
    r"""Direct connections to Destinations, and optionally via a Pipeline or a Pack"""
    pq: NotRequired[PqOffice365MsgTraceTypedDict]
    url: NotRequired[str]
    r"""URL to use when retrieving report data."""
    interval: NotRequired[float]
    r"""How often (in minutes) to run the report. Must divide evenly into 60 minutes to create a predictable schedule, or Save will fail."""
    start_date: NotRequired[str]
    r"""Backward offset for the search range's head. (E.g.: -3h@h) Message Trace data is delayed; this parameter (with Date range end) compensates for delay and gaps."""
    end_date: NotRequired[str]
    r"""Backward offset for the search range's tail. (E.g.: -2h@h) Message Trace data is delayed; this parameter (with Date range start) compensates for delay and gaps."""
    timeout: NotRequired[float]
    r"""HTTP request inactivity timeout. Maximum is 2400 (40 minutes); enter 0 to wait indefinitely."""
    disable_time_filter: NotRequired[bool]
    r"""Disables time filtering of events when a date range is specified."""
    auth_type: NotRequired[AuthenticationMethodOffice365MsgTrace]
    r"""Select authentication method."""
    reschedule_dropped_tasks: NotRequired[bool]
    r"""Reschedule tasks that failed with non-fatal errors"""
    max_task_reschedule: NotRequired[float]
    r"""Maximum number of times a task can be rescheduled"""
    log_level: NotRequired[LogLevelOffice365MsgTrace]
    r"""Log Level (verbosity) for collection runtime behavior."""
    job_timeout: NotRequired[str]
    r"""Maximum time the job is allowed to run (e.g., 30, 45s or 15m). Units are seconds, if not specified. Enter 0 for unlimited time."""
    keep_alive_time: NotRequired[float]
    r"""How often workers should check in with the scheduler to keep job subscription alive"""
    max_missed_keep_alives: NotRequired[float]
    r"""The number of Keep Alive Time periods before an inactive worker will have its job subscription revoked."""
    ttl: NotRequired[str]
    r"""Time to keep the job's artifacts on disk after job completion. This also affects how long a job is listed in the Job Inspector."""
    ignore_group_jobs_limit: NotRequired[bool]
    r"""When enabled, this job's artifacts are not counted toward the Worker Group's finished job artifacts limit. Artifacts will be removed only after the Collector's configured time to live."""
    metadata: NotRequired[List[MetadatumOffice365MsgTraceTypedDict]]
    r"""Fields to add to events from this input"""
    retry_rules: NotRequired[RetryRulesOffice365MsgTraceTypedDict]
    description: NotRequired[str]
    username: NotRequired[str]
    r"""Username to run Message Trace API call."""
    password: NotRequired[str]
    r"""Password to run Message Trace API call."""
    credentials_secret: NotRequired[str]
    r"""Select or create a secret that references your credentials."""
    client_secret: NotRequired[str]
    r"""client_secret to pass in the OAuth request parameter."""
    tenant_id: NotRequired[str]
    r"""Directory ID (tenant identifier) in Azure Active Directory."""
    client_id: NotRequired[str]
    r"""client_id to pass in the OAuth request parameter."""
    resource: NotRequired[str]
    r"""Resource to pass in the OAuth request parameter."""
    plan_type: NotRequired[SubscriptionPlanOffice365MsgTrace]
    r"""Office 365 subscription plan for your organization, typically Office 365 Enterprise"""
    text_secret: NotRequired[str]
    r"""Select or create a secret that references your client_secret to pass in the OAuth request parameter."""
    cert_options: NotRequired[CertOptionsTypedDict]


class InputOffice365MsgTrace(BaseModel):
    model_config = ConfigDict(
        populate_by_name=True, arbitrary_types_allowed=True, extra="allow"
    )
    __pydantic_extra__: Dict[str, Any] = pydantic.Field(init=False)

    type: TypeOffice365MsgTrace

    id: Optional[str] = None
    r"""Unique ID for this input"""

    disabled: Optional[bool] = False

    pipeline: Optional[str] = None
    r"""Pipeline to process data from this Source before sending it through the Routes"""

    send_to_routes: Annotated[Optional[bool], pydantic.Field(alias="sendToRoutes")] = (
        True
    )
    r"""Select whether to send data to Routes, or directly to Destinations."""

    environment: Optional[str] = None
    r"""Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere."""

    pq_enabled: Annotated[Optional[bool], pydantic.Field(alias="pqEnabled")] = False
    r"""Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers)."""

    streamtags: Optional[List[str]] = None
    r"""Tags for filtering and grouping in @{product}"""

    connections: Optional[List[ConnectionOffice365MsgTrace]] = None
    r"""Direct connections to Destinations, and optionally via a Pipeline or a Pack"""

    pq: Optional[PqOffice365MsgTrace] = None

    url: Optional[str] = (
        "https://reports.office365.com/ecp/reportingwebservice/reporting.svc/MessageTrace"
    )
    r"""URL to use when retrieving report data."""

    interval: Optional[float] = 60
    r"""How often (in minutes) to run the report. Must divide evenly into 60 minutes to create a predictable schedule, or Save will fail."""

    start_date: Annotated[Optional[str], pydantic.Field(alias="startDate")] = None
    r"""Backward offset for the search range's head. (E.g.: -3h@h) Message Trace data is delayed; this parameter (with Date range end) compensates for delay and gaps."""

    end_date: Annotated[Optional[str], pydantic.Field(alias="endDate")] = None
    r"""Backward offset for the search range's tail. (E.g.: -2h@h) Message Trace data is delayed; this parameter (with Date range start) compensates for delay and gaps."""

    timeout: Optional[float] = 300
    r"""HTTP request inactivity timeout. Maximum is 2400 (40 minutes); enter 0 to wait indefinitely."""

    disable_time_filter: Annotated[
        Optional[bool], pydantic.Field(alias="disableTimeFilter")
    ] = True
    r"""Disables time filtering of events when a date range is specified."""

    auth_type: Annotated[
        Annotated[
            Optional[AuthenticationMethodOffice365MsgTrace],
            PlainValidator(validate_open_enum(False)),
        ],
        pydantic.Field(alias="authType"),
    ] = AuthenticationMethodOffice365MsgTrace.OAUTH
    r"""Select authentication method."""

    reschedule_dropped_tasks: Annotated[
        Optional[bool], pydantic.Field(alias="rescheduleDroppedTasks")
    ] = True
    r"""Reschedule tasks that failed with non-fatal errors"""

    max_task_reschedule: Annotated[
        Optional[float], pydantic.Field(alias="maxTaskReschedule")
    ] = 1
    r"""Maximum number of times a task can be rescheduled"""

    log_level: Annotated[
        Annotated[
            Optional[LogLevelOffice365MsgTrace],
            PlainValidator(validate_open_enum(False)),
        ],
        pydantic.Field(alias="logLevel"),
    ] = LogLevelOffice365MsgTrace.INFO
    r"""Log Level (verbosity) for collection runtime behavior."""

    job_timeout: Annotated[Optional[str], pydantic.Field(alias="jobTimeout")] = "0"
    r"""Maximum time the job is allowed to run (e.g., 30, 45s or 15m). Units are seconds, if not specified. Enter 0 for unlimited time."""

    keep_alive_time: Annotated[
        Optional[float], pydantic.Field(alias="keepAliveTime")
    ] = 30
    r"""How often workers should check in with the scheduler to keep job subscription alive"""

    max_missed_keep_alives: Annotated[
        Optional[float], pydantic.Field(alias="maxMissedKeepAlives")
    ] = 3
    r"""The number of Keep Alive Time periods before an inactive worker will have its job subscription revoked."""

    ttl: Optional[str] = "4h"
    r"""Time to keep the job's artifacts on disk after job completion. This also affects how long a job is listed in the Job Inspector."""

    ignore_group_jobs_limit: Annotated[
        Optional[bool], pydantic.Field(alias="ignoreGroupJobsLimit")
    ] = False
    r"""When enabled, this job's artifacts are not counted toward the Worker Group's finished job artifacts limit. Artifacts will be removed only after the Collector's configured time to live."""

    metadata: Optional[List[MetadatumOffice365MsgTrace]] = None
    r"""Fields to add to events from this input"""

    retry_rules: Annotated[
        Optional[RetryRulesOffice365MsgTrace], pydantic.Field(alias="retryRules")
    ] = None

    description: Optional[str] = None

    username: Optional[str] = None
    r"""Username to run Message Trace API call."""

    password: Optional[str] = None
    r"""Password to run Message Trace API call."""

    credentials_secret: Annotated[
        Optional[str], pydantic.Field(alias="credentialsSecret")
    ] = None
    r"""Select or create a secret that references your credentials."""

    client_secret: Annotated[Optional[str], pydantic.Field(alias="clientSecret")] = None
    r"""client_secret to pass in the OAuth request parameter."""

    tenant_id: Annotated[Optional[str], pydantic.Field(alias="tenantId")] = None
    r"""Directory ID (tenant identifier) in Azure Active Directory."""

    client_id: Annotated[Optional[str], pydantic.Field(alias="clientId")] = None
    r"""client_id to pass in the OAuth request parameter."""

    resource: Optional[str] = "https://outlook.office365.com"
    r"""Resource to pass in the OAuth request parameter."""

    plan_type: Annotated[
        Annotated[
            Optional[SubscriptionPlanOffice365MsgTrace],
            PlainValidator(validate_open_enum(False)),
        ],
        pydantic.Field(alias="planType"),
    ] = SubscriptionPlanOffice365MsgTrace.ENTERPRISE_GCC
    r"""Office 365 subscription plan for your organization, typically Office 365 Enterprise"""

    text_secret: Annotated[Optional[str], pydantic.Field(alias="textSecret")] = None
    r"""Select or create a secret that references your client_secret to pass in the OAuth request parameter."""

    cert_options: Annotated[
        Optional[CertOptions], pydantic.Field(alias="certOptions")
    ] = None

    @property
    def additional_properties(self):
        return self.__pydantic_extra__

    @additional_properties.setter
    def additional_properties(self, value):
        self.__pydantic_extra__ = value  # pyright: ignore[reportIncompatibleVariableOverride]

    @field_serializer("auth_type")
    def serialize_auth_type(self, value):
        if isinstance(value, str):
            try:
                return models.AuthenticationMethodOffice365MsgTrace(value)
            except ValueError:
                return value
        return value

    @field_serializer("log_level")
    def serialize_log_level(self, value):
        if isinstance(value, str):
            try:
                return models.LogLevelOffice365MsgTrace(value)
            except ValueError:
                return value
        return value

    @field_serializer("plan_type")
    def serialize_plan_type(self, value):
        if isinstance(value, str):
            try:
                return models.SubscriptionPlanOffice365MsgTrace(value)
            except ValueError:
                return value
        return value


class TypeOffice365Service(str, Enum):
    OFFICE365_SERVICE = "office365_service"


class ConnectionOffice365ServiceTypedDict(TypedDict):
    output: str
    pipeline: NotRequired[str]


class ConnectionOffice365Service(BaseModel):
    output: str

    pipeline: Optional[str] = None


class ModeOffice365Service(str, Enum, metaclass=utils.OpenEnumMeta):
    r"""With Smart mode, PQ will write events to the filesystem only when it detects backpressure from the processing engine. With Always On mode, PQ will always write events directly to the queue before forwarding them to the processing engine."""

    # Smart
    SMART = "smart"
    # Always On
    ALWAYS = "always"


class CompressionOffice365Service(str, Enum, metaclass=utils.OpenEnumMeta):
    r"""Codec to use to compress the persisted data"""

    # None
    NONE = "none"
    # Gzip
    GZIP = "gzip"


class PqControlsOffice365ServiceTypedDict(TypedDict):
    pass


class PqControlsOffice365Service(BaseModel):
    pass


class PqOffice365ServiceTypedDict(TypedDict):
    mode: NotRequired[ModeOffice365Service]
    r"""With Smart mode, PQ will write events to the filesystem only when it detects backpressure from the processing engine. With Always On mode, PQ will always write events directly to the queue before forwarding them to the processing engine."""
    max_buffer_size: NotRequired[float]
    r"""The maximum number of events to hold in memory before writing the events to disk"""
    commit_frequency: NotRequired[float]
    r"""The number of events to send downstream before committing that Stream has read them"""
    max_file_size: NotRequired[str]
    r"""The maximum size to store in each queue file before closing and optionally compressing. Enter a numeral with units of KB, MB, etc."""
    max_size: NotRequired[str]
    r"""The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc."""
    path: NotRequired[str]
    r"""The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/inputs/<input-id>"""
    compress: NotRequired[CompressionOffice365Service]
    r"""Codec to use to compress the persisted data"""
    pq_controls: NotRequired[PqControlsOffice365ServiceTypedDict]


class PqOffice365Service(BaseModel):
    mode: Annotated[
        Optional[ModeOffice365Service], PlainValidator(validate_open_enum(False))
    ] = ModeOffice365Service.ALWAYS
    r"""With Smart mode, PQ will write events to the filesystem only when it detects backpressure from the processing engine. With Always On mode, PQ will always write events directly to the queue before forwarding them to the processing engine."""

    max_buffer_size: Annotated[
        Optional[float], pydantic.Field(alias="maxBufferSize")
    ] = 1000
    r"""The maximum number of events to hold in memory before writing the events to disk"""

    commit_frequency: Annotated[
        Optional[float], pydantic.Field(alias="commitFrequency")
    ] = 42
    r"""The number of events to send downstream before committing that Stream has read them"""

    max_file_size: Annotated[Optional[str], pydantic.Field(alias="maxFileSize")] = (
        "1 MB"
    )
    r"""The maximum size to store in each queue file before closing and optionally compressing. Enter a numeral with units of KB, MB, etc."""

    max_size: Annotated[Optional[str], pydantic.Field(alias="maxSize")] = "5GB"
    r"""The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc."""

    path: Optional[str] = "$CRIBL_HOME/state/queues"
    r"""The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/inputs/<input-id>"""

    compress: Annotated[
        Optional[CompressionOffice365Service], PlainValidator(validate_open_enum(False))
    ] = CompressionOffice365Service.NONE
    r"""Codec to use to compress the persisted data"""

    pq_controls: Annotated[
        Optional[PqControlsOffice365Service], pydantic.Field(alias="pqControls")
    ] = None

    @field_serializer("mode")
    def serialize_mode(self, value):
        if isinstance(value, str):
            try:
                return models.ModeOffice365Service(value)
            except ValueError:
                return value
        return value

    @field_serializer("compress")
    def serialize_compress(self, value):
        if isinstance(value, str):
            try:
                return models.CompressionOffice365Service(value)
            except ValueError:
                return value
        return value


class SubscriptionPlanOffice365Service(str, Enum, metaclass=utils.OpenEnumMeta):
    r"""Office 365 subscription plan for your organization, typically Office 365 Enterprise"""

    # Office 365 Enterprise
    ENTERPRISE_GCC = "enterprise_gcc"
    # Office 365 GCC
    GCC = "gcc"
    # Office 365 GCC High
    GCC_HIGH = "gcc_high"
    # Office 365 DoD
    DOD = "dod"


class MetadatumOffice365ServiceTypedDict(TypedDict):
    name: str
    value: str
    r"""JavaScript expression to compute field's value, enclosed in quotes or backticks. (Can evaluate to a constant.)"""


class MetadatumOffice365Service(BaseModel):
    name: str

    value: str
    r"""JavaScript expression to compute field's value, enclosed in quotes or backticks. (Can evaluate to a constant.)"""


class LogLevelOffice365Service(str, Enum, metaclass=utils.OpenEnumMeta):
    r"""Collector runtime Log Level"""

    ERROR = "error"
    WARN = "warn"
    INFO = "info"
    DEBUG = "debug"


class ContentConfigOffice365ServiceTypedDict(TypedDict):
    content_type: NotRequired[str]
    r"""Office 365 Services API Content Type"""
    description: NotRequired[str]
    r"""If interval type is minutes the value entered must evenly divisible by 60 or save will fail"""
    interval: NotRequired[float]
    log_level: NotRequired[LogLevelOffice365Service]
    r"""Collector runtime Log Level"""
    enabled: NotRequired[bool]


class ContentConfigOffice365Service(BaseModel):
    content_type: Annotated[Optional[str], pydantic.Field(alias="contentType")] = None
    r"""Office 365 Services API Content Type"""

    description: Optional[str] = None
    r"""If interval type is minutes the value entered must evenly divisible by 60 or save will fail"""

    interval: Optional[float] = None

    log_level: Annotated[
        Annotated[
            Optional[LogLevelOffice365Service],
            PlainValidator(validate_open_enum(False)),
        ],
        pydantic.Field(alias="logLevel"),
    ] = None
    r"""Collector runtime Log Level"""

    enabled: Optional[bool] = None

    @field_serializer("log_level")
    def serialize_log_level(self, value):
        if isinstance(value, str):
            try:
                return models.LogLevelOffice365Service(value)
            except ValueError:
                return value
        return value


class RetryTypeOffice365Service(str, Enum, metaclass=utils.OpenEnumMeta):
    r"""The algorithm to use when performing HTTP retries"""

    # Disabled
    NONE = "none"
    # Backoff
    BACKOFF = "backoff"
    # Static
    STATIC = "static"


class RetryRulesOffice365ServiceTypedDict(TypedDict):
    type: NotRequired[RetryTypeOffice365Service]
    r"""The algorithm to use when performing HTTP retries"""
    interval: NotRequired[float]
    r"""Time interval between failed request and first retry (kickoff). Maximum allowed value is 20,000 ms (1/3 minute)."""
    limit: NotRequired[float]
    r"""The maximum number of times to retry a failed HTTP request"""
    multiplier: NotRequired[float]
    r"""Base for exponential backoff, e.g., base 2 means that retries will occur after 2, then 4, then 8 seconds, and so on"""
    codes: NotRequired[List[float]]
    r"""List of http codes that trigger a retry. Leave empty to use the default list of 429, 500, and 503."""
    enable_header: NotRequired[bool]
    r"""Honor any Retry-After header that specifies a delay (in seconds) or a timestamp after which to retry the request. The delay is limited to 20 seconds, even if the Retry-After header specifies a longer delay. When disabled, all Retry-After headers are ignored."""
    retry_connect_timeout: NotRequired[bool]
    r"""Make a single retry attempt when a connection timeout (ETIMEDOUT) error occurs"""
    retry_connect_reset: NotRequired[bool]
    r"""Retry request when a connection reset (ECONNRESET) error occurs"""


class RetryRulesOffice365Service(BaseModel):
    type: Annotated[
        Optional[RetryTypeOffice365Service], PlainValidator(validate_open_enum(False))
    ] = RetryTypeOffice365Service.BACKOFF
    r"""The algorithm to use when performing HTTP retries"""

    interval: Optional[float] = 1000
    r"""Time interval between failed request and first retry (kickoff). Maximum allowed value is 20,000 ms (1/3 minute)."""

    limit: Optional[float] = 5
    r"""The maximum number of times to retry a failed HTTP request"""

    multiplier: Optional[float] = 2
    r"""Base for exponential backoff, e.g., base 2 means that retries will occur after 2, then 4, then 8 seconds, and so on"""

    codes: Optional[List[float]] = None
    r"""List of http codes that trigger a retry. Leave empty to use the default list of 429, 500, and 503."""

    enable_header: Annotated[Optional[bool], pydantic.Field(alias="enableHeader")] = (
        True
    )
    r"""Honor any Retry-After header that specifies a delay (in seconds) or a timestamp after which to retry the request. The delay is limited to 20 seconds, even if the Retry-After header specifies a longer delay. When disabled, all Retry-After headers are ignored."""

    retry_connect_timeout: Annotated[
        Optional[bool], pydantic.Field(alias="retryConnectTimeout")
    ] = False
    r"""Make a single retry attempt when a connection timeout (ETIMEDOUT) error occurs"""

    retry_connect_reset: Annotated[
        Optional[bool], pydantic.Field(alias="retryConnectReset")
    ] = False
    r"""Retry request when a connection reset (ECONNRESET) error occurs"""

    @field_serializer("type")
    def serialize_type(self, value):
        if isinstance(value, str):
            try:
                return models.RetryTypeOffice365Service(value)
            except ValueError:
                return value
        return value


class AuthenticationMethodOffice365Service(str, Enum, metaclass=utils.OpenEnumMeta):
    r"""Enter client secret directly, or select a stored secret"""

    MANUAL = "manual"
    SECRET = "secret"


class InputOffice365ServiceTypedDict(TypedDict):
    type: TypeOffice365Service
    tenant_id: str
    r"""Office 365 Azure Tenant ID"""
    app_id: str
    r"""Office 365 Azure Application ID"""
    id: NotRequired[str]
    r"""Unique ID for this input"""
    disabled: NotRequired[bool]
    pipeline: NotRequired[str]
    r"""Pipeline to process data from this Source before sending it through the Routes"""
    send_to_routes: NotRequired[bool]
    r"""Select whether to send data to Routes, or directly to Destinations."""
    environment: NotRequired[str]
    r"""Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere."""
    pq_enabled: NotRequired[bool]
    r"""Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers)."""
    streamtags: NotRequired[List[str]]
    r"""Tags for filtering and grouping in @{product}"""
    connections: NotRequired[List[ConnectionOffice365ServiceTypedDict]]
    r"""Direct connections to Destinations, and optionally via a Pipeline or a Pack"""
    pq: NotRequired[PqOffice365ServiceTypedDict]
    plan_type: NotRequired[SubscriptionPlanOffice365Service]
    r"""Office 365 subscription plan for your organization, typically Office 365 Enterprise"""
    timeout: NotRequired[float]
    r"""HTTP request inactivity timeout, use 0 to disable"""
    keep_alive_time: NotRequired[float]
    r"""How often workers should check in with the scheduler to keep job subscription alive"""
    job_timeout: NotRequired[str]
    r"""Maximum time the job is allowed to run (e.g., 30, 45s or 15m). Units are seconds, if not specified. Enter 0 for unlimited time."""
    max_missed_keep_alives: NotRequired[float]
    r"""The number of Keep Alive Time periods before an inactive worker will have its job subscription revoked."""
    ttl: NotRequired[str]
    r"""Time to keep the job's artifacts on disk after job completion. This also affects how long a job is listed in the Job Inspector."""
    ignore_group_jobs_limit: NotRequired[bool]
    r"""When enabled, this job's artifacts are not counted toward the Worker Group's finished job artifacts limit. Artifacts will be removed only after the Collector's configured time to live."""
    metadata: NotRequired[List[MetadatumOffice365ServiceTypedDict]]
    r"""Fields to add to events from this input"""
    content_config: NotRequired[List[ContentConfigOffice365ServiceTypedDict]]
    r"""Enable Office 365 Service Communication API content types and polling intervals. Polling intervals are used to set up search date range and cron schedule, e.g.: */${interval} * * * *. Because of this, intervals entered for current and historical status must be evenly divisible by 60 to give a predictable schedule."""
    retry_rules: NotRequired[RetryRulesOffice365ServiceTypedDict]
    auth_type: NotRequired[AuthenticationMethodOffice365Service]
    r"""Enter client secret directly, or select a stored secret"""
    description: NotRequired[str]
    client_secret: NotRequired[str]
    r"""Office 365 Azure client secret"""
    text_secret: NotRequired[str]
    r"""Select or create a stored text secret"""


class InputOffice365Service(BaseModel):
    model_config = ConfigDict(
        populate_by_name=True, arbitrary_types_allowed=True, extra="allow"
    )
    __pydantic_extra__: Dict[str, Any] = pydantic.Field(init=False)

    type: TypeOffice365Service

    tenant_id: Annotated[str, pydantic.Field(alias="tenantId")]
    r"""Office 365 Azure Tenant ID"""

    app_id: Annotated[str, pydantic.Field(alias="appId")]
    r"""Office 365 Azure Application ID"""

    id: Optional[str] = None
    r"""Unique ID for this input"""

    disabled: Optional[bool] = False

    pipeline: Optional[str] = None
    r"""Pipeline to process data from this Source before sending it through the Routes"""

    send_to_routes: Annotated[Optional[bool], pydantic.Field(alias="sendToRoutes")] = (
        True
    )
    r"""Select whether to send data to Routes, or directly to Destinations."""

    environment: Optional[str] = None
    r"""Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere."""

    pq_enabled: Annotated[Optional[bool], pydantic.Field(alias="pqEnabled")] = False
    r"""Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers)."""

    streamtags: Optional[List[str]] = None
    r"""Tags for filtering and grouping in @{product}"""

    connections: Optional[List[ConnectionOffice365Service]] = None
    r"""Direct connections to Destinations, and optionally via a Pipeline or a Pack"""

    pq: Optional[PqOffice365Service] = None

    plan_type: Annotated[
        Annotated[
            Optional[SubscriptionPlanOffice365Service],
            PlainValidator(validate_open_enum(False)),
        ],
        pydantic.Field(alias="planType"),
    ] = SubscriptionPlanOffice365Service.ENTERPRISE_GCC
    r"""Office 365 subscription plan for your organization, typically Office 365 Enterprise"""

    timeout: Optional[float] = 300
    r"""HTTP request inactivity timeout, use 0 to disable"""

    keep_alive_time: Annotated[
        Optional[float], pydantic.Field(alias="keepAliveTime")
    ] = 30
    r"""How often workers should check in with the scheduler to keep job subscription alive"""

    job_timeout: Annotated[Optional[str], pydantic.Field(alias="jobTimeout")] = "0"
    r"""Maximum time the job is allowed to run (e.g., 30, 45s or 15m). Units are seconds, if not specified. Enter 0 for unlimited time."""

    max_missed_keep_alives: Annotated[
        Optional[float], pydantic.Field(alias="maxMissedKeepAlives")
    ] = 3
    r"""The number of Keep Alive Time periods before an inactive worker will have its job subscription revoked."""

    ttl: Optional[str] = "4h"
    r"""Time to keep the job's artifacts on disk after job completion. This also affects how long a job is listed in the Job Inspector."""

    ignore_group_jobs_limit: Annotated[
        Optional[bool], pydantic.Field(alias="ignoreGroupJobsLimit")
    ] = False
    r"""When enabled, this job's artifacts are not counted toward the Worker Group's finished job artifacts limit. Artifacts will be removed only after the Collector's configured time to live."""

    metadata: Optional[List[MetadatumOffice365Service]] = None
    r"""Fields to add to events from this input"""

    content_config: Annotated[
        Optional[List[ContentConfigOffice365Service]],
        pydantic.Field(alias="contentConfig"),
    ] = None
    r"""Enable Office 365 Service Communication API content types and polling intervals. Polling intervals are used to set up search date range and cron schedule, e.g.: */${interval} * * * *. Because of this, intervals entered for current and historical status must be evenly divisible by 60 to give a predictable schedule."""

    retry_rules: Annotated[
        Optional[RetryRulesOffice365Service], pydantic.Field(alias="retryRules")
    ] = None

    auth_type: Annotated[
        Annotated[
            Optional[AuthenticationMethodOffice365Service],
            PlainValidator(validate_open_enum(False)),
        ],
        pydantic.Field(alias="authType"),
    ] = AuthenticationMethodOffice365Service.MANUAL
    r"""Enter client secret directly, or select a stored secret"""

    description: Optional[str] = None

    client_secret: Annotated[Optional[str], pydantic.Field(alias="clientSecret")] = None
    r"""Office 365 Azure client secret"""

    text_secret: Annotated[Optional[str], pydantic.Field(alias="textSecret")] = None
    r"""Select or create a stored text secret"""

    @property
    def additional_properties(self):
        return self.__pydantic_extra__

    @additional_properties.setter
    def additional_properties(self, value):
        self.__pydantic_extra__ = value  # pyright: ignore[reportIncompatibleVariableOverride]

    @field_serializer("plan_type")
    def serialize_plan_type(self, value):
        if isinstance(value, str):
            try:
                return models.SubscriptionPlanOffice365Service(value)
            except ValueError:
                return value
        return value

    @field_serializer("auth_type")
    def serialize_auth_type(self, value):
        if isinstance(value, str):
            try:
                return models.AuthenticationMethodOffice365Service(value)
            except ValueError:
                return value
        return value


class TypeOffice365Mgmt(str, Enum):
    OFFICE365_MGMT = "office365_mgmt"


class ConnectionOffice365MgmtTypedDict(TypedDict):
    output: str
    pipeline: NotRequired[str]


class ConnectionOffice365Mgmt(BaseModel):
    output: str

    pipeline: Optional[str] = None


class ModeOffice365Mgmt(str, Enum, metaclass=utils.OpenEnumMeta):
    r"""With Smart mode, PQ will write events to the filesystem only when it detects backpressure from the processing engine. With Always On mode, PQ will always write events directly to the queue before forwarding them to the processing engine."""

    # Smart
    SMART = "smart"
    # Always On
    ALWAYS = "always"


class CompressionOffice365Mgmt(str, Enum, metaclass=utils.OpenEnumMeta):
    r"""Codec to use to compress the persisted data"""

    # None
    NONE = "none"
    # Gzip
    GZIP = "gzip"


class PqControlsOffice365MgmtTypedDict(TypedDict):
    pass


class PqControlsOffice365Mgmt(BaseModel):
    pass


class PqOffice365MgmtTypedDict(TypedDict):
    mode: NotRequired[ModeOffice365Mgmt]
    r"""With Smart mode, PQ will write events to the filesystem only when it detects backpressure from the processing engine. With Always On mode, PQ will always write events directly to the queue before forwarding them to the processing engine."""
    max_buffer_size: NotRequired[float]
    r"""The maximum number of events to hold in memory before writing the events to disk"""
    commit_frequency: NotRequired[float]
    r"""The number of events to send downstream before committing that Stream has read them"""
    max_file_size: NotRequired[str]
    r"""The maximum size to store in each queue file before closing and optionally compressing. Enter a numeral with units of KB, MB, etc."""
    max_size: NotRequired[str]
    r"""The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc."""
    path: NotRequired[str]
    r"""The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/inputs/<input-id>"""
    compress: NotRequired[CompressionOffice365Mgmt]
    r"""Codec to use to compress the persisted data"""
    pq_controls: NotRequired[PqControlsOffice365MgmtTypedDict]


class PqOffice365Mgmt(BaseModel):
    mode: Annotated[
        Optional[ModeOffice365Mgmt], PlainValidator(validate_open_enum(False))
    ] = ModeOffice365Mgmt.ALWAYS
    r"""With Smart mode, PQ will write events to the filesystem only when it detects backpressure from the processing engine. With Always On mode, PQ will always write events directly to the queue before forwarding them to the processing engine."""

    max_buffer_size: Annotated[
        Optional[float], pydantic.Field(alias="maxBufferSize")
    ] = 1000
    r"""The maximum number of events to hold in memory before writing the events to disk"""

    commit_frequency: Annotated[
        Optional[float], pydantic.Field(alias="commitFrequency")
    ] = 42
    r"""The number of events to send downstream before committing that Stream has read them"""

    max_file_size: Annotated[Optional[str], pydantic.Field(alias="maxFileSize")] = (
        "1 MB"
    )
    r"""The maximum size to store in each queue file before closing and optionally compressing. Enter a numeral with units of KB, MB, etc."""

    max_size: Annotated[Optional[str], pydantic.Field(alias="maxSize")] = "5GB"
    r"""The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc."""

    path: Optional[str] = "$CRIBL_HOME/state/queues"
    r"""The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/inputs/<input-id>"""

    compress: Annotated[
        Optional[CompressionOffice365Mgmt], PlainValidator(validate_open_enum(False))
    ] = CompressionOffice365Mgmt.NONE
    r"""Codec to use to compress the persisted data"""

    pq_controls: Annotated[
        Optional[PqControlsOffice365Mgmt], pydantic.Field(alias="pqControls")
    ] = None

    @field_serializer("mode")
    def serialize_mode(self, value):
        if isinstance(value, str):
            try:
                return models.ModeOffice365Mgmt(value)
            except ValueError:
                return value
        return value

    @field_serializer("compress")
    def serialize_compress(self, value):
        if isinstance(value, str):
            try:
                return models.CompressionOffice365Mgmt(value)
            except ValueError:
                return value
        return value


class SubscriptionPlanOffice365Mgmt(str, Enum, metaclass=utils.OpenEnumMeta):
    r"""Office 365 subscription plan for your organization, typically Office 365 Enterprise"""

    # Office 365 Enterprise
    ENTERPRISE_GCC = "enterprise_gcc"
    # Office 365 GCC
    GCC = "gcc"
    # Office 365 GCC High
    GCC_HIGH = "gcc_high"
    # Office 365 DoD
    DOD = "dod"


class MetadatumOffice365MgmtTypedDict(TypedDict):
    name: str
    value: str
    r"""JavaScript expression to compute field's value, enclosed in quotes or backticks. (Can evaluate to a constant.)"""


class MetadatumOffice365Mgmt(BaseModel):
    name: str

    value: str
    r"""JavaScript expression to compute field's value, enclosed in quotes or backticks. (Can evaluate to a constant.)"""


class LogLevelOffice365Mgmt(str, Enum, metaclass=utils.OpenEnumMeta):
    r"""Collector runtime Log Level"""

    ERROR = "error"
    WARN = "warn"
    INFO = "info"
    DEBUG = "debug"


class ContentConfigOffice365MgmtTypedDict(TypedDict):
    content_type: NotRequired[str]
    r"""Office 365 Management Activity API Content Type"""
    description: NotRequired[str]
    r"""If interval type is minutes the value entered must evenly divisible by 60 or save will fail"""
    interval: NotRequired[float]
    log_level: NotRequired[LogLevelOffice365Mgmt]
    r"""Collector runtime Log Level"""
    enabled: NotRequired[bool]


class ContentConfigOffice365Mgmt(BaseModel):
    content_type: Annotated[Optional[str], pydantic.Field(alias="contentType")] = None
    r"""Office 365 Management Activity API Content Type"""

    description: Optional[str] = None
    r"""If interval type is minutes the value entered must evenly divisible by 60 or save will fail"""

    interval: Optional[float] = None

    log_level: Annotated[
        Annotated[
            Optional[LogLevelOffice365Mgmt], PlainValidator(validate_open_enum(False))
        ],
        pydantic.Field(alias="logLevel"),
    ] = None
    r"""Collector runtime Log Level"""

    enabled: Optional[bool] = None

    @field_serializer("log_level")
    def serialize_log_level(self, value):
        if isinstance(value, str):
            try:
                return models.LogLevelOffice365Mgmt(value)
            except ValueError:
                return value
        return value


class RetryTypeOffice365Mgmt(str, Enum, metaclass=utils.OpenEnumMeta):
    r"""The algorithm to use when performing HTTP retries"""

    # Disabled
    NONE = "none"
    # Backoff
    BACKOFF = "backoff"
    # Static
    STATIC = "static"


class RetryRulesOffice365MgmtTypedDict(TypedDict):
    type: NotRequired[RetryTypeOffice365Mgmt]
    r"""The algorithm to use when performing HTTP retries"""
    interval: NotRequired[float]
    r"""Time interval between failed request and first retry (kickoff). Maximum allowed value is 20,000 ms (1/3 minute)."""
    limit: NotRequired[float]
    r"""The maximum number of times to retry a failed HTTP request"""
    multiplier: NotRequired[float]
    r"""Base for exponential backoff, e.g., base 2 means that retries will occur after 2, then 4, then 8 seconds, and so on"""
    codes: NotRequired[List[float]]
    r"""List of http codes that trigger a retry. Leave empty to use the default list of 429, 500, and 503."""
    enable_header: NotRequired[bool]
    r"""Honor any Retry-After header that specifies a delay (in seconds) or a timestamp after which to retry the request. The delay is limited to 20 seconds, even if the Retry-After header specifies a longer delay. When disabled, all Retry-After headers are ignored."""
    retry_connect_timeout: NotRequired[bool]
    r"""Make a single retry attempt when a connection timeout (ETIMEDOUT) error occurs"""
    retry_connect_reset: NotRequired[bool]
    r"""Retry request when a connection reset (ECONNRESET) error occurs"""


class RetryRulesOffice365Mgmt(BaseModel):
    type: Annotated[
        Optional[RetryTypeOffice365Mgmt], PlainValidator(validate_open_enum(False))
    ] = RetryTypeOffice365Mgmt.BACKOFF
    r"""The algorithm to use when performing HTTP retries"""

    interval: Optional[float] = 1000
    r"""Time interval between failed request and first retry (kickoff). Maximum allowed value is 20,000 ms (1/3 minute)."""

    limit: Optional[float] = 5
    r"""The maximum number of times to retry a failed HTTP request"""

    multiplier: Optional[float] = 2
    r"""Base for exponential backoff, e.g., base 2 means that retries will occur after 2, then 4, then 8 seconds, and so on"""

    codes: Optional[List[float]] = None
    r"""List of http codes that trigger a retry. Leave empty to use the default list of 429, 500, and 503."""

    enable_header: Annotated[Optional[bool], pydantic.Field(alias="enableHeader")] = (
        True
    )
    r"""Honor any Retry-After header that specifies a delay (in seconds) or a timestamp after which to retry the request. The delay is limited to 20 seconds, even if the Retry-After header specifies a longer delay. When disabled, all Retry-After headers are ignored."""

    retry_connect_timeout: Annotated[
        Optional[bool], pydantic.Field(alias="retryConnectTimeout")
    ] = False
    r"""Make a single retry attempt when a connection timeout (ETIMEDOUT) error occurs"""

    retry_connect_reset: Annotated[
        Optional[bool], pydantic.Field(alias="retryConnectReset")
    ] = False
    r"""Retry request when a connection reset (ECONNRESET) error occurs"""

    @field_serializer("type")
    def serialize_type(self, value):
        if isinstance(value, str):
            try:
                return models.RetryTypeOffice365Mgmt(value)
            except ValueError:
                return value
        return value


class AuthenticationMethodOffice365Mgmt(str, Enum, metaclass=utils.OpenEnumMeta):
    r"""Enter client secret directly, or select a stored secret"""

    MANUAL = "manual"
    SECRET = "secret"


class InputOffice365MgmtTypedDict(TypedDict):
    type: TypeOffice365Mgmt
    tenant_id: str
    r"""Office 365 Azure Tenant ID"""
    app_id: str
    r"""Office 365 Azure Application ID"""
    id: NotRequired[str]
    r"""Unique ID for this input"""
    disabled: NotRequired[bool]
    pipeline: NotRequired[str]
    r"""Pipeline to process data from this Source before sending it through the Routes"""
    send_to_routes: NotRequired[bool]
    r"""Select whether to send data to Routes, or directly to Destinations."""
    environment: NotRequired[str]
    r"""Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere."""
    pq_enabled: NotRequired[bool]
    r"""Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers)."""
    streamtags: NotRequired[List[str]]
    r"""Tags for filtering and grouping in @{product}"""
    connections: NotRequired[List[ConnectionOffice365MgmtTypedDict]]
    r"""Direct connections to Destinations, and optionally via a Pipeline or a Pack"""
    pq: NotRequired[PqOffice365MgmtTypedDict]
    plan_type: NotRequired[SubscriptionPlanOffice365Mgmt]
    r"""Office 365 subscription plan for your organization, typically Office 365 Enterprise"""
    timeout: NotRequired[float]
    r"""HTTP request inactivity timeout, use 0 to disable"""
    keep_alive_time: NotRequired[float]
    r"""How often workers should check in with the scheduler to keep job subscription alive"""
    job_timeout: NotRequired[str]
    r"""Maximum time the job is allowed to run (e.g., 30, 45s or 15m). Units are seconds, if not specified. Enter 0 for unlimited time."""
    max_missed_keep_alives: NotRequired[float]
    r"""The number of Keep Alive Time periods before an inactive worker will have its job subscription revoked."""
    ttl: NotRequired[str]
    r"""Time to keep the job's artifacts on disk after job completion. This also affects how long a job is listed in the Job Inspector."""
    ignore_group_jobs_limit: NotRequired[bool]
    r"""When enabled, this job's artifacts are not counted toward the Worker Group's finished job artifacts limit. Artifacts will be removed only after the Collector's configured time to live."""
    metadata: NotRequired[List[MetadatumOffice365MgmtTypedDict]]
    r"""Fields to add to events from this input"""
    publisher_identifier: NotRequired[str]
    r"""Optional Publisher Identifier to use in API requests, defaults to tenant id if not defined. For more information see [here](https://docs.microsoft.com/en-us/office/office-365-management-api/office-365-management-activity-api-reference#start-a-subscription)"""
    content_config: NotRequired[List[ContentConfigOffice365MgmtTypedDict]]
    r"""Enable Office 365 Management Activity API content types and polling intervals. Polling intervals are used to set up search date range and cron schedule, e.g.: */${interval} * * * *. Because of this, intervals entered must be evenly divisible by 60 to give a predictable schedule."""
    ingestion_lag: NotRequired[float]
    r"""Use this setting to account for ingestion lag. This is necessary because there can be a lag of 60 - 90 minutes (or longer) before Office 365 events are available for retrieval."""
    retry_rules: NotRequired[RetryRulesOffice365MgmtTypedDict]
    auth_type: NotRequired[AuthenticationMethodOffice365Mgmt]
    r"""Enter client secret directly, or select a stored secret"""
    description: NotRequired[str]
    client_secret: NotRequired[str]
    r"""Office 365 Azure client secret"""
    text_secret: NotRequired[str]
    r"""Select or create a stored text secret"""


class InputOffice365Mgmt(BaseModel):
    model_config = ConfigDict(
        populate_by_name=True, arbitrary_types_allowed=True, extra="allow"
    )
    __pydantic_extra__: Dict[str, Any] = pydantic.Field(init=False)

    type: TypeOffice365Mgmt

    tenant_id: Annotated[str, pydantic.Field(alias="tenantId")]
    r"""Office 365 Azure Tenant ID"""

    app_id: Annotated[str, pydantic.Field(alias="appId")]
    r"""Office 365 Azure Application ID"""

    id: Optional[str] = None
    r"""Unique ID for this input"""

    disabled: Optional[bool] = False

    pipeline: Optional[str] = None
    r"""Pipeline to process data from this Source before sending it through the Routes"""

    send_to_routes: Annotated[Optional[bool], pydantic.Field(alias="sendToRoutes")] = (
        True
    )
    r"""Select whether to send data to Routes, or directly to Destinations."""

    environment: Optional[str] = None
    r"""Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere."""

    pq_enabled: Annotated[Optional[bool], pydantic.Field(alias="pqEnabled")] = False
    r"""Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers)."""

    streamtags: Optional[List[str]] = None
    r"""Tags for filtering and grouping in @{product}"""

    connections: Optional[List[ConnectionOffice365Mgmt]] = None
    r"""Direct connections to Destinations, and optionally via a Pipeline or a Pack"""

    pq: Optional[PqOffice365Mgmt] = None

    plan_type: Annotated[
        Annotated[
            Optional[SubscriptionPlanOffice365Mgmt],
            PlainValidator(validate_open_enum(False)),
        ],
        pydantic.Field(alias="planType"),
    ] = SubscriptionPlanOffice365Mgmt.ENTERPRISE_GCC
    r"""Office 365 subscription plan for your organization, typically Office 365 Enterprise"""

    timeout: Optional[float] = 300
    r"""HTTP request inactivity timeout, use 0 to disable"""

    keep_alive_time: Annotated[
        Optional[float], pydantic.Field(alias="keepAliveTime")
    ] = 30
    r"""How often workers should check in with the scheduler to keep job subscription alive"""

    job_timeout: Annotated[Optional[str], pydantic.Field(alias="jobTimeout")] = "0"
    r"""Maximum time the job is allowed to run (e.g., 30, 45s or 15m). Units are seconds, if not specified. Enter 0 for unlimited time."""

    max_missed_keep_alives: Annotated[
        Optional[float], pydantic.Field(alias="maxMissedKeepAlives")
    ] = 3
    r"""The number of Keep Alive Time periods before an inactive worker will have its job subscription revoked."""

    ttl: Optional[str] = "4h"
    r"""Time to keep the job's artifacts on disk after job completion. This also affects how long a job is listed in the Job Inspector."""

    ignore_group_jobs_limit: Annotated[
        Optional[bool], pydantic.Field(alias="ignoreGroupJobsLimit")
    ] = False
    r"""When enabled, this job's artifacts are not counted toward the Worker Group's finished job artifacts limit. Artifacts will be removed only after the Collector's configured time to live."""

    metadata: Optional[List[MetadatumOffice365Mgmt]] = None
    r"""Fields to add to events from this input"""

    publisher_identifier: Annotated[
        Optional[str], pydantic.Field(alias="publisherIdentifier")
    ] = None
    r"""Optional Publisher Identifier to use in API requests, defaults to tenant id if not defined. For more information see [here](https://docs.microsoft.com/en-us/office/office-365-management-api/office-365-management-activity-api-reference#start-a-subscription)"""

    content_config: Annotated[
        Optional[List[ContentConfigOffice365Mgmt]],
        pydantic.Field(alias="contentConfig"),
    ] = None
    r"""Enable Office 365 Management Activity API content types and polling intervals. Polling intervals are used to set up search date range and cron schedule, e.g.: */${interval} * * * *. Because of this, intervals entered must be evenly divisible by 60 to give a predictable schedule."""

    ingestion_lag: Annotated[Optional[float], pydantic.Field(alias="ingestionLag")] = 0
    r"""Use this setting to account for ingestion lag. This is necessary because there can be a lag of 60 - 90 minutes (or longer) before Office 365 events are available for retrieval."""

    retry_rules: Annotated[
        Optional[RetryRulesOffice365Mgmt], pydantic.Field(alias="retryRules")
    ] = None

    auth_type: Annotated[
        Annotated[
            Optional[AuthenticationMethodOffice365Mgmt],
            PlainValidator(validate_open_enum(False)),
        ],
        pydantic.Field(alias="authType"),
    ] = AuthenticationMethodOffice365Mgmt.MANUAL
    r"""Enter client secret directly, or select a stored secret"""

    description: Optional[str] = None

    client_secret: Annotated[Optional[str], pydantic.Field(alias="clientSecret")] = None
    r"""Office 365 Azure client secret"""

    text_secret: Annotated[Optional[str], pydantic.Field(alias="textSecret")] = None
    r"""Select or create a stored text secret"""

    @property
    def additional_properties(self):
        return self.__pydantic_extra__

    @additional_properties.setter
    def additional_properties(self, value):
        self.__pydantic_extra__ = value  # pyright: ignore[reportIncompatibleVariableOverride]

    @field_serializer("plan_type")
    def serialize_plan_type(self, value):
        if isinstance(value, str):
            try:
                return models.SubscriptionPlanOffice365Mgmt(value)
            except ValueError:
                return value
        return value

    @field_serializer("auth_type")
    def serialize_auth_type(self, value):
        if isinstance(value, str):
            try:
                return models.AuthenticationMethodOffice365Mgmt(value)
            except ValueError:
                return value
        return value


class TypeEdgePrometheus(str, Enum):
    EDGE_PROMETHEUS = "edge_prometheus"


class ConnectionEdgePrometheusTypedDict(TypedDict):
    output: str
    pipeline: NotRequired[str]


class ConnectionEdgePrometheus(BaseModel):
    output: str

    pipeline: Optional[str] = None


class ModeEdgePrometheus(str, Enum, metaclass=utils.OpenEnumMeta):
    r"""With Smart mode, PQ will write events to the filesystem only when it detects backpressure from the processing engine. With Always On mode, PQ will always write events directly to the queue before forwarding them to the processing engine."""

    # Smart
    SMART = "smart"
    # Always On
    ALWAYS = "always"


class PqCompressionEdgePrometheus(str, Enum, metaclass=utils.OpenEnumMeta):
    r"""Codec to use to compress the persisted data"""

    # None
    NONE = "none"
    # Gzip
    GZIP = "gzip"


class PqControlsEdgePrometheusTypedDict(TypedDict):
    pass


class PqControlsEdgePrometheus(BaseModel):
    pass


class PqEdgePrometheusTypedDict(TypedDict):
    mode: NotRequired[ModeEdgePrometheus]
    r"""With Smart mode, PQ will write events to the filesystem only when it detects backpressure from the processing engine. With Always On mode, PQ will always write events directly to the queue before forwarding them to the processing engine."""
    max_buffer_size: NotRequired[float]
    r"""The maximum number of events to hold in memory before writing the events to disk"""
    commit_frequency: NotRequired[float]
    r"""The number of events to send downstream before committing that Stream has read them"""
    max_file_size: NotRequired[str]
    r"""The maximum size to store in each queue file before closing and optionally compressing. Enter a numeral with units of KB, MB, etc."""
    max_size: NotRequired[str]
    r"""The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc."""
    path: NotRequired[str]
    r"""The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/inputs/<input-id>"""
    compress: NotRequired[PqCompressionEdgePrometheus]
    r"""Codec to use to compress the persisted data"""
    pq_controls: NotRequired[PqControlsEdgePrometheusTypedDict]


class PqEdgePrometheus(BaseModel):
    mode: Annotated[
        Optional[ModeEdgePrometheus], PlainValidator(validate_open_enum(False))
    ] = ModeEdgePrometheus.ALWAYS
    r"""With Smart mode, PQ will write events to the filesystem only when it detects backpressure from the processing engine. With Always On mode, PQ will always write events directly to the queue before forwarding them to the processing engine."""

    max_buffer_size: Annotated[
        Optional[float], pydantic.Field(alias="maxBufferSize")
    ] = 1000
    r"""The maximum number of events to hold in memory before writing the events to disk"""

    commit_frequency: Annotated[
        Optional[float], pydantic.Field(alias="commitFrequency")
    ] = 42
    r"""The number of events to send downstream before committing that Stream has read them"""

    max_file_size: Annotated[Optional[str], pydantic.Field(alias="maxFileSize")] = (
        "1 MB"
    )
    r"""The maximum size to store in each queue file before closing and optionally compressing. Enter a numeral with units of KB, MB, etc."""

    max_size: Annotated[Optional[str], pydantic.Field(alias="maxSize")] = "5GB"
    r"""The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc."""

    path: Optional[str] = "$CRIBL_HOME/state/queues"
    r"""The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/inputs/<input-id>"""

    compress: Annotated[
        Optional[PqCompressionEdgePrometheus], PlainValidator(validate_open_enum(False))
    ] = PqCompressionEdgePrometheus.NONE
    r"""Codec to use to compress the persisted data"""

    pq_controls: Annotated[
        Optional[PqControlsEdgePrometheus], pydantic.Field(alias="pqControls")
    ] = None

    @field_serializer("mode")
    def serialize_mode(self, value):
        if isinstance(value, str):
            try:
                return models.ModeEdgePrometheus(value)
            except ValueError:
                return value
        return value

    @field_serializer("compress")
    def serialize_compress(self, value):
        if isinstance(value, str):
            try:
                return models.PqCompressionEdgePrometheus(value)
            except ValueError:
                return value
        return value


class DiscoveryTypeEdgePrometheus(str, Enum, metaclass=utils.OpenEnumMeta):
    r"""Target discovery mechanism. Use static to manually enter a list of targets."""

    # Static
    STATIC = "static"
    # DNS
    DNS = "dns"
    # AWS EC2
    EC2 = "ec2"
    # Kubernetes Node
    K8S_NODE = "k8s-node"
    # Kubernetes Pods
    K8S_PODS = "k8s-pods"


class PersistenceCompressionEdgePrometheus(str, Enum, metaclass=utils.OpenEnumMeta):
    r"""Data compression format. Default is gzip."""

    NONE = "none"
    GZIP = "gzip"


class DiskSpoolingEdgePrometheusTypedDict(TypedDict):
    enable: NotRequired[bool]
    r"""Spool events on disk for Cribl Edge and Search. Default is disabled."""
    time_window: NotRequired[str]
    r"""Time period for grouping spooled events. Default is 10m."""
    max_data_size: NotRequired[str]
    r"""Maximum disk space that can be consumed before older buckets are deleted. Examples: 420MB, 4GB. Default is 1GB."""
    max_data_time: NotRequired[str]
    r"""Maximum amount of time to retain data before older buckets are deleted. Examples: 2h, 4d. Default is 24h."""
    compress: NotRequired[PersistenceCompressionEdgePrometheus]
    r"""Data compression format. Default is gzip."""


class DiskSpoolingEdgePrometheus(BaseModel):
    enable: Optional[bool] = False
    r"""Spool events on disk for Cribl Edge and Search. Default is disabled."""

    time_window: Annotated[Optional[str], pydantic.Field(alias="timeWindow")] = "10m"
    r"""Time period for grouping spooled events. Default is 10m."""

    max_data_size: Annotated[Optional[str], pydantic.Field(alias="maxDataSize")] = "1GB"
    r"""Maximum disk space that can be consumed before older buckets are deleted. Examples: 420MB, 4GB. Default is 1GB."""

    max_data_time: Annotated[Optional[str], pydantic.Field(alias="maxDataTime")] = "24h"
    r"""Maximum amount of time to retain data before older buckets are deleted. Examples: 2h, 4d. Default is 24h."""

    compress: Annotated[
        Optional[PersistenceCompressionEdgePrometheus],
        PlainValidator(validate_open_enum(False)),
    ] = PersistenceCompressionEdgePrometheus.GZIP
    r"""Data compression format. Default is gzip."""

    @field_serializer("compress")
    def serialize_compress(self, value):
        if isinstance(value, str):
            try:
                return models.PersistenceCompressionEdgePrometheus(value)
            except ValueError:
                return value
        return value


class MetadatumEdgePrometheusTypedDict(TypedDict):
    name: str
    value: str
    r"""JavaScript expression to compute field's value, enclosed in quotes or backticks. (Can evaluate to a constant.)"""


class MetadatumEdgePrometheus(BaseModel):
    name: str

    value: str
    r"""JavaScript expression to compute field's value, enclosed in quotes or backticks. (Can evaluate to a constant.)"""


class AuthTypeAuthenticationMethodEdgePrometheus(
    str, Enum, metaclass=utils.OpenEnumMeta
):
    r"""Enter credentials directly, or select a stored secret"""

    MANUAL = "manual"
    SECRET = "secret"
    KUBERNETES = "kubernetes"


class TargetProtocol(str, Enum, metaclass=utils.OpenEnumMeta):
    r"""Protocol to use when collecting metrics"""

    HTTP = "http"
    HTTPS = "https"


class TargetTypedDict(TypedDict):
    host: str
    r"""Name of host from which to pull metrics."""
    protocol: NotRequired[TargetProtocol]
    r"""Protocol to use when collecting metrics"""
    port: NotRequired[float]
    r"""The port number in the metrics URL for discovered targets."""
    path: NotRequired[str]
    r"""Path to use when collecting metrics from discovered targets"""


class Target(BaseModel):
    host: str
    r"""Name of host from which to pull metrics."""

    protocol: Annotated[
        Optional[TargetProtocol], PlainValidator(validate_open_enum(False))
    ] = TargetProtocol.HTTP
    r"""Protocol to use when collecting metrics"""

    port: Optional[float] = 9090
    r"""The port number in the metrics URL for discovered targets."""

    path: Optional[str] = "/metrics"
    r"""Path to use when collecting metrics from discovered targets"""

    @field_serializer("protocol")
    def serialize_protocol(self, value):
        if isinstance(value, str):
            try:
                return models.TargetProtocol(value)
            except ValueError:
                return value
        return value


class RecordTypeEdgePrometheus(str, Enum, metaclass=utils.OpenEnumMeta):
    r"""DNS Record type to resolve"""

    SRV = "SRV"
    A = "A"
    AAAA = "AAAA"


class ScrapeProtocolProtocol(str, Enum, metaclass=utils.OpenEnumMeta):
    r"""Protocol to use when collecting metrics"""

    HTTP = "http"
    HTTPS = "https"


class AwsAuthenticationMethodAuthenticationMethodEdgePrometheus(
    str, Enum, metaclass=utils.OpenEnumMeta
):
    r"""AWS authentication method. Choose Auto to use IAM roles."""

    # Auto
    AUTO = "auto"
    # Manual
    MANUAL = "manual"
    # Secret Key pair
    SECRET = "secret"


class SearchFilterEdgePrometheusTypedDict(TypedDict):
    name: str
    r"""Search filter attribute name, see: https://docs.aws.amazon.com/AWSEC2/latest/APIReference/API_DescribeInstances.html for more information. Attributes can be manually entered if not present in the drop down list"""
    values: List[str]
    r"""Search Filter Values, if empty only \"running\" EC2 instances will be returned"""


class SearchFilterEdgePrometheus(BaseModel):
    name: Annotated[str, pydantic.Field(alias="Name")]
    r"""Search filter attribute name, see: https://docs.aws.amazon.com/AWSEC2/latest/APIReference/API_DescribeInstances.html for more information. Attributes can be manually entered if not present in the drop down list"""

    values: Annotated[List[str], pydantic.Field(alias="Values")]
    r"""Search Filter Values, if empty only \"running\" EC2 instances will be returned"""


class SignatureVersionEdgePrometheus(str, Enum, metaclass=utils.OpenEnumMeta):
    r"""Signature version to use for signing EC2 requests"""

    V2 = "v2"
    V4 = "v4"


class PodFilterTypedDict(TypedDict):
    filter_: str
    r"""JavaScript expression applied to pods objects. Return 'true' to include it."""
    description: NotRequired[str]
    r"""Optional description of this rule's purpose"""


class PodFilter(BaseModel):
    filter_: Annotated[str, pydantic.Field(alias="filter")]
    r"""JavaScript expression applied to pods objects. Return 'true' to include it."""

    description: Optional[str] = None
    r"""Optional description of this rule's purpose"""


class InputEdgePrometheusTypedDict(TypedDict):
    type: TypeEdgePrometheus
    id: NotRequired[str]
    r"""Unique ID for this input"""
    disabled: NotRequired[bool]
    pipeline: NotRequired[str]
    r"""Pipeline to process data from this Source before sending it through the Routes"""
    send_to_routes: NotRequired[bool]
    r"""Select whether to send data to Routes, or directly to Destinations."""
    environment: NotRequired[str]
    r"""Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere."""
    pq_enabled: NotRequired[bool]
    r"""Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers)."""
    streamtags: NotRequired[List[str]]
    r"""Tags for filtering and grouping in @{product}"""
    connections: NotRequired[List[ConnectionEdgePrometheusTypedDict]]
    r"""Direct connections to Destinations, and optionally via a Pipeline or a Pack"""
    pq: NotRequired[PqEdgePrometheusTypedDict]
    dimension_list: NotRequired[List[str]]
    r"""Other dimensions to include in events"""
    discovery_type: NotRequired[DiscoveryTypeEdgePrometheus]
    r"""Target discovery mechanism. Use static to manually enter a list of targets."""
    interval: NotRequired[float]
    r"""How often in seconds to scrape targets for metrics."""
    timeout: NotRequired[float]
    r"""Timeout, in milliseconds, before aborting HTTP connection attempts; 1-60000 or 0 to disable"""
    persistence: NotRequired[DiskSpoolingEdgePrometheusTypedDict]
    metadata: NotRequired[List[MetadatumEdgePrometheusTypedDict]]
    r"""Fields to add to events from this input"""
    auth_type: NotRequired[AuthTypeAuthenticationMethodEdgePrometheus]
    r"""Enter credentials directly, or select a stored secret"""
    description: NotRequired[str]
    targets: NotRequired[List[TargetTypedDict]]
    record_type: NotRequired[RecordTypeEdgePrometheus]
    r"""DNS Record type to resolve"""
    scrape_port: NotRequired[float]
    r"""The port number in the metrics URL for discovered targets."""
    name_list: NotRequired[List[str]]
    r"""List of DNS names to resolve"""
    scrape_protocol: NotRequired[ScrapeProtocolProtocol]
    r"""Protocol to use when collecting metrics"""
    scrape_path: NotRequired[str]
    r"""Path to use when collecting metrics from discovered targets"""
    aws_authentication_method: NotRequired[
        AwsAuthenticationMethodAuthenticationMethodEdgePrometheus
    ]
    r"""AWS authentication method. Choose Auto to use IAM roles."""
    aws_api_key: NotRequired[str]
    aws_secret: NotRequired[str]
    r"""Select or create a stored secret that references your access key and secret key"""
    use_public_ip: NotRequired[bool]
    r"""Use public IP address for discovered targets. Set to false if the private IP address should be used."""
    search_filter: NotRequired[List[SearchFilterEdgePrometheusTypedDict]]
    r"""EC2 Instance Search Filter"""
    aws_secret_key: NotRequired[str]
    region: NotRequired[str]
    r"""Region where the EC2 is located"""
    endpoint: NotRequired[str]
    r"""EC2 service endpoint. If empty, defaults to the AWS Region-specific endpoint. Otherwise, it must point to EC2-compatible endpoint."""
    signature_version: NotRequired[SignatureVersionEdgePrometheus]
    r"""Signature version to use for signing EC2 requests"""
    reuse_connections: NotRequired[bool]
    r"""Reuse connections between requests, which can improve performance"""
    reject_unauthorized: NotRequired[bool]
    r"""Reject certificates that cannot be verified against a valid CA, such as self-signed certificates"""
    enable_assume_role: NotRequired[bool]
    r"""Use Assume Role credentials to access EC2"""
    assume_role_arn: NotRequired[str]
    r"""Amazon Resource Name (ARN) of the role to assume"""
    assume_role_external_id: NotRequired[str]
    r"""External ID to use when assuming role"""
    duration_seconds: NotRequired[float]
    r"""Duration of the assumed role's session, in seconds. Minimum is 900 (15 minutes), default is 3600 (1 hour), and maximum is 43200 (12 hours)."""
    scrape_protocol_expr: NotRequired[str]
    r"""Protocol to use when collecting metrics"""
    scrape_port_expr: NotRequired[str]
    r"""The port number in the metrics URL for discovered targets."""
    scrape_path_expr: NotRequired[str]
    r"""Path to use when collecting metrics from discovered targets"""
    pod_filter: NotRequired[List[PodFilterTypedDict]]
    r"""Add rules to decide which pods to discover for metrics.
    Pods are searched if no rules are given or of all the rules'
    expressions evaluate to true.

    """
    username: NotRequired[str]
    r"""Username for Prometheus Basic authentication"""
    password: NotRequired[str]
    r"""Password for Prometheus Basic authentication"""
    credentials_secret: NotRequired[str]
    r"""Select or create a secret that references your credentials"""


class InputEdgePrometheus(BaseModel):
    model_config = ConfigDict(
        populate_by_name=True, arbitrary_types_allowed=True, extra="allow"
    )
    __pydantic_extra__: Dict[str, Any] = pydantic.Field(init=False)

    type: TypeEdgePrometheus

    id: Optional[str] = None
    r"""Unique ID for this input"""

    disabled: Optional[bool] = False

    pipeline: Optional[str] = None
    r"""Pipeline to process data from this Source before sending it through the Routes"""

    send_to_routes: Annotated[Optional[bool], pydantic.Field(alias="sendToRoutes")] = (
        True
    )
    r"""Select whether to send data to Routes, or directly to Destinations."""

    environment: Optional[str] = None
    r"""Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere."""

    pq_enabled: Annotated[Optional[bool], pydantic.Field(alias="pqEnabled")] = False
    r"""Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers)."""

    streamtags: Optional[List[str]] = None
    r"""Tags for filtering and grouping in @{product}"""

    connections: Optional[List[ConnectionEdgePrometheus]] = None
    r"""Direct connections to Destinations, and optionally via a Pipeline or a Pack"""

    pq: Optional[PqEdgePrometheus] = None

    dimension_list: Annotated[
        Optional[List[str]], pydantic.Field(alias="dimensionList")
    ] = None
    r"""Other dimensions to include in events"""

    discovery_type: Annotated[
        Annotated[
            Optional[DiscoveryTypeEdgePrometheus],
            PlainValidator(validate_open_enum(False)),
        ],
        pydantic.Field(alias="discoveryType"),
    ] = DiscoveryTypeEdgePrometheus.STATIC
    r"""Target discovery mechanism. Use static to manually enter a list of targets."""

    interval: Optional[float] = 15
    r"""How often in seconds to scrape targets for metrics."""

    timeout: Optional[float] = 5000
    r"""Timeout, in milliseconds, before aborting HTTP connection attempts; 1-60000 or 0 to disable"""

    persistence: Optional[DiskSpoolingEdgePrometheus] = None

    metadata: Optional[List[MetadatumEdgePrometheus]] = None
    r"""Fields to add to events from this input"""

    auth_type: Annotated[
        Annotated[
            Optional[AuthTypeAuthenticationMethodEdgePrometheus],
            PlainValidator(validate_open_enum(False)),
        ],
        pydantic.Field(alias="authType"),
    ] = AuthTypeAuthenticationMethodEdgePrometheus.MANUAL
    r"""Enter credentials directly, or select a stored secret"""

    description: Optional[str] = None

    targets: Optional[List[Target]] = None

    record_type: Annotated[
        Annotated[
            Optional[RecordTypeEdgePrometheus],
            PlainValidator(validate_open_enum(False)),
        ],
        pydantic.Field(alias="recordType"),
    ] = RecordTypeEdgePrometheus.SRV
    r"""DNS Record type to resolve"""

    scrape_port: Annotated[Optional[float], pydantic.Field(alias="scrapePort")] = 9090
    r"""The port number in the metrics URL for discovered targets."""

    name_list: Annotated[Optional[List[str]], pydantic.Field(alias="nameList")] = None
    r"""List of DNS names to resolve"""

    scrape_protocol: Annotated[
        Annotated[
            Optional[ScrapeProtocolProtocol], PlainValidator(validate_open_enum(False))
        ],
        pydantic.Field(alias="scrapeProtocol"),
    ] = ScrapeProtocolProtocol.HTTP
    r"""Protocol to use when collecting metrics"""

    scrape_path: Annotated[Optional[str], pydantic.Field(alias="scrapePath")] = (
        "/metrics"
    )
    r"""Path to use when collecting metrics from discovered targets"""

    aws_authentication_method: Annotated[
        Annotated[
            Optional[AwsAuthenticationMethodAuthenticationMethodEdgePrometheus],
            PlainValidator(validate_open_enum(False)),
        ],
        pydantic.Field(alias="awsAuthenticationMethod"),
    ] = AwsAuthenticationMethodAuthenticationMethodEdgePrometheus.AUTO
    r"""AWS authentication method. Choose Auto to use IAM roles."""

    aws_api_key: Annotated[Optional[str], pydantic.Field(alias="awsApiKey")] = None

    aws_secret: Annotated[Optional[str], pydantic.Field(alias="awsSecret")] = None
    r"""Select or create a stored secret that references your access key and secret key"""

    use_public_ip: Annotated[Optional[bool], pydantic.Field(alias="usePublicIp")] = True
    r"""Use public IP address for discovered targets. Set to false if the private IP address should be used."""

    search_filter: Annotated[
        Optional[List[SearchFilterEdgePrometheus]], pydantic.Field(alias="searchFilter")
    ] = None
    r"""EC2 Instance Search Filter"""

    aws_secret_key: Annotated[Optional[str], pydantic.Field(alias="awsSecretKey")] = (
        None
    )

    region: Optional[str] = None
    r"""Region where the EC2 is located"""

    endpoint: Optional[str] = None
    r"""EC2 service endpoint. If empty, defaults to the AWS Region-specific endpoint. Otherwise, it must point to EC2-compatible endpoint."""

    signature_version: Annotated[
        Annotated[
            Optional[SignatureVersionEdgePrometheus],
            PlainValidator(validate_open_enum(False)),
        ],
        pydantic.Field(alias="signatureVersion"),
    ] = SignatureVersionEdgePrometheus.V4
    r"""Signature version to use for signing EC2 requests"""

    reuse_connections: Annotated[
        Optional[bool], pydantic.Field(alias="reuseConnections")
    ] = True
    r"""Reuse connections between requests, which can improve performance"""

    reject_unauthorized: Annotated[
        Optional[bool], pydantic.Field(alias="rejectUnauthorized")
    ] = True
    r"""Reject certificates that cannot be verified against a valid CA, such as self-signed certificates"""

    enable_assume_role: Annotated[
        Optional[bool], pydantic.Field(alias="enableAssumeRole")
    ] = False
    r"""Use Assume Role credentials to access EC2"""

    assume_role_arn: Annotated[Optional[str], pydantic.Field(alias="assumeRoleArn")] = (
        None
    )
    r"""Amazon Resource Name (ARN) of the role to assume"""

    assume_role_external_id: Annotated[
        Optional[str], pydantic.Field(alias="assumeRoleExternalId")
    ] = None
    r"""External ID to use when assuming role"""

    duration_seconds: Annotated[
        Optional[float], pydantic.Field(alias="durationSeconds")
    ] = 3600
    r"""Duration of the assumed role's session, in seconds. Minimum is 900 (15 minutes), default is 3600 (1 hour), and maximum is 43200 (12 hours)."""

    scrape_protocol_expr: Annotated[
        Optional[str], pydantic.Field(alias="scrapeProtocolExpr")
    ] = "metadata.annotations['prometheus.io/scheme'] || 'http'"
    r"""Protocol to use when collecting metrics"""

    scrape_port_expr: Annotated[
        Optional[str], pydantic.Field(alias="scrapePortExpr")
    ] = "metadata.annotations['prometheus.io/port'] || 9090"
    r"""The port number in the metrics URL for discovered targets."""

    scrape_path_expr: Annotated[
        Optional[str], pydantic.Field(alias="scrapePathExpr")
    ] = "metadata.annotations['prometheus.io/path'] || '/metrics'"
    r"""Path to use when collecting metrics from discovered targets"""

    pod_filter: Annotated[
        Optional[List[PodFilter]], pydantic.Field(alias="podFilter")
    ] = None
    r"""Add rules to decide which pods to discover for metrics.
    Pods are searched if no rules are given or of all the rules'
    expressions evaluate to true.

    """

    username: Optional[str] = None
    r"""Username for Prometheus Basic authentication"""

    password: Optional[str] = None
    r"""Password for Prometheus Basic authentication"""

    credentials_secret: Annotated[
        Optional[str], pydantic.Field(alias="credentialsSecret")
    ] = None
    r"""Select or create a secret that references your credentials"""

    @property
    def additional_properties(self):
        return self.__pydantic_extra__

    @additional_properties.setter
    def additional_properties(self, value):
        self.__pydantic_extra__ = value  # pyright: ignore[reportIncompatibleVariableOverride]

    @field_serializer("discovery_type")
    def serialize_discovery_type(self, value):
        if isinstance(value, str):
            try:
                return models.DiscoveryTypeEdgePrometheus(value)
            except ValueError:
                return value
        return value

    @field_serializer("auth_type")
    def serialize_auth_type(self, value):
        if isinstance(value, str):
            try:
                return models.AuthTypeAuthenticationMethodEdgePrometheus(value)
            except ValueError:
                return value
        return value

    @field_serializer("record_type")
    def serialize_record_type(self, value):
        if isinstance(value, str):
            try:
                return models.RecordTypeEdgePrometheus(value)
            except ValueError:
                return value
        return value

    @field_serializer("scrape_protocol")
    def serialize_scrape_protocol(self, value):
        if isinstance(value, str):
            try:
                return models.ScrapeProtocolProtocol(value)
            except ValueError:
                return value
        return value

    @field_serializer("aws_authentication_method")
    def serialize_aws_authentication_method(self, value):
        if isinstance(value, str):
            try:
                return models.AwsAuthenticationMethodAuthenticationMethodEdgePrometheus(
                    value
                )
            except ValueError:
                return value
        return value

    @field_serializer("signature_version")
    def serialize_signature_version(self, value):
        if isinstance(value, str):
            try:
                return models.SignatureVersionEdgePrometheus(value)
            except ValueError:
                return value
        return value


class InputTypePrometheus(str, Enum):
    PROMETHEUS = "prometheus"


class ConnectionPrometheusTypedDict(TypedDict):
    output: str
    pipeline: NotRequired[str]


class ConnectionPrometheus(BaseModel):
    output: str

    pipeline: Optional[str] = None


class PqModePrometheus(str, Enum, metaclass=utils.OpenEnumMeta):
    r"""With Smart mode, PQ will write events to the filesystem only when it detects backpressure from the processing engine. With Always On mode, PQ will always write events directly to the queue before forwarding them to the processing engine."""

    # Smart
    SMART = "smart"
    # Always On
    ALWAYS = "always"


class PqCompressionPrometheus(str, Enum, metaclass=utils.OpenEnumMeta):
    r"""Codec to use to compress the persisted data"""

    # None
    NONE = "none"
    # Gzip
    GZIP = "gzip"


class InputPqControlsPrometheusTypedDict(TypedDict):
    pass


class InputPqControlsPrometheus(BaseModel):
    pass


class PqPrometheusTypedDict(TypedDict):
    mode: NotRequired[PqModePrometheus]
    r"""With Smart mode, PQ will write events to the filesystem only when it detects backpressure from the processing engine. With Always On mode, PQ will always write events directly to the queue before forwarding them to the processing engine."""
    max_buffer_size: NotRequired[float]
    r"""The maximum number of events to hold in memory before writing the events to disk"""
    commit_frequency: NotRequired[float]
    r"""The number of events to send downstream before committing that Stream has read them"""
    max_file_size: NotRequired[str]
    r"""The maximum size to store in each queue file before closing and optionally compressing. Enter a numeral with units of KB, MB, etc."""
    max_size: NotRequired[str]
    r"""The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc."""
    path: NotRequired[str]
    r"""The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/inputs/<input-id>"""
    compress: NotRequired[PqCompressionPrometheus]
    r"""Codec to use to compress the persisted data"""
    pq_controls: NotRequired[InputPqControlsPrometheusTypedDict]


class PqPrometheus(BaseModel):
    mode: Annotated[
        Optional[PqModePrometheus], PlainValidator(validate_open_enum(False))
    ] = PqModePrometheus.ALWAYS
    r"""With Smart mode, PQ will write events to the filesystem only when it detects backpressure from the processing engine. With Always On mode, PQ will always write events directly to the queue before forwarding them to the processing engine."""

    max_buffer_size: Annotated[
        Optional[float], pydantic.Field(alias="maxBufferSize")
    ] = 1000
    r"""The maximum number of events to hold in memory before writing the events to disk"""

    commit_frequency: Annotated[
        Optional[float], pydantic.Field(alias="commitFrequency")
    ] = 42
    r"""The number of events to send downstream before committing that Stream has read them"""

    max_file_size: Annotated[Optional[str], pydantic.Field(alias="maxFileSize")] = (
        "1 MB"
    )
    r"""The maximum size to store in each queue file before closing and optionally compressing. Enter a numeral with units of KB, MB, etc."""

    max_size: Annotated[Optional[str], pydantic.Field(alias="maxSize")] = "5GB"
    r"""The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc."""

    path: Optional[str] = "$CRIBL_HOME/state/queues"
    r"""The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/inputs/<input-id>"""

    compress: Annotated[
        Optional[PqCompressionPrometheus], PlainValidator(validate_open_enum(False))
    ] = PqCompressionPrometheus.NONE
    r"""Codec to use to compress the persisted data"""

    pq_controls: Annotated[
        Optional[InputPqControlsPrometheus], pydantic.Field(alias="pqControls")
    ] = None

    @field_serializer("mode")
    def serialize_mode(self, value):
        if isinstance(value, str):
            try:
                return models.PqModePrometheus(value)
            except ValueError:
                return value
        return value

    @field_serializer("compress")
    def serialize_compress(self, value):
        if isinstance(value, str):
            try:
                return models.PqCompressionPrometheus(value)
            except ValueError:
                return value
        return value


class DiscoveryTypePrometheus(str, Enum, metaclass=utils.OpenEnumMeta):
    r"""Target discovery mechanism. Use static to manually enter a list of targets."""

    # Static
    STATIC = "static"
    # DNS
    DNS = "dns"
    # AWS EC2
    EC2 = "ec2"


class LogLevelPrometheus(str, Enum, metaclass=utils.OpenEnumMeta):
    r"""Collector runtime Log Level"""

    ERROR = "error"
    WARN = "warn"
    INFO = "info"
    DEBUG = "debug"


class MetadatumPrometheusTypedDict(TypedDict):
    name: str
    value: str
    r"""JavaScript expression to compute field's value, enclosed in quotes or backticks. (Can evaluate to a constant.)"""


class MetadatumPrometheus(BaseModel):
    name: str

    value: str
    r"""JavaScript expression to compute field's value, enclosed in quotes or backticks. (Can evaluate to a constant.)"""


class AuthTypeAuthenticationMethodPrometheus(str, Enum, metaclass=utils.OpenEnumMeta):
    r"""Enter credentials directly, or select a stored secret"""

    MANUAL = "manual"
    SECRET = "secret"


class RecordTypePrometheus(str, Enum, metaclass=utils.OpenEnumMeta):
    r"""DNS Record type to resolve"""

    SRV = "SRV"
    A = "A"
    AAAA = "AAAA"


class MetricsProtocol(str, Enum, metaclass=utils.OpenEnumMeta):
    r"""Protocol to use when collecting metrics"""

    HTTP = "http"
    HTTPS = "https"


class AwsAuthenticationMethodAuthenticationMethodPrometheus(
    str, Enum, metaclass=utils.OpenEnumMeta
):
    r"""AWS authentication method. Choose Auto to use IAM roles."""

    # Auto
    AUTO = "auto"
    # Manual
    MANUAL = "manual"
    # Secret Key pair
    SECRET = "secret"


class SearchFilterPrometheusTypedDict(TypedDict):
    name: str
    r"""Search filter attribute name, see: https://docs.aws.amazon.com/AWSEC2/latest/APIReference/API_DescribeInstances.html for more information. Attributes can be manually entered if not present in the drop down list"""
    values: List[str]
    r"""Search Filter Values, if empty only \"running\" EC2 instances will be returned"""


class SearchFilterPrometheus(BaseModel):
    name: Annotated[str, pydantic.Field(alias="Name")]
    r"""Search filter attribute name, see: https://docs.aws.amazon.com/AWSEC2/latest/APIReference/API_DescribeInstances.html for more information. Attributes can be manually entered if not present in the drop down list"""

    values: Annotated[List[str], pydantic.Field(alias="Values")]
    r"""Search Filter Values, if empty only \"running\" EC2 instances will be returned"""


class SignatureVersionPrometheus(str, Enum, metaclass=utils.OpenEnumMeta):
    r"""Signature version to use for signing EC2 requests"""

    V2 = "v2"
    V4 = "v4"


class InputPrometheusTypedDict(TypedDict):
    type: InputTypePrometheus
    id: NotRequired[str]
    r"""Unique ID for this input"""
    disabled: NotRequired[bool]
    pipeline: NotRequired[str]
    r"""Pipeline to process data from this Source before sending it through the Routes"""
    send_to_routes: NotRequired[bool]
    r"""Select whether to send data to Routes, or directly to Destinations."""
    environment: NotRequired[str]
    r"""Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere."""
    pq_enabled: NotRequired[bool]
    r"""Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers)."""
    streamtags: NotRequired[List[str]]
    r"""Tags for filtering and grouping in @{product}"""
    connections: NotRequired[List[ConnectionPrometheusTypedDict]]
    r"""Direct connections to Destinations, and optionally via a Pipeline or a Pack"""
    pq: NotRequired[PqPrometheusTypedDict]
    dimension_list: NotRequired[List[str]]
    r"""Other dimensions to include in events"""
    discovery_type: NotRequired[DiscoveryTypePrometheus]
    r"""Target discovery mechanism. Use static to manually enter a list of targets."""
    interval: NotRequired[float]
    r"""How often in minutes to scrape targets for metrics, 60 must be evenly divisible by the value or save will fail."""
    log_level: NotRequired[LogLevelPrometheus]
    r"""Collector runtime Log Level"""
    reject_unauthorized: NotRequired[bool]
    r"""Reject certificates that cannot be verified against a valid CA, such as self-signed certificates"""
    keep_alive_time: NotRequired[float]
    r"""How often workers should check in with the scheduler to keep job subscription alive"""
    job_timeout: NotRequired[str]
    r"""Maximum time the job is allowed to run (e.g., 30, 45s or 15m). Units are seconds, if not specified. Enter 0 for unlimited time."""
    max_missed_keep_alives: NotRequired[float]
    r"""The number of Keep Alive Time periods before an inactive worker will have its job subscription revoked."""
    ttl: NotRequired[str]
    r"""Time to keep the job's artifacts on disk after job completion. This also affects how long a job is listed in the Job Inspector."""
    ignore_group_jobs_limit: NotRequired[bool]
    r"""When enabled, this job's artifacts are not counted toward the Worker Group's finished job artifacts limit. Artifacts will be removed only after the Collector's configured time to live."""
    metadata: NotRequired[List[MetadatumPrometheusTypedDict]]
    r"""Fields to add to events from this input"""
    auth_type: NotRequired[AuthTypeAuthenticationMethodPrometheus]
    r"""Enter credentials directly, or select a stored secret"""
    description: NotRequired[str]
    target_list: NotRequired[List[str]]
    r"""List of Prometheus targets to pull metrics from. Values can be in URL or host[:port] format. For example: http://localhost:9090/metrics, localhost:9090, or localhost. In cases where just host[:port] is specified, the endpoint will resolve to 'http://host[:port]/metrics'."""
    record_type: NotRequired[RecordTypePrometheus]
    r"""DNS Record type to resolve"""
    scrape_port: NotRequired[float]
    r"""The port number in the metrics URL for discovered targets."""
    name_list: NotRequired[List[str]]
    r"""List of DNS names to resolve"""
    scrape_protocol: NotRequired[MetricsProtocol]
    r"""Protocol to use when collecting metrics"""
    scrape_path: NotRequired[str]
    r"""Path to use when collecting metrics from discovered targets"""
    aws_authentication_method: NotRequired[
        AwsAuthenticationMethodAuthenticationMethodPrometheus
    ]
    r"""AWS authentication method. Choose Auto to use IAM roles."""
    aws_api_key: NotRequired[str]
    aws_secret: NotRequired[str]
    r"""Select or create a stored secret that references your access key and secret key"""
    use_public_ip: NotRequired[bool]
    r"""Use public IP address for discovered targets. Set to false if the private IP address should be used."""
    search_filter: NotRequired[List[SearchFilterPrometheusTypedDict]]
    r"""EC2 Instance Search Filter"""
    aws_secret_key: NotRequired[str]
    region: NotRequired[str]
    r"""Region where the EC2 is located"""
    endpoint: NotRequired[str]
    r"""EC2 service endpoint. If empty, defaults to the AWS Region-specific endpoint. Otherwise, it must point to EC2-compatible endpoint."""
    signature_version: NotRequired[SignatureVersionPrometheus]
    r"""Signature version to use for signing EC2 requests"""
    reuse_connections: NotRequired[bool]
    r"""Reuse connections between requests, which can improve performance"""
    enable_assume_role: NotRequired[bool]
    r"""Use Assume Role credentials to access EC2"""
    assume_role_arn: NotRequired[str]
    r"""Amazon Resource Name (ARN) of the role to assume"""
    assume_role_external_id: NotRequired[str]
    r"""External ID to use when assuming role"""
    duration_seconds: NotRequired[float]
    r"""Duration of the assumed role's session, in seconds. Minimum is 900 (15 minutes), default is 3600 (1 hour), and maximum is 43200 (12 hours)."""
    username: NotRequired[str]
    r"""Username for Prometheus Basic authentication"""
    password: NotRequired[str]
    r"""Password for Prometheus Basic authentication"""
    credentials_secret: NotRequired[str]
    r"""Select or create a secret that references your credentials"""


class InputPrometheus(BaseModel):
    model_config = ConfigDict(
        populate_by_name=True, arbitrary_types_allowed=True, extra="allow"
    )
    __pydantic_extra__: Dict[str, Any] = pydantic.Field(init=False)

    type: InputTypePrometheus

    id: Optional[str] = None
    r"""Unique ID for this input"""

    disabled: Optional[bool] = False

    pipeline: Optional[str] = None
    r"""Pipeline to process data from this Source before sending it through the Routes"""

    send_to_routes: Annotated[Optional[bool], pydantic.Field(alias="sendToRoutes")] = (
        True
    )
    r"""Select whether to send data to Routes, or directly to Destinations."""

    environment: Optional[str] = None
    r"""Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere."""

    pq_enabled: Annotated[Optional[bool], pydantic.Field(alias="pqEnabled")] = False
    r"""Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers)."""

    streamtags: Optional[List[str]] = None
    r"""Tags for filtering and grouping in @{product}"""

    connections: Optional[List[ConnectionPrometheus]] = None
    r"""Direct connections to Destinations, and optionally via a Pipeline or a Pack"""

    pq: Optional[PqPrometheus] = None

    dimension_list: Annotated[
        Optional[List[str]], pydantic.Field(alias="dimensionList")
    ] = None
    r"""Other dimensions to include in events"""

    discovery_type: Annotated[
        Annotated[
            Optional[DiscoveryTypePrometheus], PlainValidator(validate_open_enum(False))
        ],
        pydantic.Field(alias="discoveryType"),
    ] = DiscoveryTypePrometheus.STATIC
    r"""Target discovery mechanism. Use static to manually enter a list of targets."""

    interval: Optional[float] = 15
    r"""How often in minutes to scrape targets for metrics, 60 must be evenly divisible by the value or save will fail."""

    log_level: Annotated[
        Annotated[
            Optional[LogLevelPrometheus], PlainValidator(validate_open_enum(False))
        ],
        pydantic.Field(alias="logLevel"),
    ] = LogLevelPrometheus.INFO
    r"""Collector runtime Log Level"""

    reject_unauthorized: Annotated[
        Optional[bool], pydantic.Field(alias="rejectUnauthorized")
    ] = True
    r"""Reject certificates that cannot be verified against a valid CA, such as self-signed certificates"""

    keep_alive_time: Annotated[
        Optional[float], pydantic.Field(alias="keepAliveTime")
    ] = 30
    r"""How often workers should check in with the scheduler to keep job subscription alive"""

    job_timeout: Annotated[Optional[str], pydantic.Field(alias="jobTimeout")] = "0"
    r"""Maximum time the job is allowed to run (e.g., 30, 45s or 15m). Units are seconds, if not specified. Enter 0 for unlimited time."""

    max_missed_keep_alives: Annotated[
        Optional[float], pydantic.Field(alias="maxMissedKeepAlives")
    ] = 3
    r"""The number of Keep Alive Time periods before an inactive worker will have its job subscription revoked."""

    ttl: Optional[str] = "4h"
    r"""Time to keep the job's artifacts on disk after job completion. This also affects how long a job is listed in the Job Inspector."""

    ignore_group_jobs_limit: Annotated[
        Optional[bool], pydantic.Field(alias="ignoreGroupJobsLimit")
    ] = False
    r"""When enabled, this job's artifacts are not counted toward the Worker Group's finished job artifacts limit. Artifacts will be removed only after the Collector's configured time to live."""

    metadata: Optional[List[MetadatumPrometheus]] = None
    r"""Fields to add to events from this input"""

    auth_type: Annotated[
        Annotated[
            Optional[AuthTypeAuthenticationMethodPrometheus],
            PlainValidator(validate_open_enum(False)),
        ],
        pydantic.Field(alias="authType"),
    ] = AuthTypeAuthenticationMethodPrometheus.MANUAL
    r"""Enter credentials directly, or select a stored secret"""

    description: Optional[str] = None

    target_list: Annotated[Optional[List[str]], pydantic.Field(alias="targetList")] = (
        None
    )
    r"""List of Prometheus targets to pull metrics from. Values can be in URL or host[:port] format. For example: http://localhost:9090/metrics, localhost:9090, or localhost. In cases where just host[:port] is specified, the endpoint will resolve to 'http://host[:port]/metrics'."""

    record_type: Annotated[
        Annotated[
            Optional[RecordTypePrometheus], PlainValidator(validate_open_enum(False))
        ],
        pydantic.Field(alias="recordType"),
    ] = RecordTypePrometheus.SRV
    r"""DNS Record type to resolve"""

    scrape_port: Annotated[Optional[float], pydantic.Field(alias="scrapePort")] = 9090
    r"""The port number in the metrics URL for discovered targets."""

    name_list: Annotated[Optional[List[str]], pydantic.Field(alias="nameList")] = None
    r"""List of DNS names to resolve"""

    scrape_protocol: Annotated[
        Annotated[Optional[MetricsProtocol], PlainValidator(validate_open_enum(False))],
        pydantic.Field(alias="scrapeProtocol"),
    ] = MetricsProtocol.HTTP
    r"""Protocol to use when collecting metrics"""

    scrape_path: Annotated[Optional[str], pydantic.Field(alias="scrapePath")] = (
        "/metrics"
    )
    r"""Path to use when collecting metrics from discovered targets"""

    aws_authentication_method: Annotated[
        Annotated[
            Optional[AwsAuthenticationMethodAuthenticationMethodPrometheus],
            PlainValidator(validate_open_enum(False)),
        ],
        pydantic.Field(alias="awsAuthenticationMethod"),
    ] = AwsAuthenticationMethodAuthenticationMethodPrometheus.AUTO
    r"""AWS authentication method. Choose Auto to use IAM roles."""

    aws_api_key: Annotated[Optional[str], pydantic.Field(alias="awsApiKey")] = None

    aws_secret: Annotated[Optional[str], pydantic.Field(alias="awsSecret")] = None
    r"""Select or create a stored secret that references your access key and secret key"""

    use_public_ip: Annotated[Optional[bool], pydantic.Field(alias="usePublicIp")] = True
    r"""Use public IP address for discovered targets. Set to false if the private IP address should be used."""

    search_filter: Annotated[
        Optional[List[SearchFilterPrometheus]], pydantic.Field(alias="searchFilter")
    ] = None
    r"""EC2 Instance Search Filter"""

    aws_secret_key: Annotated[Optional[str], pydantic.Field(alias="awsSecretKey")] = (
        None
    )

    region: Optional[str] = None
    r"""Region where the EC2 is located"""

    endpoint: Optional[str] = None
    r"""EC2 service endpoint. If empty, defaults to the AWS Region-specific endpoint. Otherwise, it must point to EC2-compatible endpoint."""

    signature_version: Annotated[
        Annotated[
            Optional[SignatureVersionPrometheus],
            PlainValidator(validate_open_enum(False)),
        ],
        pydantic.Field(alias="signatureVersion"),
    ] = SignatureVersionPrometheus.V4
    r"""Signature version to use for signing EC2 requests"""

    reuse_connections: Annotated[
        Optional[bool], pydantic.Field(alias="reuseConnections")
    ] = True
    r"""Reuse connections between requests, which can improve performance"""

    enable_assume_role: Annotated[
        Optional[bool], pydantic.Field(alias="enableAssumeRole")
    ] = False
    r"""Use Assume Role credentials to access EC2"""

    assume_role_arn: Annotated[Optional[str], pydantic.Field(alias="assumeRoleArn")] = (
        None
    )
    r"""Amazon Resource Name (ARN) of the role to assume"""

    assume_role_external_id: Annotated[
        Optional[str], pydantic.Field(alias="assumeRoleExternalId")
    ] = None
    r"""External ID to use when assuming role"""

    duration_seconds: Annotated[
        Optional[float], pydantic.Field(alias="durationSeconds")
    ] = 3600
    r"""Duration of the assumed role's session, in seconds. Minimum is 900 (15 minutes), default is 3600 (1 hour), and maximum is 43200 (12 hours)."""

    username: Optional[str] = None
    r"""Username for Prometheus Basic authentication"""

    password: Optional[str] = None
    r"""Password for Prometheus Basic authentication"""

    credentials_secret: Annotated[
        Optional[str], pydantic.Field(alias="credentialsSecret")
    ] = None
    r"""Select or create a secret that references your credentials"""

    @property
    def additional_properties(self):
        return self.__pydantic_extra__

    @additional_properties.setter
    def additional_properties(self, value):
        self.__pydantic_extra__ = value  # pyright: ignore[reportIncompatibleVariableOverride]

    @field_serializer("discovery_type")
    def serialize_discovery_type(self, value):
        if isinstance(value, str):
            try:
                return models.DiscoveryTypePrometheus(value)
            except ValueError:
                return value
        return value

    @field_serializer("log_level")
    def serialize_log_level(self, value):
        if isinstance(value, str):
            try:
                return models.LogLevelPrometheus(value)
            except ValueError:
                return value
        return value

    @field_serializer("auth_type")
    def serialize_auth_type(self, value):
        if isinstance(value, str):
            try:
                return models.AuthTypeAuthenticationMethodPrometheus(value)
            except ValueError:
                return value
        return value

    @field_serializer("record_type")
    def serialize_record_type(self, value):
        if isinstance(value, str):
            try:
                return models.RecordTypePrometheus(value)
            except ValueError:
                return value
        return value

    @field_serializer("scrape_protocol")
    def serialize_scrape_protocol(self, value):
        if isinstance(value, str):
            try:
                return models.MetricsProtocol(value)
            except ValueError:
                return value
        return value

    @field_serializer("aws_authentication_method")
    def serialize_aws_authentication_method(self, value):
        if isinstance(value, str):
            try:
                return models.AwsAuthenticationMethodAuthenticationMethodPrometheus(
                    value
                )
            except ValueError:
                return value
        return value

    @field_serializer("signature_version")
    def serialize_signature_version(self, value):
        if isinstance(value, str):
            try:
                return models.SignatureVersionPrometheus(value)
            except ValueError:
                return value
        return value


class TypePrometheusRw(str, Enum):
    PROMETHEUS_RW = "prometheus_rw"


class ConnectionPrometheusRwTypedDict(TypedDict):
    output: str
    pipeline: NotRequired[str]


class ConnectionPrometheusRw(BaseModel):
    output: str

    pipeline: Optional[str] = None


class ModePrometheusRw(str, Enum, metaclass=utils.OpenEnumMeta):
    r"""With Smart mode, PQ will write events to the filesystem only when it detects backpressure from the processing engine. With Always On mode, PQ will always write events directly to the queue before forwarding them to the processing engine."""

    # Smart
    SMART = "smart"
    # Always On
    ALWAYS = "always"


class CompressionPrometheusRw(str, Enum, metaclass=utils.OpenEnumMeta):
    r"""Codec to use to compress the persisted data"""

    # None
    NONE = "none"
    # Gzip
    GZIP = "gzip"


class PqControlsPrometheusRwTypedDict(TypedDict):
    pass


class PqControlsPrometheusRw(BaseModel):
    pass


class PqPrometheusRwTypedDict(TypedDict):
    mode: NotRequired[ModePrometheusRw]
    r"""With Smart mode, PQ will write events to the filesystem only when it detects backpressure from the processing engine. With Always On mode, PQ will always write events directly to the queue before forwarding them to the processing engine."""
    max_buffer_size: NotRequired[float]
    r"""The maximum number of events to hold in memory before writing the events to disk"""
    commit_frequency: NotRequired[float]
    r"""The number of events to send downstream before committing that Stream has read them"""
    max_file_size: NotRequired[str]
    r"""The maximum size to store in each queue file before closing and optionally compressing. Enter a numeral with units of KB, MB, etc."""
    max_size: NotRequired[str]
    r"""The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc."""
    path: NotRequired[str]
    r"""The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/inputs/<input-id>"""
    compress: NotRequired[CompressionPrometheusRw]
    r"""Codec to use to compress the persisted data"""
    pq_controls: NotRequired[PqControlsPrometheusRwTypedDict]


class PqPrometheusRw(BaseModel):
    mode: Annotated[
        Optional[ModePrometheusRw], PlainValidator(validate_open_enum(False))
    ] = ModePrometheusRw.ALWAYS
    r"""With Smart mode, PQ will write events to the filesystem only when it detects backpressure from the processing engine. With Always On mode, PQ will always write events directly to the queue before forwarding them to the processing engine."""

    max_buffer_size: Annotated[
        Optional[float], pydantic.Field(alias="maxBufferSize")
    ] = 1000
    r"""The maximum number of events to hold in memory before writing the events to disk"""

    commit_frequency: Annotated[
        Optional[float], pydantic.Field(alias="commitFrequency")
    ] = 42
    r"""The number of events to send downstream before committing that Stream has read them"""

    max_file_size: Annotated[Optional[str], pydantic.Field(alias="maxFileSize")] = (
        "1 MB"
    )
    r"""The maximum size to store in each queue file before closing and optionally compressing. Enter a numeral with units of KB, MB, etc."""

    max_size: Annotated[Optional[str], pydantic.Field(alias="maxSize")] = "5GB"
    r"""The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc."""

    path: Optional[str] = "$CRIBL_HOME/state/queues"
    r"""The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/inputs/<input-id>"""

    compress: Annotated[
        Optional[CompressionPrometheusRw], PlainValidator(validate_open_enum(False))
    ] = CompressionPrometheusRw.NONE
    r"""Codec to use to compress the persisted data"""

    pq_controls: Annotated[
        Optional[PqControlsPrometheusRw], pydantic.Field(alias="pqControls")
    ] = None

    @field_serializer("mode")
    def serialize_mode(self, value):
        if isinstance(value, str):
            try:
                return models.ModePrometheusRw(value)
            except ValueError:
                return value
        return value

    @field_serializer("compress")
    def serialize_compress(self, value):
        if isinstance(value, str):
            try:
                return models.CompressionPrometheusRw(value)
            except ValueError:
                return value
        return value


class MinimumTLSVersionPrometheusRw(str, Enum, metaclass=utils.OpenEnumMeta):
    TL_SV1 = "TLSv1"
    TL_SV1_1 = "TLSv1.1"
    TL_SV1_2 = "TLSv1.2"
    TL_SV1_3 = "TLSv1.3"


class MaximumTLSVersionPrometheusRw(str, Enum, metaclass=utils.OpenEnumMeta):
    TL_SV1 = "TLSv1"
    TL_SV1_1 = "TLSv1.1"
    TL_SV1_2 = "TLSv1.2"
    TL_SV1_3 = "TLSv1.3"


class TLSSettingsServerSidePrometheusRwTypedDict(TypedDict):
    disabled: NotRequired[bool]
    request_cert: NotRequired[bool]
    r"""Require clients to present their certificates. Used to perform client authentication using SSL certs."""
    reject_unauthorized: NotRequired[bool]
    r"""Reject certificates not authorized by a CA in the CA certificate path or by another trusted CA (such as the system's)"""
    common_name_regex: NotRequired[str]
    r"""Regex matching allowable common names in peer certificates' subject attribute"""
    certificate_name: NotRequired[str]
    r"""The name of the predefined certificate"""
    priv_key_path: NotRequired[str]
    r"""Path on server containing the private key to use. PEM format. Can reference $ENV_VARS."""
    passphrase: NotRequired[str]
    r"""Passphrase to use to decrypt private key"""
    cert_path: NotRequired[str]
    r"""Path on server containing certificates to use. PEM format. Can reference $ENV_VARS."""
    ca_path: NotRequired[str]
    r"""Path on server containing CA certificates to use. PEM format. Can reference $ENV_VARS."""
    min_version: NotRequired[MinimumTLSVersionPrometheusRw]
    max_version: NotRequired[MaximumTLSVersionPrometheusRw]


class TLSSettingsServerSidePrometheusRw(BaseModel):
    disabled: Optional[bool] = True

    request_cert: Annotated[Optional[bool], pydantic.Field(alias="requestCert")] = False
    r"""Require clients to present their certificates. Used to perform client authentication using SSL certs."""

    reject_unauthorized: Annotated[
        Optional[bool], pydantic.Field(alias="rejectUnauthorized")
    ] = True
    r"""Reject certificates not authorized by a CA in the CA certificate path or by another trusted CA (such as the system's)"""

    common_name_regex: Annotated[
        Optional[str], pydantic.Field(alias="commonNameRegex")
    ] = "/.*/"
    r"""Regex matching allowable common names in peer certificates' subject attribute"""

    certificate_name: Annotated[
        Optional[str], pydantic.Field(alias="certificateName")
    ] = None
    r"""The name of the predefined certificate"""

    priv_key_path: Annotated[Optional[str], pydantic.Field(alias="privKeyPath")] = None
    r"""Path on server containing the private key to use. PEM format. Can reference $ENV_VARS."""

    passphrase: Optional[str] = None
    r"""Passphrase to use to decrypt private key"""

    cert_path: Annotated[Optional[str], pydantic.Field(alias="certPath")] = None
    r"""Path on server containing certificates to use. PEM format. Can reference $ENV_VARS."""

    ca_path: Annotated[Optional[str], pydantic.Field(alias="caPath")] = None
    r"""Path on server containing CA certificates to use. PEM format. Can reference $ENV_VARS."""

    min_version: Annotated[
        Annotated[
            Optional[MinimumTLSVersionPrometheusRw],
            PlainValidator(validate_open_enum(False)),
        ],
        pydantic.Field(alias="minVersion"),
    ] = None

    max_version: Annotated[
        Annotated[
            Optional[MaximumTLSVersionPrometheusRw],
            PlainValidator(validate_open_enum(False)),
        ],
        pydantic.Field(alias="maxVersion"),
    ] = None

    @field_serializer("min_version")
    def serialize_min_version(self, value):
        if isinstance(value, str):
            try:
                return models.MinimumTLSVersionPrometheusRw(value)
            except ValueError:
                return value
        return value

    @field_serializer("max_version")
    def serialize_max_version(self, value):
        if isinstance(value, str):
            try:
                return models.MaximumTLSVersionPrometheusRw(value)
            except ValueError:
                return value
        return value


class AuthenticationTypePrometheusRw(str, Enum, metaclass=utils.OpenEnumMeta):
    r"""Remote Write authentication type"""

    NONE = "none"
    BASIC = "basic"
    CREDENTIALS_SECRET = "credentialsSecret"
    TOKEN = "token"
    TEXT_SECRET = "textSecret"
    OAUTH = "oauth"


class MetadatumPrometheusRwTypedDict(TypedDict):
    name: str
    value: str
    r"""JavaScript expression to compute field's value, enclosed in quotes or backticks. (Can evaluate to a constant.)"""


class MetadatumPrometheusRw(BaseModel):
    name: str

    value: str
    r"""JavaScript expression to compute field's value, enclosed in quotes or backticks. (Can evaluate to a constant.)"""


class OauthParamPrometheusRwTypedDict(TypedDict):
    name: str
    r"""OAuth parameter name"""
    value: str
    r"""OAuth parameter value"""


class OauthParamPrometheusRw(BaseModel):
    name: str
    r"""OAuth parameter name"""

    value: str
    r"""OAuth parameter value"""


class OauthHeaderPrometheusRwTypedDict(TypedDict):
    name: str
    r"""OAuth header name"""
    value: str
    r"""OAuth header value"""


class OauthHeaderPrometheusRw(BaseModel):
    name: str
    r"""OAuth header name"""

    value: str
    r"""OAuth header value"""


class InputPrometheusRwTypedDict(TypedDict):
    type: TypePrometheusRw
    port: float
    r"""Port to listen on"""
    id: NotRequired[str]
    r"""Unique ID for this input"""
    disabled: NotRequired[bool]
    pipeline: NotRequired[str]
    r"""Pipeline to process data from this Source before sending it through the Routes"""
    send_to_routes: NotRequired[bool]
    r"""Select whether to send data to Routes, or directly to Destinations."""
    environment: NotRequired[str]
    r"""Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere."""
    pq_enabled: NotRequired[bool]
    r"""Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers)."""
    streamtags: NotRequired[List[str]]
    r"""Tags for filtering and grouping in @{product}"""
    connections: NotRequired[List[ConnectionPrometheusRwTypedDict]]
    r"""Direct connections to Destinations, and optionally via a Pipeline or a Pack"""
    pq: NotRequired[PqPrometheusRwTypedDict]
    host: NotRequired[str]
    r"""Address to bind on. Defaults to 0.0.0.0 (all addresses)."""
    tls: NotRequired[TLSSettingsServerSidePrometheusRwTypedDict]
    max_active_req: NotRequired[float]
    r"""Maximum number of active requests allowed per Worker Process. Set to 0 for unlimited. Caution: Increasing the limit above the default value, or setting it to unlimited, may degrade performance and reduce throughput."""
    max_requests_per_socket: NotRequired[int]
    r"""Maximum number of requests per socket before @{product} instructs the client to close the connection. Default is 0 (unlimited)."""
    enable_proxy_header: NotRequired[bool]
    r"""Extract the client IP and port from PROXY protocol v1/v2. When enabled, the X-Forwarded-For header is ignored. Disable to use the X-Forwarded-For header for client IP extraction."""
    capture_headers: NotRequired[bool]
    r"""Add request headers to events, in the __headers field"""
    activity_log_sample_rate: NotRequired[float]
    r"""How often request activity is logged at the `info` level. A value of 1 would log every request, 10 every 10th request, etc."""
    request_timeout: NotRequired[float]
    r"""How long to wait for an incoming request to complete before aborting it. Use 0 to disable."""
    socket_timeout: NotRequired[float]
    r"""How long @{product} should wait before assuming that an inactive socket has timed out. To wait forever, set to 0."""
    keep_alive_timeout: NotRequired[float]
    r"""After the last response is sent, @{product} will wait this long for additional data before closing the socket connection. Minimum 1 second, maximum 600 seconds (10 minutes)."""
    enable_health_check: NotRequired[bool]
    r"""Expose the /cribl_health endpoint, which returns 200 OK when this Source is healthy"""
    ip_allowlist_regex: NotRequired[str]
    r"""Messages from matched IP addresses will be processed, unless also matched by the denylist"""
    ip_denylist_regex: NotRequired[str]
    r"""Messages from matched IP addresses will be ignored. This takes precedence over the allowlist."""
    prometheus_api: NotRequired[str]
    r"""Absolute path on which to listen for Prometheus requests. Defaults to /write, which will expand as: http://<your‑upstream‑URL>:<your‑port>/write."""
    auth_type: NotRequired[AuthenticationTypePrometheusRw]
    r"""Remote Write authentication type"""
    metadata: NotRequired[List[MetadatumPrometheusRwTypedDict]]
    r"""Fields to add to events from this input"""
    description: NotRequired[str]
    username: NotRequired[str]
    password: NotRequired[str]
    token: NotRequired[str]
    r"""Bearer token to include in the authorization header"""
    credentials_secret: NotRequired[str]
    r"""Select or create a secret that references your credentials"""
    text_secret: NotRequired[str]
    r"""Select or create a stored text secret"""
    login_url: NotRequired[str]
    r"""URL for OAuth"""
    secret_param_name: NotRequired[str]
    r"""Secret parameter name to pass in request body"""
    secret: NotRequired[str]
    r"""Secret parameter value to pass in request body"""
    token_attribute_name: NotRequired[str]
    r"""Name of the auth token attribute in the OAuth response. Can be top-level (e.g., 'token'); or nested, using a period (e.g., 'data.token')."""
    auth_header_expr: NotRequired[str]
    r"""JavaScript expression to compute the Authorization header value to pass in requests. The value `${token}` is used to reference the token obtained from authentication, e.g.: `Bearer ${token}`."""
    token_timeout_secs: NotRequired[float]
    r"""How often the OAuth token should be refreshed."""
    oauth_params: NotRequired[List[OauthParamPrometheusRwTypedDict]]
    r"""Additional parameters to send in the OAuth login request. @{product} will combine the secret with these parameters, and will send the URL-encoded result in a POST request to the endpoint specified in the 'Login URL'. We'll automatically add the content-type header 'application/x-www-form-urlencoded' when sending this request."""
    oauth_headers: NotRequired[List[OauthHeaderPrometheusRwTypedDict]]
    r"""Additional headers to send in the OAuth login request. @{product} will automatically add the content-type header 'application/x-www-form-urlencoded' when sending this request."""


class InputPrometheusRw(BaseModel):
    model_config = ConfigDict(
        populate_by_name=True, arbitrary_types_allowed=True, extra="allow"
    )
    __pydantic_extra__: Dict[str, Any] = pydantic.Field(init=False)

    type: TypePrometheusRw

    port: float
    r"""Port to listen on"""

    id: Optional[str] = None
    r"""Unique ID for this input"""

    disabled: Optional[bool] = False

    pipeline: Optional[str] = None
    r"""Pipeline to process data from this Source before sending it through the Routes"""

    send_to_routes: Annotated[Optional[bool], pydantic.Field(alias="sendToRoutes")] = (
        True
    )
    r"""Select whether to send data to Routes, or directly to Destinations."""

    environment: Optional[str] = None
    r"""Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere."""

    pq_enabled: Annotated[Optional[bool], pydantic.Field(alias="pqEnabled")] = False
    r"""Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers)."""

    streamtags: Optional[List[str]] = None
    r"""Tags for filtering and grouping in @{product}"""

    connections: Optional[List[ConnectionPrometheusRw]] = None
    r"""Direct connections to Destinations, and optionally via a Pipeline or a Pack"""

    pq: Optional[PqPrometheusRw] = None

    host: Optional[str] = "0.0.0.0"
    r"""Address to bind on. Defaults to 0.0.0.0 (all addresses)."""

    tls: Optional[TLSSettingsServerSidePrometheusRw] = None

    max_active_req: Annotated[Optional[float], pydantic.Field(alias="maxActiveReq")] = (
        256
    )
    r"""Maximum number of active requests allowed per Worker Process. Set to 0 for unlimited. Caution: Increasing the limit above the default value, or setting it to unlimited, may degrade performance and reduce throughput."""

    max_requests_per_socket: Annotated[
        Optional[int], pydantic.Field(alias="maxRequestsPerSocket")
    ] = 0
    r"""Maximum number of requests per socket before @{product} instructs the client to close the connection. Default is 0 (unlimited)."""

    enable_proxy_header: Annotated[
        Optional[bool], pydantic.Field(alias="enableProxyHeader")
    ] = False
    r"""Extract the client IP and port from PROXY protocol v1/v2. When enabled, the X-Forwarded-For header is ignored. Disable to use the X-Forwarded-For header for client IP extraction."""

    capture_headers: Annotated[
        Optional[bool], pydantic.Field(alias="captureHeaders")
    ] = False
    r"""Add request headers to events, in the __headers field"""

    activity_log_sample_rate: Annotated[
        Optional[float], pydantic.Field(alias="activityLogSampleRate")
    ] = 100
    r"""How often request activity is logged at the `info` level. A value of 1 would log every request, 10 every 10th request, etc."""

    request_timeout: Annotated[
        Optional[float], pydantic.Field(alias="requestTimeout")
    ] = 0
    r"""How long to wait for an incoming request to complete before aborting it. Use 0 to disable."""

    socket_timeout: Annotated[
        Optional[float], pydantic.Field(alias="socketTimeout")
    ] = 0
    r"""How long @{product} should wait before assuming that an inactive socket has timed out. To wait forever, set to 0."""

    keep_alive_timeout: Annotated[
        Optional[float], pydantic.Field(alias="keepAliveTimeout")
    ] = 5
    r"""After the last response is sent, @{product} will wait this long for additional data before closing the socket connection. Minimum 1 second, maximum 600 seconds (10 minutes)."""

    enable_health_check: Annotated[
        Optional[bool], pydantic.Field(alias="enableHealthCheck")
    ] = False
    r"""Expose the /cribl_health endpoint, which returns 200 OK when this Source is healthy"""

    ip_allowlist_regex: Annotated[
        Optional[str], pydantic.Field(alias="ipAllowlistRegex")
    ] = "/.*/"
    r"""Messages from matched IP addresses will be processed, unless also matched by the denylist"""

    ip_denylist_regex: Annotated[
        Optional[str], pydantic.Field(alias="ipDenylistRegex")
    ] = "/^$/"
    r"""Messages from matched IP addresses will be ignored. This takes precedence over the allowlist."""

    prometheus_api: Annotated[Optional[str], pydantic.Field(alias="prometheusAPI")] = (
        "/write"
    )
    r"""Absolute path on which to listen for Prometheus requests. Defaults to /write, which will expand as: http://<your‑upstream‑URL>:<your‑port>/write."""

    auth_type: Annotated[
        Annotated[
            Optional[AuthenticationTypePrometheusRw],
            PlainValidator(validate_open_enum(False)),
        ],
        pydantic.Field(alias="authType"),
    ] = AuthenticationTypePrometheusRw.NONE
    r"""Remote Write authentication type"""

    metadata: Optional[List[MetadatumPrometheusRw]] = None
    r"""Fields to add to events from this input"""

    description: Optional[str] = None

    username: Optional[str] = None

    password: Optional[str] = None

    token: Optional[str] = None
    r"""Bearer token to include in the authorization header"""

    credentials_secret: Annotated[
        Optional[str], pydantic.Field(alias="credentialsSecret")
    ] = None
    r"""Select or create a secret that references your credentials"""

    text_secret: Annotated[Optional[str], pydantic.Field(alias="textSecret")] = None
    r"""Select or create a stored text secret"""

    login_url: Annotated[Optional[str], pydantic.Field(alias="loginUrl")] = None
    r"""URL for OAuth"""

    secret_param_name: Annotated[
        Optional[str], pydantic.Field(alias="secretParamName")
    ] = None
    r"""Secret parameter name to pass in request body"""

    secret: Optional[str] = None
    r"""Secret parameter value to pass in request body"""

    token_attribute_name: Annotated[
        Optional[str], pydantic.Field(alias="tokenAttributeName")
    ] = None
    r"""Name of the auth token attribute in the OAuth response. Can be top-level (e.g., 'token'); or nested, using a period (e.g., 'data.token')."""

    auth_header_expr: Annotated[
        Optional[str], pydantic.Field(alias="authHeaderExpr")
    ] = "`Bearer ${token}`"
    r"""JavaScript expression to compute the Authorization header value to pass in requests. The value `${token}` is used to reference the token obtained from authentication, e.g.: `Bearer ${token}`."""

    token_timeout_secs: Annotated[
        Optional[float], pydantic.Field(alias="tokenTimeoutSecs")
    ] = 3600
    r"""How often the OAuth token should be refreshed."""

    oauth_params: Annotated[
        Optional[List[OauthParamPrometheusRw]], pydantic.Field(alias="oauthParams")
    ] = None
    r"""Additional parameters to send in the OAuth login request. @{product} will combine the secret with these parameters, and will send the URL-encoded result in a POST request to the endpoint specified in the 'Login URL'. We'll automatically add the content-type header 'application/x-www-form-urlencoded' when sending this request."""

    oauth_headers: Annotated[
        Optional[List[OauthHeaderPrometheusRw]], pydantic.Field(alias="oauthHeaders")
    ] = None
    r"""Additional headers to send in the OAuth login request. @{product} will automatically add the content-type header 'application/x-www-form-urlencoded' when sending this request."""

    @property
    def additional_properties(self):
        return self.__pydantic_extra__

    @additional_properties.setter
    def additional_properties(self, value):
        self.__pydantic_extra__ = value  # pyright: ignore[reportIncompatibleVariableOverride]

    @field_serializer("auth_type")
    def serialize_auth_type(self, value):
        if isinstance(value, str):
            try:
                return models.AuthenticationTypePrometheusRw(value)
            except ValueError:
                return value
        return value


class InputTypeLoki(str, Enum):
    LOKI = "loki"


class ConnectionLokiTypedDict(TypedDict):
    output: str
    pipeline: NotRequired[str]


class ConnectionLoki(BaseModel):
    output: str

    pipeline: Optional[str] = None


class PqModeLoki(str, Enum, metaclass=utils.OpenEnumMeta):
    r"""With Smart mode, PQ will write events to the filesystem only when it detects backpressure from the processing engine. With Always On mode, PQ will always write events directly to the queue before forwarding them to the processing engine."""

    # Smart
    SMART = "smart"
    # Always On
    ALWAYS = "always"


class PqCompressionLoki(str, Enum, metaclass=utils.OpenEnumMeta):
    r"""Codec to use to compress the persisted data"""

    # None
    NONE = "none"
    # Gzip
    GZIP = "gzip"


class InputPqControlsLokiTypedDict(TypedDict):
    pass


class InputPqControlsLoki(BaseModel):
    pass


class PqLokiTypedDict(TypedDict):
    mode: NotRequired[PqModeLoki]
    r"""With Smart mode, PQ will write events to the filesystem only when it detects backpressure from the processing engine. With Always On mode, PQ will always write events directly to the queue before forwarding them to the processing engine."""
    max_buffer_size: NotRequired[float]
    r"""The maximum number of events to hold in memory before writing the events to disk"""
    commit_frequency: NotRequired[float]
    r"""The number of events to send downstream before committing that Stream has read them"""
    max_file_size: NotRequired[str]
    r"""The maximum size to store in each queue file before closing and optionally compressing. Enter a numeral with units of KB, MB, etc."""
    max_size: NotRequired[str]
    r"""The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc."""
    path: NotRequired[str]
    r"""The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/inputs/<input-id>"""
    compress: NotRequired[PqCompressionLoki]
    r"""Codec to use to compress the persisted data"""
    pq_controls: NotRequired[InputPqControlsLokiTypedDict]


class PqLoki(BaseModel):
    mode: Annotated[Optional[PqModeLoki], PlainValidator(validate_open_enum(False))] = (
        PqModeLoki.ALWAYS
    )
    r"""With Smart mode, PQ will write events to the filesystem only when it detects backpressure from the processing engine. With Always On mode, PQ will always write events directly to the queue before forwarding them to the processing engine."""

    max_buffer_size: Annotated[
        Optional[float], pydantic.Field(alias="maxBufferSize")
    ] = 1000
    r"""The maximum number of events to hold in memory before writing the events to disk"""

    commit_frequency: Annotated[
        Optional[float], pydantic.Field(alias="commitFrequency")
    ] = 42
    r"""The number of events to send downstream before committing that Stream has read them"""

    max_file_size: Annotated[Optional[str], pydantic.Field(alias="maxFileSize")] = (
        "1 MB"
    )
    r"""The maximum size to store in each queue file before closing and optionally compressing. Enter a numeral with units of KB, MB, etc."""

    max_size: Annotated[Optional[str], pydantic.Field(alias="maxSize")] = "5GB"
    r"""The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc."""

    path: Optional[str] = "$CRIBL_HOME/state/queues"
    r"""The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/inputs/<input-id>"""

    compress: Annotated[
        Optional[PqCompressionLoki], PlainValidator(validate_open_enum(False))
    ] = PqCompressionLoki.NONE
    r"""Codec to use to compress the persisted data"""

    pq_controls: Annotated[
        Optional[InputPqControlsLoki], pydantic.Field(alias="pqControls")
    ] = None

    @field_serializer("mode")
    def serialize_mode(self, value):
        if isinstance(value, str):
            try:
                return models.PqModeLoki(value)
            except ValueError:
                return value
        return value

    @field_serializer("compress")
    def serialize_compress(self, value):
        if isinstance(value, str):
            try:
                return models.PqCompressionLoki(value)
            except ValueError:
                return value
        return value


class MinimumTLSVersionLoki(str, Enum, metaclass=utils.OpenEnumMeta):
    TL_SV1 = "TLSv1"
    TL_SV1_1 = "TLSv1.1"
    TL_SV1_2 = "TLSv1.2"
    TL_SV1_3 = "TLSv1.3"


class MaximumTLSVersionLoki(str, Enum, metaclass=utils.OpenEnumMeta):
    TL_SV1 = "TLSv1"
    TL_SV1_1 = "TLSv1.1"
    TL_SV1_2 = "TLSv1.2"
    TL_SV1_3 = "TLSv1.3"


class TLSSettingsServerSideLokiTypedDict(TypedDict):
    disabled: NotRequired[bool]
    request_cert: NotRequired[bool]
    r"""Require clients to present their certificates. Used to perform client authentication using SSL certs."""
    reject_unauthorized: NotRequired[bool]
    r"""Reject certificates not authorized by a CA in the CA certificate path or by another trusted CA (such as the system's)"""
    common_name_regex: NotRequired[str]
    r"""Regex matching allowable common names in peer certificates' subject attribute"""
    certificate_name: NotRequired[str]
    r"""The name of the predefined certificate"""
    priv_key_path: NotRequired[str]
    r"""Path on server containing the private key to use. PEM format. Can reference $ENV_VARS."""
    passphrase: NotRequired[str]
    r"""Passphrase to use to decrypt private key"""
    cert_path: NotRequired[str]
    r"""Path on server containing certificates to use. PEM format. Can reference $ENV_VARS."""
    ca_path: NotRequired[str]
    r"""Path on server containing CA certificates to use. PEM format. Can reference $ENV_VARS."""
    min_version: NotRequired[MinimumTLSVersionLoki]
    max_version: NotRequired[MaximumTLSVersionLoki]


class TLSSettingsServerSideLoki(BaseModel):
    disabled: Optional[bool] = True

    request_cert: Annotated[Optional[bool], pydantic.Field(alias="requestCert")] = False
    r"""Require clients to present their certificates. Used to perform client authentication using SSL certs."""

    reject_unauthorized: Annotated[
        Optional[bool], pydantic.Field(alias="rejectUnauthorized")
    ] = True
    r"""Reject certificates not authorized by a CA in the CA certificate path or by another trusted CA (such as the system's)"""

    common_name_regex: Annotated[
        Optional[str], pydantic.Field(alias="commonNameRegex")
    ] = "/.*/"
    r"""Regex matching allowable common names in peer certificates' subject attribute"""

    certificate_name: Annotated[
        Optional[str], pydantic.Field(alias="certificateName")
    ] = None
    r"""The name of the predefined certificate"""

    priv_key_path: Annotated[Optional[str], pydantic.Field(alias="privKeyPath")] = None
    r"""Path on server containing the private key to use. PEM format. Can reference $ENV_VARS."""

    passphrase: Optional[str] = None
    r"""Passphrase to use to decrypt private key"""

    cert_path: Annotated[Optional[str], pydantic.Field(alias="certPath")] = None
    r"""Path on server containing certificates to use. PEM format. Can reference $ENV_VARS."""

    ca_path: Annotated[Optional[str], pydantic.Field(alias="caPath")] = None
    r"""Path on server containing CA certificates to use. PEM format. Can reference $ENV_VARS."""

    min_version: Annotated[
        Annotated[
            Optional[MinimumTLSVersionLoki], PlainValidator(validate_open_enum(False))
        ],
        pydantic.Field(alias="minVersion"),
    ] = None

    max_version: Annotated[
        Annotated[
            Optional[MaximumTLSVersionLoki], PlainValidator(validate_open_enum(False))
        ],
        pydantic.Field(alias="maxVersion"),
    ] = None

    @field_serializer("min_version")
    def serialize_min_version(self, value):
        if isinstance(value, str):
            try:
                return models.MinimumTLSVersionLoki(value)
            except ValueError:
                return value
        return value

    @field_serializer("max_version")
    def serialize_max_version(self, value):
        if isinstance(value, str):
            try:
                return models.MaximumTLSVersionLoki(value)
            except ValueError:
                return value
        return value


class InputAuthenticationTypeLoki(str, Enum, metaclass=utils.OpenEnumMeta):
    r"""Loki logs authentication type"""

    NONE = "none"
    BASIC = "basic"
    CREDENTIALS_SECRET = "credentialsSecret"
    TOKEN = "token"
    TEXT_SECRET = "textSecret"
    OAUTH = "oauth"


class MetadatumLokiTypedDict(TypedDict):
    name: str
    value: str
    r"""JavaScript expression to compute field's value, enclosed in quotes or backticks. (Can evaluate to a constant.)"""


class MetadatumLoki(BaseModel):
    name: str

    value: str
    r"""JavaScript expression to compute field's value, enclosed in quotes or backticks. (Can evaluate to a constant.)"""


class OauthParamLokiTypedDict(TypedDict):
    name: str
    r"""OAuth parameter name"""
    value: str
    r"""OAuth parameter value"""


class OauthParamLoki(BaseModel):
    name: str
    r"""OAuth parameter name"""

    value: str
    r"""OAuth parameter value"""


class OauthHeaderLokiTypedDict(TypedDict):
    name: str
    r"""OAuth header name"""
    value: str
    r"""OAuth header value"""


class OauthHeaderLoki(BaseModel):
    name: str
    r"""OAuth header name"""

    value: str
    r"""OAuth header value"""


class InputLokiTypedDict(TypedDict):
    type: InputTypeLoki
    port: float
    r"""Port to listen on"""
    id: NotRequired[str]
    r"""Unique ID for this input"""
    disabled: NotRequired[bool]
    pipeline: NotRequired[str]
    r"""Pipeline to process data from this Source before sending it through the Routes"""
    send_to_routes: NotRequired[bool]
    r"""Select whether to send data to Routes, or directly to Destinations."""
    environment: NotRequired[str]
    r"""Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere."""
    pq_enabled: NotRequired[bool]
    r"""Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers)."""
    streamtags: NotRequired[List[str]]
    r"""Tags for filtering and grouping in @{product}"""
    connections: NotRequired[List[ConnectionLokiTypedDict]]
    r"""Direct connections to Destinations, and optionally via a Pipeline or a Pack"""
    pq: NotRequired[PqLokiTypedDict]
    host: NotRequired[str]
    r"""Address to bind on. Defaults to 0.0.0.0 (all addresses)."""
    tls: NotRequired[TLSSettingsServerSideLokiTypedDict]
    max_active_req: NotRequired[float]
    r"""Maximum number of active requests allowed per Worker Process. Set to 0 for unlimited. Caution: Increasing the limit above the default value, or setting it to unlimited, may degrade performance and reduce throughput."""
    max_requests_per_socket: NotRequired[int]
    r"""Maximum number of requests per socket before @{product} instructs the client to close the connection. Default is 0 (unlimited)."""
    enable_proxy_header: NotRequired[bool]
    r"""Extract the client IP and port from PROXY protocol v1/v2. When enabled, the X-Forwarded-For header is ignored. Disable to use the X-Forwarded-For header for client IP extraction."""
    capture_headers: NotRequired[bool]
    r"""Add request headers to events, in the __headers field"""
    activity_log_sample_rate: NotRequired[float]
    r"""How often request activity is logged at the `info` level. A value of 1 would log every request, 10 every 10th request, etc."""
    request_timeout: NotRequired[float]
    r"""How long to wait for an incoming request to complete before aborting it. Use 0 to disable."""
    socket_timeout: NotRequired[float]
    r"""How long @{product} should wait before assuming that an inactive socket has timed out. To wait forever, set to 0."""
    keep_alive_timeout: NotRequired[float]
    r"""After the last response is sent, @{product} will wait this long for additional data before closing the socket connection. Minimum 1 second, maximum 600 seconds (10 minutes)."""
    enable_health_check: NotRequired[bool]
    r"""Expose the /cribl_health endpoint, which returns 200 OK when this Source is healthy"""
    ip_allowlist_regex: NotRequired[str]
    r"""Messages from matched IP addresses will be processed, unless also matched by the denylist"""
    ip_denylist_regex: NotRequired[str]
    r"""Messages from matched IP addresses will be ignored. This takes precedence over the allowlist."""
    loki_api: NotRequired[str]
    r"""Absolute path on which to listen for Loki logs requests. Defaults to /loki/api/v1/push, which will (in this example) expand as: 'http://<your‑upstream‑URL>:<your‑port>/loki/api/v1/push'."""
    auth_type: NotRequired[InputAuthenticationTypeLoki]
    r"""Loki logs authentication type"""
    metadata: NotRequired[List[MetadatumLokiTypedDict]]
    r"""Fields to add to events from this input"""
    description: NotRequired[str]
    username: NotRequired[str]
    password: NotRequired[str]
    token: NotRequired[str]
    r"""Bearer token to include in the authorization header"""
    credentials_secret: NotRequired[str]
    r"""Select or create a secret that references your credentials"""
    text_secret: NotRequired[str]
    r"""Select or create a stored text secret"""
    login_url: NotRequired[str]
    r"""URL for OAuth"""
    secret_param_name: NotRequired[str]
    r"""Secret parameter name to pass in request body"""
    secret: NotRequired[str]
    r"""Secret parameter value to pass in request body"""
    token_attribute_name: NotRequired[str]
    r"""Name of the auth token attribute in the OAuth response. Can be top-level (e.g., 'token'); or nested, using a period (e.g., 'data.token')."""
    auth_header_expr: NotRequired[str]
    r"""JavaScript expression to compute the Authorization header value to pass in requests. The value `${token}` is used to reference the token obtained from authentication, e.g.: `Bearer ${token}`."""
    token_timeout_secs: NotRequired[float]
    r"""How often the OAuth token should be refreshed."""
    oauth_params: NotRequired[List[OauthParamLokiTypedDict]]
    r"""Additional parameters to send in the OAuth login request. @{product} will combine the secret with these parameters, and will send the URL-encoded result in a POST request to the endpoint specified in the 'Login URL'. We'll automatically add the content-type header 'application/x-www-form-urlencoded' when sending this request."""
    oauth_headers: NotRequired[List[OauthHeaderLokiTypedDict]]
    r"""Additional headers to send in the OAuth login request. @{product} will automatically add the content-type header 'application/x-www-form-urlencoded' when sending this request."""


class InputLoki(BaseModel):
    model_config = ConfigDict(
        populate_by_name=True, arbitrary_types_allowed=True, extra="allow"
    )
    __pydantic_extra__: Dict[str, Any] = pydantic.Field(init=False)

    type: InputTypeLoki

    port: float
    r"""Port to listen on"""

    id: Optional[str] = None
    r"""Unique ID for this input"""

    disabled: Optional[bool] = False

    pipeline: Optional[str] = None
    r"""Pipeline to process data from this Source before sending it through the Routes"""

    send_to_routes: Annotated[Optional[bool], pydantic.Field(alias="sendToRoutes")] = (
        True
    )
    r"""Select whether to send data to Routes, or directly to Destinations."""

    environment: Optional[str] = None
    r"""Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere."""

    pq_enabled: Annotated[Optional[bool], pydantic.Field(alias="pqEnabled")] = False
    r"""Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers)."""

    streamtags: Optional[List[str]] = None
    r"""Tags for filtering and grouping in @{product}"""

    connections: Optional[List[ConnectionLoki]] = None
    r"""Direct connections to Destinations, and optionally via a Pipeline or a Pack"""

    pq: Optional[PqLoki] = None

    host: Optional[str] = "0.0.0.0"
    r"""Address to bind on. Defaults to 0.0.0.0 (all addresses)."""

    tls: Optional[TLSSettingsServerSideLoki] = None

    max_active_req: Annotated[Optional[float], pydantic.Field(alias="maxActiveReq")] = (
        256
    )
    r"""Maximum number of active requests allowed per Worker Process. Set to 0 for unlimited. Caution: Increasing the limit above the default value, or setting it to unlimited, may degrade performance and reduce throughput."""

    max_requests_per_socket: Annotated[
        Optional[int], pydantic.Field(alias="maxRequestsPerSocket")
    ] = 0
    r"""Maximum number of requests per socket before @{product} instructs the client to close the connection. Default is 0 (unlimited)."""

    enable_proxy_header: Annotated[
        Optional[bool], pydantic.Field(alias="enableProxyHeader")
    ] = False
    r"""Extract the client IP and port from PROXY protocol v1/v2. When enabled, the X-Forwarded-For header is ignored. Disable to use the X-Forwarded-For header for client IP extraction."""

    capture_headers: Annotated[
        Optional[bool], pydantic.Field(alias="captureHeaders")
    ] = False
    r"""Add request headers to events, in the __headers field"""

    activity_log_sample_rate: Annotated[
        Optional[float], pydantic.Field(alias="activityLogSampleRate")
    ] = 100
    r"""How often request activity is logged at the `info` level. A value of 1 would log every request, 10 every 10th request, etc."""

    request_timeout: Annotated[
        Optional[float], pydantic.Field(alias="requestTimeout")
    ] = 0
    r"""How long to wait for an incoming request to complete before aborting it. Use 0 to disable."""

    socket_timeout: Annotated[
        Optional[float], pydantic.Field(alias="socketTimeout")
    ] = 0
    r"""How long @{product} should wait before assuming that an inactive socket has timed out. To wait forever, set to 0."""

    keep_alive_timeout: Annotated[
        Optional[float], pydantic.Field(alias="keepAliveTimeout")
    ] = 5
    r"""After the last response is sent, @{product} will wait this long for additional data before closing the socket connection. Minimum 1 second, maximum 600 seconds (10 minutes)."""

    enable_health_check: Annotated[
        Optional[bool], pydantic.Field(alias="enableHealthCheck")
    ] = False
    r"""Expose the /cribl_health endpoint, which returns 200 OK when this Source is healthy"""

    ip_allowlist_regex: Annotated[
        Optional[str], pydantic.Field(alias="ipAllowlistRegex")
    ] = "/.*/"
    r"""Messages from matched IP addresses will be processed, unless also matched by the denylist"""

    ip_denylist_regex: Annotated[
        Optional[str], pydantic.Field(alias="ipDenylistRegex")
    ] = "/^$/"
    r"""Messages from matched IP addresses will be ignored. This takes precedence over the allowlist."""

    loki_api: Annotated[Optional[str], pydantic.Field(alias="lokiAPI")] = (
        "/loki/api/v1/push"
    )
    r"""Absolute path on which to listen for Loki logs requests. Defaults to /loki/api/v1/push, which will (in this example) expand as: 'http://<your‑upstream‑URL>:<your‑port>/loki/api/v1/push'."""

    auth_type: Annotated[
        Annotated[
            Optional[InputAuthenticationTypeLoki],
            PlainValidator(validate_open_enum(False)),
        ],
        pydantic.Field(alias="authType"),
    ] = InputAuthenticationTypeLoki.NONE
    r"""Loki logs authentication type"""

    metadata: Optional[List[MetadatumLoki]] = None
    r"""Fields to add to events from this input"""

    description: Optional[str] = None

    username: Optional[str] = None

    password: Optional[str] = None

    token: Optional[str] = None
    r"""Bearer token to include in the authorization header"""

    credentials_secret: Annotated[
        Optional[str], pydantic.Field(alias="credentialsSecret")
    ] = None
    r"""Select or create a secret that references your credentials"""

    text_secret: Annotated[Optional[str], pydantic.Field(alias="textSecret")] = None
    r"""Select or create a stored text secret"""

    login_url: Annotated[Optional[str], pydantic.Field(alias="loginUrl")] = None
    r"""URL for OAuth"""

    secret_param_name: Annotated[
        Optional[str], pydantic.Field(alias="secretParamName")
    ] = None
    r"""Secret parameter name to pass in request body"""

    secret: Optional[str] = None
    r"""Secret parameter value to pass in request body"""

    token_attribute_name: Annotated[
        Optional[str], pydantic.Field(alias="tokenAttributeName")
    ] = None
    r"""Name of the auth token attribute in the OAuth response. Can be top-level (e.g., 'token'); or nested, using a period (e.g., 'data.token')."""

    auth_header_expr: Annotated[
        Optional[str], pydantic.Field(alias="authHeaderExpr")
    ] = "`Bearer ${token}`"
    r"""JavaScript expression to compute the Authorization header value to pass in requests. The value `${token}` is used to reference the token obtained from authentication, e.g.: `Bearer ${token}`."""

    token_timeout_secs: Annotated[
        Optional[float], pydantic.Field(alias="tokenTimeoutSecs")
    ] = 3600
    r"""How often the OAuth token should be refreshed."""

    oauth_params: Annotated[
        Optional[List[OauthParamLoki]], pydantic.Field(alias="oauthParams")
    ] = None
    r"""Additional parameters to send in the OAuth login request. @{product} will combine the secret with these parameters, and will send the URL-encoded result in a POST request to the endpoint specified in the 'Login URL'. We'll automatically add the content-type header 'application/x-www-form-urlencoded' when sending this request."""

    oauth_headers: Annotated[
        Optional[List[OauthHeaderLoki]], pydantic.Field(alias="oauthHeaders")
    ] = None
    r"""Additional headers to send in the OAuth login request. @{product} will automatically add the content-type header 'application/x-www-form-urlencoded' when sending this request."""

    @property
    def additional_properties(self):
        return self.__pydantic_extra__

    @additional_properties.setter
    def additional_properties(self, value):
        self.__pydantic_extra__ = value  # pyright: ignore[reportIncompatibleVariableOverride]

    @field_serializer("auth_type")
    def serialize_auth_type(self, value):
        if isinstance(value, str):
            try:
                return models.InputAuthenticationTypeLoki(value)
            except ValueError:
                return value
        return value


class InputGrafanaType2(str, Enum):
    GRAFANA = "grafana"


class InputGrafanaConnection2TypedDict(TypedDict):
    output: str
    pipeline: NotRequired[str]


class InputGrafanaConnection2(BaseModel):
    output: str

    pipeline: Optional[str] = None


class InputGrafanaMode2(str, Enum, metaclass=utils.OpenEnumMeta):
    r"""With Smart mode, PQ will write events to the filesystem only when it detects backpressure from the processing engine. With Always On mode, PQ will always write events directly to the queue before forwarding them to the processing engine."""

    # Smart
    SMART = "smart"
    # Always On
    ALWAYS = "always"


class InputGrafanaCompression2(str, Enum, metaclass=utils.OpenEnumMeta):
    r"""Codec to use to compress the persisted data"""

    # None
    NONE = "none"
    # Gzip
    GZIP = "gzip"


class InputGrafanaPqControls2TypedDict(TypedDict):
    pass


class InputGrafanaPqControls2(BaseModel):
    pass


class InputGrafanaPq2TypedDict(TypedDict):
    mode: NotRequired[InputGrafanaMode2]
    r"""With Smart mode, PQ will write events to the filesystem only when it detects backpressure from the processing engine. With Always On mode, PQ will always write events directly to the queue before forwarding them to the processing engine."""
    max_buffer_size: NotRequired[float]
    r"""The maximum number of events to hold in memory before writing the events to disk"""
    commit_frequency: NotRequired[float]
    r"""The number of events to send downstream before committing that Stream has read them"""
    max_file_size: NotRequired[str]
    r"""The maximum size to store in each queue file before closing and optionally compressing. Enter a numeral with units of KB, MB, etc."""
    max_size: NotRequired[str]
    r"""The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc."""
    path: NotRequired[str]
    r"""The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/inputs/<input-id>"""
    compress: NotRequired[InputGrafanaCompression2]
    r"""Codec to use to compress the persisted data"""
    pq_controls: NotRequired[InputGrafanaPqControls2TypedDict]


class InputGrafanaPq2(BaseModel):
    mode: Annotated[
        Optional[InputGrafanaMode2], PlainValidator(validate_open_enum(False))
    ] = InputGrafanaMode2.ALWAYS
    r"""With Smart mode, PQ will write events to the filesystem only when it detects backpressure from the processing engine. With Always On mode, PQ will always write events directly to the queue before forwarding them to the processing engine."""

    max_buffer_size: Annotated[
        Optional[float], pydantic.Field(alias="maxBufferSize")
    ] = 1000
    r"""The maximum number of events to hold in memory before writing the events to disk"""

    commit_frequency: Annotated[
        Optional[float], pydantic.Field(alias="commitFrequency")
    ] = 42
    r"""The number of events to send downstream before committing that Stream has read them"""

    max_file_size: Annotated[Optional[str], pydantic.Field(alias="maxFileSize")] = (
        "1 MB"
    )
    r"""The maximum size to store in each queue file before closing and optionally compressing. Enter a numeral with units of KB, MB, etc."""

    max_size: Annotated[Optional[str], pydantic.Field(alias="maxSize")] = "5GB"
    r"""The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc."""

    path: Optional[str] = "$CRIBL_HOME/state/queues"
    r"""The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/inputs/<input-id>"""

    compress: Annotated[
        Optional[InputGrafanaCompression2], PlainValidator(validate_open_enum(False))
    ] = InputGrafanaCompression2.NONE
    r"""Codec to use to compress the persisted data"""

    pq_controls: Annotated[
        Optional[InputGrafanaPqControls2], pydantic.Field(alias="pqControls")
    ] = None

    @field_serializer("mode")
    def serialize_mode(self, value):
        if isinstance(value, str):
            try:
                return models.InputGrafanaMode2(value)
            except ValueError:
                return value
        return value

    @field_serializer("compress")
    def serialize_compress(self, value):
        if isinstance(value, str):
            try:
                return models.InputGrafanaCompression2(value)
            except ValueError:
                return value
        return value


class InputGrafanaMinimumTLSVersion2(str, Enum, metaclass=utils.OpenEnumMeta):
    TL_SV1 = "TLSv1"
    TL_SV1_1 = "TLSv1.1"
    TL_SV1_2 = "TLSv1.2"
    TL_SV1_3 = "TLSv1.3"


class InputGrafanaMaximumTLSVersion2(str, Enum, metaclass=utils.OpenEnumMeta):
    TL_SV1 = "TLSv1"
    TL_SV1_1 = "TLSv1.1"
    TL_SV1_2 = "TLSv1.2"
    TL_SV1_3 = "TLSv1.3"


class InputGrafanaTLSSettingsServerSide2TypedDict(TypedDict):
    disabled: NotRequired[bool]
    request_cert: NotRequired[bool]
    r"""Require clients to present their certificates. Used to perform client authentication using SSL certs."""
    reject_unauthorized: NotRequired[bool]
    r"""Reject certificates not authorized by a CA in the CA certificate path or by another trusted CA (such as the system's)"""
    common_name_regex: NotRequired[str]
    r"""Regex matching allowable common names in peer certificates' subject attribute"""
    certificate_name: NotRequired[str]
    r"""The name of the predefined certificate"""
    priv_key_path: NotRequired[str]
    r"""Path on server containing the private key to use. PEM format. Can reference $ENV_VARS."""
    passphrase: NotRequired[str]
    r"""Passphrase to use to decrypt private key"""
    cert_path: NotRequired[str]
    r"""Path on server containing certificates to use. PEM format. Can reference $ENV_VARS."""
    ca_path: NotRequired[str]
    r"""Path on server containing CA certificates to use. PEM format. Can reference $ENV_VARS."""
    min_version: NotRequired[InputGrafanaMinimumTLSVersion2]
    max_version: NotRequired[InputGrafanaMaximumTLSVersion2]


class InputGrafanaTLSSettingsServerSide2(BaseModel):
    disabled: Optional[bool] = True

    request_cert: Annotated[Optional[bool], pydantic.Field(alias="requestCert")] = False
    r"""Require clients to present their certificates. Used to perform client authentication using SSL certs."""

    reject_unauthorized: Annotated[
        Optional[bool], pydantic.Field(alias="rejectUnauthorized")
    ] = True
    r"""Reject certificates not authorized by a CA in the CA certificate path or by another trusted CA (such as the system's)"""

    common_name_regex: Annotated[
        Optional[str], pydantic.Field(alias="commonNameRegex")
    ] = "/.*/"
    r"""Regex matching allowable common names in peer certificates' subject attribute"""

    certificate_name: Annotated[
        Optional[str], pydantic.Field(alias="certificateName")
    ] = None
    r"""The name of the predefined certificate"""

    priv_key_path: Annotated[Optional[str], pydantic.Field(alias="privKeyPath")] = None
    r"""Path on server containing the private key to use. PEM format. Can reference $ENV_VARS."""

    passphrase: Optional[str] = None
    r"""Passphrase to use to decrypt private key"""

    cert_path: Annotated[Optional[str], pydantic.Field(alias="certPath")] = None
    r"""Path on server containing certificates to use. PEM format. Can reference $ENV_VARS."""

    ca_path: Annotated[Optional[str], pydantic.Field(alias="caPath")] = None
    r"""Path on server containing CA certificates to use. PEM format. Can reference $ENV_VARS."""

    min_version: Annotated[
        Annotated[
            Optional[InputGrafanaMinimumTLSVersion2],
            PlainValidator(validate_open_enum(False)),
        ],
        pydantic.Field(alias="minVersion"),
    ] = None

    max_version: Annotated[
        Annotated[
            Optional[InputGrafanaMaximumTLSVersion2],
            PlainValidator(validate_open_enum(False)),
        ],
        pydantic.Field(alias="maxVersion"),
    ] = None

    @field_serializer("min_version")
    def serialize_min_version(self, value):
        if isinstance(value, str):
            try:
                return models.InputGrafanaMinimumTLSVersion2(value)
            except ValueError:
                return value
        return value

    @field_serializer("max_version")
    def serialize_max_version(self, value):
        if isinstance(value, str):
            try:
                return models.InputGrafanaMaximumTLSVersion2(value)
            except ValueError:
                return value
        return value


class InputGrafanaPrometheusAuthAuthenticationType2(
    str, Enum, metaclass=utils.OpenEnumMeta
):
    r"""Remote Write authentication type"""

    NONE = "none"
    BASIC = "basic"
    CREDENTIALS_SECRET = "credentialsSecret"
    TOKEN = "token"
    TEXT_SECRET = "textSecret"
    OAUTH = "oauth"


class PrometheusAuthOauthParam2TypedDict(TypedDict):
    name: str
    r"""OAuth parameter name"""
    value: str
    r"""OAuth parameter value"""


class PrometheusAuthOauthParam2(BaseModel):
    name: str
    r"""OAuth parameter name"""

    value: str
    r"""OAuth parameter value"""


class PrometheusAuthOauthHeader2TypedDict(TypedDict):
    name: str
    r"""OAuth header name"""
    value: str
    r"""OAuth header value"""


class PrometheusAuthOauthHeader2(BaseModel):
    name: str
    r"""OAuth header name"""

    value: str
    r"""OAuth header value"""


class InputPrometheusAuth2TypedDict(TypedDict):
    auth_type: NotRequired[InputGrafanaPrometheusAuthAuthenticationType2]
    r"""Remote Write authentication type"""
    username: NotRequired[str]
    password: NotRequired[str]
    token: NotRequired[str]
    r"""Bearer token to include in the authorization header"""
    credentials_secret: NotRequired[str]
    r"""Select or create a secret that references your credentials"""
    text_secret: NotRequired[str]
    r"""Select or create a stored text secret"""
    login_url: NotRequired[str]
    r"""URL for OAuth"""
    secret_param_name: NotRequired[str]
    r"""Secret parameter name to pass in request body"""
    secret: NotRequired[str]
    r"""Secret parameter value to pass in request body"""
    token_attribute_name: NotRequired[str]
    r"""Name of the auth token attribute in the OAuth response. Can be top-level (e.g., 'token'); or nested, using a period (e.g., 'data.token')."""
    auth_header_expr: NotRequired[str]
    r"""JavaScript expression to compute the Authorization header value to pass in requests. The value `${token}` is used to reference the token obtained from authentication, e.g.: `Bearer ${token}`."""
    token_timeout_secs: NotRequired[float]
    r"""How often the OAuth token should be refreshed."""
    oauth_params: NotRequired[List[PrometheusAuthOauthParam2TypedDict]]
    r"""Additional parameters to send in the OAuth login request. @{product} will combine the secret with these parameters, and will send the URL-encoded result in a POST request to the endpoint specified in the 'Login URL'. We'll automatically add the content-type header 'application/x-www-form-urlencoded' when sending this request."""
    oauth_headers: NotRequired[List[PrometheusAuthOauthHeader2TypedDict]]
    r"""Additional headers to send in the OAuth login request. @{product} will automatically add the content-type header 'application/x-www-form-urlencoded' when sending this request."""


class InputPrometheusAuth2(BaseModel):
    auth_type: Annotated[
        Annotated[
            Optional[InputGrafanaPrometheusAuthAuthenticationType2],
            PlainValidator(validate_open_enum(False)),
        ],
        pydantic.Field(alias="authType"),
    ] = InputGrafanaPrometheusAuthAuthenticationType2.NONE
    r"""Remote Write authentication type"""

    username: Optional[str] = None

    password: Optional[str] = None

    token: Optional[str] = None
    r"""Bearer token to include in the authorization header"""

    credentials_secret: Annotated[
        Optional[str], pydantic.Field(alias="credentialsSecret")
    ] = None
    r"""Select or create a secret that references your credentials"""

    text_secret: Annotated[Optional[str], pydantic.Field(alias="textSecret")] = None
    r"""Select or create a stored text secret"""

    login_url: Annotated[Optional[str], pydantic.Field(alias="loginUrl")] = None
    r"""URL for OAuth"""

    secret_param_name: Annotated[
        Optional[str], pydantic.Field(alias="secretParamName")
    ] = None
    r"""Secret parameter name to pass in request body"""

    secret: Optional[str] = None
    r"""Secret parameter value to pass in request body"""

    token_attribute_name: Annotated[
        Optional[str], pydantic.Field(alias="tokenAttributeName")
    ] = None
    r"""Name of the auth token attribute in the OAuth response. Can be top-level (e.g., 'token'); or nested, using a period (e.g., 'data.token')."""

    auth_header_expr: Annotated[
        Optional[str], pydantic.Field(alias="authHeaderExpr")
    ] = "`Bearer ${token}`"
    r"""JavaScript expression to compute the Authorization header value to pass in requests. The value `${token}` is used to reference the token obtained from authentication, e.g.: `Bearer ${token}`."""

    token_timeout_secs: Annotated[
        Optional[float], pydantic.Field(alias="tokenTimeoutSecs")
    ] = 3600
    r"""How often the OAuth token should be refreshed."""

    oauth_params: Annotated[
        Optional[List[PrometheusAuthOauthParam2]], pydantic.Field(alias="oauthParams")
    ] = None
    r"""Additional parameters to send in the OAuth login request. @{product} will combine the secret with these parameters, and will send the URL-encoded result in a POST request to the endpoint specified in the 'Login URL'. We'll automatically add the content-type header 'application/x-www-form-urlencoded' when sending this request."""

    oauth_headers: Annotated[
        Optional[List[PrometheusAuthOauthHeader2]], pydantic.Field(alias="oauthHeaders")
    ] = None
    r"""Additional headers to send in the OAuth login request. @{product} will automatically add the content-type header 'application/x-www-form-urlencoded' when sending this request."""

    @field_serializer("auth_type")
    def serialize_auth_type(self, value):
        if isinstance(value, str):
            try:
                return models.InputGrafanaPrometheusAuthAuthenticationType2(value)
            except ValueError:
                return value
        return value


class InputGrafanaLokiAuthAuthenticationType2(str, Enum, metaclass=utils.OpenEnumMeta):
    r"""Loki logs authentication type"""

    NONE = "none"
    BASIC = "basic"
    CREDENTIALS_SECRET = "credentialsSecret"
    TOKEN = "token"
    TEXT_SECRET = "textSecret"
    OAUTH = "oauth"


class LokiAuthOauthParam2TypedDict(TypedDict):
    name: str
    r"""OAuth parameter name"""
    value: str
    r"""OAuth parameter value"""


class LokiAuthOauthParam2(BaseModel):
    name: str
    r"""OAuth parameter name"""

    value: str
    r"""OAuth parameter value"""


class LokiAuthOauthHeader2TypedDict(TypedDict):
    name: str
    r"""OAuth header name"""
    value: str
    r"""OAuth header value"""


class LokiAuthOauthHeader2(BaseModel):
    name: str
    r"""OAuth header name"""

    value: str
    r"""OAuth header value"""


class InputLokiAuth2TypedDict(TypedDict):
    auth_type: NotRequired[InputGrafanaLokiAuthAuthenticationType2]
    r"""Loki logs authentication type"""
    username: NotRequired[str]
    password: NotRequired[str]
    token: NotRequired[str]
    r"""Bearer token to include in the authorization header"""
    credentials_secret: NotRequired[str]
    r"""Select or create a secret that references your credentials"""
    text_secret: NotRequired[str]
    r"""Select or create a stored text secret"""
    login_url: NotRequired[str]
    r"""URL for OAuth"""
    secret_param_name: NotRequired[str]
    r"""Secret parameter name to pass in request body"""
    secret: NotRequired[str]
    r"""Secret parameter value to pass in request body"""
    token_attribute_name: NotRequired[str]
    r"""Name of the auth token attribute in the OAuth response. Can be top-level (e.g., 'token'); or nested, using a period (e.g., 'data.token')."""
    auth_header_expr: NotRequired[str]
    r"""JavaScript expression to compute the Authorization header value to pass in requests. The value `${token}` is used to reference the token obtained from authentication, e.g.: `Bearer ${token}`."""
    token_timeout_secs: NotRequired[float]
    r"""How often the OAuth token should be refreshed."""
    oauth_params: NotRequired[List[LokiAuthOauthParam2TypedDict]]
    r"""Additional parameters to send in the OAuth login request. @{product} will combine the secret with these parameters, and will send the URL-encoded result in a POST request to the endpoint specified in the 'Login URL'. We'll automatically add the content-type header 'application/x-www-form-urlencoded' when sending this request."""
    oauth_headers: NotRequired[List[LokiAuthOauthHeader2TypedDict]]
    r"""Additional headers to send in the OAuth login request. @{product} will automatically add the content-type header 'application/x-www-form-urlencoded' when sending this request."""


class InputLokiAuth2(BaseModel):
    auth_type: Annotated[
        Annotated[
            Optional[InputGrafanaLokiAuthAuthenticationType2],
            PlainValidator(validate_open_enum(False)),
        ],
        pydantic.Field(alias="authType"),
    ] = InputGrafanaLokiAuthAuthenticationType2.NONE
    r"""Loki logs authentication type"""

    username: Optional[str] = None

    password: Optional[str] = None

    token: Optional[str] = None
    r"""Bearer token to include in the authorization header"""

    credentials_secret: Annotated[
        Optional[str], pydantic.Field(alias="credentialsSecret")
    ] = None
    r"""Select or create a secret that references your credentials"""

    text_secret: Annotated[Optional[str], pydantic.Field(alias="textSecret")] = None
    r"""Select or create a stored text secret"""

    login_url: Annotated[Optional[str], pydantic.Field(alias="loginUrl")] = None
    r"""URL for OAuth"""

    secret_param_name: Annotated[
        Optional[str], pydantic.Field(alias="secretParamName")
    ] = None
    r"""Secret parameter name to pass in request body"""

    secret: Optional[str] = None
    r"""Secret parameter value to pass in request body"""

    token_attribute_name: Annotated[
        Optional[str], pydantic.Field(alias="tokenAttributeName")
    ] = None
    r"""Name of the auth token attribute in the OAuth response. Can be top-level (e.g., 'token'); or nested, using a period (e.g., 'data.token')."""

    auth_header_expr: Annotated[
        Optional[str], pydantic.Field(alias="authHeaderExpr")
    ] = "`Bearer ${token}`"
    r"""JavaScript expression to compute the Authorization header value to pass in requests. The value `${token}` is used to reference the token obtained from authentication, e.g.: `Bearer ${token}`."""

    token_timeout_secs: Annotated[
        Optional[float], pydantic.Field(alias="tokenTimeoutSecs")
    ] = 3600
    r"""How often the OAuth token should be refreshed."""

    oauth_params: Annotated[
        Optional[List[LokiAuthOauthParam2]], pydantic.Field(alias="oauthParams")
    ] = None
    r"""Additional parameters to send in the OAuth login request. @{product} will combine the secret with these parameters, and will send the URL-encoded result in a POST request to the endpoint specified in the 'Login URL'. We'll automatically add the content-type header 'application/x-www-form-urlencoded' when sending this request."""

    oauth_headers: Annotated[
        Optional[List[LokiAuthOauthHeader2]], pydantic.Field(alias="oauthHeaders")
    ] = None
    r"""Additional headers to send in the OAuth login request. @{product} will automatically add the content-type header 'application/x-www-form-urlencoded' when sending this request."""

    @field_serializer("auth_type")
    def serialize_auth_type(self, value):
        if isinstance(value, str):
            try:
                return models.InputGrafanaLokiAuthAuthenticationType2(value)
            except ValueError:
                return value
        return value


class InputGrafanaMetadatum2TypedDict(TypedDict):
    name: str
    value: str
    r"""JavaScript expression to compute field's value, enclosed in quotes or backticks. (Can evaluate to a constant.)"""


class InputGrafanaMetadatum2(BaseModel):
    name: str

    value: str
    r"""JavaScript expression to compute field's value, enclosed in quotes or backticks. (Can evaluate to a constant.)"""


class InputGrafanaGrafana2TypedDict(TypedDict):
    type: InputGrafanaType2
    port: float
    r"""Port to listen on"""
    id: NotRequired[str]
    r"""Unique ID for this input"""
    disabled: NotRequired[bool]
    pipeline: NotRequired[str]
    r"""Pipeline to process data from this Source before sending it through the Routes"""
    send_to_routes: NotRequired[bool]
    r"""Select whether to send data to Routes, or directly to Destinations."""
    environment: NotRequired[str]
    r"""Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere."""
    pq_enabled: NotRequired[bool]
    r"""Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers)."""
    streamtags: NotRequired[List[str]]
    r"""Tags for filtering and grouping in @{product}"""
    connections: NotRequired[List[InputGrafanaConnection2TypedDict]]
    r"""Direct connections to Destinations, and optionally via a Pipeline or a Pack"""
    pq: NotRequired[InputGrafanaPq2TypedDict]
    host: NotRequired[str]
    r"""Address to bind on. Defaults to 0.0.0.0 (all addresses)."""
    tls: NotRequired[InputGrafanaTLSSettingsServerSide2TypedDict]
    max_active_req: NotRequired[float]
    r"""Maximum number of active requests allowed per Worker Process. Set to 0 for unlimited. Caution: Increasing the limit above the default value, or setting it to unlimited, may degrade performance and reduce throughput."""
    max_requests_per_socket: NotRequired[int]
    r"""Maximum number of requests per socket before @{product} instructs the client to close the connection. Default is 0 (unlimited)."""
    enable_proxy_header: NotRequired[bool]
    r"""Extract the client IP and port from PROXY protocol v1/v2. When enabled, the X-Forwarded-For header is ignored. Disable to use the X-Forwarded-For header for client IP extraction."""
    capture_headers: NotRequired[bool]
    r"""Add request headers to events, in the __headers field"""
    activity_log_sample_rate: NotRequired[float]
    r"""How often request activity is logged at the `info` level. A value of 1 would log every request, 10 every 10th request, etc."""
    request_timeout: NotRequired[float]
    r"""How long to wait for an incoming request to complete before aborting it. Use 0 to disable."""
    socket_timeout: NotRequired[float]
    r"""How long @{product} should wait before assuming that an inactive socket has timed out. To wait forever, set to 0."""
    keep_alive_timeout: NotRequired[float]
    r"""Maximum time to wait for additional data, after the last response was sent, before closing a socket connection. This can be very useful when Grafana Agent remote write's request frequency is high so, reusing connections, would help mitigating the cost of creating a new connection per request. Note that Grafana Agent's embedded Prometheus would attempt to keep connections open for up to 5 minutes."""
    enable_health_check: NotRequired[bool]
    r"""Expose the /cribl_health endpoint, which returns 200 OK when this Source is healthy"""
    ip_allowlist_regex: NotRequired[str]
    r"""Messages from matched IP addresses will be processed, unless also matched by the denylist"""
    ip_denylist_regex: NotRequired[str]
    r"""Messages from matched IP addresses will be ignored. This takes precedence over the allowlist."""
    prometheus_api: NotRequired[str]
    r"""Absolute path on which to listen for Grafana Agent's Remote Write requests. Defaults to /api/prom/push, which will expand as: 'http://<your‑upstream‑URL>:<your‑port>/api/prom/push'. Either this field or 'Logs API endpoint' must be configured."""
    loki_api: NotRequired[str]
    r"""Absolute path on which to listen for Loki logs requests. Defaults to /loki/api/v1/push, which will (in this example) expand as: 'http://<your‑upstream‑URL>:<your‑port>/loki/api/v1/push'. Either this field or 'Remote Write API endpoint' must be configured."""
    prometheus_auth: NotRequired[InputPrometheusAuth2TypedDict]
    loki_auth: NotRequired[InputLokiAuth2TypedDict]
    metadata: NotRequired[List[InputGrafanaMetadatum2TypedDict]]
    r"""Fields to add to events from this input"""
    description: NotRequired[str]


class InputGrafanaGrafana2(BaseModel):
    model_config = ConfigDict(
        populate_by_name=True, arbitrary_types_allowed=True, extra="allow"
    )
    __pydantic_extra__: Dict[str, Any] = pydantic.Field(init=False)

    type: InputGrafanaType2

    port: float
    r"""Port to listen on"""

    id: Optional[str] = None
    r"""Unique ID for this input"""

    disabled: Optional[bool] = False

    pipeline: Optional[str] = None
    r"""Pipeline to process data from this Source before sending it through the Routes"""

    send_to_routes: Annotated[Optional[bool], pydantic.Field(alias="sendToRoutes")] = (
        True
    )
    r"""Select whether to send data to Routes, or directly to Destinations."""

    environment: Optional[str] = None
    r"""Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere."""

    pq_enabled: Annotated[Optional[bool], pydantic.Field(alias="pqEnabled")] = False
    r"""Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers)."""

    streamtags: Optional[List[str]] = None
    r"""Tags for filtering and grouping in @{product}"""

    connections: Optional[List[InputGrafanaConnection2]] = None
    r"""Direct connections to Destinations, and optionally via a Pipeline or a Pack"""

    pq: Optional[InputGrafanaPq2] = None

    host: Optional[str] = "0.0.0.0"
    r"""Address to bind on. Defaults to 0.0.0.0 (all addresses)."""

    tls: Optional[InputGrafanaTLSSettingsServerSide2] = None

    max_active_req: Annotated[Optional[float], pydantic.Field(alias="maxActiveReq")] = (
        256
    )
    r"""Maximum number of active requests allowed per Worker Process. Set to 0 for unlimited. Caution: Increasing the limit above the default value, or setting it to unlimited, may degrade performance and reduce throughput."""

    max_requests_per_socket: Annotated[
        Optional[int], pydantic.Field(alias="maxRequestsPerSocket")
    ] = 0
    r"""Maximum number of requests per socket before @{product} instructs the client to close the connection. Default is 0 (unlimited)."""

    enable_proxy_header: Annotated[
        Optional[bool], pydantic.Field(alias="enableProxyHeader")
    ] = False
    r"""Extract the client IP and port from PROXY protocol v1/v2. When enabled, the X-Forwarded-For header is ignored. Disable to use the X-Forwarded-For header for client IP extraction."""

    capture_headers: Annotated[
        Optional[bool], pydantic.Field(alias="captureHeaders")
    ] = False
    r"""Add request headers to events, in the __headers field"""

    activity_log_sample_rate: Annotated[
        Optional[float], pydantic.Field(alias="activityLogSampleRate")
    ] = 100
    r"""How often request activity is logged at the `info` level. A value of 1 would log every request, 10 every 10th request, etc."""

    request_timeout: Annotated[
        Optional[float], pydantic.Field(alias="requestTimeout")
    ] = 0
    r"""How long to wait for an incoming request to complete before aborting it. Use 0 to disable."""

    socket_timeout: Annotated[
        Optional[float], pydantic.Field(alias="socketTimeout")
    ] = 0
    r"""How long @{product} should wait before assuming that an inactive socket has timed out. To wait forever, set to 0."""

    keep_alive_timeout: Annotated[
        Optional[float], pydantic.Field(alias="keepAliveTimeout")
    ] = 5
    r"""Maximum time to wait for additional data, after the last response was sent, before closing a socket connection. This can be very useful when Grafana Agent remote write's request frequency is high so, reusing connections, would help mitigating the cost of creating a new connection per request. Note that Grafana Agent's embedded Prometheus would attempt to keep connections open for up to 5 minutes."""

    enable_health_check: Annotated[
        Optional[bool], pydantic.Field(alias="enableHealthCheck")
    ] = False
    r"""Expose the /cribl_health endpoint, which returns 200 OK when this Source is healthy"""

    ip_allowlist_regex: Annotated[
        Optional[str], pydantic.Field(alias="ipAllowlistRegex")
    ] = "/.*/"
    r"""Messages from matched IP addresses will be processed, unless also matched by the denylist"""

    ip_denylist_regex: Annotated[
        Optional[str], pydantic.Field(alias="ipDenylistRegex")
    ] = "/^$/"
    r"""Messages from matched IP addresses will be ignored. This takes precedence over the allowlist."""

    prometheus_api: Annotated[Optional[str], pydantic.Field(alias="prometheusAPI")] = (
        "/api/prom/push"
    )
    r"""Absolute path on which to listen for Grafana Agent's Remote Write requests. Defaults to /api/prom/push, which will expand as: 'http://<your‑upstream‑URL>:<your‑port>/api/prom/push'. Either this field or 'Logs API endpoint' must be configured."""

    loki_api: Annotated[Optional[str], pydantic.Field(alias="lokiAPI")] = (
        "/loki/api/v1/push"
    )
    r"""Absolute path on which to listen for Loki logs requests. Defaults to /loki/api/v1/push, which will (in this example) expand as: 'http://<your‑upstream‑URL>:<your‑port>/loki/api/v1/push'. Either this field or 'Remote Write API endpoint' must be configured."""

    prometheus_auth: Annotated[
        Optional[InputPrometheusAuth2], pydantic.Field(alias="prometheusAuth")
    ] = None

    loki_auth: Annotated[Optional[InputLokiAuth2], pydantic.Field(alias="lokiAuth")] = (
        None
    )

    metadata: Optional[List[InputGrafanaMetadatum2]] = None
    r"""Fields to add to events from this input"""

    description: Optional[str] = None

    @property
    def additional_properties(self):
        return self.__pydantic_extra__

    @additional_properties.setter
    def additional_properties(self, value):
        self.__pydantic_extra__ = value  # pyright: ignore[reportIncompatibleVariableOverride]


class InputGrafanaType1(str, Enum):
    GRAFANA = "grafana"


class InputGrafanaConnection1TypedDict(TypedDict):
    output: str
    pipeline: NotRequired[str]


class InputGrafanaConnection1(BaseModel):
    output: str

    pipeline: Optional[str] = None


class InputGrafanaMode1(str, Enum, metaclass=utils.OpenEnumMeta):
    r"""With Smart mode, PQ will write events to the filesystem only when it detects backpressure from the processing engine. With Always On mode, PQ will always write events directly to the queue before forwarding them to the processing engine."""

    # Smart
    SMART = "smart"
    # Always On
    ALWAYS = "always"


class InputGrafanaCompression1(str, Enum, metaclass=utils.OpenEnumMeta):
    r"""Codec to use to compress the persisted data"""

    # None
    NONE = "none"
    # Gzip
    GZIP = "gzip"


class InputGrafanaPqControls1TypedDict(TypedDict):
    pass


class InputGrafanaPqControls1(BaseModel):
    pass


class InputGrafanaPq1TypedDict(TypedDict):
    mode: NotRequired[InputGrafanaMode1]
    r"""With Smart mode, PQ will write events to the filesystem only when it detects backpressure from the processing engine. With Always On mode, PQ will always write events directly to the queue before forwarding them to the processing engine."""
    max_buffer_size: NotRequired[float]
    r"""The maximum number of events to hold in memory before writing the events to disk"""
    commit_frequency: NotRequired[float]
    r"""The number of events to send downstream before committing that Stream has read them"""
    max_file_size: NotRequired[str]
    r"""The maximum size to store in each queue file before closing and optionally compressing. Enter a numeral with units of KB, MB, etc."""
    max_size: NotRequired[str]
    r"""The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc."""
    path: NotRequired[str]
    r"""The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/inputs/<input-id>"""
    compress: NotRequired[InputGrafanaCompression1]
    r"""Codec to use to compress the persisted data"""
    pq_controls: NotRequired[InputGrafanaPqControls1TypedDict]


class InputGrafanaPq1(BaseModel):
    mode: Annotated[
        Optional[InputGrafanaMode1], PlainValidator(validate_open_enum(False))
    ] = InputGrafanaMode1.ALWAYS
    r"""With Smart mode, PQ will write events to the filesystem only when it detects backpressure from the processing engine. With Always On mode, PQ will always write events directly to the queue before forwarding them to the processing engine."""

    max_buffer_size: Annotated[
        Optional[float], pydantic.Field(alias="maxBufferSize")
    ] = 1000
    r"""The maximum number of events to hold in memory before writing the events to disk"""

    commit_frequency: Annotated[
        Optional[float], pydantic.Field(alias="commitFrequency")
    ] = 42
    r"""The number of events to send downstream before committing that Stream has read them"""

    max_file_size: Annotated[Optional[str], pydantic.Field(alias="maxFileSize")] = (
        "1 MB"
    )
    r"""The maximum size to store in each queue file before closing and optionally compressing. Enter a numeral with units of KB, MB, etc."""

    max_size: Annotated[Optional[str], pydantic.Field(alias="maxSize")] = "5GB"
    r"""The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc."""

    path: Optional[str] = "$CRIBL_HOME/state/queues"
    r"""The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/inputs/<input-id>"""

    compress: Annotated[
        Optional[InputGrafanaCompression1], PlainValidator(validate_open_enum(False))
    ] = InputGrafanaCompression1.NONE
    r"""Codec to use to compress the persisted data"""

    pq_controls: Annotated[
        Optional[InputGrafanaPqControls1], pydantic.Field(alias="pqControls")
    ] = None

    @field_serializer("mode")
    def serialize_mode(self, value):
        if isinstance(value, str):
            try:
                return models.InputGrafanaMode1(value)
            except ValueError:
                return value
        return value

    @field_serializer("compress")
    def serialize_compress(self, value):
        if isinstance(value, str):
            try:
                return models.InputGrafanaCompression1(value)
            except ValueError:
                return value
        return value


class InputGrafanaMinimumTLSVersion1(str, Enum, metaclass=utils.OpenEnumMeta):
    TL_SV1 = "TLSv1"
    TL_SV1_1 = "TLSv1.1"
    TL_SV1_2 = "TLSv1.2"
    TL_SV1_3 = "TLSv1.3"


class InputGrafanaMaximumTLSVersion1(str, Enum, metaclass=utils.OpenEnumMeta):
    TL_SV1 = "TLSv1"
    TL_SV1_1 = "TLSv1.1"
    TL_SV1_2 = "TLSv1.2"
    TL_SV1_3 = "TLSv1.3"


class InputGrafanaTLSSettingsServerSide1TypedDict(TypedDict):
    disabled: NotRequired[bool]
    request_cert: NotRequired[bool]
    r"""Require clients to present their certificates. Used to perform client authentication using SSL certs."""
    reject_unauthorized: NotRequired[bool]
    r"""Reject certificates not authorized by a CA in the CA certificate path or by another trusted CA (such as the system's)"""
    common_name_regex: NotRequired[str]
    r"""Regex matching allowable common names in peer certificates' subject attribute"""
    certificate_name: NotRequired[str]
    r"""The name of the predefined certificate"""
    priv_key_path: NotRequired[str]
    r"""Path on server containing the private key to use. PEM format. Can reference $ENV_VARS."""
    passphrase: NotRequired[str]
    r"""Passphrase to use to decrypt private key"""
    cert_path: NotRequired[str]
    r"""Path on server containing certificates to use. PEM format. Can reference $ENV_VARS."""
    ca_path: NotRequired[str]
    r"""Path on server containing CA certificates to use. PEM format. Can reference $ENV_VARS."""
    min_version: NotRequired[InputGrafanaMinimumTLSVersion1]
    max_version: NotRequired[InputGrafanaMaximumTLSVersion1]


class InputGrafanaTLSSettingsServerSide1(BaseModel):
    disabled: Optional[bool] = True

    request_cert: Annotated[Optional[bool], pydantic.Field(alias="requestCert")] = False
    r"""Require clients to present their certificates. Used to perform client authentication using SSL certs."""

    reject_unauthorized: Annotated[
        Optional[bool], pydantic.Field(alias="rejectUnauthorized")
    ] = True
    r"""Reject certificates not authorized by a CA in the CA certificate path or by another trusted CA (such as the system's)"""

    common_name_regex: Annotated[
        Optional[str], pydantic.Field(alias="commonNameRegex")
    ] = "/.*/"
    r"""Regex matching allowable common names in peer certificates' subject attribute"""

    certificate_name: Annotated[
        Optional[str], pydantic.Field(alias="certificateName")
    ] = None
    r"""The name of the predefined certificate"""

    priv_key_path: Annotated[Optional[str], pydantic.Field(alias="privKeyPath")] = None
    r"""Path on server containing the private key to use. PEM format. Can reference $ENV_VARS."""

    passphrase: Optional[str] = None
    r"""Passphrase to use to decrypt private key"""

    cert_path: Annotated[Optional[str], pydantic.Field(alias="certPath")] = None
    r"""Path on server containing certificates to use. PEM format. Can reference $ENV_VARS."""

    ca_path: Annotated[Optional[str], pydantic.Field(alias="caPath")] = None
    r"""Path on server containing CA certificates to use. PEM format. Can reference $ENV_VARS."""

    min_version: Annotated[
        Annotated[
            Optional[InputGrafanaMinimumTLSVersion1],
            PlainValidator(validate_open_enum(False)),
        ],
        pydantic.Field(alias="minVersion"),
    ] = None

    max_version: Annotated[
        Annotated[
            Optional[InputGrafanaMaximumTLSVersion1],
            PlainValidator(validate_open_enum(False)),
        ],
        pydantic.Field(alias="maxVersion"),
    ] = None

    @field_serializer("min_version")
    def serialize_min_version(self, value):
        if isinstance(value, str):
            try:
                return models.InputGrafanaMinimumTLSVersion1(value)
            except ValueError:
                return value
        return value

    @field_serializer("max_version")
    def serialize_max_version(self, value):
        if isinstance(value, str):
            try:
                return models.InputGrafanaMaximumTLSVersion1(value)
            except ValueError:
                return value
        return value


class InputGrafanaPrometheusAuthAuthenticationType1(
    str, Enum, metaclass=utils.OpenEnumMeta
):
    r"""Remote Write authentication type"""

    NONE = "none"
    BASIC = "basic"
    CREDENTIALS_SECRET = "credentialsSecret"
    TOKEN = "token"
    TEXT_SECRET = "textSecret"
    OAUTH = "oauth"


class PrometheusAuthOauthParam1TypedDict(TypedDict):
    name: str
    r"""OAuth parameter name"""
    value: str
    r"""OAuth parameter value"""


class PrometheusAuthOauthParam1(BaseModel):
    name: str
    r"""OAuth parameter name"""

    value: str
    r"""OAuth parameter value"""


class PrometheusAuthOauthHeader1TypedDict(TypedDict):
    name: str
    r"""OAuth header name"""
    value: str
    r"""OAuth header value"""


class PrometheusAuthOauthHeader1(BaseModel):
    name: str
    r"""OAuth header name"""

    value: str
    r"""OAuth header value"""


class InputPrometheusAuth1TypedDict(TypedDict):
    auth_type: NotRequired[InputGrafanaPrometheusAuthAuthenticationType1]
    r"""Remote Write authentication type"""
    username: NotRequired[str]
    password: NotRequired[str]
    token: NotRequired[str]
    r"""Bearer token to include in the authorization header"""
    credentials_secret: NotRequired[str]
    r"""Select or create a secret that references your credentials"""
    text_secret: NotRequired[str]
    r"""Select or create a stored text secret"""
    login_url: NotRequired[str]
    r"""URL for OAuth"""
    secret_param_name: NotRequired[str]
    r"""Secret parameter name to pass in request body"""
    secret: NotRequired[str]
    r"""Secret parameter value to pass in request body"""
    token_attribute_name: NotRequired[str]
    r"""Name of the auth token attribute in the OAuth response. Can be top-level (e.g., 'token'); or nested, using a period (e.g., 'data.token')."""
    auth_header_expr: NotRequired[str]
    r"""JavaScript expression to compute the Authorization header value to pass in requests. The value `${token}` is used to reference the token obtained from authentication, e.g.: `Bearer ${token}`."""
    token_timeout_secs: NotRequired[float]
    r"""How often the OAuth token should be refreshed."""
    oauth_params: NotRequired[List[PrometheusAuthOauthParam1TypedDict]]
    r"""Additional parameters to send in the OAuth login request. @{product} will combine the secret with these parameters, and will send the URL-encoded result in a POST request to the endpoint specified in the 'Login URL'. We'll automatically add the content-type header 'application/x-www-form-urlencoded' when sending this request."""
    oauth_headers: NotRequired[List[PrometheusAuthOauthHeader1TypedDict]]
    r"""Additional headers to send in the OAuth login request. @{product} will automatically add the content-type header 'application/x-www-form-urlencoded' when sending this request."""


class InputPrometheusAuth1(BaseModel):
    auth_type: Annotated[
        Annotated[
            Optional[InputGrafanaPrometheusAuthAuthenticationType1],
            PlainValidator(validate_open_enum(False)),
        ],
        pydantic.Field(alias="authType"),
    ] = InputGrafanaPrometheusAuthAuthenticationType1.NONE
    r"""Remote Write authentication type"""

    username: Optional[str] = None

    password: Optional[str] = None

    token: Optional[str] = None
    r"""Bearer token to include in the authorization header"""

    credentials_secret: Annotated[
        Optional[str], pydantic.Field(alias="credentialsSecret")
    ] = None
    r"""Select or create a secret that references your credentials"""

    text_secret: Annotated[Optional[str], pydantic.Field(alias="textSecret")] = None
    r"""Select or create a stored text secret"""

    login_url: Annotated[Optional[str], pydantic.Field(alias="loginUrl")] = None
    r"""URL for OAuth"""

    secret_param_name: Annotated[
        Optional[str], pydantic.Field(alias="secretParamName")
    ] = None
    r"""Secret parameter name to pass in request body"""

    secret: Optional[str] = None
    r"""Secret parameter value to pass in request body"""

    token_attribute_name: Annotated[
        Optional[str], pydantic.Field(alias="tokenAttributeName")
    ] = None
    r"""Name of the auth token attribute in the OAuth response. Can be top-level (e.g., 'token'); or nested, using a period (e.g., 'data.token')."""

    auth_header_expr: Annotated[
        Optional[str], pydantic.Field(alias="authHeaderExpr")
    ] = "`Bearer ${token}`"
    r"""JavaScript expression to compute the Authorization header value to pass in requests. The value `${token}` is used to reference the token obtained from authentication, e.g.: `Bearer ${token}`."""

    token_timeout_secs: Annotated[
        Optional[float], pydantic.Field(alias="tokenTimeoutSecs")
    ] = 3600
    r"""How often the OAuth token should be refreshed."""

    oauth_params: Annotated[
        Optional[List[PrometheusAuthOauthParam1]], pydantic.Field(alias="oauthParams")
    ] = None
    r"""Additional parameters to send in the OAuth login request. @{product} will combine the secret with these parameters, and will send the URL-encoded result in a POST request to the endpoint specified in the 'Login URL'. We'll automatically add the content-type header 'application/x-www-form-urlencoded' when sending this request."""

    oauth_headers: Annotated[
        Optional[List[PrometheusAuthOauthHeader1]], pydantic.Field(alias="oauthHeaders")
    ] = None
    r"""Additional headers to send in the OAuth login request. @{product} will automatically add the content-type header 'application/x-www-form-urlencoded' when sending this request."""

    @field_serializer("auth_type")
    def serialize_auth_type(self, value):
        if isinstance(value, str):
            try:
                return models.InputGrafanaPrometheusAuthAuthenticationType1(value)
            except ValueError:
                return value
        return value


class InputGrafanaLokiAuthAuthenticationType1(str, Enum, metaclass=utils.OpenEnumMeta):
    r"""Loki logs authentication type"""

    NONE = "none"
    BASIC = "basic"
    CREDENTIALS_SECRET = "credentialsSecret"
    TOKEN = "token"
    TEXT_SECRET = "textSecret"
    OAUTH = "oauth"


class LokiAuthOauthParam1TypedDict(TypedDict):
    name: str
    r"""OAuth parameter name"""
    value: str
    r"""OAuth parameter value"""


class LokiAuthOauthParam1(BaseModel):
    name: str
    r"""OAuth parameter name"""

    value: str
    r"""OAuth parameter value"""


class LokiAuthOauthHeader1TypedDict(TypedDict):
    name: str
    r"""OAuth header name"""
    value: str
    r"""OAuth header value"""


class LokiAuthOauthHeader1(BaseModel):
    name: str
    r"""OAuth header name"""

    value: str
    r"""OAuth header value"""


class InputLokiAuth1TypedDict(TypedDict):
    auth_type: NotRequired[InputGrafanaLokiAuthAuthenticationType1]
    r"""Loki logs authentication type"""
    username: NotRequired[str]
    password: NotRequired[str]
    token: NotRequired[str]
    r"""Bearer token to include in the authorization header"""
    credentials_secret: NotRequired[str]
    r"""Select or create a secret that references your credentials"""
    text_secret: NotRequired[str]
    r"""Select or create a stored text secret"""
    login_url: NotRequired[str]
    r"""URL for OAuth"""
    secret_param_name: NotRequired[str]
    r"""Secret parameter name to pass in request body"""
    secret: NotRequired[str]
    r"""Secret parameter value to pass in request body"""
    token_attribute_name: NotRequired[str]
    r"""Name of the auth token attribute in the OAuth response. Can be top-level (e.g., 'token'); or nested, using a period (e.g., 'data.token')."""
    auth_header_expr: NotRequired[str]
    r"""JavaScript expression to compute the Authorization header value to pass in requests. The value `${token}` is used to reference the token obtained from authentication, e.g.: `Bearer ${token}`."""
    token_timeout_secs: NotRequired[float]
    r"""How often the OAuth token should be refreshed."""
    oauth_params: NotRequired[List[LokiAuthOauthParam1TypedDict]]
    r"""Additional parameters to send in the OAuth login request. @{product} will combine the secret with these parameters, and will send the URL-encoded result in a POST request to the endpoint specified in the 'Login URL'. We'll automatically add the content-type header 'application/x-www-form-urlencoded' when sending this request."""
    oauth_headers: NotRequired[List[LokiAuthOauthHeader1TypedDict]]
    r"""Additional headers to send in the OAuth login request. @{product} will automatically add the content-type header 'application/x-www-form-urlencoded' when sending this request."""


class InputLokiAuth1(BaseModel):
    auth_type: Annotated[
        Annotated[
            Optional[InputGrafanaLokiAuthAuthenticationType1],
            PlainValidator(validate_open_enum(False)),
        ],
        pydantic.Field(alias="authType"),
    ] = InputGrafanaLokiAuthAuthenticationType1.NONE
    r"""Loki logs authentication type"""

    username: Optional[str] = None

    password: Optional[str] = None

    token: Optional[str] = None
    r"""Bearer token to include in the authorization header"""

    credentials_secret: Annotated[
        Optional[str], pydantic.Field(alias="credentialsSecret")
    ] = None
    r"""Select or create a secret that references your credentials"""

    text_secret: Annotated[Optional[str], pydantic.Field(alias="textSecret")] = None
    r"""Select or create a stored text secret"""

    login_url: Annotated[Optional[str], pydantic.Field(alias="loginUrl")] = None
    r"""URL for OAuth"""

    secret_param_name: Annotated[
        Optional[str], pydantic.Field(alias="secretParamName")
    ] = None
    r"""Secret parameter name to pass in request body"""

    secret: Optional[str] = None
    r"""Secret parameter value to pass in request body"""

    token_attribute_name: Annotated[
        Optional[str], pydantic.Field(alias="tokenAttributeName")
    ] = None
    r"""Name of the auth token attribute in the OAuth response. Can be top-level (e.g., 'token'); or nested, using a period (e.g., 'data.token')."""

    auth_header_expr: Annotated[
        Optional[str], pydantic.Field(alias="authHeaderExpr")
    ] = "`Bearer ${token}`"
    r"""JavaScript expression to compute the Authorization header value to pass in requests. The value `${token}` is used to reference the token obtained from authentication, e.g.: `Bearer ${token}`."""

    token_timeout_secs: Annotated[
        Optional[float], pydantic.Field(alias="tokenTimeoutSecs")
    ] = 3600
    r"""How often the OAuth token should be refreshed."""

    oauth_params: Annotated[
        Optional[List[LokiAuthOauthParam1]], pydantic.Field(alias="oauthParams")
    ] = None
    r"""Additional parameters to send in the OAuth login request. @{product} will combine the secret with these parameters, and will send the URL-encoded result in a POST request to the endpoint specified in the 'Login URL'. We'll automatically add the content-type header 'application/x-www-form-urlencoded' when sending this request."""

    oauth_headers: Annotated[
        Optional[List[LokiAuthOauthHeader1]], pydantic.Field(alias="oauthHeaders")
    ] = None
    r"""Additional headers to send in the OAuth login request. @{product} will automatically add the content-type header 'application/x-www-form-urlencoded' when sending this request."""

    @field_serializer("auth_type")
    def serialize_auth_type(self, value):
        if isinstance(value, str):
            try:
                return models.InputGrafanaLokiAuthAuthenticationType1(value)
            except ValueError:
                return value
        return value


class InputGrafanaMetadatum1TypedDict(TypedDict):
    name: str
    value: str
    r"""JavaScript expression to compute field's value, enclosed in quotes or backticks. (Can evaluate to a constant.)"""


class InputGrafanaMetadatum1(BaseModel):
    name: str

    value: str
    r"""JavaScript expression to compute field's value, enclosed in quotes or backticks. (Can evaluate to a constant.)"""


class InputGrafanaGrafana1TypedDict(TypedDict):
    type: InputGrafanaType1
    port: float
    r"""Port to listen on"""
    id: NotRequired[str]
    r"""Unique ID for this input"""
    disabled: NotRequired[bool]
    pipeline: NotRequired[str]
    r"""Pipeline to process data from this Source before sending it through the Routes"""
    send_to_routes: NotRequired[bool]
    r"""Select whether to send data to Routes, or directly to Destinations."""
    environment: NotRequired[str]
    r"""Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere."""
    pq_enabled: NotRequired[bool]
    r"""Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers)."""
    streamtags: NotRequired[List[str]]
    r"""Tags for filtering and grouping in @{product}"""
    connections: NotRequired[List[InputGrafanaConnection1TypedDict]]
    r"""Direct connections to Destinations, and optionally via a Pipeline or a Pack"""
    pq: NotRequired[InputGrafanaPq1TypedDict]
    host: NotRequired[str]
    r"""Address to bind on. Defaults to 0.0.0.0 (all addresses)."""
    tls: NotRequired[InputGrafanaTLSSettingsServerSide1TypedDict]
    max_active_req: NotRequired[float]
    r"""Maximum number of active requests allowed per Worker Process. Set to 0 for unlimited. Caution: Increasing the limit above the default value, or setting it to unlimited, may degrade performance and reduce throughput."""
    max_requests_per_socket: NotRequired[int]
    r"""Maximum number of requests per socket before @{product} instructs the client to close the connection. Default is 0 (unlimited)."""
    enable_proxy_header: NotRequired[bool]
    r"""Extract the client IP and port from PROXY protocol v1/v2. When enabled, the X-Forwarded-For header is ignored. Disable to use the X-Forwarded-For header for client IP extraction."""
    capture_headers: NotRequired[bool]
    r"""Add request headers to events, in the __headers field"""
    activity_log_sample_rate: NotRequired[float]
    r"""How often request activity is logged at the `info` level. A value of 1 would log every request, 10 every 10th request, etc."""
    request_timeout: NotRequired[float]
    r"""How long to wait for an incoming request to complete before aborting it. Use 0 to disable."""
    socket_timeout: NotRequired[float]
    r"""How long @{product} should wait before assuming that an inactive socket has timed out. To wait forever, set to 0."""
    keep_alive_timeout: NotRequired[float]
    r"""Maximum time to wait for additional data, after the last response was sent, before closing a socket connection. This can be very useful when Grafana Agent remote write's request frequency is high so, reusing connections, would help mitigating the cost of creating a new connection per request. Note that Grafana Agent's embedded Prometheus would attempt to keep connections open for up to 5 minutes."""
    enable_health_check: NotRequired[bool]
    r"""Expose the /cribl_health endpoint, which returns 200 OK when this Source is healthy"""
    ip_allowlist_regex: NotRequired[str]
    r"""Messages from matched IP addresses will be processed, unless also matched by the denylist"""
    ip_denylist_regex: NotRequired[str]
    r"""Messages from matched IP addresses will be ignored. This takes precedence over the allowlist."""
    prometheus_api: NotRequired[str]
    r"""Absolute path on which to listen for Grafana Agent's Remote Write requests. Defaults to /api/prom/push, which will expand as: 'http://<your‑upstream‑URL>:<your‑port>/api/prom/push'. Either this field or 'Logs API endpoint' must be configured."""
    loki_api: NotRequired[str]
    r"""Absolute path on which to listen for Loki logs requests. Defaults to /loki/api/v1/push, which will (in this example) expand as: 'http://<your‑upstream‑URL>:<your‑port>/loki/api/v1/push'. Either this field or 'Remote Write API endpoint' must be configured."""
    prometheus_auth: NotRequired[InputPrometheusAuth1TypedDict]
    loki_auth: NotRequired[InputLokiAuth1TypedDict]
    metadata: NotRequired[List[InputGrafanaMetadatum1TypedDict]]
    r"""Fields to add to events from this input"""
    description: NotRequired[str]


class InputGrafanaGrafana1(BaseModel):
    model_config = ConfigDict(
        populate_by_name=True, arbitrary_types_allowed=True, extra="allow"
    )
    __pydantic_extra__: Dict[str, Any] = pydantic.Field(init=False)

    type: InputGrafanaType1

    port: float
    r"""Port to listen on"""

    id: Optional[str] = None
    r"""Unique ID for this input"""

    disabled: Optional[bool] = False

    pipeline: Optional[str] = None
    r"""Pipeline to process data from this Source before sending it through the Routes"""

    send_to_routes: Annotated[Optional[bool], pydantic.Field(alias="sendToRoutes")] = (
        True
    )
    r"""Select whether to send data to Routes, or directly to Destinations."""

    environment: Optional[str] = None
    r"""Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere."""

    pq_enabled: Annotated[Optional[bool], pydantic.Field(alias="pqEnabled")] = False
    r"""Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers)."""

    streamtags: Optional[List[str]] = None
    r"""Tags for filtering and grouping in @{product}"""

    connections: Optional[List[InputGrafanaConnection1]] = None
    r"""Direct connections to Destinations, and optionally via a Pipeline or a Pack"""

    pq: Optional[InputGrafanaPq1] = None

    host: Optional[str] = "0.0.0.0"
    r"""Address to bind on. Defaults to 0.0.0.0 (all addresses)."""

    tls: Optional[InputGrafanaTLSSettingsServerSide1] = None

    max_active_req: Annotated[Optional[float], pydantic.Field(alias="maxActiveReq")] = (
        256
    )
    r"""Maximum number of active requests allowed per Worker Process. Set to 0 for unlimited. Caution: Increasing the limit above the default value, or setting it to unlimited, may degrade performance and reduce throughput."""

    max_requests_per_socket: Annotated[
        Optional[int], pydantic.Field(alias="maxRequestsPerSocket")
    ] = 0
    r"""Maximum number of requests per socket before @{product} instructs the client to close the connection. Default is 0 (unlimited)."""

    enable_proxy_header: Annotated[
        Optional[bool], pydantic.Field(alias="enableProxyHeader")
    ] = False
    r"""Extract the client IP and port from PROXY protocol v1/v2. When enabled, the X-Forwarded-For header is ignored. Disable to use the X-Forwarded-For header for client IP extraction."""

    capture_headers: Annotated[
        Optional[bool], pydantic.Field(alias="captureHeaders")
    ] = False
    r"""Add request headers to events, in the __headers field"""

    activity_log_sample_rate: Annotated[
        Optional[float], pydantic.Field(alias="activityLogSampleRate")
    ] = 100
    r"""How often request activity is logged at the `info` level. A value of 1 would log every request, 10 every 10th request, etc."""

    request_timeout: Annotated[
        Optional[float], pydantic.Field(alias="requestTimeout")
    ] = 0
    r"""How long to wait for an incoming request to complete before aborting it. Use 0 to disable."""

    socket_timeout: Annotated[
        Optional[float], pydantic.Field(alias="socketTimeout")
    ] = 0
    r"""How long @{product} should wait before assuming that an inactive socket has timed out. To wait forever, set to 0."""

    keep_alive_timeout: Annotated[
        Optional[float], pydantic.Field(alias="keepAliveTimeout")
    ] = 5
    r"""Maximum time to wait for additional data, after the last response was sent, before closing a socket connection. This can be very useful when Grafana Agent remote write's request frequency is high so, reusing connections, would help mitigating the cost of creating a new connection per request. Note that Grafana Agent's embedded Prometheus would attempt to keep connections open for up to 5 minutes."""

    enable_health_check: Annotated[
        Optional[bool], pydantic.Field(alias="enableHealthCheck")
    ] = False
    r"""Expose the /cribl_health endpoint, which returns 200 OK when this Source is healthy"""

    ip_allowlist_regex: Annotated[
        Optional[str], pydantic.Field(alias="ipAllowlistRegex")
    ] = "/.*/"
    r"""Messages from matched IP addresses will be processed, unless also matched by the denylist"""

    ip_denylist_regex: Annotated[
        Optional[str], pydantic.Field(alias="ipDenylistRegex")
    ] = "/^$/"
    r"""Messages from matched IP addresses will be ignored. This takes precedence over the allowlist."""

    prometheus_api: Annotated[Optional[str], pydantic.Field(alias="prometheusAPI")] = (
        "/api/prom/push"
    )
    r"""Absolute path on which to listen for Grafana Agent's Remote Write requests. Defaults to /api/prom/push, which will expand as: 'http://<your‑upstream‑URL>:<your‑port>/api/prom/push'. Either this field or 'Logs API endpoint' must be configured."""

    loki_api: Annotated[Optional[str], pydantic.Field(alias="lokiAPI")] = (
        "/loki/api/v1/push"
    )
    r"""Absolute path on which to listen for Loki logs requests. Defaults to /loki/api/v1/push, which will (in this example) expand as: 'http://<your‑upstream‑URL>:<your‑port>/loki/api/v1/push'. Either this field or 'Remote Write API endpoint' must be configured."""

    prometheus_auth: Annotated[
        Optional[InputPrometheusAuth1], pydantic.Field(alias="prometheusAuth")
    ] = None

    loki_auth: Annotated[Optional[InputLokiAuth1], pydantic.Field(alias="lokiAuth")] = (
        None
    )

    metadata: Optional[List[InputGrafanaMetadatum1]] = None
    r"""Fields to add to events from this input"""

    description: Optional[str] = None

    @property
    def additional_properties(self):
        return self.__pydantic_extra__

    @additional_properties.setter
    def additional_properties(self, value):
        self.__pydantic_extra__ = value  # pyright: ignore[reportIncompatibleVariableOverride]


InputGrafanaTypedDict = TypeAliasType(
    "InputGrafanaTypedDict",
    Union[InputGrafanaGrafana1TypedDict, InputGrafanaGrafana2TypedDict],
)


InputGrafana = TypeAliasType(
    "InputGrafana", Union[InputGrafanaGrafana1, InputGrafanaGrafana2]
)


class InputTypeConfluentCloud(str, Enum):
    CONFLUENT_CLOUD = "confluent_cloud"


class ConnectionConfluentCloudTypedDict(TypedDict):
    output: str
    pipeline: NotRequired[str]


class ConnectionConfluentCloud(BaseModel):
    output: str

    pipeline: Optional[str] = None


class PqModeConfluentCloud(str, Enum, metaclass=utils.OpenEnumMeta):
    r"""With Smart mode, PQ will write events to the filesystem only when it detects backpressure from the processing engine. With Always On mode, PQ will always write events directly to the queue before forwarding them to the processing engine."""

    # Smart
    SMART = "smart"
    # Always On
    ALWAYS = "always"


class PqCompressionConfluentCloud(str, Enum, metaclass=utils.OpenEnumMeta):
    r"""Codec to use to compress the persisted data"""

    # None
    NONE = "none"
    # Gzip
    GZIP = "gzip"


class InputPqControlsConfluentCloudTypedDict(TypedDict):
    pass


class InputPqControlsConfluentCloud(BaseModel):
    pass


class PqConfluentCloudTypedDict(TypedDict):
    mode: NotRequired[PqModeConfluentCloud]
    r"""With Smart mode, PQ will write events to the filesystem only when it detects backpressure from the processing engine. With Always On mode, PQ will always write events directly to the queue before forwarding them to the processing engine."""
    max_buffer_size: NotRequired[float]
    r"""The maximum number of events to hold in memory before writing the events to disk"""
    commit_frequency: NotRequired[float]
    r"""The number of events to send downstream before committing that Stream has read them"""
    max_file_size: NotRequired[str]
    r"""The maximum size to store in each queue file before closing and optionally compressing. Enter a numeral with units of KB, MB, etc."""
    max_size: NotRequired[str]
    r"""The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc."""
    path: NotRequired[str]
    r"""The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/inputs/<input-id>"""
    compress: NotRequired[PqCompressionConfluentCloud]
    r"""Codec to use to compress the persisted data"""
    pq_controls: NotRequired[InputPqControlsConfluentCloudTypedDict]


class PqConfluentCloud(BaseModel):
    mode: Annotated[
        Optional[PqModeConfluentCloud], PlainValidator(validate_open_enum(False))
    ] = PqModeConfluentCloud.ALWAYS
    r"""With Smart mode, PQ will write events to the filesystem only when it detects backpressure from the processing engine. With Always On mode, PQ will always write events directly to the queue before forwarding them to the processing engine."""

    max_buffer_size: Annotated[
        Optional[float], pydantic.Field(alias="maxBufferSize")
    ] = 1000
    r"""The maximum number of events to hold in memory before writing the events to disk"""

    commit_frequency: Annotated[
        Optional[float], pydantic.Field(alias="commitFrequency")
    ] = 42
    r"""The number of events to send downstream before committing that Stream has read them"""

    max_file_size: Annotated[Optional[str], pydantic.Field(alias="maxFileSize")] = (
        "1 MB"
    )
    r"""The maximum size to store in each queue file before closing and optionally compressing. Enter a numeral with units of KB, MB, etc."""

    max_size: Annotated[Optional[str], pydantic.Field(alias="maxSize")] = "5GB"
    r"""The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc."""

    path: Optional[str] = "$CRIBL_HOME/state/queues"
    r"""The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/inputs/<input-id>"""

    compress: Annotated[
        Optional[PqCompressionConfluentCloud], PlainValidator(validate_open_enum(False))
    ] = PqCompressionConfluentCloud.NONE
    r"""Codec to use to compress the persisted data"""

    pq_controls: Annotated[
        Optional[InputPqControlsConfluentCloud], pydantic.Field(alias="pqControls")
    ] = None

    @field_serializer("mode")
    def serialize_mode(self, value):
        if isinstance(value, str):
            try:
                return models.PqModeConfluentCloud(value)
            except ValueError:
                return value
        return value

    @field_serializer("compress")
    def serialize_compress(self, value):
        if isinstance(value, str):
            try:
                return models.PqCompressionConfluentCloud(value)
            except ValueError:
                return value
        return value


class InputMinimumTLSVersionConfluentCloud(str, Enum, metaclass=utils.OpenEnumMeta):
    TL_SV1 = "TLSv1"
    TL_SV1_1 = "TLSv1.1"
    TL_SV1_2 = "TLSv1.2"
    TL_SV1_3 = "TLSv1.3"


class InputMaximumTLSVersionConfluentCloud(str, Enum, metaclass=utils.OpenEnumMeta):
    TL_SV1 = "TLSv1"
    TL_SV1_1 = "TLSv1.1"
    TL_SV1_2 = "TLSv1.2"
    TL_SV1_3 = "TLSv1.3"


class InputTLSSettingsClientSideConfluentCloudTypedDict(TypedDict):
    disabled: NotRequired[bool]
    reject_unauthorized: NotRequired[bool]
    r"""Reject certificates that are not authorized by a CA in the CA certificate path, or by another
    trusted CA (such as the system's). Defaults to Enabled. Overrides the toggle from Advanced Settings, when also present.
    """
    servername: NotRequired[str]
    r"""Server name for the SNI (Server Name Indication) TLS extension. It must be a host name, and not an IP address."""
    certificate_name: NotRequired[str]
    r"""The name of the predefined certificate"""
    ca_path: NotRequired[str]
    r"""Path on client in which to find CA certificates to verify the server's cert. PEM format. Can reference $ENV_VARS."""
    priv_key_path: NotRequired[str]
    r"""Path on client in which to find the private key to use. PEM format. Can reference $ENV_VARS."""
    cert_path: NotRequired[str]
    r"""Path on client in which to find certificates to use. PEM format. Can reference $ENV_VARS."""
    passphrase: NotRequired[str]
    r"""Passphrase to use to decrypt private key"""
    min_version: NotRequired[InputMinimumTLSVersionConfluentCloud]
    max_version: NotRequired[InputMaximumTLSVersionConfluentCloud]


class InputTLSSettingsClientSideConfluentCloud(BaseModel):
    disabled: Optional[bool] = False

    reject_unauthorized: Annotated[
        Optional[bool], pydantic.Field(alias="rejectUnauthorized")
    ] = True
    r"""Reject certificates that are not authorized by a CA in the CA certificate path, or by another
    trusted CA (such as the system's). Defaults to Enabled. Overrides the toggle from Advanced Settings, when also present.
    """

    servername: Optional[str] = None
    r"""Server name for the SNI (Server Name Indication) TLS extension. It must be a host name, and not an IP address."""

    certificate_name: Annotated[
        Optional[str], pydantic.Field(alias="certificateName")
    ] = None
    r"""The name of the predefined certificate"""

    ca_path: Annotated[Optional[str], pydantic.Field(alias="caPath")] = None
    r"""Path on client in which to find CA certificates to verify the server's cert. PEM format. Can reference $ENV_VARS."""

    priv_key_path: Annotated[Optional[str], pydantic.Field(alias="privKeyPath")] = None
    r"""Path on client in which to find the private key to use. PEM format. Can reference $ENV_VARS."""

    cert_path: Annotated[Optional[str], pydantic.Field(alias="certPath")] = None
    r"""Path on client in which to find certificates to use. PEM format. Can reference $ENV_VARS."""

    passphrase: Optional[str] = None
    r"""Passphrase to use to decrypt private key"""

    min_version: Annotated[
        Annotated[
            Optional[InputMinimumTLSVersionConfluentCloud],
            PlainValidator(validate_open_enum(False)),
        ],
        pydantic.Field(alias="minVersion"),
    ] = None

    max_version: Annotated[
        Annotated[
            Optional[InputMaximumTLSVersionConfluentCloud],
            PlainValidator(validate_open_enum(False)),
        ],
        pydantic.Field(alias="maxVersion"),
    ] = None

    @field_serializer("min_version")
    def serialize_min_version(self, value):
        if isinstance(value, str):
            try:
                return models.InputMinimumTLSVersionConfluentCloud(value)
            except ValueError:
                return value
        return value

    @field_serializer("max_version")
    def serialize_max_version(self, value):
        if isinstance(value, str):
            try:
                return models.InputMaximumTLSVersionConfluentCloud(value)
            except ValueError:
                return value
        return value


class InputAuthConfluentCloudTypedDict(TypedDict):
    r"""Credentials to use when authenticating with the schema registry using basic HTTP authentication"""

    disabled: NotRequired[bool]
    credentials_secret: NotRequired[str]
    r"""Select or create a secret that references your credentials"""


class InputAuthConfluentCloud(BaseModel):
    r"""Credentials to use when authenticating with the schema registry using basic HTTP authentication"""

    disabled: Optional[bool] = True

    credentials_secret: Annotated[
        Optional[str], pydantic.Field(alias="credentialsSecret")
    ] = None
    r"""Select or create a secret that references your credentials"""


class InputKafkaSchemaRegistryMinimumTLSVersionConfluentCloud(
    str, Enum, metaclass=utils.OpenEnumMeta
):
    TL_SV1 = "TLSv1"
    TL_SV1_1 = "TLSv1.1"
    TL_SV1_2 = "TLSv1.2"
    TL_SV1_3 = "TLSv1.3"


class InputKafkaSchemaRegistryMaximumTLSVersionConfluentCloud(
    str, Enum, metaclass=utils.OpenEnumMeta
):
    TL_SV1 = "TLSv1"
    TL_SV1_1 = "TLSv1.1"
    TL_SV1_2 = "TLSv1.2"
    TL_SV1_3 = "TLSv1.3"


class InputKafkaSchemaRegistryTLSSettingsClientSideConfluentCloudTypedDict(TypedDict):
    disabled: NotRequired[bool]
    reject_unauthorized: NotRequired[bool]
    r"""Reject certificates that are not authorized by a CA in the CA certificate path, or by another
    trusted CA (such as the system's). Defaults to Enabled. Overrides the toggle from Advanced Settings, when also present.
    """
    servername: NotRequired[str]
    r"""Server name for the SNI (Server Name Indication) TLS extension. It must be a host name, and not an IP address."""
    certificate_name: NotRequired[str]
    r"""The name of the predefined certificate"""
    ca_path: NotRequired[str]
    r"""Path on client in which to find CA certificates to verify the server's cert. PEM format. Can reference $ENV_VARS."""
    priv_key_path: NotRequired[str]
    r"""Path on client in which to find the private key to use. PEM format. Can reference $ENV_VARS."""
    cert_path: NotRequired[str]
    r"""Path on client in which to find certificates to use. PEM format. Can reference $ENV_VARS."""
    passphrase: NotRequired[str]
    r"""Passphrase to use to decrypt private key"""
    min_version: NotRequired[InputKafkaSchemaRegistryMinimumTLSVersionConfluentCloud]
    max_version: NotRequired[InputKafkaSchemaRegistryMaximumTLSVersionConfluentCloud]


class InputKafkaSchemaRegistryTLSSettingsClientSideConfluentCloud(BaseModel):
    disabled: Optional[bool] = True

    reject_unauthorized: Annotated[
        Optional[bool], pydantic.Field(alias="rejectUnauthorized")
    ] = True
    r"""Reject certificates that are not authorized by a CA in the CA certificate path, or by another
    trusted CA (such as the system's). Defaults to Enabled. Overrides the toggle from Advanced Settings, when also present.
    """

    servername: Optional[str] = None
    r"""Server name for the SNI (Server Name Indication) TLS extension. It must be a host name, and not an IP address."""

    certificate_name: Annotated[
        Optional[str], pydantic.Field(alias="certificateName")
    ] = None
    r"""The name of the predefined certificate"""

    ca_path: Annotated[Optional[str], pydantic.Field(alias="caPath")] = None
    r"""Path on client in which to find CA certificates to verify the server's cert. PEM format. Can reference $ENV_VARS."""

    priv_key_path: Annotated[Optional[str], pydantic.Field(alias="privKeyPath")] = None
    r"""Path on client in which to find the private key to use. PEM format. Can reference $ENV_VARS."""

    cert_path: Annotated[Optional[str], pydantic.Field(alias="certPath")] = None
    r"""Path on client in which to find certificates to use. PEM format. Can reference $ENV_VARS."""

    passphrase: Optional[str] = None
    r"""Passphrase to use to decrypt private key"""

    min_version: Annotated[
        Annotated[
            Optional[InputKafkaSchemaRegistryMinimumTLSVersionConfluentCloud],
            PlainValidator(validate_open_enum(False)),
        ],
        pydantic.Field(alias="minVersion"),
    ] = None

    max_version: Annotated[
        Annotated[
            Optional[InputKafkaSchemaRegistryMaximumTLSVersionConfluentCloud],
            PlainValidator(validate_open_enum(False)),
        ],
        pydantic.Field(alias="maxVersion"),
    ] = None

    @field_serializer("min_version")
    def serialize_min_version(self, value):
        if isinstance(value, str):
            try:
                return models.InputKafkaSchemaRegistryMinimumTLSVersionConfluentCloud(
                    value
                )
            except ValueError:
                return value
        return value

    @field_serializer("max_version")
    def serialize_max_version(self, value):
        if isinstance(value, str):
            try:
                return models.InputKafkaSchemaRegistryMaximumTLSVersionConfluentCloud(
                    value
                )
            except ValueError:
                return value
        return value


class InputKafkaSchemaRegistryAuthenticationConfluentCloudTypedDict(TypedDict):
    disabled: NotRequired[bool]
    schema_registry_url: NotRequired[str]
    r"""URL for accessing the Confluent Schema Registry. Example: http://localhost:8081. To connect over TLS, use https instead of http."""
    connection_timeout: NotRequired[float]
    r"""Maximum time to wait for a Schema Registry connection to complete successfully"""
    request_timeout: NotRequired[float]
    r"""Maximum time to wait for the Schema Registry to respond to a request"""
    max_retries: NotRequired[float]
    r"""Maximum number of times to try fetching schemas from the Schema Registry"""
    auth: NotRequired[InputAuthConfluentCloudTypedDict]
    r"""Credentials to use when authenticating with the schema registry using basic HTTP authentication"""
    tls: NotRequired[
        InputKafkaSchemaRegistryTLSSettingsClientSideConfluentCloudTypedDict
    ]


class InputKafkaSchemaRegistryAuthenticationConfluentCloud(BaseModel):
    disabled: Optional[bool] = True

    schema_registry_url: Annotated[
        Optional[str], pydantic.Field(alias="schemaRegistryURL")
    ] = "http://localhost:8081"
    r"""URL for accessing the Confluent Schema Registry. Example: http://localhost:8081. To connect over TLS, use https instead of http."""

    connection_timeout: Annotated[
        Optional[float], pydantic.Field(alias="connectionTimeout")
    ] = 30000
    r"""Maximum time to wait for a Schema Registry connection to complete successfully"""

    request_timeout: Annotated[
        Optional[float], pydantic.Field(alias="requestTimeout")
    ] = 30000
    r"""Maximum time to wait for the Schema Registry to respond to a request"""

    max_retries: Annotated[Optional[float], pydantic.Field(alias="maxRetries")] = 1
    r"""Maximum number of times to try fetching schemas from the Schema Registry"""

    auth: Optional[InputAuthConfluentCloud] = None
    r"""Credentials to use when authenticating with the schema registry using basic HTTP authentication"""

    tls: Optional[InputKafkaSchemaRegistryTLSSettingsClientSideConfluentCloud] = None


class InputAuthenticationMethodConfluentCloud(str, Enum, metaclass=utils.OpenEnumMeta):
    r"""Enter credentials directly, or select a stored secret"""

    MANUAL = "manual"
    SECRET = "secret"


class InputSASLMechanismConfluentCloud(str, Enum, metaclass=utils.OpenEnumMeta):
    # PLAIN
    PLAIN = "plain"
    # SCRAM-SHA-256
    SCRAM_SHA_256 = "scram-sha-256"
    # SCRAM-SHA-512
    SCRAM_SHA_512 = "scram-sha-512"
    # GSSAPI/Kerberos
    KERBEROS = "kerberos"


class InputOauthParamConfluentCloudTypedDict(TypedDict):
    name: str
    value: str


class InputOauthParamConfluentCloud(BaseModel):
    name: str

    value: str


class InputSaslExtensionConfluentCloudTypedDict(TypedDict):
    name: str
    value: str


class InputSaslExtensionConfluentCloud(BaseModel):
    name: str

    value: str


class InputAuthenticationConfluentCloudTypedDict(TypedDict):
    r"""Authentication parameters to use when connecting to brokers. Using TLS is highly recommended."""

    disabled: NotRequired[bool]
    username: NotRequired[str]
    password: NotRequired[str]
    auth_type: NotRequired[InputAuthenticationMethodConfluentCloud]
    r"""Enter credentials directly, or select a stored secret"""
    credentials_secret: NotRequired[str]
    r"""Select or create a secret that references your credentials"""
    mechanism: NotRequired[InputSASLMechanismConfluentCloud]
    keytab_location: NotRequired[str]
    r"""Location of keytab file for authentication principal"""
    principal: NotRequired[str]
    r"""Authentication principal, such as `kafka_user@example.com`"""
    broker_service_class: NotRequired[str]
    r"""Kerberos service class for Kafka brokers, such as `kafka`"""
    oauth_enabled: NotRequired[bool]
    r"""Enable OAuth authentication"""
    token_url: NotRequired[str]
    r"""URL of the token endpoint to use for OAuth authentication"""
    client_id: NotRequired[str]
    r"""Client ID to use for OAuth authentication"""
    oauth_secret_type: NotRequired[str]
    client_text_secret: NotRequired[str]
    r"""Select or create a stored text secret"""
    oauth_params: NotRequired[List[InputOauthParamConfluentCloudTypedDict]]
    r"""Additional fields to send to the token endpoint, such as scope or audience"""
    sasl_extensions: NotRequired[List[InputSaslExtensionConfluentCloudTypedDict]]
    r"""Additional SASL extension fields, such as Confluent's logicalCluster or identityPoolId"""


class InputAuthenticationConfluentCloud(BaseModel):
    r"""Authentication parameters to use when connecting to brokers. Using TLS is highly recommended."""

    disabled: Optional[bool] = True

    username: Optional[str] = None

    password: Optional[str] = None

    auth_type: Annotated[
        Annotated[
            Optional[InputAuthenticationMethodConfluentCloud],
            PlainValidator(validate_open_enum(False)),
        ],
        pydantic.Field(alias="authType"),
    ] = InputAuthenticationMethodConfluentCloud.MANUAL
    r"""Enter credentials directly, or select a stored secret"""

    credentials_secret: Annotated[
        Optional[str], pydantic.Field(alias="credentialsSecret")
    ] = None
    r"""Select or create a secret that references your credentials"""

    mechanism: Annotated[
        Optional[InputSASLMechanismConfluentCloud],
        PlainValidator(validate_open_enum(False)),
    ] = InputSASLMechanismConfluentCloud.PLAIN

    keytab_location: Annotated[
        Optional[str], pydantic.Field(alias="keytabLocation")
    ] = None
    r"""Location of keytab file for authentication principal"""

    principal: Optional[str] = None
    r"""Authentication principal, such as `kafka_user@example.com`"""

    broker_service_class: Annotated[
        Optional[str], pydantic.Field(alias="brokerServiceClass")
    ] = None
    r"""Kerberos service class for Kafka brokers, such as `kafka`"""

    oauth_enabled: Annotated[Optional[bool], pydantic.Field(alias="oauthEnabled")] = (
        False
    )
    r"""Enable OAuth authentication"""

    token_url: Annotated[Optional[str], pydantic.Field(alias="tokenUrl")] = None
    r"""URL of the token endpoint to use for OAuth authentication"""

    client_id: Annotated[Optional[str], pydantic.Field(alias="clientId")] = None
    r"""Client ID to use for OAuth authentication"""

    oauth_secret_type: Annotated[
        Optional[str], pydantic.Field(alias="oauthSecretType")
    ] = "secret"

    client_text_secret: Annotated[
        Optional[str], pydantic.Field(alias="clientTextSecret")
    ] = None
    r"""Select or create a stored text secret"""

    oauth_params: Annotated[
        Optional[List[InputOauthParamConfluentCloud]],
        pydantic.Field(alias="oauthParams"),
    ] = None
    r"""Additional fields to send to the token endpoint, such as scope or audience"""

    sasl_extensions: Annotated[
        Optional[List[InputSaslExtensionConfluentCloud]],
        pydantic.Field(alias="saslExtensions"),
    ] = None
    r"""Additional SASL extension fields, such as Confluent's logicalCluster or identityPoolId"""

    @field_serializer("auth_type")
    def serialize_auth_type(self, value):
        if isinstance(value, str):
            try:
                return models.InputAuthenticationMethodConfluentCloud(value)
            except ValueError:
                return value
        return value

    @field_serializer("mechanism")
    def serialize_mechanism(self, value):
        if isinstance(value, str):
            try:
                return models.InputSASLMechanismConfluentCloud(value)
            except ValueError:
                return value
        return value


class MetadatumConfluentCloudTypedDict(TypedDict):
    name: str
    value: str
    r"""JavaScript expression to compute field's value, enclosed in quotes or backticks. (Can evaluate to a constant.)"""


class MetadatumConfluentCloud(BaseModel):
    name: str

    value: str
    r"""JavaScript expression to compute field's value, enclosed in quotes or backticks. (Can evaluate to a constant.)"""


class InputConfluentCloudTypedDict(TypedDict):
    type: InputTypeConfluentCloud
    brokers: List[str]
    r"""List of Confluent Cloud bootstrap servers to use, such as yourAccount.confluent.cloud:9092"""
    topics: List[str]
    r"""Topic to subscribe to. Warning: To optimize performance, Cribl suggests subscribing each Kafka Source to a single topic only."""
    id: NotRequired[str]
    r"""Unique ID for this input"""
    disabled: NotRequired[bool]
    pipeline: NotRequired[str]
    r"""Pipeline to process data from this Source before sending it through the Routes"""
    send_to_routes: NotRequired[bool]
    r"""Select whether to send data to Routes, or directly to Destinations."""
    environment: NotRequired[str]
    r"""Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere."""
    pq_enabled: NotRequired[bool]
    r"""Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers)."""
    streamtags: NotRequired[List[str]]
    r"""Tags for filtering and grouping in @{product}"""
    connections: NotRequired[List[ConnectionConfluentCloudTypedDict]]
    r"""Direct connections to Destinations, and optionally via a Pipeline or a Pack"""
    pq: NotRequired[PqConfluentCloudTypedDict]
    tls: NotRequired[InputTLSSettingsClientSideConfluentCloudTypedDict]
    group_id: NotRequired[str]
    r"""The consumer group to which this instance belongs. Defaults to 'Cribl'."""
    from_beginning: NotRequired[bool]
    r"""Leave enabled if you want the Source, upon first subscribing to a topic, to read starting with the earliest available message"""
    kafka_schema_registry: NotRequired[
        InputKafkaSchemaRegistryAuthenticationConfluentCloudTypedDict
    ]
    connection_timeout: NotRequired[float]
    r"""Maximum time to wait for a connection to complete successfully"""
    request_timeout: NotRequired[float]
    r"""Maximum time to wait for Kafka to respond to a request"""
    max_retries: NotRequired[float]
    r"""If messages are failing, you can set the maximum number of retries as high as 100 to prevent loss of data"""
    max_back_off: NotRequired[float]
    r"""The maximum wait time for a retry, in milliseconds. Default (and minimum) is 30,000 ms (30 seconds); maximum is 180,000 ms (180 seconds)."""
    initial_backoff: NotRequired[float]
    r"""Initial value used to calculate the retry, in milliseconds. Maximum is 600,000 ms (10 minutes)."""
    backoff_rate: NotRequired[float]
    r"""Set the backoff multiplier (2-20) to control the retry frequency for failed messages. For faster retries, use a lower multiplier. For slower retries with more delay between attempts, use a higher multiplier. The multiplier is used in an exponential backoff formula; see the Kafka [documentation](https://kafka.js.org/docs/retry-detailed) for details."""
    authentication_timeout: NotRequired[float]
    r"""Maximum time to wait for Kafka to respond to an authentication request"""
    reauthentication_threshold: NotRequired[float]
    r"""Specifies a time window during which @{product} can reauthenticate if needed. Creates the window measuring backward from the moment when credentials are set to expire."""
    sasl: NotRequired[InputAuthenticationConfluentCloudTypedDict]
    r"""Authentication parameters to use when connecting to brokers. Using TLS is highly recommended."""
    session_timeout: NotRequired[float]
    r"""Timeout used to detect client failures when using Kafka's group-management facilities.
    If the client sends no heartbeats to the broker before the timeout expires,
    the broker will remove the client from the group and initiate a rebalance.
    Value must be between the broker's configured group.min.session.timeout.ms and group.max.session.timeout.ms.
    See [Kafka's documentation](https://kafka.apache.org/documentation/#consumerconfigs_session.timeout.ms) for details.
    """
    rebalance_timeout: NotRequired[float]
    r"""Maximum allowed time for each worker to join the group after a rebalance begins.
    If the timeout is exceeded, the coordinator broker will remove the worker from the group.
    See [Kafka's documentation](https://kafka.apache.org/documentation/#connectconfigs_rebalance.timeout.ms) for details.
    """
    heartbeat_interval: NotRequired[float]
    r"""Expected time between heartbeats to the consumer coordinator when using Kafka's group-management facilities.
    Value must be lower than sessionTimeout and typically should not exceed 1/3 of the sessionTimeout value.
    See [Kafka's documentation](https://kafka.apache.org/documentation/#consumerconfigs_heartbeat.interval.ms) for details.
    """
    auto_commit_interval: NotRequired[float]
    r"""How often to commit offsets. If both this and Offset commit threshold are set, @{product} commits offsets when either condition is met. If both are empty, @{product} commits offsets after each batch."""
    auto_commit_threshold: NotRequired[float]
    r"""How many events are needed to trigger an offset commit. If both this and Offset commit interval are set, @{product} commits offsets when either condition is met. If both are empty, @{product} commits offsets after each batch."""
    max_bytes_per_partition: NotRequired[float]
    r"""Maximum amount of data that Kafka will return per partition, per fetch request. Must equal or exceed the maximum message size (maxBytesPerPartition) that Kafka is configured to allow. Otherwise, @{product} can get stuck trying to retrieve messages. Defaults to 1048576 (1 MB)."""
    max_bytes: NotRequired[float]
    r"""Maximum number of bytes that Kafka will return per fetch request. Defaults to 10485760 (10 MB)."""
    max_socket_errors: NotRequired[float]
    r"""Maximum number of network errors before the consumer re-creates a socket"""
    metadata: NotRequired[List[MetadatumConfluentCloudTypedDict]]
    r"""Fields to add to events from this input"""
    description: NotRequired[str]


class InputConfluentCloud(BaseModel):
    model_config = ConfigDict(
        populate_by_name=True, arbitrary_types_allowed=True, extra="allow"
    )
    __pydantic_extra__: Dict[str, Any] = pydantic.Field(init=False)

    type: InputTypeConfluentCloud

    brokers: List[str]
    r"""List of Confluent Cloud bootstrap servers to use, such as yourAccount.confluent.cloud:9092"""

    topics: List[str]
    r"""Topic to subscribe to. Warning: To optimize performance, Cribl suggests subscribing each Kafka Source to a single topic only."""

    id: Optional[str] = None
    r"""Unique ID for this input"""

    disabled: Optional[bool] = False

    pipeline: Optional[str] = None
    r"""Pipeline to process data from this Source before sending it through the Routes"""

    send_to_routes: Annotated[Optional[bool], pydantic.Field(alias="sendToRoutes")] = (
        True
    )
    r"""Select whether to send data to Routes, or directly to Destinations."""

    environment: Optional[str] = None
    r"""Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere."""

    pq_enabled: Annotated[Optional[bool], pydantic.Field(alias="pqEnabled")] = False
    r"""Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers)."""

    streamtags: Optional[List[str]] = None
    r"""Tags for filtering and grouping in @{product}"""

    connections: Optional[List[ConnectionConfluentCloud]] = None
    r"""Direct connections to Destinations, and optionally via a Pipeline or a Pack"""

    pq: Optional[PqConfluentCloud] = None

    tls: Optional[InputTLSSettingsClientSideConfluentCloud] = None

    group_id: Annotated[Optional[str], pydantic.Field(alias="groupId")] = "Cribl"
    r"""The consumer group to which this instance belongs. Defaults to 'Cribl'."""

    from_beginning: Annotated[Optional[bool], pydantic.Field(alias="fromBeginning")] = (
        True
    )
    r"""Leave enabled if you want the Source, upon first subscribing to a topic, to read starting with the earliest available message"""

    kafka_schema_registry: Annotated[
        Optional[InputKafkaSchemaRegistryAuthenticationConfluentCloud],
        pydantic.Field(alias="kafkaSchemaRegistry"),
    ] = None

    connection_timeout: Annotated[
        Optional[float], pydantic.Field(alias="connectionTimeout")
    ] = 10000
    r"""Maximum time to wait for a connection to complete successfully"""

    request_timeout: Annotated[
        Optional[float], pydantic.Field(alias="requestTimeout")
    ] = 60000
    r"""Maximum time to wait for Kafka to respond to a request"""

    max_retries: Annotated[Optional[float], pydantic.Field(alias="maxRetries")] = 5
    r"""If messages are failing, you can set the maximum number of retries as high as 100 to prevent loss of data"""

    max_back_off: Annotated[Optional[float], pydantic.Field(alias="maxBackOff")] = 30000
    r"""The maximum wait time for a retry, in milliseconds. Default (and minimum) is 30,000 ms (30 seconds); maximum is 180,000 ms (180 seconds)."""

    initial_backoff: Annotated[
        Optional[float], pydantic.Field(alias="initialBackoff")
    ] = 300
    r"""Initial value used to calculate the retry, in milliseconds. Maximum is 600,000 ms (10 minutes)."""

    backoff_rate: Annotated[Optional[float], pydantic.Field(alias="backoffRate")] = 2
    r"""Set the backoff multiplier (2-20) to control the retry frequency for failed messages. For faster retries, use a lower multiplier. For slower retries with more delay between attempts, use a higher multiplier. The multiplier is used in an exponential backoff formula; see the Kafka [documentation](https://kafka.js.org/docs/retry-detailed) for details."""

    authentication_timeout: Annotated[
        Optional[float], pydantic.Field(alias="authenticationTimeout")
    ] = 10000
    r"""Maximum time to wait for Kafka to respond to an authentication request"""

    reauthentication_threshold: Annotated[
        Optional[float], pydantic.Field(alias="reauthenticationThreshold")
    ] = 10000
    r"""Specifies a time window during which @{product} can reauthenticate if needed. Creates the window measuring backward from the moment when credentials are set to expire."""

    sasl: Optional[InputAuthenticationConfluentCloud] = None
    r"""Authentication parameters to use when connecting to brokers. Using TLS is highly recommended."""

    session_timeout: Annotated[
        Optional[float], pydantic.Field(alias="sessionTimeout")
    ] = 30000
    r"""Timeout used to detect client failures when using Kafka's group-management facilities.
    If the client sends no heartbeats to the broker before the timeout expires,
    the broker will remove the client from the group and initiate a rebalance.
    Value must be between the broker's configured group.min.session.timeout.ms and group.max.session.timeout.ms.
    See [Kafka's documentation](https://kafka.apache.org/documentation/#consumerconfigs_session.timeout.ms) for details.
    """

    rebalance_timeout: Annotated[
        Optional[float], pydantic.Field(alias="rebalanceTimeout")
    ] = 60000
    r"""Maximum allowed time for each worker to join the group after a rebalance begins.
    If the timeout is exceeded, the coordinator broker will remove the worker from the group.
    See [Kafka's documentation](https://kafka.apache.org/documentation/#connectconfigs_rebalance.timeout.ms) for details.
    """

    heartbeat_interval: Annotated[
        Optional[float], pydantic.Field(alias="heartbeatInterval")
    ] = 3000
    r"""Expected time between heartbeats to the consumer coordinator when using Kafka's group-management facilities.
    Value must be lower than sessionTimeout and typically should not exceed 1/3 of the sessionTimeout value.
    See [Kafka's documentation](https://kafka.apache.org/documentation/#consumerconfigs_heartbeat.interval.ms) for details.
    """

    auto_commit_interval: Annotated[
        Optional[float], pydantic.Field(alias="autoCommitInterval")
    ] = None
    r"""How often to commit offsets. If both this and Offset commit threshold are set, @{product} commits offsets when either condition is met. If both are empty, @{product} commits offsets after each batch."""

    auto_commit_threshold: Annotated[
        Optional[float], pydantic.Field(alias="autoCommitThreshold")
    ] = None
    r"""How many events are needed to trigger an offset commit. If both this and Offset commit interval are set, @{product} commits offsets when either condition is met. If both are empty, @{product} commits offsets after each batch."""

    max_bytes_per_partition: Annotated[
        Optional[float], pydantic.Field(alias="maxBytesPerPartition")
    ] = 1048576
    r"""Maximum amount of data that Kafka will return per partition, per fetch request. Must equal or exceed the maximum message size (maxBytesPerPartition) that Kafka is configured to allow. Otherwise, @{product} can get stuck trying to retrieve messages. Defaults to 1048576 (1 MB)."""

    max_bytes: Annotated[Optional[float], pydantic.Field(alias="maxBytes")] = 10485760
    r"""Maximum number of bytes that Kafka will return per fetch request. Defaults to 10485760 (10 MB)."""

    max_socket_errors: Annotated[
        Optional[float], pydantic.Field(alias="maxSocketErrors")
    ] = 0
    r"""Maximum number of network errors before the consumer re-creates a socket"""

    metadata: Optional[List[MetadatumConfluentCloud]] = None
    r"""Fields to add to events from this input"""

    description: Optional[str] = None

    @property
    def additional_properties(self):
        return self.__pydantic_extra__

    @additional_properties.setter
    def additional_properties(self, value):
        self.__pydantic_extra__ = value  # pyright: ignore[reportIncompatibleVariableOverride]


class InputTypeElastic(str, Enum):
    ELASTIC = "elastic"


class ConnectionElasticTypedDict(TypedDict):
    output: str
    pipeline: NotRequired[str]


class ConnectionElastic(BaseModel):
    output: str

    pipeline: Optional[str] = None


class PqModeElastic(str, Enum, metaclass=utils.OpenEnumMeta):
    r"""With Smart mode, PQ will write events to the filesystem only when it detects backpressure from the processing engine. With Always On mode, PQ will always write events directly to the queue before forwarding them to the processing engine."""

    # Smart
    SMART = "smart"
    # Always On
    ALWAYS = "always"


class PqCompressionElastic(str, Enum, metaclass=utils.OpenEnumMeta):
    r"""Codec to use to compress the persisted data"""

    # None
    NONE = "none"
    # Gzip
    GZIP = "gzip"


class InputPqControlsElasticTypedDict(TypedDict):
    pass


class InputPqControlsElastic(BaseModel):
    pass


class PqElasticTypedDict(TypedDict):
    mode: NotRequired[PqModeElastic]
    r"""With Smart mode, PQ will write events to the filesystem only when it detects backpressure from the processing engine. With Always On mode, PQ will always write events directly to the queue before forwarding them to the processing engine."""
    max_buffer_size: NotRequired[float]
    r"""The maximum number of events to hold in memory before writing the events to disk"""
    commit_frequency: NotRequired[float]
    r"""The number of events to send downstream before committing that Stream has read them"""
    max_file_size: NotRequired[str]
    r"""The maximum size to store in each queue file before closing and optionally compressing. Enter a numeral with units of KB, MB, etc."""
    max_size: NotRequired[str]
    r"""The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc."""
    path: NotRequired[str]
    r"""The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/inputs/<input-id>"""
    compress: NotRequired[PqCompressionElastic]
    r"""Codec to use to compress the persisted data"""
    pq_controls: NotRequired[InputPqControlsElasticTypedDict]


class PqElastic(BaseModel):
    mode: Annotated[
        Optional[PqModeElastic], PlainValidator(validate_open_enum(False))
    ] = PqModeElastic.ALWAYS
    r"""With Smart mode, PQ will write events to the filesystem only when it detects backpressure from the processing engine. With Always On mode, PQ will always write events directly to the queue before forwarding them to the processing engine."""

    max_buffer_size: Annotated[
        Optional[float], pydantic.Field(alias="maxBufferSize")
    ] = 1000
    r"""The maximum number of events to hold in memory before writing the events to disk"""

    commit_frequency: Annotated[
        Optional[float], pydantic.Field(alias="commitFrequency")
    ] = 42
    r"""The number of events to send downstream before committing that Stream has read them"""

    max_file_size: Annotated[Optional[str], pydantic.Field(alias="maxFileSize")] = (
        "1 MB"
    )
    r"""The maximum size to store in each queue file before closing and optionally compressing. Enter a numeral with units of KB, MB, etc."""

    max_size: Annotated[Optional[str], pydantic.Field(alias="maxSize")] = "5GB"
    r"""The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc."""

    path: Optional[str] = "$CRIBL_HOME/state/queues"
    r"""The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/inputs/<input-id>"""

    compress: Annotated[
        Optional[PqCompressionElastic], PlainValidator(validate_open_enum(False))
    ] = PqCompressionElastic.NONE
    r"""Codec to use to compress the persisted data"""

    pq_controls: Annotated[
        Optional[InputPqControlsElastic], pydantic.Field(alias="pqControls")
    ] = None

    @field_serializer("mode")
    def serialize_mode(self, value):
        if isinstance(value, str):
            try:
                return models.PqModeElastic(value)
            except ValueError:
                return value
        return value

    @field_serializer("compress")
    def serialize_compress(self, value):
        if isinstance(value, str):
            try:
                return models.PqCompressionElastic(value)
            except ValueError:
                return value
        return value


class MinimumTLSVersionElastic(str, Enum, metaclass=utils.OpenEnumMeta):
    TL_SV1 = "TLSv1"
    TL_SV1_1 = "TLSv1.1"
    TL_SV1_2 = "TLSv1.2"
    TL_SV1_3 = "TLSv1.3"


class MaximumTLSVersionElastic(str, Enum, metaclass=utils.OpenEnumMeta):
    TL_SV1 = "TLSv1"
    TL_SV1_1 = "TLSv1.1"
    TL_SV1_2 = "TLSv1.2"
    TL_SV1_3 = "TLSv1.3"


class TLSSettingsServerSideElasticTypedDict(TypedDict):
    disabled: NotRequired[bool]
    request_cert: NotRequired[bool]
    r"""Require clients to present their certificates. Used to perform client authentication using SSL certs."""
    reject_unauthorized: NotRequired[bool]
    r"""Reject certificates not authorized by a CA in the CA certificate path or by another trusted CA (such as the system's)"""
    common_name_regex: NotRequired[str]
    r"""Regex matching allowable common names in peer certificates' subject attribute"""
    certificate_name: NotRequired[str]
    r"""The name of the predefined certificate"""
    priv_key_path: NotRequired[str]
    r"""Path on server containing the private key to use. PEM format. Can reference $ENV_VARS."""
    passphrase: NotRequired[str]
    r"""Passphrase to use to decrypt private key"""
    cert_path: NotRequired[str]
    r"""Path on server containing certificates to use. PEM format. Can reference $ENV_VARS."""
    ca_path: NotRequired[str]
    r"""Path on server containing CA certificates to use. PEM format. Can reference $ENV_VARS."""
    min_version: NotRequired[MinimumTLSVersionElastic]
    max_version: NotRequired[MaximumTLSVersionElastic]


class TLSSettingsServerSideElastic(BaseModel):
    disabled: Optional[bool] = True

    request_cert: Annotated[Optional[bool], pydantic.Field(alias="requestCert")] = False
    r"""Require clients to present their certificates. Used to perform client authentication using SSL certs."""

    reject_unauthorized: Annotated[
        Optional[bool], pydantic.Field(alias="rejectUnauthorized")
    ] = True
    r"""Reject certificates not authorized by a CA in the CA certificate path or by another trusted CA (such as the system's)"""

    common_name_regex: Annotated[
        Optional[str], pydantic.Field(alias="commonNameRegex")
    ] = "/.*/"
    r"""Regex matching allowable common names in peer certificates' subject attribute"""

    certificate_name: Annotated[
        Optional[str], pydantic.Field(alias="certificateName")
    ] = None
    r"""The name of the predefined certificate"""

    priv_key_path: Annotated[Optional[str], pydantic.Field(alias="privKeyPath")] = None
    r"""Path on server containing the private key to use. PEM format. Can reference $ENV_VARS."""

    passphrase: Optional[str] = None
    r"""Passphrase to use to decrypt private key"""

    cert_path: Annotated[Optional[str], pydantic.Field(alias="certPath")] = None
    r"""Path on server containing certificates to use. PEM format. Can reference $ENV_VARS."""

    ca_path: Annotated[Optional[str], pydantic.Field(alias="caPath")] = None
    r"""Path on server containing CA certificates to use. PEM format. Can reference $ENV_VARS."""

    min_version: Annotated[
        Annotated[
            Optional[MinimumTLSVersionElastic],
            PlainValidator(validate_open_enum(False)),
        ],
        pydantic.Field(alias="minVersion"),
    ] = None

    max_version: Annotated[
        Annotated[
            Optional[MaximumTLSVersionElastic],
            PlainValidator(validate_open_enum(False)),
        ],
        pydantic.Field(alias="maxVersion"),
    ] = None

    @field_serializer("min_version")
    def serialize_min_version(self, value):
        if isinstance(value, str):
            try:
                return models.MinimumTLSVersionElastic(value)
            except ValueError:
                return value
        return value

    @field_serializer("max_version")
    def serialize_max_version(self, value):
        if isinstance(value, str):
            try:
                return models.MaximumTLSVersionElastic(value)
            except ValueError:
                return value
        return value


class AuthenticationTypeElastic(str, Enum, metaclass=utils.OpenEnumMeta):
    # None
    NONE = "none"
    # Basic
    BASIC = "basic"
    # Basic (credentials secret)
    CREDENTIALS_SECRET = "credentialsSecret"
    # Auth Tokens
    AUTH_TOKENS = "authTokens"


class InputAPIVersion(str, Enum, metaclass=utils.OpenEnumMeta):
    r"""The API version to use for communicating with the server"""

    # 6.8.4
    SIX_DOT_8_DOT_4 = "6.8.4"
    # 8.3.2
    EIGHT_DOT_3_DOT_2 = "8.3.2"
    # Custom
    CUSTOM = "custom"


class InputExtraHTTPHeaderTypedDict(TypedDict):
    value: str
    name: NotRequired[str]


class InputExtraHTTPHeader(BaseModel):
    value: str

    name: Optional[str] = None


class MetadatumElasticTypedDict(TypedDict):
    name: str
    value: str
    r"""JavaScript expression to compute field's value, enclosed in quotes or backticks. (Can evaluate to a constant.)"""


class MetadatumElastic(BaseModel):
    name: str

    value: str
    r"""JavaScript expression to compute field's value, enclosed in quotes or backticks. (Can evaluate to a constant.)"""


class ProxyModeAuthenticationMethod(str, Enum, metaclass=utils.OpenEnumMeta):
    r"""Enter credentials directly, or select a stored secret"""

    NONE = "none"
    MANUAL = "manual"
    SECRET = "secret"


class ProxyModeElasticTypedDict(TypedDict):
    enabled: NotRequired[bool]
    r"""Enable proxying of non-bulk API requests to an external Elastic server. Enable this only if you understand the implications. See [Cribl Docs](https://docs.cribl.io/stream/sources-elastic/#proxy-mode) for more details."""
    auth_type: NotRequired[ProxyModeAuthenticationMethod]
    r"""Enter credentials directly, or select a stored secret"""
    username: NotRequired[str]
    password: NotRequired[str]
    credentials_secret: NotRequired[str]
    r"""Select or create a secret that references your credentials"""
    url: NotRequired[str]
    r"""URL of the Elastic server to proxy non-bulk requests to, such as http://elastic:9200"""
    reject_unauthorized: NotRequired[bool]
    r"""Reject certificates that cannot be verified against a valid CA (such as self-signed certificates)"""
    remove_headers: NotRequired[List[str]]
    r"""List of headers to remove from the request to proxy"""
    timeout_sec: NotRequired[float]
    r"""Amount of time, in seconds, to wait for a proxy request to complete before canceling it"""


class ProxyModeElastic(BaseModel):
    enabled: Optional[bool] = False
    r"""Enable proxying of non-bulk API requests to an external Elastic server. Enable this only if you understand the implications. See [Cribl Docs](https://docs.cribl.io/stream/sources-elastic/#proxy-mode) for more details."""

    auth_type: Annotated[
        Annotated[
            Optional[ProxyModeAuthenticationMethod],
            PlainValidator(validate_open_enum(False)),
        ],
        pydantic.Field(alias="authType"),
    ] = ProxyModeAuthenticationMethod.NONE
    r"""Enter credentials directly, or select a stored secret"""

    username: Optional[str] = None

    password: Optional[str] = None

    credentials_secret: Annotated[
        Optional[str], pydantic.Field(alias="credentialsSecret")
    ] = None
    r"""Select or create a secret that references your credentials"""

    url: Optional[str] = None
    r"""URL of the Elastic server to proxy non-bulk requests to, such as http://elastic:9200"""

    reject_unauthorized: Annotated[
        Optional[bool], pydantic.Field(alias="rejectUnauthorized")
    ] = False
    r"""Reject certificates that cannot be verified against a valid CA (such as self-signed certificates)"""

    remove_headers: Annotated[
        Optional[List[str]], pydantic.Field(alias="removeHeaders")
    ] = None
    r"""List of headers to remove from the request to proxy"""

    timeout_sec: Annotated[Optional[float], pydantic.Field(alias="timeoutSec")] = 60
    r"""Amount of time, in seconds, to wait for a proxy request to complete before canceling it"""

    @field_serializer("auth_type")
    def serialize_auth_type(self, value):
        if isinstance(value, str):
            try:
                return models.ProxyModeAuthenticationMethod(value)
            except ValueError:
                return value
        return value


class InputElasticTypedDict(TypedDict):
    type: InputTypeElastic
    port: float
    r"""Port to listen on"""
    id: NotRequired[str]
    r"""Unique ID for this input"""
    disabled: NotRequired[bool]
    pipeline: NotRequired[str]
    r"""Pipeline to process data from this Source before sending it through the Routes"""
    send_to_routes: NotRequired[bool]
    r"""Select whether to send data to Routes, or directly to Destinations."""
    environment: NotRequired[str]
    r"""Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere."""
    pq_enabled: NotRequired[bool]
    r"""Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers)."""
    streamtags: NotRequired[List[str]]
    r"""Tags for filtering and grouping in @{product}"""
    connections: NotRequired[List[ConnectionElasticTypedDict]]
    r"""Direct connections to Destinations, and optionally via a Pipeline or a Pack"""
    pq: NotRequired[PqElasticTypedDict]
    host: NotRequired[str]
    r"""Address to bind on. Defaults to 0.0.0.0 (all addresses)."""
    tls: NotRequired[TLSSettingsServerSideElasticTypedDict]
    max_active_req: NotRequired[float]
    r"""Maximum number of active requests allowed per Worker Process. Set to 0 for unlimited. Caution: Increasing the limit above the default value, or setting it to unlimited, may degrade performance and reduce throughput."""
    max_requests_per_socket: NotRequired[int]
    r"""Maximum number of requests per socket before @{product} instructs the client to close the connection. Default is 0 (unlimited)."""
    enable_proxy_header: NotRequired[bool]
    r"""Extract the client IP and port from PROXY protocol v1/v2. When enabled, the X-Forwarded-For header is ignored. Disable to use the X-Forwarded-For header for client IP extraction."""
    capture_headers: NotRequired[bool]
    r"""Add request headers to events, in the __headers field"""
    activity_log_sample_rate: NotRequired[float]
    r"""How often request activity is logged at the `info` level. A value of 1 would log every request, 10 every 10th request, etc."""
    request_timeout: NotRequired[float]
    r"""How long to wait for an incoming request to complete before aborting it. Use 0 to disable."""
    socket_timeout: NotRequired[float]
    r"""How long @{product} should wait before assuming that an inactive socket has timed out. To wait forever, set to 0."""
    keep_alive_timeout: NotRequired[float]
    r"""After the last response is sent, @{product} will wait this long for additional data before closing the socket connection. Minimum 1 second, maximum 600 seconds (10 minutes)."""
    enable_health_check: NotRequired[bool]
    r"""Expose the /cribl_health endpoint, which returns 200 OK when this Source is healthy"""
    ip_allowlist_regex: NotRequired[str]
    r"""Messages from matched IP addresses will be processed, unless also matched by the denylist"""
    ip_denylist_regex: NotRequired[str]
    r"""Messages from matched IP addresses will be ignored. This takes precedence over the allowlist."""
    elastic_api: NotRequired[str]
    r"""Absolute path on which to listen for Elasticsearch API requests. Defaults to /. _bulk will be appended automatically. For example, /myPath becomes /myPath/_bulk. Requests can then be made to either /myPath/_bulk or /myPath/<myIndexName>/_bulk. Other entries are faked as success."""
    auth_type: NotRequired[AuthenticationTypeElastic]
    api_version: NotRequired[InputAPIVersion]
    r"""The API version to use for communicating with the server"""
    extra_http_headers: NotRequired[List[InputExtraHTTPHeaderTypedDict]]
    r"""Headers to add to all events"""
    metadata: NotRequired[List[MetadatumElasticTypedDict]]
    r"""Fields to add to events from this input"""
    proxy_mode: NotRequired[ProxyModeElasticTypedDict]
    description: NotRequired[str]
    username: NotRequired[str]
    password: NotRequired[str]
    credentials_secret: NotRequired[str]
    r"""Select or create a secret that references your credentials"""
    auth_tokens: NotRequired[List[str]]
    r"""Bearer tokens to include in the authorization header"""
    custom_api_version: NotRequired[str]
    r"""Custom version information to respond to requests"""


class InputElastic(BaseModel):
    model_config = ConfigDict(
        populate_by_name=True, arbitrary_types_allowed=True, extra="allow"
    )
    __pydantic_extra__: Dict[str, Any] = pydantic.Field(init=False)

    type: InputTypeElastic

    port: float
    r"""Port to listen on"""

    id: Optional[str] = None
    r"""Unique ID for this input"""

    disabled: Optional[bool] = False

    pipeline: Optional[str] = None
    r"""Pipeline to process data from this Source before sending it through the Routes"""

    send_to_routes: Annotated[Optional[bool], pydantic.Field(alias="sendToRoutes")] = (
        True
    )
    r"""Select whether to send data to Routes, or directly to Destinations."""

    environment: Optional[str] = None
    r"""Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere."""

    pq_enabled: Annotated[Optional[bool], pydantic.Field(alias="pqEnabled")] = False
    r"""Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers)."""

    streamtags: Optional[List[str]] = None
    r"""Tags for filtering and grouping in @{product}"""

    connections: Optional[List[ConnectionElastic]] = None
    r"""Direct connections to Destinations, and optionally via a Pipeline or a Pack"""

    pq: Optional[PqElastic] = None

    host: Optional[str] = "0.0.0.0"
    r"""Address to bind on. Defaults to 0.0.0.0 (all addresses)."""

    tls: Optional[TLSSettingsServerSideElastic] = None

    max_active_req: Annotated[Optional[float], pydantic.Field(alias="maxActiveReq")] = (
        256
    )
    r"""Maximum number of active requests allowed per Worker Process. Set to 0 for unlimited. Caution: Increasing the limit above the default value, or setting it to unlimited, may degrade performance and reduce throughput."""

    max_requests_per_socket: Annotated[
        Optional[int], pydantic.Field(alias="maxRequestsPerSocket")
    ] = 0
    r"""Maximum number of requests per socket before @{product} instructs the client to close the connection. Default is 0 (unlimited)."""

    enable_proxy_header: Annotated[
        Optional[bool], pydantic.Field(alias="enableProxyHeader")
    ] = False
    r"""Extract the client IP and port from PROXY protocol v1/v2. When enabled, the X-Forwarded-For header is ignored. Disable to use the X-Forwarded-For header for client IP extraction."""

    capture_headers: Annotated[
        Optional[bool], pydantic.Field(alias="captureHeaders")
    ] = False
    r"""Add request headers to events, in the __headers field"""

    activity_log_sample_rate: Annotated[
        Optional[float], pydantic.Field(alias="activityLogSampleRate")
    ] = 100
    r"""How often request activity is logged at the `info` level. A value of 1 would log every request, 10 every 10th request, etc."""

    request_timeout: Annotated[
        Optional[float], pydantic.Field(alias="requestTimeout")
    ] = 0
    r"""How long to wait for an incoming request to complete before aborting it. Use 0 to disable."""

    socket_timeout: Annotated[
        Optional[float], pydantic.Field(alias="socketTimeout")
    ] = 0
    r"""How long @{product} should wait before assuming that an inactive socket has timed out. To wait forever, set to 0."""

    keep_alive_timeout: Annotated[
        Optional[float], pydantic.Field(alias="keepAliveTimeout")
    ] = 5
    r"""After the last response is sent, @{product} will wait this long for additional data before closing the socket connection. Minimum 1 second, maximum 600 seconds (10 minutes)."""

    enable_health_check: Annotated[
        Optional[bool], pydantic.Field(alias="enableHealthCheck")
    ] = False
    r"""Expose the /cribl_health endpoint, which returns 200 OK when this Source is healthy"""

    ip_allowlist_regex: Annotated[
        Optional[str], pydantic.Field(alias="ipAllowlistRegex")
    ] = "/.*/"
    r"""Messages from matched IP addresses will be processed, unless also matched by the denylist"""

    ip_denylist_regex: Annotated[
        Optional[str], pydantic.Field(alias="ipDenylistRegex")
    ] = "/^$/"
    r"""Messages from matched IP addresses will be ignored. This takes precedence over the allowlist."""

    elastic_api: Annotated[Optional[str], pydantic.Field(alias="elasticAPI")] = "/"
    r"""Absolute path on which to listen for Elasticsearch API requests. Defaults to /. _bulk will be appended automatically. For example, /myPath becomes /myPath/_bulk. Requests can then be made to either /myPath/_bulk or /myPath/<myIndexName>/_bulk. Other entries are faked as success."""

    auth_type: Annotated[
        Annotated[
            Optional[AuthenticationTypeElastic],
            PlainValidator(validate_open_enum(False)),
        ],
        pydantic.Field(alias="authType"),
    ] = AuthenticationTypeElastic.NONE

    api_version: Annotated[
        Annotated[Optional[InputAPIVersion], PlainValidator(validate_open_enum(False))],
        pydantic.Field(alias="apiVersion"),
    ] = InputAPIVersion.EIGHT_DOT_3_DOT_2
    r"""The API version to use for communicating with the server"""

    extra_http_headers: Annotated[
        Optional[List[InputExtraHTTPHeader]], pydantic.Field(alias="extraHttpHeaders")
    ] = None
    r"""Headers to add to all events"""

    metadata: Optional[List[MetadatumElastic]] = None
    r"""Fields to add to events from this input"""

    proxy_mode: Annotated[
        Optional[ProxyModeElastic], pydantic.Field(alias="proxyMode")
    ] = None

    description: Optional[str] = None

    username: Optional[str] = None

    password: Optional[str] = None

    credentials_secret: Annotated[
        Optional[str], pydantic.Field(alias="credentialsSecret")
    ] = None
    r"""Select or create a secret that references your credentials"""

    auth_tokens: Annotated[Optional[List[str]], pydantic.Field(alias="authTokens")] = (
        None
    )
    r"""Bearer tokens to include in the authorization header"""

    custom_api_version: Annotated[
        Optional[str], pydantic.Field(alias="customAPIVersion")
    ] = (
        "{\n"
        '    "name": "AzU84iL",\n'
        '    "cluster_name": "cribl",\n'
        '    "cluster_uuid": "Js6_Z2VKS3KbfRSxPmPbaw",\n'
        '    "version": {\n'
        '        "number": "8.3.2",\n'
        '        "build_type": "tar",\n'
        '        "build_hash": "bca0c8d",\n'
        '        "build_date": "2019-10-16T06:19:49.319352Z",\n'
        '        "build_snapshot": false,\n'
        '        "lucene_version": "9.7.2",\n'
        '        "minimum_wire_compatibility_version": "7.17.0",\n'
        '        "minimum_index_compatibility_version": "7.0.0"\n'
        "    },\n"
        '    "tagline": "You Know, for Search"\n'
        "}"
    )
    r"""Custom version information to respond to requests"""

    @property
    def additional_properties(self):
        return self.__pydantic_extra__

    @additional_properties.setter
    def additional_properties(self, value):
        self.__pydantic_extra__ = value  # pyright: ignore[reportIncompatibleVariableOverride]

    @field_serializer("auth_type")
    def serialize_auth_type(self, value):
        if isinstance(value, str):
            try:
                return models.AuthenticationTypeElastic(value)
            except ValueError:
                return value
        return value

    @field_serializer("api_version")
    def serialize_api_version(self, value):
        if isinstance(value, str):
            try:
                return models.InputAPIVersion(value)
            except ValueError:
                return value
        return value


class InputTypeAzureBlob(str, Enum):
    AZURE_BLOB = "azure_blob"


class ConnectionAzureBlobTypedDict(TypedDict):
    output: str
    pipeline: NotRequired[str]


class ConnectionAzureBlob(BaseModel):
    output: str

    pipeline: Optional[str] = None


class ModeAzureBlob(str, Enum, metaclass=utils.OpenEnumMeta):
    r"""With Smart mode, PQ will write events to the filesystem only when it detects backpressure from the processing engine. With Always On mode, PQ will always write events directly to the queue before forwarding them to the processing engine."""

    # Smart
    SMART = "smart"
    # Always On
    ALWAYS = "always"


class PqCompressionAzureBlob(str, Enum, metaclass=utils.OpenEnumMeta):
    r"""Codec to use to compress the persisted data"""

    # None
    NONE = "none"
    # Gzip
    GZIP = "gzip"


class PqControlsAzureBlobTypedDict(TypedDict):
    pass


class PqControlsAzureBlob(BaseModel):
    pass


class PqAzureBlobTypedDict(TypedDict):
    mode: NotRequired[ModeAzureBlob]
    r"""With Smart mode, PQ will write events to the filesystem only when it detects backpressure from the processing engine. With Always On mode, PQ will always write events directly to the queue before forwarding them to the processing engine."""
    max_buffer_size: NotRequired[float]
    r"""The maximum number of events to hold in memory before writing the events to disk"""
    commit_frequency: NotRequired[float]
    r"""The number of events to send downstream before committing that Stream has read them"""
    max_file_size: NotRequired[str]
    r"""The maximum size to store in each queue file before closing and optionally compressing. Enter a numeral with units of KB, MB, etc."""
    max_size: NotRequired[str]
    r"""The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc."""
    path: NotRequired[str]
    r"""The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/inputs/<input-id>"""
    compress: NotRequired[PqCompressionAzureBlob]
    r"""Codec to use to compress the persisted data"""
    pq_controls: NotRequired[PqControlsAzureBlobTypedDict]


class PqAzureBlob(BaseModel):
    mode: Annotated[
        Optional[ModeAzureBlob], PlainValidator(validate_open_enum(False))
    ] = ModeAzureBlob.ALWAYS
    r"""With Smart mode, PQ will write events to the filesystem only when it detects backpressure from the processing engine. With Always On mode, PQ will always write events directly to the queue before forwarding them to the processing engine."""

    max_buffer_size: Annotated[
        Optional[float], pydantic.Field(alias="maxBufferSize")
    ] = 1000
    r"""The maximum number of events to hold in memory before writing the events to disk"""

    commit_frequency: Annotated[
        Optional[float], pydantic.Field(alias="commitFrequency")
    ] = 42
    r"""The number of events to send downstream before committing that Stream has read them"""

    max_file_size: Annotated[Optional[str], pydantic.Field(alias="maxFileSize")] = (
        "1 MB"
    )
    r"""The maximum size to store in each queue file before closing and optionally compressing. Enter a numeral with units of KB, MB, etc."""

    max_size: Annotated[Optional[str], pydantic.Field(alias="maxSize")] = "5GB"
    r"""The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc."""

    path: Optional[str] = "$CRIBL_HOME/state/queues"
    r"""The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/inputs/<input-id>"""

    compress: Annotated[
        Optional[PqCompressionAzureBlob], PlainValidator(validate_open_enum(False))
    ] = PqCompressionAzureBlob.NONE
    r"""Codec to use to compress the persisted data"""

    pq_controls: Annotated[
        Optional[PqControlsAzureBlob], pydantic.Field(alias="pqControls")
    ] = None

    @field_serializer("mode")
    def serialize_mode(self, value):
        if isinstance(value, str):
            try:
                return models.ModeAzureBlob(value)
            except ValueError:
                return value
        return value

    @field_serializer("compress")
    def serialize_compress(self, value):
        if isinstance(value, str):
            try:
                return models.PqCompressionAzureBlob(value)
            except ValueError:
                return value
        return value


class MetadatumAzureBlobTypedDict(TypedDict):
    name: str
    value: str
    r"""JavaScript expression to compute field's value, enclosed in quotes or backticks. (Can evaluate to a constant.)"""


class MetadatumAzureBlob(BaseModel):
    name: str

    value: str
    r"""JavaScript expression to compute field's value, enclosed in quotes or backticks. (Can evaluate to a constant.)"""


class InputAuthenticationMethodAzureBlob(str, Enum, metaclass=utils.OpenEnumMeta):
    MANUAL = "manual"
    SECRET = "secret"
    CLIENT_SECRET = "clientSecret"
    CLIENT_CERT = "clientCert"


class InputCertificateTypedDict(TypedDict):
    certificate_name: str
    r"""The certificate you registered as credentials for your app in the Azure portal"""


class InputCertificate(BaseModel):
    certificate_name: Annotated[str, pydantic.Field(alias="certificateName")]
    r"""The certificate you registered as credentials for your app in the Azure portal"""


class InputAzureBlobTypedDict(TypedDict):
    type: InputTypeAzureBlob
    queue_name: str
    r"""The storage account queue name blob notifications will be read from. Value must be a JavaScript expression (which can evaluate to a constant value), enclosed in quotes or backticks. Can be evaluated only at initialization time. Example referencing a Global Variable: `myQueue-${C.vars.myVar}`"""
    id: NotRequired[str]
    r"""Unique ID for this input"""
    disabled: NotRequired[bool]
    pipeline: NotRequired[str]
    r"""Pipeline to process data from this Source before sending it through the Routes"""
    send_to_routes: NotRequired[bool]
    r"""Select whether to send data to Routes, or directly to Destinations."""
    environment: NotRequired[str]
    r"""Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere."""
    pq_enabled: NotRequired[bool]
    r"""Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers)."""
    streamtags: NotRequired[List[str]]
    r"""Tags for filtering and grouping in @{product}"""
    connections: NotRequired[List[ConnectionAzureBlobTypedDict]]
    r"""Direct connections to Destinations, and optionally via a Pipeline or a Pack"""
    pq: NotRequired[PqAzureBlobTypedDict]
    file_filter: NotRequired[str]
    r"""Regex matching file names to download and process. Defaults to: .*"""
    visibility_timeout: NotRequired[float]
    r"""The duration (in seconds) that the received messages are hidden from subsequent retrieve requests after being retrieved by a ReceiveMessage request."""
    num_receivers: NotRequired[float]
    r"""How many receiver processes to run. The higher the number, the better the throughput - at the expense of CPU overhead."""
    max_messages: NotRequired[float]
    r"""The maximum number of messages to return in a poll request. Azure storage queues never returns more messages than this value (however, fewer messages might be returned). Valid values: 1 to 32."""
    service_period_secs: NotRequired[float]
    r"""The duration (in seconds) which pollers should be validated and restarted if exited"""
    skip_on_error: NotRequired[bool]
    r"""Skip files that trigger a processing error. Disabled by default, which allows retries after processing errors."""
    metadata: NotRequired[List[MetadatumAzureBlobTypedDict]]
    r"""Fields to add to events from this input"""
    breaker_rulesets: NotRequired[List[str]]
    r"""A list of event-breaking rulesets that will be applied, in order, to the input data stream"""
    stale_channel_flush_ms: NotRequired[float]
    r"""How long (in milliseconds) the Event Breaker will wait for new data to be sent to a specific channel before flushing the data stream out, as is, to the Pipelines"""
    parquet_chunk_size_mb: NotRequired[float]
    r"""Maximum file size for each Parquet chunk"""
    parquet_chunk_download_timeout: NotRequired[float]
    r"""The maximum time allowed for downloading a Parquet chunk. Processing will stop if a chunk cannot be downloaded within the time specified."""
    auth_type: NotRequired[InputAuthenticationMethodAzureBlob]
    description: NotRequired[str]
    connection_string: NotRequired[str]
    r"""Enter your Azure Storage account connection string. If left blank, Stream will fall back to env.AZURE_STORAGE_CONNECTION_STRING."""
    text_secret: NotRequired[str]
    r"""Select or create a stored text secret"""
    storage_account_name: NotRequired[str]
    r"""The name of your Azure storage account"""
    tenant_id: NotRequired[str]
    r"""The service principal's tenant ID"""
    client_id: NotRequired[str]
    r"""The service principal's client ID"""
    azure_cloud: NotRequired[str]
    r"""The Azure cloud to use. Defaults to Azure Public Cloud."""
    endpoint_suffix: NotRequired[str]
    r"""Endpoint suffix for the service URL. Takes precedence over the Azure Cloud setting. Defaults to core.windows.net."""
    client_text_secret: NotRequired[str]
    r"""Select or create a stored text secret"""
    certificate: NotRequired[InputCertificateTypedDict]


class InputAzureBlob(BaseModel):
    model_config = ConfigDict(
        populate_by_name=True, arbitrary_types_allowed=True, extra="allow"
    )
    __pydantic_extra__: Dict[str, Any] = pydantic.Field(init=False)

    type: InputTypeAzureBlob

    queue_name: Annotated[str, pydantic.Field(alias="queueName")]
    r"""The storage account queue name blob notifications will be read from. Value must be a JavaScript expression (which can evaluate to a constant value), enclosed in quotes or backticks. Can be evaluated only at initialization time. Example referencing a Global Variable: `myQueue-${C.vars.myVar}`"""

    id: Optional[str] = None
    r"""Unique ID for this input"""

    disabled: Optional[bool] = False

    pipeline: Optional[str] = None
    r"""Pipeline to process data from this Source before sending it through the Routes"""

    send_to_routes: Annotated[Optional[bool], pydantic.Field(alias="sendToRoutes")] = (
        True
    )
    r"""Select whether to send data to Routes, or directly to Destinations."""

    environment: Optional[str] = None
    r"""Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere."""

    pq_enabled: Annotated[Optional[bool], pydantic.Field(alias="pqEnabled")] = False
    r"""Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers)."""

    streamtags: Optional[List[str]] = None
    r"""Tags for filtering and grouping in @{product}"""

    connections: Optional[List[ConnectionAzureBlob]] = None
    r"""Direct connections to Destinations, and optionally via a Pipeline or a Pack"""

    pq: Optional[PqAzureBlob] = None

    file_filter: Annotated[Optional[str], pydantic.Field(alias="fileFilter")] = "/.*/"
    r"""Regex matching file names to download and process. Defaults to: .*"""

    visibility_timeout: Annotated[
        Optional[float], pydantic.Field(alias="visibilityTimeout")
    ] = 600
    r"""The duration (in seconds) that the received messages are hidden from subsequent retrieve requests after being retrieved by a ReceiveMessage request."""

    num_receivers: Annotated[Optional[float], pydantic.Field(alias="numReceivers")] = 1
    r"""How many receiver processes to run. The higher the number, the better the throughput - at the expense of CPU overhead."""

    max_messages: Annotated[Optional[float], pydantic.Field(alias="maxMessages")] = 1
    r"""The maximum number of messages to return in a poll request. Azure storage queues never returns more messages than this value (however, fewer messages might be returned). Valid values: 1 to 32."""

    service_period_secs: Annotated[
        Optional[float], pydantic.Field(alias="servicePeriodSecs")
    ] = 5
    r"""The duration (in seconds) which pollers should be validated and restarted if exited"""

    skip_on_error: Annotated[Optional[bool], pydantic.Field(alias="skipOnError")] = (
        False
    )
    r"""Skip files that trigger a processing error. Disabled by default, which allows retries after processing errors."""

    metadata: Optional[List[MetadatumAzureBlob]] = None
    r"""Fields to add to events from this input"""

    breaker_rulesets: Annotated[
        Optional[List[str]], pydantic.Field(alias="breakerRulesets")
    ] = None
    r"""A list of event-breaking rulesets that will be applied, in order, to the input data stream"""

    stale_channel_flush_ms: Annotated[
        Optional[float], pydantic.Field(alias="staleChannelFlushMs")
    ] = 10000
    r"""How long (in milliseconds) the Event Breaker will wait for new data to be sent to a specific channel before flushing the data stream out, as is, to the Pipelines"""

    parquet_chunk_size_mb: Annotated[
        Optional[float], pydantic.Field(alias="parquetChunkSizeMB")
    ] = 5
    r"""Maximum file size for each Parquet chunk"""

    parquet_chunk_download_timeout: Annotated[
        Optional[float], pydantic.Field(alias="parquetChunkDownloadTimeout")
    ] = 600
    r"""The maximum time allowed for downloading a Parquet chunk. Processing will stop if a chunk cannot be downloaded within the time specified."""

    auth_type: Annotated[
        Annotated[
            Optional[InputAuthenticationMethodAzureBlob],
            PlainValidator(validate_open_enum(False)),
        ],
        pydantic.Field(alias="authType"),
    ] = InputAuthenticationMethodAzureBlob.MANUAL

    description: Optional[str] = None

    connection_string: Annotated[
        Optional[str], pydantic.Field(alias="connectionString")
    ] = None
    r"""Enter your Azure Storage account connection string. If left blank, Stream will fall back to env.AZURE_STORAGE_CONNECTION_STRING."""

    text_secret: Annotated[Optional[str], pydantic.Field(alias="textSecret")] = None
    r"""Select or create a stored text secret"""

    storage_account_name: Annotated[
        Optional[str], pydantic.Field(alias="storageAccountName")
    ] = None
    r"""The name of your Azure storage account"""

    tenant_id: Annotated[Optional[str], pydantic.Field(alias="tenantId")] = None
    r"""The service principal's tenant ID"""

    client_id: Annotated[Optional[str], pydantic.Field(alias="clientId")] = None
    r"""The service principal's client ID"""

    azure_cloud: Annotated[Optional[str], pydantic.Field(alias="azureCloud")] = None
    r"""The Azure cloud to use. Defaults to Azure Public Cloud."""

    endpoint_suffix: Annotated[
        Optional[str], pydantic.Field(alias="endpointSuffix")
    ] = None
    r"""Endpoint suffix for the service URL. Takes precedence over the Azure Cloud setting. Defaults to core.windows.net."""

    client_text_secret: Annotated[
        Optional[str], pydantic.Field(alias="clientTextSecret")
    ] = None
    r"""Select or create a stored text secret"""

    certificate: Optional[InputCertificate] = None

    @property
    def additional_properties(self):
        return self.__pydantic_extra__

    @additional_properties.setter
    def additional_properties(self, value):
        self.__pydantic_extra__ = value  # pyright: ignore[reportIncompatibleVariableOverride]

    @field_serializer("auth_type")
    def serialize_auth_type(self, value):
        if isinstance(value, str):
            try:
                return models.InputAuthenticationMethodAzureBlob(value)
            except ValueError:
                return value
        return value


class InputTypeSplunkHec(str, Enum):
    SPLUNK_HEC = "splunk_hec"


class ConnectionSplunkHecTypedDict(TypedDict):
    output: str
    pipeline: NotRequired[str]


class ConnectionSplunkHec(BaseModel):
    output: str

    pipeline: Optional[str] = None


class PqModeSplunkHec(str, Enum, metaclass=utils.OpenEnumMeta):
    r"""With Smart mode, PQ will write events to the filesystem only when it detects backpressure from the processing engine. With Always On mode, PQ will always write events directly to the queue before forwarding them to the processing engine."""

    # Smart
    SMART = "smart"
    # Always On
    ALWAYS = "always"


class PqCompressionSplunkHec(str, Enum, metaclass=utils.OpenEnumMeta):
    r"""Codec to use to compress the persisted data"""

    # None
    NONE = "none"
    # Gzip
    GZIP = "gzip"


class InputPqControlsSplunkHecTypedDict(TypedDict):
    pass


class InputPqControlsSplunkHec(BaseModel):
    pass


class PqSplunkHecTypedDict(TypedDict):
    mode: NotRequired[PqModeSplunkHec]
    r"""With Smart mode, PQ will write events to the filesystem only when it detects backpressure from the processing engine. With Always On mode, PQ will always write events directly to the queue before forwarding them to the processing engine."""
    max_buffer_size: NotRequired[float]
    r"""The maximum number of events to hold in memory before writing the events to disk"""
    commit_frequency: NotRequired[float]
    r"""The number of events to send downstream before committing that Stream has read them"""
    max_file_size: NotRequired[str]
    r"""The maximum size to store in each queue file before closing and optionally compressing. Enter a numeral with units of KB, MB, etc."""
    max_size: NotRequired[str]
    r"""The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc."""
    path: NotRequired[str]
    r"""The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/inputs/<input-id>"""
    compress: NotRequired[PqCompressionSplunkHec]
    r"""Codec to use to compress the persisted data"""
    pq_controls: NotRequired[InputPqControlsSplunkHecTypedDict]


class PqSplunkHec(BaseModel):
    mode: Annotated[
        Optional[PqModeSplunkHec], PlainValidator(validate_open_enum(False))
    ] = PqModeSplunkHec.ALWAYS
    r"""With Smart mode, PQ will write events to the filesystem only when it detects backpressure from the processing engine. With Always On mode, PQ will always write events directly to the queue before forwarding them to the processing engine."""

    max_buffer_size: Annotated[
        Optional[float], pydantic.Field(alias="maxBufferSize")
    ] = 1000
    r"""The maximum number of events to hold in memory before writing the events to disk"""

    commit_frequency: Annotated[
        Optional[float], pydantic.Field(alias="commitFrequency")
    ] = 42
    r"""The number of events to send downstream before committing that Stream has read them"""

    max_file_size: Annotated[Optional[str], pydantic.Field(alias="maxFileSize")] = (
        "1 MB"
    )
    r"""The maximum size to store in each queue file before closing and optionally compressing. Enter a numeral with units of KB, MB, etc."""

    max_size: Annotated[Optional[str], pydantic.Field(alias="maxSize")] = "5GB"
    r"""The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc."""

    path: Optional[str] = "$CRIBL_HOME/state/queues"
    r"""The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/inputs/<input-id>"""

    compress: Annotated[
        Optional[PqCompressionSplunkHec], PlainValidator(validate_open_enum(False))
    ] = PqCompressionSplunkHec.NONE
    r"""Codec to use to compress the persisted data"""

    pq_controls: Annotated[
        Optional[InputPqControlsSplunkHec], pydantic.Field(alias="pqControls")
    ] = None

    @field_serializer("mode")
    def serialize_mode(self, value):
        if isinstance(value, str):
            try:
                return models.PqModeSplunkHec(value)
            except ValueError:
                return value
        return value

    @field_serializer("compress")
    def serialize_compress(self, value):
        if isinstance(value, str):
            try:
                return models.PqCompressionSplunkHec(value)
            except ValueError:
                return value
        return value


class AuthTokenAuthenticationMethodSplunkHec(str, Enum, metaclass=utils.OpenEnumMeta):
    r"""Select Manual to enter an auth token directly, or select Secret to use a text secret to authenticate"""

    MANUAL = "manual"
    SECRET = "secret"


class AuthTokenMetadatumSplunkHecTypedDict(TypedDict):
    name: str
    value: str
    r"""JavaScript expression to compute field's value, enclosed in quotes or backticks. (Can evaluate to a constant.)"""


class AuthTokenMetadatumSplunkHec(BaseModel):
    name: str

    value: str
    r"""JavaScript expression to compute field's value, enclosed in quotes or backticks. (Can evaluate to a constant.)"""


class AuthTokenSplunkHecTypedDict(TypedDict):
    token: Any
    auth_type: NotRequired[AuthTokenAuthenticationMethodSplunkHec]
    r"""Select Manual to enter an auth token directly, or select Secret to use a text secret to authenticate"""
    token_secret: NotRequired[Any]
    enabled: NotRequired[bool]
    description: NotRequired[str]
    r"""Optional token description"""
    allowed_indexes_at_token: NotRequired[List[str]]
    r"""Enter the values you want to allow in the HEC event index field at the token level. Supports wildcards. To skip validation, leave blank."""
    metadata: NotRequired[List[AuthTokenMetadatumSplunkHecTypedDict]]
    r"""Fields to add to events referencing this token"""


class AuthTokenSplunkHec(BaseModel):
    token: Any

    auth_type: Annotated[
        Annotated[
            Optional[AuthTokenAuthenticationMethodSplunkHec],
            PlainValidator(validate_open_enum(False)),
        ],
        pydantic.Field(alias="authType"),
    ] = AuthTokenAuthenticationMethodSplunkHec.MANUAL
    r"""Select Manual to enter an auth token directly, or select Secret to use a text secret to authenticate"""

    token_secret: Annotated[Optional[Any], pydantic.Field(alias="tokenSecret")] = None

    enabled: Optional[bool] = True

    description: Optional[str] = None
    r"""Optional token description"""

    allowed_indexes_at_token: Annotated[
        Optional[List[str]], pydantic.Field(alias="allowedIndexesAtToken")
    ] = None
    r"""Enter the values you want to allow in the HEC event index field at the token level. Supports wildcards. To skip validation, leave blank."""

    metadata: Optional[List[AuthTokenMetadatumSplunkHec]] = None
    r"""Fields to add to events referencing this token"""

    @field_serializer("auth_type")
    def serialize_auth_type(self, value):
        if isinstance(value, str):
            try:
                return models.AuthTokenAuthenticationMethodSplunkHec(value)
            except ValueError:
                return value
        return value


class InputMinimumTLSVersionSplunkHec(str, Enum, metaclass=utils.OpenEnumMeta):
    TL_SV1 = "TLSv1"
    TL_SV1_1 = "TLSv1.1"
    TL_SV1_2 = "TLSv1.2"
    TL_SV1_3 = "TLSv1.3"


class InputMaximumTLSVersionSplunkHec(str, Enum, metaclass=utils.OpenEnumMeta):
    TL_SV1 = "TLSv1"
    TL_SV1_1 = "TLSv1.1"
    TL_SV1_2 = "TLSv1.2"
    TL_SV1_3 = "TLSv1.3"


class TLSSettingsServerSideSplunkHecTypedDict(TypedDict):
    disabled: NotRequired[bool]
    request_cert: NotRequired[bool]
    r"""Require clients to present their certificates. Used to perform client authentication using SSL certs."""
    reject_unauthorized: NotRequired[bool]
    r"""Reject certificates not authorized by a CA in the CA certificate path or by another trusted CA (such as the system's)"""
    common_name_regex: NotRequired[str]
    r"""Regex matching allowable common names in peer certificates' subject attribute"""
    certificate_name: NotRequired[str]
    r"""The name of the predefined certificate"""
    priv_key_path: NotRequired[str]
    r"""Path on server containing the private key to use. PEM format. Can reference $ENV_VARS."""
    passphrase: NotRequired[str]
    r"""Passphrase to use to decrypt private key"""
    cert_path: NotRequired[str]
    r"""Path on server containing certificates to use. PEM format. Can reference $ENV_VARS."""
    ca_path: NotRequired[str]
    r"""Path on server containing CA certificates to use. PEM format. Can reference $ENV_VARS."""
    min_version: NotRequired[InputMinimumTLSVersionSplunkHec]
    max_version: NotRequired[InputMaximumTLSVersionSplunkHec]


class TLSSettingsServerSideSplunkHec(BaseModel):
    disabled: Optional[bool] = True

    request_cert: Annotated[Optional[bool], pydantic.Field(alias="requestCert")] = False
    r"""Require clients to present their certificates. Used to perform client authentication using SSL certs."""

    reject_unauthorized: Annotated[
        Optional[bool], pydantic.Field(alias="rejectUnauthorized")
    ] = True
    r"""Reject certificates not authorized by a CA in the CA certificate path or by another trusted CA (such as the system's)"""

    common_name_regex: Annotated[
        Optional[str], pydantic.Field(alias="commonNameRegex")
    ] = "/.*/"
    r"""Regex matching allowable common names in peer certificates' subject attribute"""

    certificate_name: Annotated[
        Optional[str], pydantic.Field(alias="certificateName")
    ] = None
    r"""The name of the predefined certificate"""

    priv_key_path: Annotated[Optional[str], pydantic.Field(alias="privKeyPath")] = None
    r"""Path on server containing the private key to use. PEM format. Can reference $ENV_VARS."""

    passphrase: Optional[str] = None
    r"""Passphrase to use to decrypt private key"""

    cert_path: Annotated[Optional[str], pydantic.Field(alias="certPath")] = None
    r"""Path on server containing certificates to use. PEM format. Can reference $ENV_VARS."""

    ca_path: Annotated[Optional[str], pydantic.Field(alias="caPath")] = None
    r"""Path on server containing CA certificates to use. PEM format. Can reference $ENV_VARS."""

    min_version: Annotated[
        Annotated[
            Optional[InputMinimumTLSVersionSplunkHec],
            PlainValidator(validate_open_enum(False)),
        ],
        pydantic.Field(alias="minVersion"),
    ] = None

    max_version: Annotated[
        Annotated[
            Optional[InputMaximumTLSVersionSplunkHec],
            PlainValidator(validate_open_enum(False)),
        ],
        pydantic.Field(alias="maxVersion"),
    ] = None

    @field_serializer("min_version")
    def serialize_min_version(self, value):
        if isinstance(value, str):
            try:
                return models.InputMinimumTLSVersionSplunkHec(value)
            except ValueError:
                return value
        return value

    @field_serializer("max_version")
    def serialize_max_version(self, value):
        if isinstance(value, str):
            try:
                return models.InputMaximumTLSVersionSplunkHec(value)
            except ValueError:
                return value
        return value


class MetadatumSplunkHecTypedDict(TypedDict):
    name: str
    value: str
    r"""JavaScript expression to compute field's value, enclosed in quotes or backticks. (Can evaluate to a constant.)"""


class MetadatumSplunkHec(BaseModel):
    name: str

    value: str
    r"""JavaScript expression to compute field's value, enclosed in quotes or backticks. (Can evaluate to a constant.)"""


class InputInputSplunkHecTypedDict(TypedDict):
    type: InputTypeSplunkHec
    port: float
    r"""Port to listen on"""
    id: NotRequired[str]
    r"""Unique ID for this input"""
    disabled: NotRequired[bool]
    pipeline: NotRequired[str]
    r"""Pipeline to process data from this Source before sending it through the Routes"""
    send_to_routes: NotRequired[bool]
    r"""Select whether to send data to Routes, or directly to Destinations."""
    environment: NotRequired[str]
    r"""Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere."""
    pq_enabled: NotRequired[bool]
    r"""Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers)."""
    streamtags: NotRequired[List[str]]
    r"""Tags for filtering and grouping in @{product}"""
    connections: NotRequired[List[ConnectionSplunkHecTypedDict]]
    r"""Direct connections to Destinations, and optionally via a Pipeline or a Pack"""
    pq: NotRequired[PqSplunkHecTypedDict]
    host: NotRequired[str]
    r"""Address to bind on. Defaults to 0.0.0.0 (all addresses)."""
    auth_tokens: NotRequired[List[AuthTokenSplunkHecTypedDict]]
    r"""Shared secrets to be provided by any client (Authorization: <token>). If empty, unauthorized access is permitted."""
    tls: NotRequired[TLSSettingsServerSideSplunkHecTypedDict]
    max_active_req: NotRequired[float]
    r"""Maximum number of active requests allowed per Worker Process. Set to 0 for unlimited. Caution: Increasing the limit above the default value, or setting it to unlimited, may degrade performance and reduce throughput."""
    max_requests_per_socket: NotRequired[int]
    r"""Maximum number of requests per socket before @{product} instructs the client to close the connection. Default is 0 (unlimited)."""
    enable_proxy_header: NotRequired[bool]
    r"""Extract the client IP and port from PROXY protocol v1/v2. When enabled, the X-Forwarded-For header is ignored. Disable to use the X-Forwarded-For header for client IP extraction."""
    capture_headers: NotRequired[bool]
    r"""Add request headers to events, in the __headers field"""
    activity_log_sample_rate: NotRequired[float]
    r"""How often request activity is logged at the `info` level. A value of 1 would log every request, 10 every 10th request, etc."""
    request_timeout: NotRequired[float]
    r"""How long to wait for an incoming request to complete before aborting it. Use 0 to disable."""
    socket_timeout: NotRequired[float]
    r"""How long @{product} should wait before assuming that an inactive socket has timed out. To wait forever, set to 0."""
    keep_alive_timeout: NotRequired[float]
    r"""After the last response is sent, @{product} will wait this long for additional data before closing the socket connection. Minimum 1 second, maximum 600 seconds (10 minutes)."""
    enable_health_check: NotRequired[Any]
    ip_allowlist_regex: NotRequired[str]
    r"""Messages from matched IP addresses will be processed, unless also matched by the denylist"""
    ip_denylist_regex: NotRequired[str]
    r"""Messages from matched IP addresses will be ignored. This takes precedence over the allowlist."""
    splunk_hec_api: NotRequired[str]
    r"""Absolute path on which to listen for the Splunk HTTP Event Collector API requests. This input supports the /event, /raw and /s2s endpoints."""
    metadata: NotRequired[List[MetadatumSplunkHecTypedDict]]
    r"""Fields to add to every event. Overrides fields added at the token or request level. See [the Source documentation](https://docs.cribl.io/stream/sources-splunk-hec/#fields) for more info."""
    allowed_indexes: NotRequired[List[str]]
    r"""List values allowed in HEC event index field. Leave blank to skip validation. Supports wildcards. The values here can expand index validation at the token level."""
    splunk_hec_acks: NotRequired[bool]
    r"""Enable Splunk HEC acknowledgements"""
    breaker_rulesets: NotRequired[List[str]]
    r"""A list of event-breaking rulesets that will be applied, in order, to the input data stream"""
    stale_channel_flush_ms: NotRequired[float]
    r"""How long (in milliseconds) the Event Breaker will wait for new data to be sent to a specific channel before flushing the data stream out, as is, to the Pipelines"""
    use_fwd_timezone: NotRequired[bool]
    r"""Event Breakers will determine events' time zone from UF-provided metadata, when TZ can't be inferred from the raw event"""
    drop_control_fields: NotRequired[bool]
    r"""Drop Splunk control fields such as `crcSalt` and `_savedPort`. If disabled, control fields are stored in the internal field `__ctrlFields`."""
    extract_metrics: NotRequired[bool]
    r"""Extract and process Splunk-generated metrics as Cribl metrics"""
    access_control_allow_origin: NotRequired[List[str]]
    r"""Optionally, list HTTP origins to which @{product} should send CORS (cross-origin resource sharing) Access-Control-Allow-* headers. Supports wildcards."""
    access_control_allow_headers: NotRequired[List[str]]
    r"""Optionally, list HTTP headers that @{product} will send to allowed origins as \"Access-Control-Allow-Headers\" in a CORS preflight response. Use \"*\" to allow all headers."""
    emit_token_metrics: NotRequired[bool]
    r"""Emit per-token (<prefix>.http.perToken) and summary (<prefix>.http.summary) request metrics"""
    description: NotRequired[str]


class InputInputSplunkHec(BaseModel):
    model_config = ConfigDict(
        populate_by_name=True, arbitrary_types_allowed=True, extra="allow"
    )
    __pydantic_extra__: Dict[str, Any] = pydantic.Field(init=False)

    type: InputTypeSplunkHec

    port: float
    r"""Port to listen on"""

    id: Optional[str] = None
    r"""Unique ID for this input"""

    disabled: Optional[bool] = False

    pipeline: Optional[str] = None
    r"""Pipeline to process data from this Source before sending it through the Routes"""

    send_to_routes: Annotated[Optional[bool], pydantic.Field(alias="sendToRoutes")] = (
        True
    )
    r"""Select whether to send data to Routes, or directly to Destinations."""

    environment: Optional[str] = None
    r"""Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere."""

    pq_enabled: Annotated[Optional[bool], pydantic.Field(alias="pqEnabled")] = False
    r"""Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers)."""

    streamtags: Optional[List[str]] = None
    r"""Tags for filtering and grouping in @{product}"""

    connections: Optional[List[ConnectionSplunkHec]] = None
    r"""Direct connections to Destinations, and optionally via a Pipeline or a Pack"""

    pq: Optional[PqSplunkHec] = None

    host: Optional[str] = "0.0.0.0"
    r"""Address to bind on. Defaults to 0.0.0.0 (all addresses)."""

    auth_tokens: Annotated[
        Optional[List[AuthTokenSplunkHec]], pydantic.Field(alias="authTokens")
    ] = None
    r"""Shared secrets to be provided by any client (Authorization: <token>). If empty, unauthorized access is permitted."""

    tls: Optional[TLSSettingsServerSideSplunkHec] = None

    max_active_req: Annotated[Optional[float], pydantic.Field(alias="maxActiveReq")] = (
        256
    )
    r"""Maximum number of active requests allowed per Worker Process. Set to 0 for unlimited. Caution: Increasing the limit above the default value, or setting it to unlimited, may degrade performance and reduce throughput."""

    max_requests_per_socket: Annotated[
        Optional[int], pydantic.Field(alias="maxRequestsPerSocket")
    ] = 0
    r"""Maximum number of requests per socket before @{product} instructs the client to close the connection. Default is 0 (unlimited)."""

    enable_proxy_header: Annotated[
        Optional[bool], pydantic.Field(alias="enableProxyHeader")
    ] = False
    r"""Extract the client IP and port from PROXY protocol v1/v2. When enabled, the X-Forwarded-For header is ignored. Disable to use the X-Forwarded-For header for client IP extraction."""

    capture_headers: Annotated[
        Optional[bool], pydantic.Field(alias="captureHeaders")
    ] = False
    r"""Add request headers to events, in the __headers field"""

    activity_log_sample_rate: Annotated[
        Optional[float], pydantic.Field(alias="activityLogSampleRate")
    ] = 100
    r"""How often request activity is logged at the `info` level. A value of 1 would log every request, 10 every 10th request, etc."""

    request_timeout: Annotated[
        Optional[float], pydantic.Field(alias="requestTimeout")
    ] = 0
    r"""How long to wait for an incoming request to complete before aborting it. Use 0 to disable."""

    socket_timeout: Annotated[
        Optional[float], pydantic.Field(alias="socketTimeout")
    ] = 0
    r"""How long @{product} should wait before assuming that an inactive socket has timed out. To wait forever, set to 0."""

    keep_alive_timeout: Annotated[
        Optional[float], pydantic.Field(alias="keepAliveTimeout")
    ] = 5
    r"""After the last response is sent, @{product} will wait this long for additional data before closing the socket connection. Minimum 1 second, maximum 600 seconds (10 minutes)."""

    enable_health_check: Annotated[
        Optional[Any], pydantic.Field(alias="enableHealthCheck")
    ] = None

    ip_allowlist_regex: Annotated[
        Optional[str], pydantic.Field(alias="ipAllowlistRegex")
    ] = "/.*/"
    r"""Messages from matched IP addresses will be processed, unless also matched by the denylist"""

    ip_denylist_regex: Annotated[
        Optional[str], pydantic.Field(alias="ipDenylistRegex")
    ] = "/^$/"
    r"""Messages from matched IP addresses will be ignored. This takes precedence over the allowlist."""

    splunk_hec_api: Annotated[Optional[str], pydantic.Field(alias="splunkHecAPI")] = (
        "/services/collector"
    )
    r"""Absolute path on which to listen for the Splunk HTTP Event Collector API requests. This input supports the /event, /raw and /s2s endpoints."""

    metadata: Optional[List[MetadatumSplunkHec]] = None
    r"""Fields to add to every event. Overrides fields added at the token or request level. See [the Source documentation](https://docs.cribl.io/stream/sources-splunk-hec/#fields) for more info."""

    allowed_indexes: Annotated[
        Optional[List[str]], pydantic.Field(alias="allowedIndexes")
    ] = None
    r"""List values allowed in HEC event index field. Leave blank to skip validation. Supports wildcards. The values here can expand index validation at the token level."""

    splunk_hec_acks: Annotated[
        Optional[bool], pydantic.Field(alias="splunkHecAcks")
    ] = False
    r"""Enable Splunk HEC acknowledgements"""

    breaker_rulesets: Annotated[
        Optional[List[str]], pydantic.Field(alias="breakerRulesets")
    ] = None
    r"""A list of event-breaking rulesets that will be applied, in order, to the input data stream"""

    stale_channel_flush_ms: Annotated[
        Optional[float], pydantic.Field(alias="staleChannelFlushMs")
    ] = 10000
    r"""How long (in milliseconds) the Event Breaker will wait for new data to be sent to a specific channel before flushing the data stream out, as is, to the Pipelines"""

    use_fwd_timezone: Annotated[
        Optional[bool], pydantic.Field(alias="useFwdTimezone")
    ] = True
    r"""Event Breakers will determine events' time zone from UF-provided metadata, when TZ can't be inferred from the raw event"""

    drop_control_fields: Annotated[
        Optional[bool], pydantic.Field(alias="dropControlFields")
    ] = True
    r"""Drop Splunk control fields such as `crcSalt` and `_savedPort`. If disabled, control fields are stored in the internal field `__ctrlFields`."""

    extract_metrics: Annotated[
        Optional[bool], pydantic.Field(alias="extractMetrics")
    ] = False
    r"""Extract and process Splunk-generated metrics as Cribl metrics"""

    access_control_allow_origin: Annotated[
        Optional[List[str]], pydantic.Field(alias="accessControlAllowOrigin")
    ] = None
    r"""Optionally, list HTTP origins to which @{product} should send CORS (cross-origin resource sharing) Access-Control-Allow-* headers. Supports wildcards."""

    access_control_allow_headers: Annotated[
        Optional[List[str]], pydantic.Field(alias="accessControlAllowHeaders")
    ] = None
    r"""Optionally, list HTTP headers that @{product} will send to allowed origins as \"Access-Control-Allow-Headers\" in a CORS preflight response. Use \"*\" to allow all headers."""

    emit_token_metrics: Annotated[
        Optional[bool], pydantic.Field(alias="emitTokenMetrics")
    ] = False
    r"""Emit per-token (<prefix>.http.perToken) and summary (<prefix>.http.summary) request metrics"""

    description: Optional[str] = None

    @property
    def additional_properties(self):
        return self.__pydantic_extra__

    @additional_properties.setter
    def additional_properties(self, value):
        self.__pydantic_extra__ = value  # pyright: ignore[reportIncompatibleVariableOverride]


class TypeSplunkSearch(str, Enum):
    SPLUNK_SEARCH = "splunk_search"


class ConnectionSplunkSearchTypedDict(TypedDict):
    output: str
    pipeline: NotRequired[str]


class ConnectionSplunkSearch(BaseModel):
    output: str

    pipeline: Optional[str] = None


class ModeSplunkSearch(str, Enum, metaclass=utils.OpenEnumMeta):
    r"""With Smart mode, PQ will write events to the filesystem only when it detects backpressure from the processing engine. With Always On mode, PQ will always write events directly to the queue before forwarding them to the processing engine."""

    # Smart
    SMART = "smart"
    # Always On
    ALWAYS = "always"


class CompressionSplunkSearch(str, Enum, metaclass=utils.OpenEnumMeta):
    r"""Codec to use to compress the persisted data"""

    # None
    NONE = "none"
    # Gzip
    GZIP = "gzip"


class PqControlsSplunkSearchTypedDict(TypedDict):
    pass


class PqControlsSplunkSearch(BaseModel):
    pass


class PqSplunkSearchTypedDict(TypedDict):
    mode: NotRequired[ModeSplunkSearch]
    r"""With Smart mode, PQ will write events to the filesystem only when it detects backpressure from the processing engine. With Always On mode, PQ will always write events directly to the queue before forwarding them to the processing engine."""
    max_buffer_size: NotRequired[float]
    r"""The maximum number of events to hold in memory before writing the events to disk"""
    commit_frequency: NotRequired[float]
    r"""The number of events to send downstream before committing that Stream has read them"""
    max_file_size: NotRequired[str]
    r"""The maximum size to store in each queue file before closing and optionally compressing. Enter a numeral with units of KB, MB, etc."""
    max_size: NotRequired[str]
    r"""The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc."""
    path: NotRequired[str]
    r"""The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/inputs/<input-id>"""
    compress: NotRequired[CompressionSplunkSearch]
    r"""Codec to use to compress the persisted data"""
    pq_controls: NotRequired[PqControlsSplunkSearchTypedDict]


class PqSplunkSearch(BaseModel):
    mode: Annotated[
        Optional[ModeSplunkSearch], PlainValidator(validate_open_enum(False))
    ] = ModeSplunkSearch.ALWAYS
    r"""With Smart mode, PQ will write events to the filesystem only when it detects backpressure from the processing engine. With Always On mode, PQ will always write events directly to the queue before forwarding them to the processing engine."""

    max_buffer_size: Annotated[
        Optional[float], pydantic.Field(alias="maxBufferSize")
    ] = 1000
    r"""The maximum number of events to hold in memory before writing the events to disk"""

    commit_frequency: Annotated[
        Optional[float], pydantic.Field(alias="commitFrequency")
    ] = 42
    r"""The number of events to send downstream before committing that Stream has read them"""

    max_file_size: Annotated[Optional[str], pydantic.Field(alias="maxFileSize")] = (
        "1 MB"
    )
    r"""The maximum size to store in each queue file before closing and optionally compressing. Enter a numeral with units of KB, MB, etc."""

    max_size: Annotated[Optional[str], pydantic.Field(alias="maxSize")] = "5GB"
    r"""The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc."""

    path: Optional[str] = "$CRIBL_HOME/state/queues"
    r"""The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/inputs/<input-id>"""

    compress: Annotated[
        Optional[CompressionSplunkSearch], PlainValidator(validate_open_enum(False))
    ] = CompressionSplunkSearch.NONE
    r"""Codec to use to compress the persisted data"""

    pq_controls: Annotated[
        Optional[PqControlsSplunkSearch], pydantic.Field(alias="pqControls")
    ] = None

    @field_serializer("mode")
    def serialize_mode(self, value):
        if isinstance(value, str):
            try:
                return models.ModeSplunkSearch(value)
            except ValueError:
                return value
        return value

    @field_serializer("compress")
    def serialize_compress(self, value):
        if isinstance(value, str):
            try:
                return models.CompressionSplunkSearch(value)
            except ValueError:
                return value
        return value


class OutputMode(str, Enum, metaclass=utils.OpenEnumMeta):
    r"""Format of the returned output"""

    CSV = "csv"
    JSON = "json"


class EndpointParamTypedDict(TypedDict):
    name: str
    value: str
    r"""JavaScript expression to compute the parameter's value, normally enclosed in backticks (e.g., `${earliest}`). If a constant, use single quotes (e.g., 'earliest'). Values without delimiters (e.g., earliest) are evaluated as strings."""


class EndpointParam(BaseModel):
    name: str

    value: str
    r"""JavaScript expression to compute the parameter's value, normally enclosed in backticks (e.g., `${earliest}`). If a constant, use single quotes (e.g., 'earliest'). Values without delimiters (e.g., earliest) are evaluated as strings."""


class EndpointHeaderTypedDict(TypedDict):
    name: str
    value: str
    r"""JavaScript expression to compute the header's value, normally enclosed in backticks (e.g., `${earliest}`). If a constant, use single quotes (e.g., 'earliest'). Values without delimiters (e.g., earliest) are evaluated as strings."""


class EndpointHeader(BaseModel):
    name: str

    value: str
    r"""JavaScript expression to compute the header's value, normally enclosed in backticks (e.g., `${earliest}`). If a constant, use single quotes (e.g., 'earliest'). Values without delimiters (e.g., earliest) are evaluated as strings."""


class LogLevelSplunkSearch(str, Enum, metaclass=utils.OpenEnumMeta):
    r"""Collector runtime log level (verbosity)"""

    ERROR = "error"
    WARN = "warn"
    INFO = "info"
    DEBUG = "debug"


class MetadatumSplunkSearchTypedDict(TypedDict):
    name: str
    value: str
    r"""JavaScript expression to compute field's value, enclosed in quotes or backticks. (Can evaluate to a constant.)"""


class MetadatumSplunkSearch(BaseModel):
    name: str

    value: str
    r"""JavaScript expression to compute field's value, enclosed in quotes or backticks. (Can evaluate to a constant.)"""


class RetryTypeSplunkSearch(str, Enum, metaclass=utils.OpenEnumMeta):
    r"""The algorithm to use when performing HTTP retries"""

    # Disabled
    NONE = "none"
    # Backoff
    BACKOFF = "backoff"
    # Static
    STATIC = "static"


class RetryRulesSplunkSearchTypedDict(TypedDict):
    type: NotRequired[RetryTypeSplunkSearch]
    r"""The algorithm to use when performing HTTP retries"""
    interval: NotRequired[float]
    r"""Time interval between failed request and first retry (kickoff). Maximum allowed value is 20,000 ms (1/3 minute)."""
    limit: NotRequired[float]
    r"""The maximum number of times to retry a failed HTTP request"""
    multiplier: NotRequired[float]
    r"""Base for exponential backoff, e.g., base 2 means that retries will occur after 2, then 4, then 8 seconds, and so on"""
    codes: NotRequired[List[float]]
    r"""List of HTTP codes that trigger a retry. Leave empty to use the default list of 429 and 503."""
    enable_header: NotRequired[bool]
    r"""Honor any Retry-After header that specifies a delay (in seconds) or a timestamp after which to retry the request. The delay is limited to 20 seconds, even if the Retry-After header specifies a longer delay. When disabled, all Retry-After headers are ignored."""
    retry_connect_timeout: NotRequired[bool]
    r"""Make a single retry attempt when a connection timeout (ETIMEDOUT) error occurs"""
    retry_connect_reset: NotRequired[bool]
    r"""Retry request when a connection reset (ECONNRESET) error occurs"""


class RetryRulesSplunkSearch(BaseModel):
    type: Annotated[
        Optional[RetryTypeSplunkSearch], PlainValidator(validate_open_enum(False))
    ] = RetryTypeSplunkSearch.BACKOFF
    r"""The algorithm to use when performing HTTP retries"""

    interval: Optional[float] = 1000
    r"""Time interval between failed request and first retry (kickoff). Maximum allowed value is 20,000 ms (1/3 minute)."""

    limit: Optional[float] = 5
    r"""The maximum number of times to retry a failed HTTP request"""

    multiplier: Optional[float] = 2
    r"""Base for exponential backoff, e.g., base 2 means that retries will occur after 2, then 4, then 8 seconds, and so on"""

    codes: Optional[List[float]] = None
    r"""List of HTTP codes that trigger a retry. Leave empty to use the default list of 429 and 503."""

    enable_header: Annotated[Optional[bool], pydantic.Field(alias="enableHeader")] = (
        True
    )
    r"""Honor any Retry-After header that specifies a delay (in seconds) or a timestamp after which to retry the request. The delay is limited to 20 seconds, even if the Retry-After header specifies a longer delay. When disabled, all Retry-After headers are ignored."""

    retry_connect_timeout: Annotated[
        Optional[bool], pydantic.Field(alias="retryConnectTimeout")
    ] = False
    r"""Make a single retry attempt when a connection timeout (ETIMEDOUT) error occurs"""

    retry_connect_reset: Annotated[
        Optional[bool], pydantic.Field(alias="retryConnectReset")
    ] = False
    r"""Retry request when a connection reset (ECONNRESET) error occurs"""

    @field_serializer("type")
    def serialize_type(self, value):
        if isinstance(value, str):
            try:
                return models.RetryTypeSplunkSearch(value)
            except ValueError:
                return value
        return value


class AuthenticationTypeSplunkSearch(str, Enum, metaclass=utils.OpenEnumMeta):
    r"""Splunk Search authentication type"""

    NONE = "none"
    BASIC = "basic"
    CREDENTIALS_SECRET = "credentialsSecret"
    TOKEN = "token"
    TEXT_SECRET = "textSecret"
    OAUTH = "oauth"


class OauthParamSplunkSearchTypedDict(TypedDict):
    name: str
    r"""OAuth parameter name"""
    value: str
    r"""OAuth parameter value"""


class OauthParamSplunkSearch(BaseModel):
    name: str
    r"""OAuth parameter name"""

    value: str
    r"""OAuth parameter value"""


class OauthHeaderSplunkSearchTypedDict(TypedDict):
    name: str
    r"""OAuth header name"""
    value: str
    r"""OAuth header value"""


class OauthHeaderSplunkSearch(BaseModel):
    name: str
    r"""OAuth header name"""

    value: str
    r"""OAuth header value"""


class InputSplunkSearchTypedDict(TypedDict):
    type: TypeSplunkSearch
    search: str
    r"""Enter Splunk search here. Examples: 'index=myAppLogs level=error channel=myApp' OR '| mstats avg(myStat) as myStat WHERE index=myStatsIndex.'"""
    id: NotRequired[str]
    r"""Unique ID for this input"""
    disabled: NotRequired[bool]
    pipeline: NotRequired[str]
    r"""Pipeline to process data from this Source before sending it through the Routes"""
    send_to_routes: NotRequired[bool]
    r"""Select whether to send data to Routes, or directly to Destinations."""
    environment: NotRequired[str]
    r"""Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere."""
    pq_enabled: NotRequired[bool]
    r"""Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers)."""
    streamtags: NotRequired[List[str]]
    r"""Tags for filtering and grouping in @{product}"""
    connections: NotRequired[List[ConnectionSplunkSearchTypedDict]]
    r"""Direct connections to Destinations, and optionally via a Pipeline or a Pack"""
    pq: NotRequired[PqSplunkSearchTypedDict]
    search_head: NotRequired[str]
    r"""Search head base URL. Can be an expression. Default is https://localhost:8089."""
    earliest: NotRequired[str]
    r"""The earliest time boundary for the search. Can be an exact or relative time. Examples: '2022-01-14T12:00:00Z' or '-16m@m'"""
    latest: NotRequired[str]
    r"""The latest time boundary for the search. Can be an exact or relative time. Examples: '2022-01-14T12:00:00Z' or '-1m@m'"""
    cron_schedule: NotRequired[str]
    r"""A cron schedule on which to run this job"""
    endpoint: NotRequired[str]
    r"""REST API used to create a search"""
    output_mode: NotRequired[OutputMode]
    r"""Format of the returned output"""
    endpoint_params: NotRequired[List[EndpointParamTypedDict]]
    r"""Optional request parameters to send to the endpoint"""
    endpoint_headers: NotRequired[List[EndpointHeaderTypedDict]]
    r"""Optional request headers to send to the endpoint"""
    log_level: NotRequired[LogLevelSplunkSearch]
    r"""Collector runtime log level (verbosity)"""
    request_timeout: NotRequired[float]
    r"""HTTP request inactivity timeout. Use 0 for no timeout."""
    use_round_robin_dns: NotRequired[bool]
    r"""When a DNS server returns multiple addresses, @{product} will cycle through them in the order returned"""
    reject_unauthorized: NotRequired[bool]
    r"""Reject certificates that cannot be verified against a valid CA (such as self-signed certificates)"""
    encoding: NotRequired[str]
    r"""Character encoding to use when parsing ingested data. When not set, @{product} will default to UTF-8 but may incorrectly interpret multi-byte characters."""
    keep_alive_time: NotRequired[float]
    r"""How often workers should check in with the scheduler to keep job subscription alive"""
    job_timeout: NotRequired[str]
    r"""Maximum time the job is allowed to run (e.g., 30, 45s or 15m). Units are seconds, if not specified. Enter 0 for unlimited time."""
    max_missed_keep_alives: NotRequired[float]
    r"""The number of Keep Alive Time periods before an inactive worker will have its job subscription revoked."""
    ttl: NotRequired[str]
    r"""Time to keep the job's artifacts on disk after job completion. This also affects how long a job is listed in the Job Inspector."""
    ignore_group_jobs_limit: NotRequired[bool]
    r"""When enabled, this job's artifacts are not counted toward the Worker Group's finished job artifacts limit. Artifacts will be removed only after the Collector's configured time to live."""
    metadata: NotRequired[List[MetadatumSplunkSearchTypedDict]]
    r"""Fields to add to events from this input"""
    retry_rules: NotRequired[RetryRulesSplunkSearchTypedDict]
    breaker_rulesets: NotRequired[List[str]]
    r"""A list of event-breaking rulesets that will be applied, in order, to the input data stream"""
    stale_channel_flush_ms: NotRequired[float]
    r"""How long (in milliseconds) the Event Breaker will wait for new data to be sent to a specific channel before flushing the data stream out, as is, to the Pipelines"""
    auth_type: NotRequired[AuthenticationTypeSplunkSearch]
    r"""Splunk Search authentication type"""
    description: NotRequired[str]
    username: NotRequired[str]
    password: NotRequired[str]
    token: NotRequired[str]
    r"""Bearer token to include in the authorization header"""
    credentials_secret: NotRequired[str]
    r"""Select or create a secret that references your credentials"""
    text_secret: NotRequired[str]
    r"""Select or create a stored text secret"""
    login_url: NotRequired[str]
    r"""URL for OAuth"""
    secret_param_name: NotRequired[str]
    r"""Secret parameter name to pass in request body"""
    secret: NotRequired[str]
    r"""Secret parameter value to pass in request body"""
    token_attribute_name: NotRequired[str]
    r"""Name of the auth token attribute in the OAuth response. Can be top-level (e.g., 'token'); or nested, using a period (e.g., 'data.token')."""
    auth_header_expr: NotRequired[str]
    r"""JavaScript expression to compute the Authorization header value to pass in requests. The value `${token}` is used to reference the token obtained from authentication, e.g.: `Bearer ${token}`."""
    token_timeout_secs: NotRequired[float]
    r"""How often the OAuth token should be refreshed."""
    oauth_params: NotRequired[List[OauthParamSplunkSearchTypedDict]]
    r"""Additional parameters to send in the OAuth login request. @{product} will combine the secret with these parameters, and will send the URL-encoded result in a POST request to the endpoint specified in the 'Login URL'. We'll automatically add the content-type header 'application/x-www-form-urlencoded' when sending this request."""
    oauth_headers: NotRequired[List[OauthHeaderSplunkSearchTypedDict]]
    r"""Additional headers to send in the OAuth login request. @{product} will automatically add the content-type header 'application/x-www-form-urlencoded' when sending this request."""


class InputSplunkSearch(BaseModel):
    model_config = ConfigDict(
        populate_by_name=True, arbitrary_types_allowed=True, extra="allow"
    )
    __pydantic_extra__: Dict[str, Any] = pydantic.Field(init=False)

    type: TypeSplunkSearch

    search: str
    r"""Enter Splunk search here. Examples: 'index=myAppLogs level=error channel=myApp' OR '| mstats avg(myStat) as myStat WHERE index=myStatsIndex.'"""

    id: Optional[str] = None
    r"""Unique ID for this input"""

    disabled: Optional[bool] = False

    pipeline: Optional[str] = None
    r"""Pipeline to process data from this Source before sending it through the Routes"""

    send_to_routes: Annotated[Optional[bool], pydantic.Field(alias="sendToRoutes")] = (
        True
    )
    r"""Select whether to send data to Routes, or directly to Destinations."""

    environment: Optional[str] = None
    r"""Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere."""

    pq_enabled: Annotated[Optional[bool], pydantic.Field(alias="pqEnabled")] = False
    r"""Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers)."""

    streamtags: Optional[List[str]] = None
    r"""Tags for filtering and grouping in @{product}"""

    connections: Optional[List[ConnectionSplunkSearch]] = None
    r"""Direct connections to Destinations, and optionally via a Pipeline or a Pack"""

    pq: Optional[PqSplunkSearch] = None

    search_head: Annotated[Optional[str], pydantic.Field(alias="searchHead")] = (
        "https://localhost:8089"
    )
    r"""Search head base URL. Can be an expression. Default is https://localhost:8089."""

    earliest: Optional[str] = "-16m@m"
    r"""The earliest time boundary for the search. Can be an exact or relative time. Examples: '2022-01-14T12:00:00Z' or '-16m@m'"""

    latest: Optional[str] = "-1m@m"
    r"""The latest time boundary for the search. Can be an exact or relative time. Examples: '2022-01-14T12:00:00Z' or '-1m@m'"""

    cron_schedule: Annotated[Optional[str], pydantic.Field(alias="cronSchedule")] = (
        "*/15 * * * *"
    )
    r"""A cron schedule on which to run this job"""

    endpoint: Optional[str] = "/services/search/v2/jobs/export"
    r"""REST API used to create a search"""

    output_mode: Annotated[
        Annotated[Optional[OutputMode], PlainValidator(validate_open_enum(False))],
        pydantic.Field(alias="outputMode"),
    ] = OutputMode.JSON
    r"""Format of the returned output"""

    endpoint_params: Annotated[
        Optional[List[EndpointParam]], pydantic.Field(alias="endpointParams")
    ] = None
    r"""Optional request parameters to send to the endpoint"""

    endpoint_headers: Annotated[
        Optional[List[EndpointHeader]], pydantic.Field(alias="endpointHeaders")
    ] = None
    r"""Optional request headers to send to the endpoint"""

    log_level: Annotated[
        Annotated[
            Optional[LogLevelSplunkSearch], PlainValidator(validate_open_enum(False))
        ],
        pydantic.Field(alias="logLevel"),
    ] = None
    r"""Collector runtime log level (verbosity)"""

    request_timeout: Annotated[
        Optional[float], pydantic.Field(alias="requestTimeout")
    ] = 0
    r"""HTTP request inactivity timeout. Use 0 for no timeout."""

    use_round_robin_dns: Annotated[
        Optional[bool], pydantic.Field(alias="useRoundRobinDns")
    ] = False
    r"""When a DNS server returns multiple addresses, @{product} will cycle through them in the order returned"""

    reject_unauthorized: Annotated[
        Optional[bool], pydantic.Field(alias="rejectUnauthorized")
    ] = False
    r"""Reject certificates that cannot be verified against a valid CA (such as self-signed certificates)"""

    encoding: Optional[str] = None
    r"""Character encoding to use when parsing ingested data. When not set, @{product} will default to UTF-8 but may incorrectly interpret multi-byte characters."""

    keep_alive_time: Annotated[
        Optional[float], pydantic.Field(alias="keepAliveTime")
    ] = 30
    r"""How often workers should check in with the scheduler to keep job subscription alive"""

    job_timeout: Annotated[Optional[str], pydantic.Field(alias="jobTimeout")] = "0"
    r"""Maximum time the job is allowed to run (e.g., 30, 45s or 15m). Units are seconds, if not specified. Enter 0 for unlimited time."""

    max_missed_keep_alives: Annotated[
        Optional[float], pydantic.Field(alias="maxMissedKeepAlives")
    ] = 3
    r"""The number of Keep Alive Time periods before an inactive worker will have its job subscription revoked."""

    ttl: Optional[str] = "4h"
    r"""Time to keep the job's artifacts on disk after job completion. This also affects how long a job is listed in the Job Inspector."""

    ignore_group_jobs_limit: Annotated[
        Optional[bool], pydantic.Field(alias="ignoreGroupJobsLimit")
    ] = False
    r"""When enabled, this job's artifacts are not counted toward the Worker Group's finished job artifacts limit. Artifacts will be removed only after the Collector's configured time to live."""

    metadata: Optional[List[MetadatumSplunkSearch]] = None
    r"""Fields to add to events from this input"""

    retry_rules: Annotated[
        Optional[RetryRulesSplunkSearch], pydantic.Field(alias="retryRules")
    ] = None

    breaker_rulesets: Annotated[
        Optional[List[str]], pydantic.Field(alias="breakerRulesets")
    ] = None
    r"""A list of event-breaking rulesets that will be applied, in order, to the input data stream"""

    stale_channel_flush_ms: Annotated[
        Optional[float], pydantic.Field(alias="staleChannelFlushMs")
    ] = 10000
    r"""How long (in milliseconds) the Event Breaker will wait for new data to be sent to a specific channel before flushing the data stream out, as is, to the Pipelines"""

    auth_type: Annotated[
        Annotated[
            Optional[AuthenticationTypeSplunkSearch],
            PlainValidator(validate_open_enum(False)),
        ],
        pydantic.Field(alias="authType"),
    ] = AuthenticationTypeSplunkSearch.BASIC
    r"""Splunk Search authentication type"""

    description: Optional[str] = None

    username: Optional[str] = None

    password: Optional[str] = None

    token: Optional[str] = None
    r"""Bearer token to include in the authorization header"""

    credentials_secret: Annotated[
        Optional[str], pydantic.Field(alias="credentialsSecret")
    ] = None
    r"""Select or create a secret that references your credentials"""

    text_secret: Annotated[Optional[str], pydantic.Field(alias="textSecret")] = None
    r"""Select or create a stored text secret"""

    login_url: Annotated[Optional[str], pydantic.Field(alias="loginUrl")] = None
    r"""URL for OAuth"""

    secret_param_name: Annotated[
        Optional[str], pydantic.Field(alias="secretParamName")
    ] = None
    r"""Secret parameter name to pass in request body"""

    secret: Optional[str] = None
    r"""Secret parameter value to pass in request body"""

    token_attribute_name: Annotated[
        Optional[str], pydantic.Field(alias="tokenAttributeName")
    ] = None
    r"""Name of the auth token attribute in the OAuth response. Can be top-level (e.g., 'token'); or nested, using a period (e.g., 'data.token')."""

    auth_header_expr: Annotated[
        Optional[str], pydantic.Field(alias="authHeaderExpr")
    ] = "`Bearer ${token}`"
    r"""JavaScript expression to compute the Authorization header value to pass in requests. The value `${token}` is used to reference the token obtained from authentication, e.g.: `Bearer ${token}`."""

    token_timeout_secs: Annotated[
        Optional[float], pydantic.Field(alias="tokenTimeoutSecs")
    ] = 3600
    r"""How often the OAuth token should be refreshed."""

    oauth_params: Annotated[
        Optional[List[OauthParamSplunkSearch]], pydantic.Field(alias="oauthParams")
    ] = None
    r"""Additional parameters to send in the OAuth login request. @{product} will combine the secret with these parameters, and will send the URL-encoded result in a POST request to the endpoint specified in the 'Login URL'. We'll automatically add the content-type header 'application/x-www-form-urlencoded' when sending this request."""

    oauth_headers: Annotated[
        Optional[List[OauthHeaderSplunkSearch]], pydantic.Field(alias="oauthHeaders")
    ] = None
    r"""Additional headers to send in the OAuth login request. @{product} will automatically add the content-type header 'application/x-www-form-urlencoded' when sending this request."""

    @property
    def additional_properties(self):
        return self.__pydantic_extra__

    @additional_properties.setter
    def additional_properties(self, value):
        self.__pydantic_extra__ = value  # pyright: ignore[reportIncompatibleVariableOverride]

    @field_serializer("output_mode")
    def serialize_output_mode(self, value):
        if isinstance(value, str):
            try:
                return models.OutputMode(value)
            except ValueError:
                return value
        return value

    @field_serializer("log_level")
    def serialize_log_level(self, value):
        if isinstance(value, str):
            try:
                return models.LogLevelSplunkSearch(value)
            except ValueError:
                return value
        return value

    @field_serializer("auth_type")
    def serialize_auth_type(self, value):
        if isinstance(value, str):
            try:
                return models.AuthenticationTypeSplunkSearch(value)
            except ValueError:
                return value
        return value


class InputTypeSplunk(str, Enum):
    SPLUNK = "splunk"


class ConnectionSplunkTypedDict(TypedDict):
    output: str
    pipeline: NotRequired[str]


class ConnectionSplunk(BaseModel):
    output: str

    pipeline: Optional[str] = None


class PqModeSplunk(str, Enum, metaclass=utils.OpenEnumMeta):
    r"""With Smart mode, PQ will write events to the filesystem only when it detects backpressure from the processing engine. With Always On mode, PQ will always write events directly to the queue before forwarding them to the processing engine."""

    # Smart
    SMART = "smart"
    # Always On
    ALWAYS = "always"


class PqCompressionSplunk(str, Enum, metaclass=utils.OpenEnumMeta):
    r"""Codec to use to compress the persisted data"""

    # None
    NONE = "none"
    # Gzip
    GZIP = "gzip"


class InputPqControlsSplunkTypedDict(TypedDict):
    pass


class InputPqControlsSplunk(BaseModel):
    pass


class PqSplunkTypedDict(TypedDict):
    mode: NotRequired[PqModeSplunk]
    r"""With Smart mode, PQ will write events to the filesystem only when it detects backpressure from the processing engine. With Always On mode, PQ will always write events directly to the queue before forwarding them to the processing engine."""
    max_buffer_size: NotRequired[float]
    r"""The maximum number of events to hold in memory before writing the events to disk"""
    commit_frequency: NotRequired[float]
    r"""The number of events to send downstream before committing that Stream has read them"""
    max_file_size: NotRequired[str]
    r"""The maximum size to store in each queue file before closing and optionally compressing. Enter a numeral with units of KB, MB, etc."""
    max_size: NotRequired[str]
    r"""The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc."""
    path: NotRequired[str]
    r"""The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/inputs/<input-id>"""
    compress: NotRequired[PqCompressionSplunk]
    r"""Codec to use to compress the persisted data"""
    pq_controls: NotRequired[InputPqControlsSplunkTypedDict]


class PqSplunk(BaseModel):
    mode: Annotated[
        Optional[PqModeSplunk], PlainValidator(validate_open_enum(False))
    ] = PqModeSplunk.ALWAYS
    r"""With Smart mode, PQ will write events to the filesystem only when it detects backpressure from the processing engine. With Always On mode, PQ will always write events directly to the queue before forwarding them to the processing engine."""

    max_buffer_size: Annotated[
        Optional[float], pydantic.Field(alias="maxBufferSize")
    ] = 1000
    r"""The maximum number of events to hold in memory before writing the events to disk"""

    commit_frequency: Annotated[
        Optional[float], pydantic.Field(alias="commitFrequency")
    ] = 42
    r"""The number of events to send downstream before committing that Stream has read them"""

    max_file_size: Annotated[Optional[str], pydantic.Field(alias="maxFileSize")] = (
        "1 MB"
    )
    r"""The maximum size to store in each queue file before closing and optionally compressing. Enter a numeral with units of KB, MB, etc."""

    max_size: Annotated[Optional[str], pydantic.Field(alias="maxSize")] = "5GB"
    r"""The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc."""

    path: Optional[str] = "$CRIBL_HOME/state/queues"
    r"""The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/inputs/<input-id>"""

    compress: Annotated[
        Optional[PqCompressionSplunk], PlainValidator(validate_open_enum(False))
    ] = PqCompressionSplunk.NONE
    r"""Codec to use to compress the persisted data"""

    pq_controls: Annotated[
        Optional[InputPqControlsSplunk], pydantic.Field(alias="pqControls")
    ] = None

    @field_serializer("mode")
    def serialize_mode(self, value):
        if isinstance(value, str):
            try:
                return models.PqModeSplunk(value)
            except ValueError:
                return value
        return value

    @field_serializer("compress")
    def serialize_compress(self, value):
        if isinstance(value, str):
            try:
                return models.PqCompressionSplunk(value)
            except ValueError:
                return value
        return value


class InputMinimumTLSVersionSplunk(str, Enum, metaclass=utils.OpenEnumMeta):
    TL_SV1 = "TLSv1"
    TL_SV1_1 = "TLSv1.1"
    TL_SV1_2 = "TLSv1.2"
    TL_SV1_3 = "TLSv1.3"


class InputMaximumTLSVersionSplunk(str, Enum, metaclass=utils.OpenEnumMeta):
    TL_SV1 = "TLSv1"
    TL_SV1_1 = "TLSv1.1"
    TL_SV1_2 = "TLSv1.2"
    TL_SV1_3 = "TLSv1.3"


class TLSSettingsServerSideSplunkTypedDict(TypedDict):
    disabled: NotRequired[bool]
    request_cert: NotRequired[bool]
    r"""Require clients to present their certificates. Used to perform client authentication using SSL certs."""
    reject_unauthorized: NotRequired[bool]
    r"""Reject certificates not authorized by a CA in the CA certificate path or by another trusted CA (such as the system's)"""
    common_name_regex: NotRequired[str]
    r"""Regex matching allowable common names in peer certificates' subject attribute"""
    certificate_name: NotRequired[str]
    r"""The name of the predefined certificate"""
    priv_key_path: NotRequired[str]
    r"""Path on server containing the private key to use. PEM format. Can reference $ENV_VARS."""
    passphrase: NotRequired[str]
    r"""Passphrase to use to decrypt private key"""
    cert_path: NotRequired[str]
    r"""Path on server containing certificates to use. PEM format. Can reference $ENV_VARS."""
    ca_path: NotRequired[str]
    r"""Path on server containing CA certificates to use. PEM format. Can reference $ENV_VARS."""
    min_version: NotRequired[InputMinimumTLSVersionSplunk]
    max_version: NotRequired[InputMaximumTLSVersionSplunk]


class TLSSettingsServerSideSplunk(BaseModel):
    disabled: Optional[bool] = True

    request_cert: Annotated[Optional[bool], pydantic.Field(alias="requestCert")] = False
    r"""Require clients to present their certificates. Used to perform client authentication using SSL certs."""

    reject_unauthorized: Annotated[
        Optional[bool], pydantic.Field(alias="rejectUnauthorized")
    ] = True
    r"""Reject certificates not authorized by a CA in the CA certificate path or by another trusted CA (such as the system's)"""

    common_name_regex: Annotated[
        Optional[str], pydantic.Field(alias="commonNameRegex")
    ] = "/.*/"
    r"""Regex matching allowable common names in peer certificates' subject attribute"""

    certificate_name: Annotated[
        Optional[str], pydantic.Field(alias="certificateName")
    ] = None
    r"""The name of the predefined certificate"""

    priv_key_path: Annotated[Optional[str], pydantic.Field(alias="privKeyPath")] = None
    r"""Path on server containing the private key to use. PEM format. Can reference $ENV_VARS."""

    passphrase: Optional[str] = None
    r"""Passphrase to use to decrypt private key"""

    cert_path: Annotated[Optional[str], pydantic.Field(alias="certPath")] = None
    r"""Path on server containing certificates to use. PEM format. Can reference $ENV_VARS."""

    ca_path: Annotated[Optional[str], pydantic.Field(alias="caPath")] = None
    r"""Path on server containing CA certificates to use. PEM format. Can reference $ENV_VARS."""

    min_version: Annotated[
        Annotated[
            Optional[InputMinimumTLSVersionSplunk],
            PlainValidator(validate_open_enum(False)),
        ],
        pydantic.Field(alias="minVersion"),
    ] = None

    max_version: Annotated[
        Annotated[
            Optional[InputMaximumTLSVersionSplunk],
            PlainValidator(validate_open_enum(False)),
        ],
        pydantic.Field(alias="maxVersion"),
    ] = None

    @field_serializer("min_version")
    def serialize_min_version(self, value):
        if isinstance(value, str):
            try:
                return models.InputMinimumTLSVersionSplunk(value)
            except ValueError:
                return value
        return value

    @field_serializer("max_version")
    def serialize_max_version(self, value):
        if isinstance(value, str):
            try:
                return models.InputMaximumTLSVersionSplunk(value)
            except ValueError:
                return value
        return value


class MetadatumSplunkTypedDict(TypedDict):
    name: str
    value: str
    r"""JavaScript expression to compute field's value, enclosed in quotes or backticks. (Can evaluate to a constant.)"""


class MetadatumSplunk(BaseModel):
    name: str

    value: str
    r"""JavaScript expression to compute field's value, enclosed in quotes or backticks. (Can evaluate to a constant.)"""


class AuthTokenSplunkTypedDict(TypedDict):
    token: str
    r"""Shared secrets to be provided by any Splunk forwarder. If empty, unauthorized access is permitted."""
    description: NotRequired[str]


class AuthTokenSplunk(BaseModel):
    token: str
    r"""Shared secrets to be provided by any Splunk forwarder. If empty, unauthorized access is permitted."""

    description: Optional[str] = None


class InputMaxS2SVersion(str, Enum, metaclass=utils.OpenEnumMeta):
    r"""The highest S2S protocol version to advertise during handshake"""

    # v3
    V3 = "v3"
    # v4
    V4 = "v4"


class InputCompressionSplunk(str, Enum, metaclass=utils.OpenEnumMeta):
    r"""Controls whether to support reading compressed data from a forwarder. Select 'Automatic' to match the forwarder's configuration, or 'Disabled' to reject compressed connections."""

    # Disabled
    DISABLED = "disabled"
    # Automatic
    AUTO = "auto"
    # Always
    ALWAYS = "always"


class InputSplunkTypedDict(TypedDict):
    type: InputTypeSplunk
    port: float
    r"""Port to listen on"""
    id: NotRequired[str]
    r"""Unique ID for this input"""
    disabled: NotRequired[bool]
    pipeline: NotRequired[str]
    r"""Pipeline to process data from this Source before sending it through the Routes"""
    send_to_routes: NotRequired[bool]
    r"""Select whether to send data to Routes, or directly to Destinations."""
    environment: NotRequired[str]
    r"""Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere."""
    pq_enabled: NotRequired[bool]
    r"""Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers)."""
    streamtags: NotRequired[List[str]]
    r"""Tags for filtering and grouping in @{product}"""
    connections: NotRequired[List[ConnectionSplunkTypedDict]]
    r"""Direct connections to Destinations, and optionally via a Pipeline or a Pack"""
    pq: NotRequired[PqSplunkTypedDict]
    host: NotRequired[str]
    r"""Address to bind on. Defaults to 0.0.0.0 (all addresses)."""
    tls: NotRequired[TLSSettingsServerSideSplunkTypedDict]
    ip_whitelist_regex: NotRequired[str]
    r"""Regex matching IP addresses that are allowed to establish a connection"""
    max_active_cxn: NotRequired[float]
    r"""Maximum number of active connections allowed per Worker Process. Use 0 for unlimited."""
    socket_idle_timeout: NotRequired[float]
    r"""How long @{product} should wait before assuming that an inactive socket has timed out. After this time, the connection will be closed. Leave at 0 for no inactive socket monitoring."""
    socket_ending_max_wait: NotRequired[float]
    r"""How long the server will wait after initiating a closure for a client to close its end of the connection. If the client doesn't close the connection within this time, the server will forcefully terminate the socket to prevent resource leaks and ensure efficient connection cleanup and system stability. Leave at 0 for no inactive socket monitoring."""
    socket_max_lifespan: NotRequired[float]
    r"""The maximum duration a socket can remain open, even if active. This helps manage resources and mitigate issues caused by TCP pinning. Set to 0 to disable."""
    enable_proxy_header: NotRequired[bool]
    r"""Enable if the connection is proxied by a device that supports proxy protocol v1 or v2"""
    metadata: NotRequired[List[MetadatumSplunkTypedDict]]
    r"""Fields to add to events from this input"""
    breaker_rulesets: NotRequired[List[str]]
    r"""A list of event-breaking rulesets that will be applied, in order, to the input data stream"""
    stale_channel_flush_ms: NotRequired[float]
    r"""How long (in milliseconds) the Event Breaker will wait for new data to be sent to a specific channel before flushing the data stream out, as is, to the Pipelines"""
    auth_tokens: NotRequired[List[AuthTokenSplunkTypedDict]]
    r"""Shared secrets to be provided by any Splunk forwarder. If empty, unauthorized access is permitted."""
    max_s2_sversion: NotRequired[InputMaxS2SVersion]
    r"""The highest S2S protocol version to advertise during handshake"""
    description: NotRequired[str]
    use_fwd_timezone: NotRequired[bool]
    r"""Event Breakers will determine events' time zone from UF-provided metadata, when TZ can't be inferred from the raw event"""
    drop_control_fields: NotRequired[bool]
    r"""Drop Splunk control fields such as `crcSalt` and `_savedPort`. If disabled, control fields are stored in the internal field `__ctrlFields`."""
    extract_metrics: NotRequired[bool]
    r"""Extract and process Splunk-generated metrics as Cribl metrics"""
    compress: NotRequired[InputCompressionSplunk]
    r"""Controls whether to support reading compressed data from a forwarder. Select 'Automatic' to match the forwarder's configuration, or 'Disabled' to reject compressed connections."""


class InputSplunk(BaseModel):
    model_config = ConfigDict(
        populate_by_name=True, arbitrary_types_allowed=True, extra="allow"
    )
    __pydantic_extra__: Dict[str, Any] = pydantic.Field(init=False)

    type: InputTypeSplunk

    port: float
    r"""Port to listen on"""

    id: Optional[str] = None
    r"""Unique ID for this input"""

    disabled: Optional[bool] = False

    pipeline: Optional[str] = None
    r"""Pipeline to process data from this Source before sending it through the Routes"""

    send_to_routes: Annotated[Optional[bool], pydantic.Field(alias="sendToRoutes")] = (
        True
    )
    r"""Select whether to send data to Routes, or directly to Destinations."""

    environment: Optional[str] = None
    r"""Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere."""

    pq_enabled: Annotated[Optional[bool], pydantic.Field(alias="pqEnabled")] = False
    r"""Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers)."""

    streamtags: Optional[List[str]] = None
    r"""Tags for filtering and grouping in @{product}"""

    connections: Optional[List[ConnectionSplunk]] = None
    r"""Direct connections to Destinations, and optionally via a Pipeline or a Pack"""

    pq: Optional[PqSplunk] = None

    host: Optional[str] = "0.0.0.0"
    r"""Address to bind on. Defaults to 0.0.0.0 (all addresses)."""

    tls: Optional[TLSSettingsServerSideSplunk] = None

    ip_whitelist_regex: Annotated[
        Optional[str], pydantic.Field(alias="ipWhitelistRegex")
    ] = "/.*/"
    r"""Regex matching IP addresses that are allowed to establish a connection"""

    max_active_cxn: Annotated[Optional[float], pydantic.Field(alias="maxActiveCxn")] = (
        1000
    )
    r"""Maximum number of active connections allowed per Worker Process. Use 0 for unlimited."""

    socket_idle_timeout: Annotated[
        Optional[float], pydantic.Field(alias="socketIdleTimeout")
    ] = 0
    r"""How long @{product} should wait before assuming that an inactive socket has timed out. After this time, the connection will be closed. Leave at 0 for no inactive socket monitoring."""

    socket_ending_max_wait: Annotated[
        Optional[float], pydantic.Field(alias="socketEndingMaxWait")
    ] = 30
    r"""How long the server will wait after initiating a closure for a client to close its end of the connection. If the client doesn't close the connection within this time, the server will forcefully terminate the socket to prevent resource leaks and ensure efficient connection cleanup and system stability. Leave at 0 for no inactive socket monitoring."""

    socket_max_lifespan: Annotated[
        Optional[float], pydantic.Field(alias="socketMaxLifespan")
    ] = 0
    r"""The maximum duration a socket can remain open, even if active. This helps manage resources and mitigate issues caused by TCP pinning. Set to 0 to disable."""

    enable_proxy_header: Annotated[
        Optional[bool], pydantic.Field(alias="enableProxyHeader")
    ] = False
    r"""Enable if the connection is proxied by a device that supports proxy protocol v1 or v2"""

    metadata: Optional[List[MetadatumSplunk]] = None
    r"""Fields to add to events from this input"""

    breaker_rulesets: Annotated[
        Optional[List[str]], pydantic.Field(alias="breakerRulesets")
    ] = None
    r"""A list of event-breaking rulesets that will be applied, in order, to the input data stream"""

    stale_channel_flush_ms: Annotated[
        Optional[float], pydantic.Field(alias="staleChannelFlushMs")
    ] = 10000
    r"""How long (in milliseconds) the Event Breaker will wait for new data to be sent to a specific channel before flushing the data stream out, as is, to the Pipelines"""

    auth_tokens: Annotated[
        Optional[List[AuthTokenSplunk]], pydantic.Field(alias="authTokens")
    ] = None
    r"""Shared secrets to be provided by any Splunk forwarder. If empty, unauthorized access is permitted."""

    max_s2_sversion: Annotated[
        Annotated[
            Optional[InputMaxS2SVersion], PlainValidator(validate_open_enum(False))
        ],
        pydantic.Field(alias="maxS2Sversion"),
    ] = InputMaxS2SVersion.V3
    r"""The highest S2S protocol version to advertise during handshake"""

    description: Optional[str] = None

    use_fwd_timezone: Annotated[
        Optional[bool], pydantic.Field(alias="useFwdTimezone")
    ] = True
    r"""Event Breakers will determine events' time zone from UF-provided metadata, when TZ can't be inferred from the raw event"""

    drop_control_fields: Annotated[
        Optional[bool], pydantic.Field(alias="dropControlFields")
    ] = True
    r"""Drop Splunk control fields such as `crcSalt` and `_savedPort`. If disabled, control fields are stored in the internal field `__ctrlFields`."""

    extract_metrics: Annotated[
        Optional[bool], pydantic.Field(alias="extractMetrics")
    ] = False
    r"""Extract and process Splunk-generated metrics as Cribl metrics"""

    compress: Annotated[
        Optional[InputCompressionSplunk], PlainValidator(validate_open_enum(False))
    ] = InputCompressionSplunk.DISABLED
    r"""Controls whether to support reading compressed data from a forwarder. Select 'Automatic' to match the forwarder's configuration, or 'Disabled' to reject compressed connections."""

    @property
    def additional_properties(self):
        return self.__pydantic_extra__

    @additional_properties.setter
    def additional_properties(self, value):
        self.__pydantic_extra__ = value  # pyright: ignore[reportIncompatibleVariableOverride]

    @field_serializer("max_s2_sversion")
    def serialize_max_s2_sversion(self, value):
        if isinstance(value, str):
            try:
                return models.InputMaxS2SVersion(value)
            except ValueError:
                return value
        return value

    @field_serializer("compress")
    def serialize_compress(self, value):
        if isinstance(value, str):
            try:
                return models.InputCompressionSplunk(value)
            except ValueError:
                return value
        return value


class TypeHTTP(str, Enum):
    HTTP = "http"


class ConnectionHTTPTypedDict(TypedDict):
    output: str
    pipeline: NotRequired[str]


class ConnectionHTTP(BaseModel):
    output: str

    pipeline: Optional[str] = None


class ModeHTTP(str, Enum, metaclass=utils.OpenEnumMeta):
    r"""With Smart mode, PQ will write events to the filesystem only when it detects backpressure from the processing engine. With Always On mode, PQ will always write events directly to the queue before forwarding them to the processing engine."""

    # Smart
    SMART = "smart"
    # Always On
    ALWAYS = "always"


class CompressionHTTP(str, Enum, metaclass=utils.OpenEnumMeta):
    r"""Codec to use to compress the persisted data"""

    # None
    NONE = "none"
    # Gzip
    GZIP = "gzip"


class PqControlsHTTPTypedDict(TypedDict):
    pass


class PqControlsHTTP(BaseModel):
    pass


class PqHTTPTypedDict(TypedDict):
    mode: NotRequired[ModeHTTP]
    r"""With Smart mode, PQ will write events to the filesystem only when it detects backpressure from the processing engine. With Always On mode, PQ will always write events directly to the queue before forwarding them to the processing engine."""
    max_buffer_size: NotRequired[float]
    r"""The maximum number of events to hold in memory before writing the events to disk"""
    commit_frequency: NotRequired[float]
    r"""The number of events to send downstream before committing that Stream has read them"""
    max_file_size: NotRequired[str]
    r"""The maximum size to store in each queue file before closing and optionally compressing. Enter a numeral with units of KB, MB, etc."""
    max_size: NotRequired[str]
    r"""The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc."""
    path: NotRequired[str]
    r"""The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/inputs/<input-id>"""
    compress: NotRequired[CompressionHTTP]
    r"""Codec to use to compress the persisted data"""
    pq_controls: NotRequired[PqControlsHTTPTypedDict]


class PqHTTP(BaseModel):
    mode: Annotated[Optional[ModeHTTP], PlainValidator(validate_open_enum(False))] = (
        ModeHTTP.ALWAYS
    )
    r"""With Smart mode, PQ will write events to the filesystem only when it detects backpressure from the processing engine. With Always On mode, PQ will always write events directly to the queue before forwarding them to the processing engine."""

    max_buffer_size: Annotated[
        Optional[float], pydantic.Field(alias="maxBufferSize")
    ] = 1000
    r"""The maximum number of events to hold in memory before writing the events to disk"""

    commit_frequency: Annotated[
        Optional[float], pydantic.Field(alias="commitFrequency")
    ] = 42
    r"""The number of events to send downstream before committing that Stream has read them"""

    max_file_size: Annotated[Optional[str], pydantic.Field(alias="maxFileSize")] = (
        "1 MB"
    )
    r"""The maximum size to store in each queue file before closing and optionally compressing. Enter a numeral with units of KB, MB, etc."""

    max_size: Annotated[Optional[str], pydantic.Field(alias="maxSize")] = "5GB"
    r"""The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc."""

    path: Optional[str] = "$CRIBL_HOME/state/queues"
    r"""The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/inputs/<input-id>"""

    compress: Annotated[
        Optional[CompressionHTTP], PlainValidator(validate_open_enum(False))
    ] = CompressionHTTP.NONE
    r"""Codec to use to compress the persisted data"""

    pq_controls: Annotated[
        Optional[PqControlsHTTP], pydantic.Field(alias="pqControls")
    ] = None

    @field_serializer("mode")
    def serialize_mode(self, value):
        if isinstance(value, str):
            try:
                return models.ModeHTTP(value)
            except ValueError:
                return value
        return value

    @field_serializer("compress")
    def serialize_compress(self, value):
        if isinstance(value, str):
            try:
                return models.CompressionHTTP(value)
            except ValueError:
                return value
        return value


class MinimumTLSVersionHTTP(str, Enum, metaclass=utils.OpenEnumMeta):
    TL_SV1 = "TLSv1"
    TL_SV1_1 = "TLSv1.1"
    TL_SV1_2 = "TLSv1.2"
    TL_SV1_3 = "TLSv1.3"


class MaximumTLSVersionHTTP(str, Enum, metaclass=utils.OpenEnumMeta):
    TL_SV1 = "TLSv1"
    TL_SV1_1 = "TLSv1.1"
    TL_SV1_2 = "TLSv1.2"
    TL_SV1_3 = "TLSv1.3"


class TLSSettingsServerSideHTTPTypedDict(TypedDict):
    disabled: NotRequired[bool]
    request_cert: NotRequired[bool]
    r"""Require clients to present their certificates. Used to perform client authentication using SSL certs."""
    reject_unauthorized: NotRequired[bool]
    r"""Reject certificates not authorized by a CA in the CA certificate path or by another trusted CA (such as the system's)"""
    common_name_regex: NotRequired[str]
    r"""Regex matching allowable common names in peer certificates' subject attribute"""
    certificate_name: NotRequired[str]
    r"""The name of the predefined certificate"""
    priv_key_path: NotRequired[str]
    r"""Path on server containing the private key to use. PEM format. Can reference $ENV_VARS."""
    passphrase: NotRequired[str]
    r"""Passphrase to use to decrypt private key"""
    cert_path: NotRequired[str]
    r"""Path on server containing certificates to use. PEM format. Can reference $ENV_VARS."""
    ca_path: NotRequired[str]
    r"""Path on server containing CA certificates to use. PEM format. Can reference $ENV_VARS."""
    min_version: NotRequired[MinimumTLSVersionHTTP]
    max_version: NotRequired[MaximumTLSVersionHTTP]


class TLSSettingsServerSideHTTP(BaseModel):
    disabled: Optional[bool] = True

    request_cert: Annotated[Optional[bool], pydantic.Field(alias="requestCert")] = False
    r"""Require clients to present their certificates. Used to perform client authentication using SSL certs."""

    reject_unauthorized: Annotated[
        Optional[bool], pydantic.Field(alias="rejectUnauthorized")
    ] = True
    r"""Reject certificates not authorized by a CA in the CA certificate path or by another trusted CA (such as the system's)"""

    common_name_regex: Annotated[
        Optional[str], pydantic.Field(alias="commonNameRegex")
    ] = "/.*/"
    r"""Regex matching allowable common names in peer certificates' subject attribute"""

    certificate_name: Annotated[
        Optional[str], pydantic.Field(alias="certificateName")
    ] = None
    r"""The name of the predefined certificate"""

    priv_key_path: Annotated[Optional[str], pydantic.Field(alias="privKeyPath")] = None
    r"""Path on server containing the private key to use. PEM format. Can reference $ENV_VARS."""

    passphrase: Optional[str] = None
    r"""Passphrase to use to decrypt private key"""

    cert_path: Annotated[Optional[str], pydantic.Field(alias="certPath")] = None
    r"""Path on server containing certificates to use. PEM format. Can reference $ENV_VARS."""

    ca_path: Annotated[Optional[str], pydantic.Field(alias="caPath")] = None
    r"""Path on server containing CA certificates to use. PEM format. Can reference $ENV_VARS."""

    min_version: Annotated[
        Annotated[
            Optional[MinimumTLSVersionHTTP], PlainValidator(validate_open_enum(False))
        ],
        pydantic.Field(alias="minVersion"),
    ] = None

    max_version: Annotated[
        Annotated[
            Optional[MaximumTLSVersionHTTP], PlainValidator(validate_open_enum(False))
        ],
        pydantic.Field(alias="maxVersion"),
    ] = None

    @field_serializer("min_version")
    def serialize_min_version(self, value):
        if isinstance(value, str):
            try:
                return models.MinimumTLSVersionHTTP(value)
            except ValueError:
                return value
        return value

    @field_serializer("max_version")
    def serialize_max_version(self, value):
        if isinstance(value, str):
            try:
                return models.MaximumTLSVersionHTTP(value)
            except ValueError:
                return value
        return value


class MetadatumHTTPTypedDict(TypedDict):
    name: str
    value: str
    r"""JavaScript expression to compute field's value, enclosed in quotes or backticks. (Can evaluate to a constant.)"""


class MetadatumHTTP(BaseModel):
    name: str

    value: str
    r"""JavaScript expression to compute field's value, enclosed in quotes or backticks. (Can evaluate to a constant.)"""


class AuthTokensExtMetadatumHTTPTypedDict(TypedDict):
    name: str
    value: str
    r"""JavaScript expression to compute field's value, enclosed in quotes or backticks. (Can evaluate to a constant.)"""


class AuthTokensExtMetadatumHTTP(BaseModel):
    name: str

    value: str
    r"""JavaScript expression to compute field's value, enclosed in quotes or backticks. (Can evaluate to a constant.)"""


class AuthTokensExtHTTPTypedDict(TypedDict):
    token: str
    r"""Shared secret to be provided by any client (Authorization: <token>)"""
    description: NotRequired[str]
    metadata: NotRequired[List[AuthTokensExtMetadatumHTTPTypedDict]]
    r"""Fields to add to events referencing this token"""


class AuthTokensExtHTTP(BaseModel):
    token: str
    r"""Shared secret to be provided by any client (Authorization: <token>)"""

    description: Optional[str] = None

    metadata: Optional[List[AuthTokensExtMetadatumHTTP]] = None
    r"""Fields to add to events referencing this token"""


class InputHTTPTypedDict(TypedDict):
    type: TypeHTTP
    port: float
    r"""Port to listen on"""
    id: NotRequired[str]
    r"""Unique ID for this input"""
    disabled: NotRequired[bool]
    pipeline: NotRequired[str]
    r"""Pipeline to process data from this Source before sending it through the Routes"""
    send_to_routes: NotRequired[bool]
    r"""Select whether to send data to Routes, or directly to Destinations."""
    environment: NotRequired[str]
    r"""Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere."""
    pq_enabled: NotRequired[bool]
    r"""Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers)."""
    streamtags: NotRequired[List[str]]
    r"""Tags for filtering and grouping in @{product}"""
    connections: NotRequired[List[ConnectionHTTPTypedDict]]
    r"""Direct connections to Destinations, and optionally via a Pipeline or a Pack"""
    pq: NotRequired[PqHTTPTypedDict]
    host: NotRequired[str]
    r"""Address to bind on. Defaults to 0.0.0.0 (all addresses)."""
    auth_tokens: NotRequired[List[str]]
    r"""Shared secrets to be provided by any client (Authorization: <token>). If empty, unauthorized access is permitted."""
    tls: NotRequired[TLSSettingsServerSideHTTPTypedDict]
    max_active_req: NotRequired[float]
    r"""Maximum number of active requests allowed per Worker Process. Set to 0 for unlimited. Caution: Increasing the limit above the default value, or setting it to unlimited, may degrade performance and reduce throughput."""
    max_requests_per_socket: NotRequired[int]
    r"""Maximum number of requests per socket before @{product} instructs the client to close the connection. Default is 0 (unlimited)."""
    enable_proxy_header: NotRequired[bool]
    r"""Extract the client IP and port from PROXY protocol v1/v2. When enabled, the X-Forwarded-For header is ignored. Disable to use the X-Forwarded-For header for client IP extraction."""
    capture_headers: NotRequired[bool]
    r"""Add request headers to events, in the __headers field"""
    activity_log_sample_rate: NotRequired[float]
    r"""How often request activity is logged at the `info` level. A value of 1 would log every request, 10 every 10th request, etc."""
    request_timeout: NotRequired[float]
    r"""How long to wait for an incoming request to complete before aborting it. Use 0 to disable."""
    socket_timeout: NotRequired[float]
    r"""How long @{product} should wait before assuming that an inactive socket has timed out. To wait forever, set to 0."""
    keep_alive_timeout: NotRequired[float]
    r"""After the last response is sent, @{product} will wait this long for additional data before closing the socket connection. Minimum 1 second, maximum 600 seconds (10 minutes)."""
    enable_health_check: NotRequired[bool]
    r"""Expose the /cribl_health endpoint, which returns 200 OK when this Source is healthy"""
    ip_allowlist_regex: NotRequired[str]
    r"""Messages from matched IP addresses will be processed, unless also matched by the denylist"""
    ip_denylist_regex: NotRequired[str]
    r"""Messages from matched IP addresses will be ignored. This takes precedence over the allowlist."""
    cribl_api: NotRequired[str]
    r"""Absolute path on which to listen for the Cribl HTTP API requests. Only _bulk (default /cribl/_bulk) is available. Use empty string to disable."""
    elastic_api: NotRequired[str]
    r"""Absolute path on which to listen for the Elasticsearch API requests. Only _bulk (default /elastic/_bulk) is available. Use empty string to disable."""
    splunk_hec_api: NotRequired[str]
    r"""Absolute path on which listen for the Splunk HTTP Event Collector API requests. Use empty string to disable."""
    splunk_hec_acks: NotRequired[bool]
    metadata: NotRequired[List[MetadatumHTTPTypedDict]]
    r"""Fields to add to events from this input"""
    auth_tokens_ext: NotRequired[List[AuthTokensExtHTTPTypedDict]]
    r"""Shared secrets to be provided by any client (Authorization: <token>). If empty, unauthorized access is permitted."""
    description: NotRequired[str]


class InputHTTP(BaseModel):
    model_config = ConfigDict(
        populate_by_name=True, arbitrary_types_allowed=True, extra="allow"
    )
    __pydantic_extra__: Dict[str, Any] = pydantic.Field(init=False)

    type: TypeHTTP

    port: float
    r"""Port to listen on"""

    id: Optional[str] = None
    r"""Unique ID for this input"""

    disabled: Optional[bool] = False

    pipeline: Optional[str] = None
    r"""Pipeline to process data from this Source before sending it through the Routes"""

    send_to_routes: Annotated[Optional[bool], pydantic.Field(alias="sendToRoutes")] = (
        True
    )
    r"""Select whether to send data to Routes, or directly to Destinations."""

    environment: Optional[str] = None
    r"""Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere."""

    pq_enabled: Annotated[Optional[bool], pydantic.Field(alias="pqEnabled")] = False
    r"""Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers)."""

    streamtags: Optional[List[str]] = None
    r"""Tags for filtering and grouping in @{product}"""

    connections: Optional[List[ConnectionHTTP]] = None
    r"""Direct connections to Destinations, and optionally via a Pipeline or a Pack"""

    pq: Optional[PqHTTP] = None

    host: Optional[str] = "0.0.0.0"
    r"""Address to bind on. Defaults to 0.0.0.0 (all addresses)."""

    auth_tokens: Annotated[Optional[List[str]], pydantic.Field(alias="authTokens")] = (
        None
    )
    r"""Shared secrets to be provided by any client (Authorization: <token>). If empty, unauthorized access is permitted."""

    tls: Optional[TLSSettingsServerSideHTTP] = None

    max_active_req: Annotated[Optional[float], pydantic.Field(alias="maxActiveReq")] = (
        256
    )
    r"""Maximum number of active requests allowed per Worker Process. Set to 0 for unlimited. Caution: Increasing the limit above the default value, or setting it to unlimited, may degrade performance and reduce throughput."""

    max_requests_per_socket: Annotated[
        Optional[int], pydantic.Field(alias="maxRequestsPerSocket")
    ] = 0
    r"""Maximum number of requests per socket before @{product} instructs the client to close the connection. Default is 0 (unlimited)."""

    enable_proxy_header: Annotated[
        Optional[bool], pydantic.Field(alias="enableProxyHeader")
    ] = False
    r"""Extract the client IP and port from PROXY protocol v1/v2. When enabled, the X-Forwarded-For header is ignored. Disable to use the X-Forwarded-For header for client IP extraction."""

    capture_headers: Annotated[
        Optional[bool], pydantic.Field(alias="captureHeaders")
    ] = False
    r"""Add request headers to events, in the __headers field"""

    activity_log_sample_rate: Annotated[
        Optional[float], pydantic.Field(alias="activityLogSampleRate")
    ] = 100
    r"""How often request activity is logged at the `info` level. A value of 1 would log every request, 10 every 10th request, etc."""

    request_timeout: Annotated[
        Optional[float], pydantic.Field(alias="requestTimeout")
    ] = 0
    r"""How long to wait for an incoming request to complete before aborting it. Use 0 to disable."""

    socket_timeout: Annotated[
        Optional[float], pydantic.Field(alias="socketTimeout")
    ] = 0
    r"""How long @{product} should wait before assuming that an inactive socket has timed out. To wait forever, set to 0."""

    keep_alive_timeout: Annotated[
        Optional[float], pydantic.Field(alias="keepAliveTimeout")
    ] = 5
    r"""After the last response is sent, @{product} will wait this long for additional data before closing the socket connection. Minimum 1 second, maximum 600 seconds (10 minutes)."""

    enable_health_check: Annotated[
        Optional[bool], pydantic.Field(alias="enableHealthCheck")
    ] = False
    r"""Expose the /cribl_health endpoint, which returns 200 OK when this Source is healthy"""

    ip_allowlist_regex: Annotated[
        Optional[str], pydantic.Field(alias="ipAllowlistRegex")
    ] = "/.*/"
    r"""Messages from matched IP addresses will be processed, unless also matched by the denylist"""

    ip_denylist_regex: Annotated[
        Optional[str], pydantic.Field(alias="ipDenylistRegex")
    ] = "/^$/"
    r"""Messages from matched IP addresses will be ignored. This takes precedence over the allowlist."""

    cribl_api: Annotated[Optional[str], pydantic.Field(alias="criblAPI")] = "/cribl"
    r"""Absolute path on which to listen for the Cribl HTTP API requests. Only _bulk (default /cribl/_bulk) is available. Use empty string to disable."""

    elastic_api: Annotated[Optional[str], pydantic.Field(alias="elasticAPI")] = (
        "/elastic"
    )
    r"""Absolute path on which to listen for the Elasticsearch API requests. Only _bulk (default /elastic/_bulk) is available. Use empty string to disable."""

    splunk_hec_api: Annotated[Optional[str], pydantic.Field(alias="splunkHecAPI")] = (
        "/services/collector"
    )
    r"""Absolute path on which listen for the Splunk HTTP Event Collector API requests. Use empty string to disable."""

    splunk_hec_acks: Annotated[
        Optional[bool], pydantic.Field(alias="splunkHecAcks")
    ] = False

    metadata: Optional[List[MetadatumHTTP]] = None
    r"""Fields to add to events from this input"""

    auth_tokens_ext: Annotated[
        Optional[List[AuthTokensExtHTTP]], pydantic.Field(alias="authTokensExt")
    ] = None
    r"""Shared secrets to be provided by any client (Authorization: <token>). If empty, unauthorized access is permitted."""

    description: Optional[str] = None

    @property
    def additional_properties(self):
        return self.__pydantic_extra__

    @additional_properties.setter
    def additional_properties(self, value):
        self.__pydantic_extra__ = value  # pyright: ignore[reportIncompatibleVariableOverride]


class InputTypeMsk(str, Enum):
    MSK = "msk"


class ConnectionMskTypedDict(TypedDict):
    output: str
    pipeline: NotRequired[str]


class ConnectionMsk(BaseModel):
    output: str

    pipeline: Optional[str] = None


class PqModeMsk(str, Enum, metaclass=utils.OpenEnumMeta):
    r"""With Smart mode, PQ will write events to the filesystem only when it detects backpressure from the processing engine. With Always On mode, PQ will always write events directly to the queue before forwarding them to the processing engine."""

    # Smart
    SMART = "smart"
    # Always On
    ALWAYS = "always"


class PqCompressionMsk(str, Enum, metaclass=utils.OpenEnumMeta):
    r"""Codec to use to compress the persisted data"""

    # None
    NONE = "none"
    # Gzip
    GZIP = "gzip"


class InputPqControlsMskTypedDict(TypedDict):
    pass


class InputPqControlsMsk(BaseModel):
    pass


class PqMskTypedDict(TypedDict):
    mode: NotRequired[PqModeMsk]
    r"""With Smart mode, PQ will write events to the filesystem only when it detects backpressure from the processing engine. With Always On mode, PQ will always write events directly to the queue before forwarding them to the processing engine."""
    max_buffer_size: NotRequired[float]
    r"""The maximum number of events to hold in memory before writing the events to disk"""
    commit_frequency: NotRequired[float]
    r"""The number of events to send downstream before committing that Stream has read them"""
    max_file_size: NotRequired[str]
    r"""The maximum size to store in each queue file before closing and optionally compressing. Enter a numeral with units of KB, MB, etc."""
    max_size: NotRequired[str]
    r"""The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc."""
    path: NotRequired[str]
    r"""The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/inputs/<input-id>"""
    compress: NotRequired[PqCompressionMsk]
    r"""Codec to use to compress the persisted data"""
    pq_controls: NotRequired[InputPqControlsMskTypedDict]


class PqMsk(BaseModel):
    mode: Annotated[Optional[PqModeMsk], PlainValidator(validate_open_enum(False))] = (
        PqModeMsk.ALWAYS
    )
    r"""With Smart mode, PQ will write events to the filesystem only when it detects backpressure from the processing engine. With Always On mode, PQ will always write events directly to the queue before forwarding them to the processing engine."""

    max_buffer_size: Annotated[
        Optional[float], pydantic.Field(alias="maxBufferSize")
    ] = 1000
    r"""The maximum number of events to hold in memory before writing the events to disk"""

    commit_frequency: Annotated[
        Optional[float], pydantic.Field(alias="commitFrequency")
    ] = 42
    r"""The number of events to send downstream before committing that Stream has read them"""

    max_file_size: Annotated[Optional[str], pydantic.Field(alias="maxFileSize")] = (
        "1 MB"
    )
    r"""The maximum size to store in each queue file before closing and optionally compressing. Enter a numeral with units of KB, MB, etc."""

    max_size: Annotated[Optional[str], pydantic.Field(alias="maxSize")] = "5GB"
    r"""The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc."""

    path: Optional[str] = "$CRIBL_HOME/state/queues"
    r"""The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/inputs/<input-id>"""

    compress: Annotated[
        Optional[PqCompressionMsk], PlainValidator(validate_open_enum(False))
    ] = PqCompressionMsk.NONE
    r"""Codec to use to compress the persisted data"""

    pq_controls: Annotated[
        Optional[InputPqControlsMsk], pydantic.Field(alias="pqControls")
    ] = None

    @field_serializer("mode")
    def serialize_mode(self, value):
        if isinstance(value, str):
            try:
                return models.PqModeMsk(value)
            except ValueError:
                return value
        return value

    @field_serializer("compress")
    def serialize_compress(self, value):
        if isinstance(value, str):
            try:
                return models.PqCompressionMsk(value)
            except ValueError:
                return value
        return value


class MetadatumMskTypedDict(TypedDict):
    name: str
    value: str
    r"""JavaScript expression to compute field's value, enclosed in quotes or backticks. (Can evaluate to a constant.)"""


class MetadatumMsk(BaseModel):
    name: str

    value: str
    r"""JavaScript expression to compute field's value, enclosed in quotes or backticks. (Can evaluate to a constant.)"""


class InputAuthMskTypedDict(TypedDict):
    r"""Credentials to use when authenticating with the schema registry using basic HTTP authentication"""

    disabled: NotRequired[bool]
    credentials_secret: NotRequired[str]
    r"""Select or create a secret that references your credentials"""


class InputAuthMsk(BaseModel):
    r"""Credentials to use when authenticating with the schema registry using basic HTTP authentication"""

    disabled: Optional[bool] = True

    credentials_secret: Annotated[
        Optional[str], pydantic.Field(alias="credentialsSecret")
    ] = None
    r"""Select or create a secret that references your credentials"""


class InputKafkaSchemaRegistryMinimumTLSVersionMsk(
    str, Enum, metaclass=utils.OpenEnumMeta
):
    TL_SV1 = "TLSv1"
    TL_SV1_1 = "TLSv1.1"
    TL_SV1_2 = "TLSv1.2"
    TL_SV1_3 = "TLSv1.3"


class InputKafkaSchemaRegistryMaximumTLSVersionMsk(
    str, Enum, metaclass=utils.OpenEnumMeta
):
    TL_SV1 = "TLSv1"
    TL_SV1_1 = "TLSv1.1"
    TL_SV1_2 = "TLSv1.2"
    TL_SV1_3 = "TLSv1.3"


class InputKafkaSchemaRegistryTLSSettingsClientSideMskTypedDict(TypedDict):
    disabled: NotRequired[bool]
    reject_unauthorized: NotRequired[bool]
    r"""Reject certificates that are not authorized by a CA in the CA certificate path, or by another
    trusted CA (such as the system's). Defaults to Enabled. Overrides the toggle from Advanced Settings, when also present.
    """
    servername: NotRequired[str]
    r"""Server name for the SNI (Server Name Indication) TLS extension. It must be a host name, and not an IP address."""
    certificate_name: NotRequired[str]
    r"""The name of the predefined certificate"""
    ca_path: NotRequired[str]
    r"""Path on client in which to find CA certificates to verify the server's cert. PEM format. Can reference $ENV_VARS."""
    priv_key_path: NotRequired[str]
    r"""Path on client in which to find the private key to use. PEM format. Can reference $ENV_VARS."""
    cert_path: NotRequired[str]
    r"""Path on client in which to find certificates to use. PEM format. Can reference $ENV_VARS."""
    passphrase: NotRequired[str]
    r"""Passphrase to use to decrypt private key"""
    min_version: NotRequired[InputKafkaSchemaRegistryMinimumTLSVersionMsk]
    max_version: NotRequired[InputKafkaSchemaRegistryMaximumTLSVersionMsk]


class InputKafkaSchemaRegistryTLSSettingsClientSideMsk(BaseModel):
    disabled: Optional[bool] = True

    reject_unauthorized: Annotated[
        Optional[bool], pydantic.Field(alias="rejectUnauthorized")
    ] = True
    r"""Reject certificates that are not authorized by a CA in the CA certificate path, or by another
    trusted CA (such as the system's). Defaults to Enabled. Overrides the toggle from Advanced Settings, when also present.
    """

    servername: Optional[str] = None
    r"""Server name for the SNI (Server Name Indication) TLS extension. It must be a host name, and not an IP address."""

    certificate_name: Annotated[
        Optional[str], pydantic.Field(alias="certificateName")
    ] = None
    r"""The name of the predefined certificate"""

    ca_path: Annotated[Optional[str], pydantic.Field(alias="caPath")] = None
    r"""Path on client in which to find CA certificates to verify the server's cert. PEM format. Can reference $ENV_VARS."""

    priv_key_path: Annotated[Optional[str], pydantic.Field(alias="privKeyPath")] = None
    r"""Path on client in which to find the private key to use. PEM format. Can reference $ENV_VARS."""

    cert_path: Annotated[Optional[str], pydantic.Field(alias="certPath")] = None
    r"""Path on client in which to find certificates to use. PEM format. Can reference $ENV_VARS."""

    passphrase: Optional[str] = None
    r"""Passphrase to use to decrypt private key"""

    min_version: Annotated[
        Annotated[
            Optional[InputKafkaSchemaRegistryMinimumTLSVersionMsk],
            PlainValidator(validate_open_enum(False)),
        ],
        pydantic.Field(alias="minVersion"),
    ] = None

    max_version: Annotated[
        Annotated[
            Optional[InputKafkaSchemaRegistryMaximumTLSVersionMsk],
            PlainValidator(validate_open_enum(False)),
        ],
        pydantic.Field(alias="maxVersion"),
    ] = None

    @field_serializer("min_version")
    def serialize_min_version(self, value):
        if isinstance(value, str):
            try:
                return models.InputKafkaSchemaRegistryMinimumTLSVersionMsk(value)
            except ValueError:
                return value
        return value

    @field_serializer("max_version")
    def serialize_max_version(self, value):
        if isinstance(value, str):
            try:
                return models.InputKafkaSchemaRegistryMaximumTLSVersionMsk(value)
            except ValueError:
                return value
        return value


class InputKafkaSchemaRegistryAuthenticationMskTypedDict(TypedDict):
    disabled: NotRequired[bool]
    schema_registry_url: NotRequired[str]
    r"""URL for accessing the Confluent Schema Registry. Example: http://localhost:8081. To connect over TLS, use https instead of http."""
    connection_timeout: NotRequired[float]
    r"""Maximum time to wait for a Schema Registry connection to complete successfully"""
    request_timeout: NotRequired[float]
    r"""Maximum time to wait for the Schema Registry to respond to a request"""
    max_retries: NotRequired[float]
    r"""Maximum number of times to try fetching schemas from the Schema Registry"""
    auth: NotRequired[InputAuthMskTypedDict]
    r"""Credentials to use when authenticating with the schema registry using basic HTTP authentication"""
    tls: NotRequired[InputKafkaSchemaRegistryTLSSettingsClientSideMskTypedDict]


class InputKafkaSchemaRegistryAuthenticationMsk(BaseModel):
    disabled: Optional[bool] = True

    schema_registry_url: Annotated[
        Optional[str], pydantic.Field(alias="schemaRegistryURL")
    ] = "http://localhost:8081"
    r"""URL for accessing the Confluent Schema Registry. Example: http://localhost:8081. To connect over TLS, use https instead of http."""

    connection_timeout: Annotated[
        Optional[float], pydantic.Field(alias="connectionTimeout")
    ] = 30000
    r"""Maximum time to wait for a Schema Registry connection to complete successfully"""

    request_timeout: Annotated[
        Optional[float], pydantic.Field(alias="requestTimeout")
    ] = 30000
    r"""Maximum time to wait for the Schema Registry to respond to a request"""

    max_retries: Annotated[Optional[float], pydantic.Field(alias="maxRetries")] = 1
    r"""Maximum number of times to try fetching schemas from the Schema Registry"""

    auth: Optional[InputAuthMsk] = None
    r"""Credentials to use when authenticating with the schema registry using basic HTTP authentication"""

    tls: Optional[InputKafkaSchemaRegistryTLSSettingsClientSideMsk] = None


class InputAuthenticationMethodMsk(str, Enum, metaclass=utils.OpenEnumMeta):
    r"""AWS authentication method. Choose Auto to use IAM roles."""

    # Auto
    AUTO = "auto"
    # Manual
    MANUAL = "manual"
    # Secret Key pair
    SECRET = "secret"


class InputSignatureVersionMsk(str, Enum, metaclass=utils.OpenEnumMeta):
    r"""Signature version to use for signing MSK cluster requests"""

    V2 = "v2"
    V4 = "v4"


class InputMinimumTLSVersionMsk(str, Enum, metaclass=utils.OpenEnumMeta):
    TL_SV1 = "TLSv1"
    TL_SV1_1 = "TLSv1.1"
    TL_SV1_2 = "TLSv1.2"
    TL_SV1_3 = "TLSv1.3"


class InputMaximumTLSVersionMsk(str, Enum, metaclass=utils.OpenEnumMeta):
    TL_SV1 = "TLSv1"
    TL_SV1_1 = "TLSv1.1"
    TL_SV1_2 = "TLSv1.2"
    TL_SV1_3 = "TLSv1.3"


class InputTLSSettingsClientSideMskTypedDict(TypedDict):
    disabled: NotRequired[bool]
    reject_unauthorized: NotRequired[bool]
    r"""Reject certificates that are not authorized by a CA in the CA certificate path, or by another
    trusted CA (such as the system's). Defaults to Enabled. Overrides the toggle from Advanced Settings, when also present.
    """
    servername: NotRequired[str]
    r"""Server name for the SNI (Server Name Indication) TLS extension. It must be a host name, and not an IP address."""
    certificate_name: NotRequired[str]
    r"""The name of the predefined certificate"""
    ca_path: NotRequired[str]
    r"""Path on client in which to find CA certificates to verify the server's cert. PEM format. Can reference $ENV_VARS."""
    priv_key_path: NotRequired[str]
    r"""Path on client in which to find the private key to use. PEM format. Can reference $ENV_VARS."""
    cert_path: NotRequired[str]
    r"""Path on client in which to find certificates to use. PEM format. Can reference $ENV_VARS."""
    passphrase: NotRequired[str]
    r"""Passphrase to use to decrypt private key"""
    min_version: NotRequired[InputMinimumTLSVersionMsk]
    max_version: NotRequired[InputMaximumTLSVersionMsk]


class InputTLSSettingsClientSideMsk(BaseModel):
    disabled: Optional[bool] = False

    reject_unauthorized: Annotated[
        Optional[bool], pydantic.Field(alias="rejectUnauthorized")
    ] = True
    r"""Reject certificates that are not authorized by a CA in the CA certificate path, or by another
    trusted CA (such as the system's). Defaults to Enabled. Overrides the toggle from Advanced Settings, when also present.
    """

    servername: Optional[str] = None
    r"""Server name for the SNI (Server Name Indication) TLS extension. It must be a host name, and not an IP address."""

    certificate_name: Annotated[
        Optional[str], pydantic.Field(alias="certificateName")
    ] = None
    r"""The name of the predefined certificate"""

    ca_path: Annotated[Optional[str], pydantic.Field(alias="caPath")] = None
    r"""Path on client in which to find CA certificates to verify the server's cert. PEM format. Can reference $ENV_VARS."""

    priv_key_path: Annotated[Optional[str], pydantic.Field(alias="privKeyPath")] = None
    r"""Path on client in which to find the private key to use. PEM format. Can reference $ENV_VARS."""

    cert_path: Annotated[Optional[str], pydantic.Field(alias="certPath")] = None
    r"""Path on client in which to find certificates to use. PEM format. Can reference $ENV_VARS."""

    passphrase: Optional[str] = None
    r"""Passphrase to use to decrypt private key"""

    min_version: Annotated[
        Annotated[
            Optional[InputMinimumTLSVersionMsk],
            PlainValidator(validate_open_enum(False)),
        ],
        pydantic.Field(alias="minVersion"),
    ] = None

    max_version: Annotated[
        Annotated[
            Optional[InputMaximumTLSVersionMsk],
            PlainValidator(validate_open_enum(False)),
        ],
        pydantic.Field(alias="maxVersion"),
    ] = None

    @field_serializer("min_version")
    def serialize_min_version(self, value):
        if isinstance(value, str):
            try:
                return models.InputMinimumTLSVersionMsk(value)
            except ValueError:
                return value
        return value

    @field_serializer("max_version")
    def serialize_max_version(self, value):
        if isinstance(value, str):
            try:
                return models.InputMaximumTLSVersionMsk(value)
            except ValueError:
                return value
        return value


class InputMskTypedDict(TypedDict):
    type: InputTypeMsk
    brokers: List[str]
    r"""Enter each Kafka bootstrap server you want to use. Specify the hostname and port (such as mykafkabroker:9092) or just the hostname (in which case @{product} will assign port 9092)."""
    topics: List[str]
    r"""Topic to subscribe to. Warning: To optimize performance, Cribl suggests subscribing each Kafka Source to a single topic only."""
    region: str
    r"""Region where the MSK cluster is located"""
    id: NotRequired[str]
    r"""Unique ID for this input"""
    disabled: NotRequired[bool]
    pipeline: NotRequired[str]
    r"""Pipeline to process data from this Source before sending it through the Routes"""
    send_to_routes: NotRequired[bool]
    r"""Select whether to send data to Routes, or directly to Destinations."""
    environment: NotRequired[str]
    r"""Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere."""
    pq_enabled: NotRequired[bool]
    r"""Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers)."""
    streamtags: NotRequired[List[str]]
    r"""Tags for filtering and grouping in @{product}"""
    connections: NotRequired[List[ConnectionMskTypedDict]]
    r"""Direct connections to Destinations, and optionally via a Pipeline or a Pack"""
    pq: NotRequired[PqMskTypedDict]
    group_id: NotRequired[str]
    r"""The consumer group to which this instance belongs. Defaults to 'Cribl'."""
    from_beginning: NotRequired[bool]
    r"""Leave enabled if you want the Source, upon first subscribing to a topic, to read starting with the earliest available message"""
    session_timeout: NotRequired[float]
    r"""Timeout used to detect client failures when using Kafka's group-management facilities.
    If the client sends no heartbeats to the broker before the timeout expires,
    the broker will remove the client from the group and initiate a rebalance.
    Value must be between the broker's configured group.min.session.timeout.ms and group.max.session.timeout.ms.
    See [Kafka's documentation](https://kafka.apache.org/documentation/#consumerconfigs_session.timeout.ms) for details.
    """
    rebalance_timeout: NotRequired[float]
    r"""Maximum allowed time for each worker to join the group after a rebalance begins.
    If the timeout is exceeded, the coordinator broker will remove the worker from the group.
    See [Kafka's documentation](https://kafka.apache.org/documentation/#connectconfigs_rebalance.timeout.ms) for details.
    """
    heartbeat_interval: NotRequired[float]
    r"""Expected time between heartbeats to the consumer coordinator when using Kafka's group-management facilities.
    Value must be lower than sessionTimeout and typically should not exceed 1/3 of the sessionTimeout value.
    See [Kafka's documentation](https://kafka.apache.org/documentation/#consumerconfigs_heartbeat.interval.ms) for details.
    """
    metadata: NotRequired[List[MetadatumMskTypedDict]]
    r"""Fields to add to events from this input"""
    kafka_schema_registry: NotRequired[
        InputKafkaSchemaRegistryAuthenticationMskTypedDict
    ]
    connection_timeout: NotRequired[float]
    r"""Maximum time to wait for a connection to complete successfully"""
    request_timeout: NotRequired[float]
    r"""Maximum time to wait for Kafka to respond to a request"""
    max_retries: NotRequired[float]
    r"""If messages are failing, you can set the maximum number of retries as high as 100 to prevent loss of data"""
    max_back_off: NotRequired[float]
    r"""The maximum wait time for a retry, in milliseconds. Default (and minimum) is 30,000 ms (30 seconds); maximum is 180,000 ms (180 seconds)."""
    initial_backoff: NotRequired[float]
    r"""Initial value used to calculate the retry, in milliseconds. Maximum is 600,000 ms (10 minutes)."""
    backoff_rate: NotRequired[float]
    r"""Set the backoff multiplier (2-20) to control the retry frequency for failed messages. For faster retries, use a lower multiplier. For slower retries with more delay between attempts, use a higher multiplier. The multiplier is used in an exponential backoff formula; see the Kafka [documentation](https://kafka.js.org/docs/retry-detailed) for details."""
    authentication_timeout: NotRequired[float]
    r"""Maximum time to wait for Kafka to respond to an authentication request"""
    reauthentication_threshold: NotRequired[float]
    r"""Specifies a time window during which @{product} can reauthenticate if needed. Creates the window measuring backward from the moment when credentials are set to expire."""
    aws_authentication_method: NotRequired[InputAuthenticationMethodMsk]
    r"""AWS authentication method. Choose Auto to use IAM roles."""
    aws_secret_key: NotRequired[str]
    endpoint: NotRequired[str]
    r"""MSK cluster service endpoint. If empty, defaults to the AWS Region-specific endpoint. Otherwise, it must point to MSK cluster-compatible endpoint."""
    signature_version: NotRequired[InputSignatureVersionMsk]
    r"""Signature version to use for signing MSK cluster requests"""
    reuse_connections: NotRequired[bool]
    r"""Reuse connections between requests, which can improve performance"""
    reject_unauthorized: NotRequired[bool]
    r"""Reject certificates that cannot be verified against a valid CA, such as self-signed certificates"""
    enable_assume_role: NotRequired[bool]
    r"""Use Assume Role credentials to access MSK"""
    assume_role_arn: NotRequired[str]
    r"""Amazon Resource Name (ARN) of the role to assume"""
    assume_role_external_id: NotRequired[str]
    r"""External ID to use when assuming role"""
    duration_seconds: NotRequired[float]
    r"""Duration of the assumed role's session, in seconds. Minimum is 900 (15 minutes), default is 3600 (1 hour), and maximum is 43200 (12 hours)."""
    tls: NotRequired[InputTLSSettingsClientSideMskTypedDict]
    auto_commit_interval: NotRequired[float]
    r"""How often to commit offsets. If both this and Offset commit threshold are set, @{product} commits offsets when either condition is met. If both are empty, @{product} commits offsets after each batch."""
    auto_commit_threshold: NotRequired[float]
    r"""How many events are needed to trigger an offset commit. If both this and Offset commit interval are set, @{product} commits offsets when either condition is met. If both are empty, @{product} commits offsets after each batch."""
    max_bytes_per_partition: NotRequired[float]
    r"""Maximum amount of data that Kafka will return per partition, per fetch request. Must equal or exceed the maximum message size (maxBytesPerPartition) that Kafka is configured to allow. Otherwise, @{product} can get stuck trying to retrieve messages. Defaults to 1048576 (1 MB)."""
    max_bytes: NotRequired[float]
    r"""Maximum number of bytes that Kafka will return per fetch request. Defaults to 10485760 (10 MB)."""
    max_socket_errors: NotRequired[float]
    r"""Maximum number of network errors before the consumer re-creates a socket"""
    description: NotRequired[str]
    aws_api_key: NotRequired[str]
    aws_secret: NotRequired[str]
    r"""Select or create a stored secret that references your access key and secret key"""


class InputMsk(BaseModel):
    model_config = ConfigDict(
        populate_by_name=True, arbitrary_types_allowed=True, extra="allow"
    )
    __pydantic_extra__: Dict[str, Any] = pydantic.Field(init=False)

    type: InputTypeMsk

    brokers: List[str]
    r"""Enter each Kafka bootstrap server you want to use. Specify the hostname and port (such as mykafkabroker:9092) or just the hostname (in which case @{product} will assign port 9092)."""

    topics: List[str]
    r"""Topic to subscribe to. Warning: To optimize performance, Cribl suggests subscribing each Kafka Source to a single topic only."""

    region: str
    r"""Region where the MSK cluster is located"""

    id: Optional[str] = None
    r"""Unique ID for this input"""

    disabled: Optional[bool] = False

    pipeline: Optional[str] = None
    r"""Pipeline to process data from this Source before sending it through the Routes"""

    send_to_routes: Annotated[Optional[bool], pydantic.Field(alias="sendToRoutes")] = (
        True
    )
    r"""Select whether to send data to Routes, or directly to Destinations."""

    environment: Optional[str] = None
    r"""Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere."""

    pq_enabled: Annotated[Optional[bool], pydantic.Field(alias="pqEnabled")] = False
    r"""Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers)."""

    streamtags: Optional[List[str]] = None
    r"""Tags for filtering and grouping in @{product}"""

    connections: Optional[List[ConnectionMsk]] = None
    r"""Direct connections to Destinations, and optionally via a Pipeline or a Pack"""

    pq: Optional[PqMsk] = None

    group_id: Annotated[Optional[str], pydantic.Field(alias="groupId")] = "Cribl"
    r"""The consumer group to which this instance belongs. Defaults to 'Cribl'."""

    from_beginning: Annotated[Optional[bool], pydantic.Field(alias="fromBeginning")] = (
        True
    )
    r"""Leave enabled if you want the Source, upon first subscribing to a topic, to read starting with the earliest available message"""

    session_timeout: Annotated[
        Optional[float], pydantic.Field(alias="sessionTimeout")
    ] = 30000
    r"""Timeout used to detect client failures when using Kafka's group-management facilities.
    If the client sends no heartbeats to the broker before the timeout expires,
    the broker will remove the client from the group and initiate a rebalance.
    Value must be between the broker's configured group.min.session.timeout.ms and group.max.session.timeout.ms.
    See [Kafka's documentation](https://kafka.apache.org/documentation/#consumerconfigs_session.timeout.ms) for details.
    """

    rebalance_timeout: Annotated[
        Optional[float], pydantic.Field(alias="rebalanceTimeout")
    ] = 60000
    r"""Maximum allowed time for each worker to join the group after a rebalance begins.
    If the timeout is exceeded, the coordinator broker will remove the worker from the group.
    See [Kafka's documentation](https://kafka.apache.org/documentation/#connectconfigs_rebalance.timeout.ms) for details.
    """

    heartbeat_interval: Annotated[
        Optional[float], pydantic.Field(alias="heartbeatInterval")
    ] = 3000
    r"""Expected time between heartbeats to the consumer coordinator when using Kafka's group-management facilities.
    Value must be lower than sessionTimeout and typically should not exceed 1/3 of the sessionTimeout value.
    See [Kafka's documentation](https://kafka.apache.org/documentation/#consumerconfigs_heartbeat.interval.ms) for details.
    """

    metadata: Optional[List[MetadatumMsk]] = None
    r"""Fields to add to events from this input"""

    kafka_schema_registry: Annotated[
        Optional[InputKafkaSchemaRegistryAuthenticationMsk],
        pydantic.Field(alias="kafkaSchemaRegistry"),
    ] = None

    connection_timeout: Annotated[
        Optional[float], pydantic.Field(alias="connectionTimeout")
    ] = 10000
    r"""Maximum time to wait for a connection to complete successfully"""

    request_timeout: Annotated[
        Optional[float], pydantic.Field(alias="requestTimeout")
    ] = 60000
    r"""Maximum time to wait for Kafka to respond to a request"""

    max_retries: Annotated[Optional[float], pydantic.Field(alias="maxRetries")] = 5
    r"""If messages are failing, you can set the maximum number of retries as high as 100 to prevent loss of data"""

    max_back_off: Annotated[Optional[float], pydantic.Field(alias="maxBackOff")] = 30000
    r"""The maximum wait time for a retry, in milliseconds. Default (and minimum) is 30,000 ms (30 seconds); maximum is 180,000 ms (180 seconds)."""

    initial_backoff: Annotated[
        Optional[float], pydantic.Field(alias="initialBackoff")
    ] = 300
    r"""Initial value used to calculate the retry, in milliseconds. Maximum is 600,000 ms (10 minutes)."""

    backoff_rate: Annotated[Optional[float], pydantic.Field(alias="backoffRate")] = 2
    r"""Set the backoff multiplier (2-20) to control the retry frequency for failed messages. For faster retries, use a lower multiplier. For slower retries with more delay between attempts, use a higher multiplier. The multiplier is used in an exponential backoff formula; see the Kafka [documentation](https://kafka.js.org/docs/retry-detailed) for details."""

    authentication_timeout: Annotated[
        Optional[float], pydantic.Field(alias="authenticationTimeout")
    ] = 10000
    r"""Maximum time to wait for Kafka to respond to an authentication request"""

    reauthentication_threshold: Annotated[
        Optional[float], pydantic.Field(alias="reauthenticationThreshold")
    ] = 10000
    r"""Specifies a time window during which @{product} can reauthenticate if needed. Creates the window measuring backward from the moment when credentials are set to expire."""

    aws_authentication_method: Annotated[
        Annotated[
            Optional[InputAuthenticationMethodMsk],
            PlainValidator(validate_open_enum(False)),
        ],
        pydantic.Field(alias="awsAuthenticationMethod"),
    ] = InputAuthenticationMethodMsk.AUTO
    r"""AWS authentication method. Choose Auto to use IAM roles."""

    aws_secret_key: Annotated[Optional[str], pydantic.Field(alias="awsSecretKey")] = (
        None
    )

    endpoint: Optional[str] = None
    r"""MSK cluster service endpoint. If empty, defaults to the AWS Region-specific endpoint. Otherwise, it must point to MSK cluster-compatible endpoint."""

    signature_version: Annotated[
        Annotated[
            Optional[InputSignatureVersionMsk],
            PlainValidator(validate_open_enum(False)),
        ],
        pydantic.Field(alias="signatureVersion"),
    ] = InputSignatureVersionMsk.V4
    r"""Signature version to use for signing MSK cluster requests"""

    reuse_connections: Annotated[
        Optional[bool], pydantic.Field(alias="reuseConnections")
    ] = True
    r"""Reuse connections between requests, which can improve performance"""

    reject_unauthorized: Annotated[
        Optional[bool], pydantic.Field(alias="rejectUnauthorized")
    ] = True
    r"""Reject certificates that cannot be verified against a valid CA, such as self-signed certificates"""

    enable_assume_role: Annotated[
        Optional[bool], pydantic.Field(alias="enableAssumeRole")
    ] = False
    r"""Use Assume Role credentials to access MSK"""

    assume_role_arn: Annotated[Optional[str], pydantic.Field(alias="assumeRoleArn")] = (
        None
    )
    r"""Amazon Resource Name (ARN) of the role to assume"""

    assume_role_external_id: Annotated[
        Optional[str], pydantic.Field(alias="assumeRoleExternalId")
    ] = None
    r"""External ID to use when assuming role"""

    duration_seconds: Annotated[
        Optional[float], pydantic.Field(alias="durationSeconds")
    ] = 3600
    r"""Duration of the assumed role's session, in seconds. Minimum is 900 (15 minutes), default is 3600 (1 hour), and maximum is 43200 (12 hours)."""

    tls: Optional[InputTLSSettingsClientSideMsk] = None

    auto_commit_interval: Annotated[
        Optional[float], pydantic.Field(alias="autoCommitInterval")
    ] = None
    r"""How often to commit offsets. If both this and Offset commit threshold are set, @{product} commits offsets when either condition is met. If both are empty, @{product} commits offsets after each batch."""

    auto_commit_threshold: Annotated[
        Optional[float], pydantic.Field(alias="autoCommitThreshold")
    ] = None
    r"""How many events are needed to trigger an offset commit. If both this and Offset commit interval are set, @{product} commits offsets when either condition is met. If both are empty, @{product} commits offsets after each batch."""

    max_bytes_per_partition: Annotated[
        Optional[float], pydantic.Field(alias="maxBytesPerPartition")
    ] = 1048576
    r"""Maximum amount of data that Kafka will return per partition, per fetch request. Must equal or exceed the maximum message size (maxBytesPerPartition) that Kafka is configured to allow. Otherwise, @{product} can get stuck trying to retrieve messages. Defaults to 1048576 (1 MB)."""

    max_bytes: Annotated[Optional[float], pydantic.Field(alias="maxBytes")] = 10485760
    r"""Maximum number of bytes that Kafka will return per fetch request. Defaults to 10485760 (10 MB)."""

    max_socket_errors: Annotated[
        Optional[float], pydantic.Field(alias="maxSocketErrors")
    ] = 0
    r"""Maximum number of network errors before the consumer re-creates a socket"""

    description: Optional[str] = None

    aws_api_key: Annotated[Optional[str], pydantic.Field(alias="awsApiKey")] = None

    aws_secret: Annotated[Optional[str], pydantic.Field(alias="awsSecret")] = None
    r"""Select or create a stored secret that references your access key and secret key"""

    @property
    def additional_properties(self):
        return self.__pydantic_extra__

    @additional_properties.setter
    def additional_properties(self, value):
        self.__pydantic_extra__ = value  # pyright: ignore[reportIncompatibleVariableOverride]

    @field_serializer("aws_authentication_method")
    def serialize_aws_authentication_method(self, value):
        if isinstance(value, str):
            try:
                return models.InputAuthenticationMethodMsk(value)
            except ValueError:
                return value
        return value

    @field_serializer("signature_version")
    def serialize_signature_version(self, value):
        if isinstance(value, str):
            try:
                return models.InputSignatureVersionMsk(value)
            except ValueError:
                return value
        return value


class InputTypeKafka(str, Enum):
    KAFKA = "kafka"


class ConnectionKafkaTypedDict(TypedDict):
    output: str
    pipeline: NotRequired[str]


class ConnectionKafka(BaseModel):
    output: str

    pipeline: Optional[str] = None


class PqModeKafka(str, Enum, metaclass=utils.OpenEnumMeta):
    r"""With Smart mode, PQ will write events to the filesystem only when it detects backpressure from the processing engine. With Always On mode, PQ will always write events directly to the queue before forwarding them to the processing engine."""

    # Smart
    SMART = "smart"
    # Always On
    ALWAYS = "always"


class PqCompressionKafka(str, Enum, metaclass=utils.OpenEnumMeta):
    r"""Codec to use to compress the persisted data"""

    # None
    NONE = "none"
    # Gzip
    GZIP = "gzip"


class InputPqControlsKafkaTypedDict(TypedDict):
    pass


class InputPqControlsKafka(BaseModel):
    pass


class PqKafkaTypedDict(TypedDict):
    mode: NotRequired[PqModeKafka]
    r"""With Smart mode, PQ will write events to the filesystem only when it detects backpressure from the processing engine. With Always On mode, PQ will always write events directly to the queue before forwarding them to the processing engine."""
    max_buffer_size: NotRequired[float]
    r"""The maximum number of events to hold in memory before writing the events to disk"""
    commit_frequency: NotRequired[float]
    r"""The number of events to send downstream before committing that Stream has read them"""
    max_file_size: NotRequired[str]
    r"""The maximum size to store in each queue file before closing and optionally compressing. Enter a numeral with units of KB, MB, etc."""
    max_size: NotRequired[str]
    r"""The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc."""
    path: NotRequired[str]
    r"""The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/inputs/<input-id>"""
    compress: NotRequired[PqCompressionKafka]
    r"""Codec to use to compress the persisted data"""
    pq_controls: NotRequired[InputPqControlsKafkaTypedDict]


class PqKafka(BaseModel):
    mode: Annotated[
        Optional[PqModeKafka], PlainValidator(validate_open_enum(False))
    ] = PqModeKafka.ALWAYS
    r"""With Smart mode, PQ will write events to the filesystem only when it detects backpressure from the processing engine. With Always On mode, PQ will always write events directly to the queue before forwarding them to the processing engine."""

    max_buffer_size: Annotated[
        Optional[float], pydantic.Field(alias="maxBufferSize")
    ] = 1000
    r"""The maximum number of events to hold in memory before writing the events to disk"""

    commit_frequency: Annotated[
        Optional[float], pydantic.Field(alias="commitFrequency")
    ] = 42
    r"""The number of events to send downstream before committing that Stream has read them"""

    max_file_size: Annotated[Optional[str], pydantic.Field(alias="maxFileSize")] = (
        "1 MB"
    )
    r"""The maximum size to store in each queue file before closing and optionally compressing. Enter a numeral with units of KB, MB, etc."""

    max_size: Annotated[Optional[str], pydantic.Field(alias="maxSize")] = "5GB"
    r"""The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc."""

    path: Optional[str] = "$CRIBL_HOME/state/queues"
    r"""The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/inputs/<input-id>"""

    compress: Annotated[
        Optional[PqCompressionKafka], PlainValidator(validate_open_enum(False))
    ] = PqCompressionKafka.NONE
    r"""Codec to use to compress the persisted data"""

    pq_controls: Annotated[
        Optional[InputPqControlsKafka], pydantic.Field(alias="pqControls")
    ] = None

    @field_serializer("mode")
    def serialize_mode(self, value):
        if isinstance(value, str):
            try:
                return models.PqModeKafka(value)
            except ValueError:
                return value
        return value

    @field_serializer("compress")
    def serialize_compress(self, value):
        if isinstance(value, str):
            try:
                return models.PqCompressionKafka(value)
            except ValueError:
                return value
        return value


class InputAuthKafkaTypedDict(TypedDict):
    r"""Credentials to use when authenticating with the schema registry using basic HTTP authentication"""

    disabled: NotRequired[bool]
    credentials_secret: NotRequired[str]
    r"""Select or create a secret that references your credentials"""


class InputAuthKafka(BaseModel):
    r"""Credentials to use when authenticating with the schema registry using basic HTTP authentication"""

    disabled: Optional[bool] = True

    credentials_secret: Annotated[
        Optional[str], pydantic.Field(alias="credentialsSecret")
    ] = None
    r"""Select or create a secret that references your credentials"""


class InputKafkaSchemaRegistryMinimumTLSVersionKafka(
    str, Enum, metaclass=utils.OpenEnumMeta
):
    TL_SV1 = "TLSv1"
    TL_SV1_1 = "TLSv1.1"
    TL_SV1_2 = "TLSv1.2"
    TL_SV1_3 = "TLSv1.3"


class InputKafkaSchemaRegistryMaximumTLSVersionKafka(
    str, Enum, metaclass=utils.OpenEnumMeta
):
    TL_SV1 = "TLSv1"
    TL_SV1_1 = "TLSv1.1"
    TL_SV1_2 = "TLSv1.2"
    TL_SV1_3 = "TLSv1.3"


class InputKafkaSchemaRegistryTLSSettingsClientSideKafkaTypedDict(TypedDict):
    disabled: NotRequired[bool]
    reject_unauthorized: NotRequired[bool]
    r"""Reject certificates that are not authorized by a CA in the CA certificate path, or by another
    trusted CA (such as the system's). Defaults to Enabled. Overrides the toggle from Advanced Settings, when also present.
    """
    servername: NotRequired[str]
    r"""Server name for the SNI (Server Name Indication) TLS extension. It must be a host name, and not an IP address."""
    certificate_name: NotRequired[str]
    r"""The name of the predefined certificate"""
    ca_path: NotRequired[str]
    r"""Path on client in which to find CA certificates to verify the server's cert. PEM format. Can reference $ENV_VARS."""
    priv_key_path: NotRequired[str]
    r"""Path on client in which to find the private key to use. PEM format. Can reference $ENV_VARS."""
    cert_path: NotRequired[str]
    r"""Path on client in which to find certificates to use. PEM format. Can reference $ENV_VARS."""
    passphrase: NotRequired[str]
    r"""Passphrase to use to decrypt private key"""
    min_version: NotRequired[InputKafkaSchemaRegistryMinimumTLSVersionKafka]
    max_version: NotRequired[InputKafkaSchemaRegistryMaximumTLSVersionKafka]


class InputKafkaSchemaRegistryTLSSettingsClientSideKafka(BaseModel):
    disabled: Optional[bool] = True

    reject_unauthorized: Annotated[
        Optional[bool], pydantic.Field(alias="rejectUnauthorized")
    ] = True
    r"""Reject certificates that are not authorized by a CA in the CA certificate path, or by another
    trusted CA (such as the system's). Defaults to Enabled. Overrides the toggle from Advanced Settings, when also present.
    """

    servername: Optional[str] = None
    r"""Server name for the SNI (Server Name Indication) TLS extension. It must be a host name, and not an IP address."""

    certificate_name: Annotated[
        Optional[str], pydantic.Field(alias="certificateName")
    ] = None
    r"""The name of the predefined certificate"""

    ca_path: Annotated[Optional[str], pydantic.Field(alias="caPath")] = None
    r"""Path on client in which to find CA certificates to verify the server's cert. PEM format. Can reference $ENV_VARS."""

    priv_key_path: Annotated[Optional[str], pydantic.Field(alias="privKeyPath")] = None
    r"""Path on client in which to find the private key to use. PEM format. Can reference $ENV_VARS."""

    cert_path: Annotated[Optional[str], pydantic.Field(alias="certPath")] = None
    r"""Path on client in which to find certificates to use. PEM format. Can reference $ENV_VARS."""

    passphrase: Optional[str] = None
    r"""Passphrase to use to decrypt private key"""

    min_version: Annotated[
        Annotated[
            Optional[InputKafkaSchemaRegistryMinimumTLSVersionKafka],
            PlainValidator(validate_open_enum(False)),
        ],
        pydantic.Field(alias="minVersion"),
    ] = None

    max_version: Annotated[
        Annotated[
            Optional[InputKafkaSchemaRegistryMaximumTLSVersionKafka],
            PlainValidator(validate_open_enum(False)),
        ],
        pydantic.Field(alias="maxVersion"),
    ] = None

    @field_serializer("min_version")
    def serialize_min_version(self, value):
        if isinstance(value, str):
            try:
                return models.InputKafkaSchemaRegistryMinimumTLSVersionKafka(value)
            except ValueError:
                return value
        return value

    @field_serializer("max_version")
    def serialize_max_version(self, value):
        if isinstance(value, str):
            try:
                return models.InputKafkaSchemaRegistryMaximumTLSVersionKafka(value)
            except ValueError:
                return value
        return value


class InputKafkaSchemaRegistryAuthenticationKafkaTypedDict(TypedDict):
    disabled: NotRequired[bool]
    schema_registry_url: NotRequired[str]
    r"""URL for accessing the Confluent Schema Registry. Example: http://localhost:8081. To connect over TLS, use https instead of http."""
    connection_timeout: NotRequired[float]
    r"""Maximum time to wait for a Schema Registry connection to complete successfully"""
    request_timeout: NotRequired[float]
    r"""Maximum time to wait for the Schema Registry to respond to a request"""
    max_retries: NotRequired[float]
    r"""Maximum number of times to try fetching schemas from the Schema Registry"""
    auth: NotRequired[InputAuthKafkaTypedDict]
    r"""Credentials to use when authenticating with the schema registry using basic HTTP authentication"""
    tls: NotRequired[InputKafkaSchemaRegistryTLSSettingsClientSideKafkaTypedDict]


class InputKafkaSchemaRegistryAuthenticationKafka(BaseModel):
    disabled: Optional[bool] = True

    schema_registry_url: Annotated[
        Optional[str], pydantic.Field(alias="schemaRegistryURL")
    ] = "http://localhost:8081"
    r"""URL for accessing the Confluent Schema Registry. Example: http://localhost:8081. To connect over TLS, use https instead of http."""

    connection_timeout: Annotated[
        Optional[float], pydantic.Field(alias="connectionTimeout")
    ] = 30000
    r"""Maximum time to wait for a Schema Registry connection to complete successfully"""

    request_timeout: Annotated[
        Optional[float], pydantic.Field(alias="requestTimeout")
    ] = 30000
    r"""Maximum time to wait for the Schema Registry to respond to a request"""

    max_retries: Annotated[Optional[float], pydantic.Field(alias="maxRetries")] = 1
    r"""Maximum number of times to try fetching schemas from the Schema Registry"""

    auth: Optional[InputAuthKafka] = None
    r"""Credentials to use when authenticating with the schema registry using basic HTTP authentication"""

    tls: Optional[InputKafkaSchemaRegistryTLSSettingsClientSideKafka] = None


class InputAuthenticationMethodKafka(str, Enum, metaclass=utils.OpenEnumMeta):
    r"""Enter credentials directly, or select a stored secret"""

    MANUAL = "manual"
    SECRET = "secret"


class InputSASLMechanismKafka(str, Enum, metaclass=utils.OpenEnumMeta):
    # PLAIN
    PLAIN = "plain"
    # SCRAM-SHA-256
    SCRAM_SHA_256 = "scram-sha-256"
    # SCRAM-SHA-512
    SCRAM_SHA_512 = "scram-sha-512"
    # GSSAPI/Kerberos
    KERBEROS = "kerberos"


class InputOauthParamKafkaTypedDict(TypedDict):
    name: str
    value: str


class InputOauthParamKafka(BaseModel):
    name: str

    value: str


class InputSaslExtensionKafkaTypedDict(TypedDict):
    name: str
    value: str


class InputSaslExtensionKafka(BaseModel):
    name: str

    value: str


class InputAuthenticationKafkaTypedDict(TypedDict):
    r"""Authentication parameters to use when connecting to brokers. Using TLS is highly recommended."""

    disabled: NotRequired[bool]
    username: NotRequired[str]
    password: NotRequired[str]
    auth_type: NotRequired[InputAuthenticationMethodKafka]
    r"""Enter credentials directly, or select a stored secret"""
    credentials_secret: NotRequired[str]
    r"""Select or create a secret that references your credentials"""
    mechanism: NotRequired[InputSASLMechanismKafka]
    keytab_location: NotRequired[str]
    r"""Location of keytab file for authentication principal"""
    principal: NotRequired[str]
    r"""Authentication principal, such as `kafka_user@example.com`"""
    broker_service_class: NotRequired[str]
    r"""Kerberos service class for Kafka brokers, such as `kafka`"""
    oauth_enabled: NotRequired[bool]
    r"""Enable OAuth authentication"""
    token_url: NotRequired[str]
    r"""URL of the token endpoint to use for OAuth authentication"""
    client_id: NotRequired[str]
    r"""Client ID to use for OAuth authentication"""
    oauth_secret_type: NotRequired[str]
    client_text_secret: NotRequired[str]
    r"""Select or create a stored text secret"""
    oauth_params: NotRequired[List[InputOauthParamKafkaTypedDict]]
    r"""Additional fields to send to the token endpoint, such as scope or audience"""
    sasl_extensions: NotRequired[List[InputSaslExtensionKafkaTypedDict]]
    r"""Additional SASL extension fields, such as Confluent's logicalCluster or identityPoolId"""


class InputAuthenticationKafka(BaseModel):
    r"""Authentication parameters to use when connecting to brokers. Using TLS is highly recommended."""

    disabled: Optional[bool] = True

    username: Optional[str] = None

    password: Optional[str] = None

    auth_type: Annotated[
        Annotated[
            Optional[InputAuthenticationMethodKafka],
            PlainValidator(validate_open_enum(False)),
        ],
        pydantic.Field(alias="authType"),
    ] = InputAuthenticationMethodKafka.MANUAL
    r"""Enter credentials directly, or select a stored secret"""

    credentials_secret: Annotated[
        Optional[str], pydantic.Field(alias="credentialsSecret")
    ] = None
    r"""Select or create a secret that references your credentials"""

    mechanism: Annotated[
        Optional[InputSASLMechanismKafka], PlainValidator(validate_open_enum(False))
    ] = InputSASLMechanismKafka.PLAIN

    keytab_location: Annotated[
        Optional[str], pydantic.Field(alias="keytabLocation")
    ] = None
    r"""Location of keytab file for authentication principal"""

    principal: Optional[str] = None
    r"""Authentication principal, such as `kafka_user@example.com`"""

    broker_service_class: Annotated[
        Optional[str], pydantic.Field(alias="brokerServiceClass")
    ] = None
    r"""Kerberos service class for Kafka brokers, such as `kafka`"""

    oauth_enabled: Annotated[Optional[bool], pydantic.Field(alias="oauthEnabled")] = (
        False
    )
    r"""Enable OAuth authentication"""

    token_url: Annotated[Optional[str], pydantic.Field(alias="tokenUrl")] = None
    r"""URL of the token endpoint to use for OAuth authentication"""

    client_id: Annotated[Optional[str], pydantic.Field(alias="clientId")] = None
    r"""Client ID to use for OAuth authentication"""

    oauth_secret_type: Annotated[
        Optional[str], pydantic.Field(alias="oauthSecretType")
    ] = "secret"

    client_text_secret: Annotated[
        Optional[str], pydantic.Field(alias="clientTextSecret")
    ] = None
    r"""Select or create a stored text secret"""

    oauth_params: Annotated[
        Optional[List[InputOauthParamKafka]], pydantic.Field(alias="oauthParams")
    ] = None
    r"""Additional fields to send to the token endpoint, such as scope or audience"""

    sasl_extensions: Annotated[
        Optional[List[InputSaslExtensionKafka]], pydantic.Field(alias="saslExtensions")
    ] = None
    r"""Additional SASL extension fields, such as Confluent's logicalCluster or identityPoolId"""

    @field_serializer("auth_type")
    def serialize_auth_type(self, value):
        if isinstance(value, str):
            try:
                return models.InputAuthenticationMethodKafka(value)
            except ValueError:
                return value
        return value

    @field_serializer("mechanism")
    def serialize_mechanism(self, value):
        if isinstance(value, str):
            try:
                return models.InputSASLMechanismKafka(value)
            except ValueError:
                return value
        return value


class InputMinimumTLSVersionKafka(str, Enum, metaclass=utils.OpenEnumMeta):
    TL_SV1 = "TLSv1"
    TL_SV1_1 = "TLSv1.1"
    TL_SV1_2 = "TLSv1.2"
    TL_SV1_3 = "TLSv1.3"


class InputMaximumTLSVersionKafka(str, Enum, metaclass=utils.OpenEnumMeta):
    TL_SV1 = "TLSv1"
    TL_SV1_1 = "TLSv1.1"
    TL_SV1_2 = "TLSv1.2"
    TL_SV1_3 = "TLSv1.3"


class InputTLSSettingsClientSideKafkaTypedDict(TypedDict):
    disabled: NotRequired[bool]
    reject_unauthorized: NotRequired[bool]
    r"""Reject certificates that are not authorized by a CA in the CA certificate path, or by another
    trusted CA (such as the system's). Defaults to Enabled. Overrides the toggle from Advanced Settings, when also present.
    """
    servername: NotRequired[str]
    r"""Server name for the SNI (Server Name Indication) TLS extension. It must be a host name, and not an IP address."""
    certificate_name: NotRequired[str]
    r"""The name of the predefined certificate"""
    ca_path: NotRequired[str]
    r"""Path on client in which to find CA certificates to verify the server's cert. PEM format. Can reference $ENV_VARS."""
    priv_key_path: NotRequired[str]
    r"""Path on client in which to find the private key to use. PEM format. Can reference $ENV_VARS."""
    cert_path: NotRequired[str]
    r"""Path on client in which to find certificates to use. PEM format. Can reference $ENV_VARS."""
    passphrase: NotRequired[str]
    r"""Passphrase to use to decrypt private key"""
    min_version: NotRequired[InputMinimumTLSVersionKafka]
    max_version: NotRequired[InputMaximumTLSVersionKafka]


class InputTLSSettingsClientSideKafka(BaseModel):
    disabled: Optional[bool] = True

    reject_unauthorized: Annotated[
        Optional[bool], pydantic.Field(alias="rejectUnauthorized")
    ] = True
    r"""Reject certificates that are not authorized by a CA in the CA certificate path, or by another
    trusted CA (such as the system's). Defaults to Enabled. Overrides the toggle from Advanced Settings, when also present.
    """

    servername: Optional[str] = None
    r"""Server name for the SNI (Server Name Indication) TLS extension. It must be a host name, and not an IP address."""

    certificate_name: Annotated[
        Optional[str], pydantic.Field(alias="certificateName")
    ] = None
    r"""The name of the predefined certificate"""

    ca_path: Annotated[Optional[str], pydantic.Field(alias="caPath")] = None
    r"""Path on client in which to find CA certificates to verify the server's cert. PEM format. Can reference $ENV_VARS."""

    priv_key_path: Annotated[Optional[str], pydantic.Field(alias="privKeyPath")] = None
    r"""Path on client in which to find the private key to use. PEM format. Can reference $ENV_VARS."""

    cert_path: Annotated[Optional[str], pydantic.Field(alias="certPath")] = None
    r"""Path on client in which to find certificates to use. PEM format. Can reference $ENV_VARS."""

    passphrase: Optional[str] = None
    r"""Passphrase to use to decrypt private key"""

    min_version: Annotated[
        Annotated[
            Optional[InputMinimumTLSVersionKafka],
            PlainValidator(validate_open_enum(False)),
        ],
        pydantic.Field(alias="minVersion"),
    ] = None

    max_version: Annotated[
        Annotated[
            Optional[InputMaximumTLSVersionKafka],
            PlainValidator(validate_open_enum(False)),
        ],
        pydantic.Field(alias="maxVersion"),
    ] = None

    @field_serializer("min_version")
    def serialize_min_version(self, value):
        if isinstance(value, str):
            try:
                return models.InputMinimumTLSVersionKafka(value)
            except ValueError:
                return value
        return value

    @field_serializer("max_version")
    def serialize_max_version(self, value):
        if isinstance(value, str):
            try:
                return models.InputMaximumTLSVersionKafka(value)
            except ValueError:
                return value
        return value


class MetadatumKafkaTypedDict(TypedDict):
    name: str
    value: str
    r"""JavaScript expression to compute field's value, enclosed in quotes or backticks. (Can evaluate to a constant.)"""


class MetadatumKafka(BaseModel):
    name: str

    value: str
    r"""JavaScript expression to compute field's value, enclosed in quotes or backticks. (Can evaluate to a constant.)"""


class InputKafkaTypedDict(TypedDict):
    type: InputTypeKafka
    brokers: List[str]
    r"""Enter each Kafka bootstrap server you want to use. Specify the hostname and port (such as mykafkabroker:9092) or just the hostname (in which case @{product} will assign port 9092)."""
    topics: List[str]
    r"""Topic to subscribe to. Warning: To optimize performance, Cribl suggests subscribing each Kafka Source to a single topic only."""
    id: NotRequired[str]
    r"""Unique ID for this input"""
    disabled: NotRequired[bool]
    pipeline: NotRequired[str]
    r"""Pipeline to process data from this Source before sending it through the Routes"""
    send_to_routes: NotRequired[bool]
    r"""Select whether to send data to Routes, or directly to Destinations."""
    environment: NotRequired[str]
    r"""Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere."""
    pq_enabled: NotRequired[bool]
    r"""Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers)."""
    streamtags: NotRequired[List[str]]
    r"""Tags for filtering and grouping in @{product}"""
    connections: NotRequired[List[ConnectionKafkaTypedDict]]
    r"""Direct connections to Destinations, and optionally via a Pipeline or a Pack"""
    pq: NotRequired[PqKafkaTypedDict]
    group_id: NotRequired[str]
    r"""The consumer group to which this instance belongs. Defaults to 'Cribl'."""
    from_beginning: NotRequired[bool]
    r"""Leave enabled if you want the Source, upon first subscribing to a topic, to read starting with the earliest available message"""
    kafka_schema_registry: NotRequired[
        InputKafkaSchemaRegistryAuthenticationKafkaTypedDict
    ]
    connection_timeout: NotRequired[float]
    r"""Maximum time to wait for a connection to complete successfully"""
    request_timeout: NotRequired[float]
    r"""Maximum time to wait for Kafka to respond to a request"""
    max_retries: NotRequired[float]
    r"""If messages are failing, you can set the maximum number of retries as high as 100 to prevent loss of data"""
    max_back_off: NotRequired[float]
    r"""The maximum wait time for a retry, in milliseconds. Default (and minimum) is 30,000 ms (30 seconds); maximum is 180,000 ms (180 seconds)."""
    initial_backoff: NotRequired[float]
    r"""Initial value used to calculate the retry, in milliseconds. Maximum is 600,000 ms (10 minutes)."""
    backoff_rate: NotRequired[float]
    r"""Set the backoff multiplier (2-20) to control the retry frequency for failed messages. For faster retries, use a lower multiplier. For slower retries with more delay between attempts, use a higher multiplier. The multiplier is used in an exponential backoff formula; see the Kafka [documentation](https://kafka.js.org/docs/retry-detailed) for details."""
    authentication_timeout: NotRequired[float]
    r"""Maximum time to wait for Kafka to respond to an authentication request"""
    reauthentication_threshold: NotRequired[float]
    r"""Specifies a time window during which @{product} can reauthenticate if needed. Creates the window measuring backward from the moment when credentials are set to expire."""
    sasl: NotRequired[InputAuthenticationKafkaTypedDict]
    r"""Authentication parameters to use when connecting to brokers. Using TLS is highly recommended."""
    tls: NotRequired[InputTLSSettingsClientSideKafkaTypedDict]
    session_timeout: NotRequired[float]
    r"""Timeout used to detect client failures when using Kafka's group-management facilities.
    If the client sends no heartbeats to the broker before the timeout expires,
    the broker will remove the client from the group and initiate a rebalance.
    Value must be between the broker's configured group.min.session.timeout.ms and group.max.session.timeout.ms.
    See [Kafka's documentation](https://kafka.apache.org/documentation/#consumerconfigs_session.timeout.ms) for details.
    """
    rebalance_timeout: NotRequired[float]
    r"""Maximum allowed time for each worker to join the group after a rebalance begins.
    If the timeout is exceeded, the coordinator broker will remove the worker from the group.
    See [Kafka's documentation](https://kafka.apache.org/documentation/#connectconfigs_rebalance.timeout.ms) for details.
    """
    heartbeat_interval: NotRequired[float]
    r"""Expected time between heartbeats to the consumer coordinator when using Kafka's group-management facilities.
    Value must be lower than sessionTimeout and typically should not exceed 1/3 of the sessionTimeout value.
    See [Kafka's documentation](https://kafka.apache.org/documentation/#consumerconfigs_heartbeat.interval.ms) for details.
    """
    auto_commit_interval: NotRequired[float]
    r"""How often to commit offsets. If both this and Offset commit threshold are set, @{product} commits offsets when either condition is met. If both are empty, @{product} commits offsets after each batch."""
    auto_commit_threshold: NotRequired[float]
    r"""How many events are needed to trigger an offset commit. If both this and Offset commit interval are set, @{product} commits offsets when either condition is met. If both are empty, @{product} commits offsets after each batch."""
    max_bytes_per_partition: NotRequired[float]
    r"""Maximum amount of data that Kafka will return per partition, per fetch request. Must equal or exceed the maximum message size (maxBytesPerPartition) that Kafka is configured to allow. Otherwise, @{product} can get stuck trying to retrieve messages. Defaults to 1048576 (1 MB)."""
    max_bytes: NotRequired[float]
    r"""Maximum number of bytes that Kafka will return per fetch request. Defaults to 10485760 (10 MB)."""
    max_socket_errors: NotRequired[float]
    r"""Maximum number of network errors before the consumer re-creates a socket"""
    metadata: NotRequired[List[MetadatumKafkaTypedDict]]
    r"""Fields to add to events from this input"""
    description: NotRequired[str]


class InputKafka(BaseModel):
    model_config = ConfigDict(
        populate_by_name=True, arbitrary_types_allowed=True, extra="allow"
    )
    __pydantic_extra__: Dict[str, Any] = pydantic.Field(init=False)

    type: InputTypeKafka

    brokers: List[str]
    r"""Enter each Kafka bootstrap server you want to use. Specify the hostname and port (such as mykafkabroker:9092) or just the hostname (in which case @{product} will assign port 9092)."""

    topics: List[str]
    r"""Topic to subscribe to. Warning: To optimize performance, Cribl suggests subscribing each Kafka Source to a single topic only."""

    id: Optional[str] = None
    r"""Unique ID for this input"""

    disabled: Optional[bool] = False

    pipeline: Optional[str] = None
    r"""Pipeline to process data from this Source before sending it through the Routes"""

    send_to_routes: Annotated[Optional[bool], pydantic.Field(alias="sendToRoutes")] = (
        True
    )
    r"""Select whether to send data to Routes, or directly to Destinations."""

    environment: Optional[str] = None
    r"""Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere."""

    pq_enabled: Annotated[Optional[bool], pydantic.Field(alias="pqEnabled")] = False
    r"""Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers)."""

    streamtags: Optional[List[str]] = None
    r"""Tags for filtering and grouping in @{product}"""

    connections: Optional[List[ConnectionKafka]] = None
    r"""Direct connections to Destinations, and optionally via a Pipeline or a Pack"""

    pq: Optional[PqKafka] = None

    group_id: Annotated[Optional[str], pydantic.Field(alias="groupId")] = "Cribl"
    r"""The consumer group to which this instance belongs. Defaults to 'Cribl'."""

    from_beginning: Annotated[Optional[bool], pydantic.Field(alias="fromBeginning")] = (
        True
    )
    r"""Leave enabled if you want the Source, upon first subscribing to a topic, to read starting with the earliest available message"""

    kafka_schema_registry: Annotated[
        Optional[InputKafkaSchemaRegistryAuthenticationKafka],
        pydantic.Field(alias="kafkaSchemaRegistry"),
    ] = None

    connection_timeout: Annotated[
        Optional[float], pydantic.Field(alias="connectionTimeout")
    ] = 10000
    r"""Maximum time to wait for a connection to complete successfully"""

    request_timeout: Annotated[
        Optional[float], pydantic.Field(alias="requestTimeout")
    ] = 60000
    r"""Maximum time to wait for Kafka to respond to a request"""

    max_retries: Annotated[Optional[float], pydantic.Field(alias="maxRetries")] = 5
    r"""If messages are failing, you can set the maximum number of retries as high as 100 to prevent loss of data"""

    max_back_off: Annotated[Optional[float], pydantic.Field(alias="maxBackOff")] = 30000
    r"""The maximum wait time for a retry, in milliseconds. Default (and minimum) is 30,000 ms (30 seconds); maximum is 180,000 ms (180 seconds)."""

    initial_backoff: Annotated[
        Optional[float], pydantic.Field(alias="initialBackoff")
    ] = 300
    r"""Initial value used to calculate the retry, in milliseconds. Maximum is 600,000 ms (10 minutes)."""

    backoff_rate: Annotated[Optional[float], pydantic.Field(alias="backoffRate")] = 2
    r"""Set the backoff multiplier (2-20) to control the retry frequency for failed messages. For faster retries, use a lower multiplier. For slower retries with more delay between attempts, use a higher multiplier. The multiplier is used in an exponential backoff formula; see the Kafka [documentation](https://kafka.js.org/docs/retry-detailed) for details."""

    authentication_timeout: Annotated[
        Optional[float], pydantic.Field(alias="authenticationTimeout")
    ] = 10000
    r"""Maximum time to wait for Kafka to respond to an authentication request"""

    reauthentication_threshold: Annotated[
        Optional[float], pydantic.Field(alias="reauthenticationThreshold")
    ] = 10000
    r"""Specifies a time window during which @{product} can reauthenticate if needed. Creates the window measuring backward from the moment when credentials are set to expire."""

    sasl: Optional[InputAuthenticationKafka] = None
    r"""Authentication parameters to use when connecting to brokers. Using TLS is highly recommended."""

    tls: Optional[InputTLSSettingsClientSideKafka] = None

    session_timeout: Annotated[
        Optional[float], pydantic.Field(alias="sessionTimeout")
    ] = 30000
    r"""Timeout used to detect client failures when using Kafka's group-management facilities.
    If the client sends no heartbeats to the broker before the timeout expires,
    the broker will remove the client from the group and initiate a rebalance.
    Value must be between the broker's configured group.min.session.timeout.ms and group.max.session.timeout.ms.
    See [Kafka's documentation](https://kafka.apache.org/documentation/#consumerconfigs_session.timeout.ms) for details.
    """

    rebalance_timeout: Annotated[
        Optional[float], pydantic.Field(alias="rebalanceTimeout")
    ] = 60000
    r"""Maximum allowed time for each worker to join the group after a rebalance begins.
    If the timeout is exceeded, the coordinator broker will remove the worker from the group.
    See [Kafka's documentation](https://kafka.apache.org/documentation/#connectconfigs_rebalance.timeout.ms) for details.
    """

    heartbeat_interval: Annotated[
        Optional[float], pydantic.Field(alias="heartbeatInterval")
    ] = 3000
    r"""Expected time between heartbeats to the consumer coordinator when using Kafka's group-management facilities.
    Value must be lower than sessionTimeout and typically should not exceed 1/3 of the sessionTimeout value.
    See [Kafka's documentation](https://kafka.apache.org/documentation/#consumerconfigs_heartbeat.interval.ms) for details.
    """

    auto_commit_interval: Annotated[
        Optional[float], pydantic.Field(alias="autoCommitInterval")
    ] = None
    r"""How often to commit offsets. If both this and Offset commit threshold are set, @{product} commits offsets when either condition is met. If both are empty, @{product} commits offsets after each batch."""

    auto_commit_threshold: Annotated[
        Optional[float], pydantic.Field(alias="autoCommitThreshold")
    ] = None
    r"""How many events are needed to trigger an offset commit. If both this and Offset commit interval are set, @{product} commits offsets when either condition is met. If both are empty, @{product} commits offsets after each batch."""

    max_bytes_per_partition: Annotated[
        Optional[float], pydantic.Field(alias="maxBytesPerPartition")
    ] = 1048576
    r"""Maximum amount of data that Kafka will return per partition, per fetch request. Must equal or exceed the maximum message size (maxBytesPerPartition) that Kafka is configured to allow. Otherwise, @{product} can get stuck trying to retrieve messages. Defaults to 1048576 (1 MB)."""

    max_bytes: Annotated[Optional[float], pydantic.Field(alias="maxBytes")] = 10485760
    r"""Maximum number of bytes that Kafka will return per fetch request. Defaults to 10485760 (10 MB)."""

    max_socket_errors: Annotated[
        Optional[float], pydantic.Field(alias="maxSocketErrors")
    ] = 0
    r"""Maximum number of network errors before the consumer re-creates a socket"""

    metadata: Optional[List[MetadatumKafka]] = None
    r"""Fields to add to events from this input"""

    description: Optional[str] = None

    @property
    def additional_properties(self):
        return self.__pydantic_extra__

    @additional_properties.setter
    def additional_properties(self, value):
        self.__pydantic_extra__ = value  # pyright: ignore[reportIncompatibleVariableOverride]


class InputTypeCollection(str, Enum):
    COLLECTION = "collection"


class ConnectionCollectionTypedDict(TypedDict):
    output: str
    pipeline: NotRequired[str]


class ConnectionCollection(BaseModel):
    output: str

    pipeline: Optional[str] = None


class ModeCollection(str, Enum, metaclass=utils.OpenEnumMeta):
    r"""With Smart mode, PQ will write events to the filesystem only when it detects backpressure from the processing engine. With Always On mode, PQ will always write events directly to the queue before forwarding them to the processing engine."""

    # Smart
    SMART = "smart"
    # Always On
    ALWAYS = "always"


class CompressionCollection(str, Enum, metaclass=utils.OpenEnumMeta):
    r"""Codec to use to compress the persisted data"""

    # None
    NONE = "none"
    # Gzip
    GZIP = "gzip"


class PqControlsCollectionTypedDict(TypedDict):
    pass


class PqControlsCollection(BaseModel):
    pass


class PqCollectionTypedDict(TypedDict):
    mode: NotRequired[ModeCollection]
    r"""With Smart mode, PQ will write events to the filesystem only when it detects backpressure from the processing engine. With Always On mode, PQ will always write events directly to the queue before forwarding them to the processing engine."""
    max_buffer_size: NotRequired[float]
    r"""The maximum number of events to hold in memory before writing the events to disk"""
    commit_frequency: NotRequired[float]
    r"""The number of events to send downstream before committing that Stream has read them"""
    max_file_size: NotRequired[str]
    r"""The maximum size to store in each queue file before closing and optionally compressing. Enter a numeral with units of KB, MB, etc."""
    max_size: NotRequired[str]
    r"""The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc."""
    path: NotRequired[str]
    r"""The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/inputs/<input-id>"""
    compress: NotRequired[CompressionCollection]
    r"""Codec to use to compress the persisted data"""
    pq_controls: NotRequired[PqControlsCollectionTypedDict]


class PqCollection(BaseModel):
    mode: Annotated[
        Optional[ModeCollection], PlainValidator(validate_open_enum(False))
    ] = ModeCollection.ALWAYS
    r"""With Smart mode, PQ will write events to the filesystem only when it detects backpressure from the processing engine. With Always On mode, PQ will always write events directly to the queue before forwarding them to the processing engine."""

    max_buffer_size: Annotated[
        Optional[float], pydantic.Field(alias="maxBufferSize")
    ] = 1000
    r"""The maximum number of events to hold in memory before writing the events to disk"""

    commit_frequency: Annotated[
        Optional[float], pydantic.Field(alias="commitFrequency")
    ] = 42
    r"""The number of events to send downstream before committing that Stream has read them"""

    max_file_size: Annotated[Optional[str], pydantic.Field(alias="maxFileSize")] = (
        "1 MB"
    )
    r"""The maximum size to store in each queue file before closing and optionally compressing. Enter a numeral with units of KB, MB, etc."""

    max_size: Annotated[Optional[str], pydantic.Field(alias="maxSize")] = "5GB"
    r"""The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc."""

    path: Optional[str] = "$CRIBL_HOME/state/queues"
    r"""The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/inputs/<input-id>"""

    compress: Annotated[
        Optional[CompressionCollection], PlainValidator(validate_open_enum(False))
    ] = CompressionCollection.NONE
    r"""Codec to use to compress the persisted data"""

    pq_controls: Annotated[
        Optional[PqControlsCollection], pydantic.Field(alias="pqControls")
    ] = None

    @field_serializer("mode")
    def serialize_mode(self, value):
        if isinstance(value, str):
            try:
                return models.ModeCollection(value)
            except ValueError:
                return value
        return value

    @field_serializer("compress")
    def serialize_compress(self, value):
        if isinstance(value, str):
            try:
                return models.CompressionCollection(value)
            except ValueError:
                return value
        return value


class PreprocessCollectionTypedDict(TypedDict):
    disabled: NotRequired[bool]
    command: NotRequired[str]
    r"""Command to feed the data through (via stdin) and process its output (stdout)"""
    args: NotRequired[List[str]]
    r"""Arguments to be added to the custom command"""


class PreprocessCollection(BaseModel):
    disabled: Optional[bool] = True

    command: Optional[str] = None
    r"""Command to feed the data through (via stdin) and process its output (stdout)"""

    args: Optional[List[str]] = None
    r"""Arguments to be added to the custom command"""


class MetadatumCollectionTypedDict(TypedDict):
    name: str
    value: str
    r"""JavaScript expression to compute field's value, enclosed in quotes or backticks. (Can evaluate to a constant.)"""


class MetadatumCollection(BaseModel):
    name: str

    value: str
    r"""JavaScript expression to compute field's value, enclosed in quotes or backticks. (Can evaluate to a constant.)"""


class InputCollectionTypedDict(TypedDict):
    id: NotRequired[str]
    r"""Unique ID for this input"""
    type: NotRequired[InputTypeCollection]
    disabled: NotRequired[bool]
    pipeline: NotRequired[str]
    r"""Pipeline to process results"""
    send_to_routes: NotRequired[bool]
    r"""Send events to normal routing and event processing. Disable to select a specific Pipeline/Destination combination."""
    environment: NotRequired[str]
    r"""Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere."""
    pq_enabled: NotRequired[bool]
    r"""Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers)."""
    streamtags: NotRequired[List[str]]
    r"""Tags for filtering and grouping in @{product}"""
    connections: NotRequired[List[ConnectionCollectionTypedDict]]
    r"""Direct connections to Destinations, and optionally via a Pipeline or a Pack"""
    pq: NotRequired[PqCollectionTypedDict]
    breaker_rulesets: NotRequired[List[str]]
    r"""A list of event-breaking rulesets that will be applied, in order, to the input data stream"""
    stale_channel_flush_ms: NotRequired[float]
    r"""How long (in milliseconds) the Event Breaker will wait for new data to be sent to a specific channel before flushing the data stream out, as is, to the Pipelines"""
    preprocess: NotRequired[PreprocessCollectionTypedDict]
    throttle_rate_per_sec: NotRequired[str]
    r"""Rate (in bytes per second) to throttle while writing to an output. Accepts values with multiple-byte units, such as KB, MB, and GB. (Example: 42 MB) Default value of 0 specifies no throttling."""
    metadata: NotRequired[List[MetadatumCollectionTypedDict]]
    r"""Fields to add to events from this input"""
    output: NotRequired[str]
    r"""Destination to send results to"""


class InputCollection(BaseModel):
    model_config = ConfigDict(
        populate_by_name=True, arbitrary_types_allowed=True, extra="allow"
    )
    __pydantic_extra__: Dict[str, Any] = pydantic.Field(init=False)

    id: Optional[str] = None
    r"""Unique ID for this input"""

    type: Optional[InputTypeCollection] = InputTypeCollection.COLLECTION

    disabled: Optional[bool] = False

    pipeline: Optional[str] = None
    r"""Pipeline to process results"""

    send_to_routes: Annotated[Optional[bool], pydantic.Field(alias="sendToRoutes")] = (
        True
    )
    r"""Send events to normal routing and event processing. Disable to select a specific Pipeline/Destination combination."""

    environment: Optional[str] = None
    r"""Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere."""

    pq_enabled: Annotated[Optional[bool], pydantic.Field(alias="pqEnabled")] = False
    r"""Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers)."""

    streamtags: Optional[List[str]] = None
    r"""Tags for filtering and grouping in @{product}"""

    connections: Optional[List[ConnectionCollection]] = None
    r"""Direct connections to Destinations, and optionally via a Pipeline or a Pack"""

    pq: Optional[PqCollection] = None

    breaker_rulesets: Annotated[
        Optional[List[str]], pydantic.Field(alias="breakerRulesets")
    ] = None
    r"""A list of event-breaking rulesets that will be applied, in order, to the input data stream"""

    stale_channel_flush_ms: Annotated[
        Optional[float], pydantic.Field(alias="staleChannelFlushMs")
    ] = 10000
    r"""How long (in milliseconds) the Event Breaker will wait for new data to be sent to a specific channel before flushing the data stream out, as is, to the Pipelines"""

    preprocess: Optional[PreprocessCollection] = None

    throttle_rate_per_sec: Annotated[
        Optional[str], pydantic.Field(alias="throttleRatePerSec")
    ] = "0"
    r"""Rate (in bytes per second) to throttle while writing to an output. Accepts values with multiple-byte units, such as KB, MB, and GB. (Example: 42 MB) Default value of 0 specifies no throttling."""

    metadata: Optional[List[MetadatumCollection]] = None
    r"""Fields to add to events from this input"""

    output: Optional[str] = None
    r"""Destination to send results to"""

    @property
    def additional_properties(self):
        return self.__pydantic_extra__

    @additional_properties.setter
    def additional_properties(self, value):
        self.__pydantic_extra__ = value  # pyright: ignore[reportIncompatibleVariableOverride]


InputTypedDict = TypeAliasType(
    "InputTypedDict",
    Union[
        InputCriblTypedDict,
        InputKubeEventsTypedDict,
        InputDatagenTypedDict,
        InputCriblmetricsTypedDict,
        InputKubeMetricsTypedDict,
        InputSystemStateTypedDict,
        InputCollectionTypedDict,
        InputModelDrivenTelemetryTypedDict,
        InputWindowsMetricsTypedDict,
        InputSystemMetricsTypedDict,
        InputJournalFilesTypedDict,
        InputRawUDPTypedDict,
        InputKubeLogsTypedDict,
        InputExecTypedDict,
        InputMetricsTypedDict,
        InputSnmpTypedDict,
        InputWinEventLogsTypedDict,
        InputCriblTCPTypedDict,
        InputNetflowTypedDict,
        InputGooglePubsubTypedDict,
        InputTcpjsonTypedDict,
        InputOffice365ServiceTypedDict,
        InputWizTypedDict,
        InputFirehoseTypedDict,
        InputCriblHTTPTypedDict,
        InputOffice365MgmtTypedDict,
        InputDatadogAgentTypedDict,
        InputTCPTypedDict,
        InputSplunkTypedDict,
        InputFileTypedDict,
        InputWefTypedDict,
        InputAppscopeTypedDict,
        InputWizWebhookTypedDict,
        InputHTTPTypedDict,
        InputHTTPRawTypedDict,
        InputCriblLakeHTTPTypedDict,
        InputAzureBlobTypedDict,
        InputZscalerHecTypedDict,
        InputSqsTypedDict,
        InputCloudflareHecTypedDict,
        InputConfluentCloudTypedDict,
        InputKafkaTypedDict,
        InputEventhubTypedDict,
        InputKinesisTypedDict,
        InputElasticTypedDict,
        InputOffice365MsgTraceTypedDict,
        InputInputSplunkHecTypedDict,
        InputPrometheusRwTypedDict,
        InputLokiTypedDict,
        InputCrowdstrikeTypedDict,
        InputEdgePrometheusTypedDict,
        InputOpenTelemetryTypedDict,
        InputPrometheusTypedDict,
        InputS3TypedDict,
        InputSecurityLakeTypedDict,
        InputMskTypedDict,
        InputS3InventoryTypedDict,
        InputSplunkSearchTypedDict,
        InputSyslogTypedDict,
        InputGrafanaTypedDict,
    ],
)


Input = Annotated[
    Union[
        Annotated[InputCollection, Tag("collection")],
        Annotated[InputKafka, Tag("kafka")],
        Annotated[InputMsk, Tag("msk")],
        Annotated[InputHTTP, Tag("http")],
        Annotated[InputSplunk, Tag("splunk")],
        Annotated[InputSplunkSearch, Tag("splunk_search")],
        Annotated[InputInputSplunkHec, Tag("splunk_hec")],
        Annotated[InputAzureBlob, Tag("azure_blob")],
        Annotated[InputElastic, Tag("elastic")],
        Annotated[InputConfluentCloud, Tag("confluent_cloud")],
        Annotated[InputGrafana, Tag("grafana")],
        Annotated[InputLoki, Tag("loki")],
        Annotated[InputPrometheusRw, Tag("prometheus_rw")],
        Annotated[InputPrometheus, Tag("prometheus")],
        Annotated[InputEdgePrometheus, Tag("edge_prometheus")],
        Annotated[InputOffice365Mgmt, Tag("office365_mgmt")],
        Annotated[InputOffice365Service, Tag("office365_service")],
        Annotated[InputOffice365MsgTrace, Tag("office365_msg_trace")],
        Annotated[InputEventhub, Tag("eventhub")],
        Annotated[InputExec, Tag("exec")],
        Annotated[InputFirehose, Tag("firehose")],
        Annotated[InputGooglePubsub, Tag("google_pubsub")],
        Annotated[InputCribl, Tag("cribl")],
        Annotated[InputCriblTCP, Tag("cribl_tcp")],
        Annotated[InputCriblHTTP, Tag("cribl_http")],
        Annotated[InputCriblLakeHTTP, Tag("cribl_lake_http")],
        Annotated[InputTcpjson, Tag("tcpjson")],
        Annotated[InputSystemMetrics, Tag("system_metrics")],
        Annotated[InputSystemState, Tag("system_state")],
        Annotated[InputKubeMetrics, Tag("kube_metrics")],
        Annotated[InputKubeLogs, Tag("kube_logs")],
        Annotated[InputKubeEvents, Tag("kube_events")],
        Annotated[InputWindowsMetrics, Tag("windows_metrics")],
        Annotated[InputCrowdstrike, Tag("crowdstrike")],
        Annotated[InputDatadogAgent, Tag("datadog_agent")],
        Annotated[InputDatagen, Tag("datagen")],
        Annotated[InputHTTPRaw, Tag("http_raw")],
        Annotated[InputKinesis, Tag("kinesis")],
        Annotated[InputCriblmetrics, Tag("criblmetrics")],
        Annotated[InputMetrics, Tag("metrics")],
        Annotated[InputS3, Tag("s3")],
        Annotated[InputS3Inventory, Tag("s3_inventory")],
        Annotated[InputSnmp, Tag("snmp")],
        Annotated[InputOpenTelemetry, Tag("open_telemetry")],
        Annotated[InputModelDrivenTelemetry, Tag("model_driven_telemetry")],
        Annotated[InputSqs, Tag("sqs")],
        Annotated[InputSyslog, Tag("syslog")],
        Annotated[InputFile, Tag("file")],
        Annotated[InputTCP, Tag("tcp")],
        Annotated[InputAppscope, Tag("appscope")],
        Annotated[InputWef, Tag("wef")],
        Annotated[InputWinEventLogs, Tag("win_event_logs")],
        Annotated[InputRawUDP, Tag("raw_udp")],
        Annotated[InputJournalFiles, Tag("journal_files")],
        Annotated[InputWiz, Tag("wiz")],
        Annotated[InputWizWebhook, Tag("wiz_webhook")],
        Annotated[InputNetflow, Tag("netflow")],
        Annotated[InputSecurityLake, Tag("security_lake")],
        Annotated[InputZscalerHec, Tag("zscaler_hec")],
        Annotated[InputCloudflareHec, Tag("cloudflare_hec")],
    ],
    Discriminator(lambda m: get_discriminator(m, "type", "type")),
]

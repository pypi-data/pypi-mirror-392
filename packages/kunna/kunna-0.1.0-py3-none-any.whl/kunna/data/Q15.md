# Q15

## **Question 15 – (Solar.csv)**

---

## **1. (Using Solar.csv)**

- Read the data
- Identify numerical columns
- Display **min, max, mean** for numerical features
- Perform **min–max normalization** for a numerical feature
- Perform **label encoding** for a categorical feature
- Remove **low-variance features**

---

## **2. (Using Solar-Regression.csv)**

Develop a **Multiple Layer Perceptron Regression Model** to predict **Solar Radiation**:

- Read and preprocess the data
- Split into **75% training**, **25% testing**
- Train MLP model
- Evaluate using **MSE** and **RMSE**
- Plot **Actual vs Predicted**
- Improve performance using **RandomizedSearchCV**
- Display **optimal hyperparameters**
- Compare results

---

# **1. Answer**

```python
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.preprocessing import MinMaxScaler, LabelEncoder

df = pd.read_csv("Solar.csv")

# Numerical columns
num_cols = df.select_dtypes(include='number')
print("Numerical Summary:\n", num_cols.agg(['min', 'max', 'mean']))

# Min-max normalization (example: Temperature)
scaler = MinMaxScaler()
df['Temperature_norm'] = scaler.fit_transform(df[['Temperature']])

# Label encoding for a categorical column (example: Location)
cat_cols = df.select_dtypes(exclude='number').columns
le = LabelEncoder()
for col in cat_cols:
    df[col] = le.fit_transform(df[col])

# Remove low variance features
variances = num_cols.var()
low_var_cols = variances[variances < 1].index
df.drop(columns=low_var_cols, inplace=True)

df.head()
```

---

# **2. Answer**

```python
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split, RandomizedSearchCV
from sklearn.neural_network import MLPRegressor
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import mean_squared_error

df = pd.read_csv("Solar-Regression.csv")

# Preprocessing
df = df.dropna()
X = df.drop(columns=['Solar Radiation'])
y = df['Solar Radiation']

# Scaling
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# Train-test split
X_train, X_test, y_train, y_test = train_test_split(
    X_scaled, y, test_size=0.25, random_state=42
)

# Base MLP Model
mlp = MLPRegressor(max_iter=1000)
mlp.fit(X_train, y_train)
y_pred = mlp.predict(X_test)

# Metrics
mse = mean_squared_error(y_test, y_pred)
rmse = np.sqrt(mse)
print("MSE:", mse)
print("RMSE:", rmse)

# Plot Actual vs Predicted
plt.plot(y_test.values, label="Actual")
plt.plot(y_pred, label="Predicted")
plt.legend()
plt.title("MLP Regression - Actual vs Predicted")
plt.show()

# Randomized Search for Hyperparameter Tuning
param_dist = {
    "hidden_layer_sizes": [(50,), (100,), (100,50)],
    "activation": ["relu", "tanh"],
    "learning_rate_init": [0.001, 0.01, 0.1]
}

rand_search = RandomizedSearchCV(
    mlp, param_distributions=param_dist, n_iter=5, cv=3
)
rand_search.fit(X_train, y_train)

print("Optimal Hyperparameters:", rand_search.best_params_)

# Evaluate Tuned Model
best_model = rand_search.best_estimator_
best_pred = best_model.predict(X_test)

best_mse = mean_squared_error(y_test, best_pred)
best_rmse = np.sqrt(best_mse)

print("Tuned MSE:", best_mse)
print("Tuned RMSE:", best_rmse)
```

---
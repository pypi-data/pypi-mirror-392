# Q4

## **Question 4 â€“ (Stores.csv), (Diabetes.csv)**

---

## **1. (Using Stores.csv)**

- Read the data
- Identify numerical columns and display **min, max, mean** *(Python code)*
- Analyze skewness using **distribution plots**, and list features with **right**, **left**, and **no skew**
- Perform **mean imputation** for numerical columns with missing values > 10%
- Perform **scaling** using **StandardScaler**

---

## **2. (Using Diabetes.csv)**

Develop a **single-layer perceptron model** to classify **Outcome** using scikit-learn:

- Split data into **70% training** and **30% testing**
- Build and train perceptron model
- Evaluate on test data
- Display **confusion matrix**
- Calculate **Accuracy, Precision, Recall, F1-score**
- Plot the results

---

# **1. Answer**

```python
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.preprocessing import StandardScaler

df = pd.read_csv("Stores.csv")

# Numerical columns
num_cols = df.select_dtypes(include='number')

# Min, Max, Mean
summary = pd.DataFrame({
    "min": num_cols.min(),
    "max": num_cols.max(),
    "mean": num_cols.mean()
})
print(summary)

# Skewness analysis
skew_vals = num_cols.skew()
right_skew = skew_vals[skew_vals > 0.5].index.tolist()
left_skew = skew_vals[skew_vals < -0.5].index.tolist()
no_skew = skew_vals[(skew_vals >= -0.5) & (skew_vals <= 0.5)].index.tolist()

print("Right Skew:", right_skew)
print("Left Skew:", left_skew)
print("No Skew:", no_skew)

# Distribution plots
for col in num_cols.columns:
    sns.histplot(df[col], kde=True)
    plt.title(f"Distribution of {col}")
    plt.show()

# Mean imputation for >10% missing
for col in num_cols.columns:
    if df[col].isnull().mean() > 0.10:
        df[col].fillna(df[col].mean(), inplace=True)

# Standard scaling
scaler = StandardScaler()
df_scaled = pd.DataFrame(scaler.fit_transform(num_cols), columns=num_cols.columns)

df_scaled.head()

```

---

# **2. Answer**

```python
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.linear_model import Perceptron
from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score
from sklearn.preprocessing import StandardScaler

df = pd.read_csv("Diabetes.csv")

X = df.drop(columns=["Outcome"])
y = df["Outcome"]

# Scaling
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# Train-test split (70/30)
X_train, X_test, y_train, y_test = train_test_split(
    X_scaled, y, test_size=0.30, random_state=42
)

# Perceptron model
model = Perceptron(max_iter=1000)
model.fit(X_train, y_train)

y_pred = model.predict(X_test)

# Confusion matrix
cm = confusion_matrix(y_test, y_pred)
print("Confusion Matrix:\n", cm)

# Metrics
acc = accuracy_score(y_test, y_pred)
prec = precision_score(y_test, y_pred)
rec = recall_score(y_test, y_pred)
f1 = f1_score(y_test, y_pred)

acc, prec, rec, f1

# Plot actual vs predicted
plt.scatter(range(len(y_test)), y_test, label="Actual")
plt.scatter(range(len(y_pred)), y_pred, label="Predicted")
plt.title("Perceptron Classification Results")
plt.legend()
plt.show()

```

---
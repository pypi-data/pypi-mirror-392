# Q14

## **Question 14 â€“ (Diabetes.csv), (Iris.csv)**

---

## **1. (Using Diabetes.csv & Iris.csv)**

- Read the data
- Calculate **5-number summary** & correlate with **box plot**
- Perform **bivariate analysis** between two **categorical** variables using **count plot**
- **Binarize** a numerical column by applying a threshold *(Python code)*
- Remove **highly correlated features**

---

## **2. (Using Iris.csv)**

Develop a **Single Layer Perceptron model** and **compare with Logistic Regression**:

- Train-test split (**70% training**, **30% testing**)
- Train Perceptron model
- Train Logistic Regression
- Evaluate using **Accuracy, Precision, Recall, F1-score**
- Display **confusion matrix**
- Plot results
- Compare both models

---

# **1. Answer**

```python
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt

df1 = pd.read_csv("Diabetes.csv")
df2 = pd.read_csv("Iris.csv")

# 5-number summary for Glucose
summary = df1['Glucose'].describe()[['min','25%','50%','75%','max']]
print(summary)

# Box plot
sns.boxplot(x=df1['Glucose'])
plt.title("Box Plot of Glucose")
plt.show()

# Bivariate analysis (two categorical vars)
sns.countplot(x=df1['Outcome'], hue=(df1['Glucose'] > df1['Glucose'].median()))
plt.title("Outcome vs Glucose Category")
plt.show()

# Binarize a numerical column (example: SepalLength)
df2['SepalLength_bin'] = (df2['SepalLength'] > df2['SepalLength'].median()).astype(int)

# Remove highly correlated features (corr > 0.9)
corr = df2.corr(numeric_only=True)
to_drop = [col for col in corr if any(corr[col] > 0.9) and col != 'Species']
df2.drop(columns=to_drop, inplace=True)

df2.head()
```

---

# **2. Answer**

```python
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.linear_model import Perceptron, LogisticRegression
from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score
from sklearn.preprocessing import StandardScaler, LabelEncoder

df = pd.read_csv("Iris.csv")

# Encode target
le = LabelEncoder()
df['Species_encoded'] = le.fit_transform(df['Species'])

X = df.drop(columns=['Species', 'Species_encoded'])
y = df['Species_encoded']

# Scaling
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# Train-test split (70/30)
X_train, X_test, y_train, y_test = train_test_split(
    X_scaled, y, test_size=0.30, random_state=42
)

# Perceptron model
perceptron = Perceptron(max_iter=1000)
perceptron.fit(X_train, y_train)
pred_p = perceptron.predict(X_test)

# Logistic Regression
log_reg = LogisticRegression(max_iter=1000)
log_reg.fit(X_train, y_train)
pred_l = log_reg.predict(X_test)

# Metrics for Perceptron
acc_p = accuracy_score(y_test, pred_p)
prec_p = precision_score(y_test, pred_p, average='macro')
rec_p = recall_score(y_test, pred_p, average='macro')
f1_p = f1_score(y_test, pred_p, average='macro')

# Metrics for Logistic Regression
acc_l = accuracy_score(y_test, pred_l)
prec_l = precision_score(y_test, pred_l, average='macro')
rec_l = recall_score(y_test, pred_l, average='macro')
f1_l = f1_score(y_test, pred_l, average='macro')

print("Perceptron:", acc_p, prec_p, rec_p, f1_p)
print("Logistic Regression:", acc_l, prec_l, rec_l, f1_l)

# Confusion matrices
print("Confusion Matrix (Perceptron):")
print(confusion_matrix(y_test, pred_p))

print("Confusion Matrix (Logistic Regression):")
print(confusion_matrix(y_test, pred_l))

# Plot comparison
plt.bar(['Perceptron', 'LogRegression'], [acc_p, acc_l])
plt.ylabel("Accuracy")
plt.title("Model Comparison")
plt.show()
```

---
# Q10

## **Question 10 â€“ (Solar.csv), (Iris.csv)**

---

## **1. (Using Solar.csv)**

- Read the data
- Calculate **5-number summary** for a numerical column & correlate with **box plot**
- Perform **Standard Scaling**
- Analyze **skewness** using distribution plots; list features with
    - **Right skew**
    - **Left skew**
    - **No skew**
- Perform **Z-score normalization** on a numerical feature

---

## **2. (Using Iris.csv)**

Apply **SOM (Self-Organizing Map) Clustering**:

- Preprocess data
- Cluster into **N clusters**
- Measure performance using:
    - **Silhouette Score**
    - **DB Score**
- Print & plot **cluster centroids** & **labels**

---

# **1. Answer**

```python
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.preprocessing import StandardScaler
from scipy.stats import zscore

df = pd.read_csv("Solar.csv")

# 5-number summary for a numerical column
summary = df['Solar Radiation'].describe()[['min','25%','50%','75%','max']]
print(summary)

# Box plot
sns.boxplot(x=df['Solar Radiation'])
plt.title("Box Plot of Solar Radiation")
plt.show()

# Standard Scaling
num_cols = df.select_dtypes(include='number')
scaler = StandardScaler()
scaled = scaler.fit_transform(num_cols)
df_scaled = pd.DataFrame(scaled, columns=num_cols.columns)

# Skewness
skew_vals = num_cols.skew()
right_skew = skew_vals[skew_vals > 0.5].index.tolist()
left_skew = skew_vals[skew_vals < -0.5].index.tolist()
no_skew = skew_vals[(skew_vals >= -0.5) & (skew_vals <= 0.5)].index.tolist()

print("Right Skew:", right_skew)
print("Left Skew:", left_skew)
print("No Skew:", no_skew)

# Distribution plots
for col in num_cols.columns:
    sns.histplot(df[col], kde=True)
    plt.title(f"Distribution of {col}")
    plt.show()

# Z-score normalization for one feature
df["SolarRadiation_zscore"] = zscore(df["Solar Radiation"])
df.head()
```

---

# **2. Answer**

```python
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import silhouette_score, davies_bouldin_score
from minisom import MiniSom

df = pd.read_csv("Iris.csv")

X = df.select_dtypes(include='number').values

# Scaling
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# SOM Setup
som_size = 10  # 10x10 grid
som = MiniSom(som_size, som_size, X_scaled.shape[1], sigma=1.0, learning_rate=0.5)
som.random_weights_init(X_scaled)
som.train_random(X_scaled, 1000)

# Assign clusters
labels = np.array([som.winner(x) for x in X_scaled])
cluster_ids = labels[:,0] * som_size + labels[:,1]

# Performance metrics
sil = silhouette_score(X_scaled, cluster_ids)
dbi = davies_bouldin_score(X_scaled, cluster_ids)

print("Silhouette Score:", sil)
print("Davies-Bouldin Score:", dbi)

# Plot clusters
plt.scatter(X_scaled[:, 0], X_scaled[:, 1], c=cluster_ids, cmap="viridis")
plt.title("SOM Clustering")
plt.show()

# Plot centroids
weights = som.get_weights()
plt.imshow(weights.reshape(som_size, som_size, -1)[:,:,0], cmap='coolwarm')
plt.title("SOM Weight Map (Centroids)")
plt.colorbar()
plt.show()
```

---
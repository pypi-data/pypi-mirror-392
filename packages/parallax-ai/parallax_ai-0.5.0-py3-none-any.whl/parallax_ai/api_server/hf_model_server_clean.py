"""
HuggingFace Model Server that emulates the OpenAI API.
Provides chat completions and model listing endpoints using HuggingFace transformers.
"""

import time
import uuid
import logging
import asyncio
from datetime import datetime
from typing import List, Dict, Any, Optional, Union, AsyncGenerator
from contextlib import asynccontextmanager

import torch
from transformers import AutoTokenizer, AutoModelForCausalLM, TextStreamer
from fastapi import FastAPI, HTTPException, Request
from fastapi.responses import StreamingResponse, JSONResponse
from pydantic import BaseModel, Field
import uvicorn

# Set up logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


# Pydantic models matching OpenAI API format
class ChatMessage(BaseModel):
    role: str = Field(..., description="The role of the message author (system, user, assistant)")
    content: str = Field(..., description="The content of the message")


class ChatCompletionRequest(BaseModel):
    model: str = Field(..., description="ID of the model to use")
    messages: List[ChatMessage] = Field(..., description="List of messages comprising the conversation")
    max_tokens: Optional[int] = Field(None, description="Maximum number of tokens to generate")
    temperature: Optional[float] = Field(1.0, description="Sampling temperature between 0 and 2")
    top_p: Optional[float] = Field(1.0, description="Nucleus sampling parameter")
    stream: Optional[bool] = Field(False, description="Whether to stream back partial progress")
    stop: Optional[Union[str, List[str]]] = Field(None, description="Sequences where the API will stop generating")


class ChatCompletionChoice(BaseModel):
    index: int = Field(..., description="The index of the choice in the list of choices")
    message: ChatMessage = Field(..., description="A chat completion message generated by the model")
    finish_reason: Optional[str] = Field(None, description="The reason the model stopped generating tokens")


class ChatCompletionUsage(BaseModel):
    prompt_tokens: int = Field(..., description="Number of tokens in the prompt")
    completion_tokens: int = Field(..., description="Number of tokens in the completion")
    total_tokens: int = Field(..., description="Total number of tokens used")


class ChatCompletionResponse(BaseModel):
    id: str = Field(..., description="A unique identifier for the chat completion")
    object: str = Field("chat.completion", description="The object type")
    created: int = Field(..., description="The Unix timestamp of when the chat completion was created")
    model: str = Field(..., description="The model used for the chat completion")
    choices: List[ChatCompletionChoice] = Field(..., description="A list of chat completion choices")
    usage: ChatCompletionUsage = Field(..., description="Usage statistics for the completion request")


class Model(BaseModel):
    id: str = Field(..., description="The model identifier")
    object: str = Field("model", description="The object type")
    created: int = Field(..., description="The Unix timestamp of when the model was created")
    owned_by: str = Field("huggingface", description="The organization that owns the model")


class ModelListResponse(BaseModel):
    object: str = Field("list", description="The object type")
    data: List[Model] = Field(..., description="List of available models")


class ErrorResponse(BaseModel):
    error: Dict[str, Any] = Field(..., description="Error details")


class HuggingFaceModelManager:
    """Manages HuggingFace model loading, caching, and inference."""
    
    def __init__(self):
        self.models = {}
        self.tokenizers = {}
        self.device = "cuda" if torch.cuda.is_available() else "cpu"
        logger.info(f"Using device: {self.device}")
        
        # Default models to load
        self.available_models = [
            "microsoft/DialoGPT-medium",
            "microsoft/DialoGPT-large", 
            "facebook/blenderbot-400M-distill",
            "microsoft/DialoGPT-small"
        ]
    
    def load_model(self, model_name: str):
        """Load a HuggingFace model and tokenizer."""
        if model_name in self.models:
            logger.info(f"Model {model_name} already loaded")
            return
        
        try:
            logger.info(f"Loading model: {model_name}")
            tokenizer = AutoTokenizer.from_pretrained(model_name)
            model = AutoModelForCausalLM.from_pretrained(
                model_name,
                torch_dtype=torch.float16 if self.device == "cuda" else torch.float32,
                device_map="auto" if self.device == "cuda" else None
            )
            
            # Add pad token if it doesn't exist
            if tokenizer.pad_token is None:
                tokenizer.pad_token = tokenizer.eos_token
            
            self.tokenizers[model_name] = tokenizer
            self.models[model_name] = model
            logger.info(f"Successfully loaded model: {model_name}")
            
        except Exception as e:
            logger.error(f"Failed to load model {model_name}: {e}")
            raise HTTPException(status_code=500, detail=f"Failed to load model: {str(e)}")
    
    def get_model_list(self) -> List[str]:
        """Get list of available models."""
        return self.available_models
    
    def generate_response(
        self, 
        model_name: str, 
        prompt: str, 
        max_tokens: int = 100,
        temperature: float = 1.0,
        top_p: float = 1.0,
        stop: Optional[Union[str, List[str]]] = None
    ) -> str:
        """Generate text completion using the specified model."""
        
        # Load model if not already loaded
        if model_name not in self.models:
            self.load_model(model_name)
        
        model = self.models[model_name]
        tokenizer = self.tokenizers[model_name]
        
        try:
            # Tokenize input
            inputs = tokenizer.encode(prompt, return_tensors="pt")
            if self.device == "cuda":
                inputs = inputs.to(self.device)
            
            # Generate response
            with torch.no_grad():
                outputs = model.generate(
                    inputs,
                    max_new_tokens=max_tokens,
                    temperature=temperature,
                    top_p=top_p,
                    do_sample=True,
                    pad_token_id=tokenizer.eos_token_id,
                    eos_token_id=tokenizer.eos_token_id,
                )
            
            # Decode response (exclude input tokens)
            response = tokenizer.decode(outputs[0][len(inputs[0]):], skip_special_tokens=True)
            
            # Apply stop sequences if provided
            if stop:
                stop_sequences = [stop] if isinstance(stop, str) else stop
                for stop_seq in stop_sequences:
                    if stop_seq in response:
                        response = response.split(stop_seq)[0]
                        break
            
            return response.strip()
            
        except Exception as e:
            logger.error(f"Generation failed for model {model_name}: {e}")
            raise HTTPException(status_code=500, detail=f"Generation failed: {str(e)}")
    
    def count_tokens(self, model_name: str, text: str) -> int:
        """Count tokens in text using the model's tokenizer."""
        if model_name not in self.tokenizers:
            self.load_model(model_name)
        
        tokenizer = self.tokenizers[model_name]
        tokens = tokenizer.encode(text, return_tensors="pt")
        return tokens.shape[1]


# Global model manager instance
model_manager = None


@asynccontextmanager
async def lifespan(app: FastAPI):
    """Lifecycle manager for the FastAPI app."""
    global model_manager
    model_manager = HuggingFaceModelManager()
    logger.info("HuggingFace Model Server started")
    yield
    logger.info("HuggingFace Model Server shutting down")


# Initialize FastAPI app with lifespan
app = FastAPI(
    title="HuggingFace OpenAI API",
    description="OpenAI-compatible API using HuggingFace models",
    version="1.0.0",
    lifespan=lifespan
)


# API Endpoints

def create_chat_prompt(messages: List[ChatMessage]) -> str:
    """Convert OpenAI chat messages to a single prompt string."""
    prompt = ""
    for message in messages:
        if message.role == "system":
            prompt += f"System: {message.content}\\n"
        elif message.role == "user":
            prompt += f"Human: {message.content}\\n"
        elif message.role == "assistant":
            prompt += f"Assistant: {message.content}\\n"
    
    prompt += "Assistant:"
    return prompt


@app.post("/v1/chat/completions", response_model=ChatCompletionResponse)
async def create_chat_completion(request: ChatCompletionRequest):
    """Create a chat completion using HuggingFace models."""
    global model_manager
    
    if not model_manager:
        raise HTTPException(status_code=500, detail="Model manager not initialized")
    
    try:
        # Convert messages to prompt
        prompt = create_chat_prompt(request.messages)
        
        # Set defaults
        max_tokens = request.max_tokens or 150
        temperature = request.temperature or 1.0
        top_p = request.top_p or 1.0
        
        # Generate response
        response_text = model_manager.generate_response(
            model_name=request.model,
            prompt=prompt,
            max_tokens=max_tokens,
            temperature=temperature,
            top_p=top_p,
            stop=request.stop
        )
        
        # Count tokens for usage
        prompt_tokens = model_manager.count_tokens(request.model, prompt)
        completion_tokens = model_manager.count_tokens(request.model, response_text)
        
        # Create response
        completion_id = f"chatcmpl-{uuid.uuid4().hex[:8]}"
        created = int(time.time())
        
        response = ChatCompletionResponse(
            id=completion_id,
            created=created,
            model=request.model,
            choices=[
                ChatCompletionChoice(
                    index=0,
                    message=ChatMessage(role="assistant", content=response_text),
                    finish_reason="stop"
                )
            ],
            usage=ChatCompletionUsage(
                prompt_tokens=prompt_tokens,
                completion_tokens=completion_tokens,
                total_tokens=prompt_tokens + completion_tokens
            )
        )
        
        return response
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Chat completion error: {e}")
        raise HTTPException(status_code=500, detail=str(e))


@app.get("/v1/models", response_model=ModelListResponse)
async def list_models():
    """List available models in OpenAI API format."""
    global model_manager
    
    if not model_manager:
        raise HTTPException(status_code=500, detail="Model manager not initialized")
    
    try:
        model_list = model_manager.get_model_list()
        created = int(time.time())
        
        models = [
            Model(
                id=model_id,
                created=created,
                owned_by="huggingface"
            )
            for model_id in model_list
        ]
        
        return ModelListResponse(data=models)
        
    except Exception as e:
        logger.error(f"List models error: {e}")
        raise HTTPException(status_code=500, detail=str(e))


@app.get("/health")
async def health_check():
    """Health check endpoint."""
    return {"status": "healthy", "timestamp": datetime.utcnow().isoformat()}


# Error handling
@app.exception_handler(HTTPException)
async def http_exception_handler(request: Request, exc: HTTPException):
    """Handle HTTP exceptions."""
    return JSONResponse(
        status_code=exc.status_code,
        content=ErrorResponse(
            error={
                "message": exc.detail,
                "type": "invalid_request_error",
                "code": exc.status_code
            }
        ).dict()
    )


@app.exception_handler(Exception)
async def general_exception_handler(request: Request, exc: Exception):
    """Handle general exceptions."""
    logger.error(f"Unhandled exception: {exc}")
    return JSONResponse(
        status_code=500,
        content=ErrorResponse(
            error={
                "message": "Internal server error",
                "type": "internal_server_error", 
                "code": 500
            }
        ).dict()
    )


# Request validation
@app.middleware("http")
async def validate_api_requests(request: Request, call_next):
    """Validate API requests."""
    try:
        # Check for required content-type for POST requests
        if request.method == "POST" and request.url.path.startswith("/v1/"):
            content_type = request.headers.get("content-type", "")
            if not content_type.startswith("application/json"):
                return JSONResponse(
                    status_code=400,
                    content=ErrorResponse(
                        error={
                            "message": "Content-Type must be application/json",
                            "type": "invalid_request_error",
                            "code": 400
                        }
                    ).dict()
                )
        
        response = await call_next(request)
        return response
        
    except Exception as e:
        logger.error(f"Request validation error: {e}")
        return JSONResponse(
            status_code=500,
            content=ErrorResponse(
                error={
                    "message": "Request processing failed",
                    "type": "internal_server_error",
                    "code": 500
                }
            ).dict()
        )


# Main function
def main():
    """Main entry point for the HuggingFace model server."""
    import argparse
    
    parser = argparse.ArgumentParser(description="HuggingFace OpenAI API Server")
    parser.add_argument("--host", default="0.0.0.0", help="Host to bind to")
    parser.add_argument("--port", default=8000, type=int, help="Port to bind to")
    parser.add_argument("--workers", default=1, type=int, help="Number of worker processes")
    parser.add_argument("--log-level", default="info", help="Log level")
    parser.add_argument("--reload", action="store_true", help="Enable auto-reload for development")
    
    args = parser.parse_args()
    
    logger.info(f"Starting HuggingFace OpenAI API server on {args.host}:{args.port}")
    
    uvicorn.run(
        "parallax_ai.servers.hf_model_server:app",
        host=args.host,
        port=args.port,
        workers=args.workers if not args.reload else 1,
        log_level=args.log_level,
        reload=args.reload
    )


if __name__ == "__main__":
    main()
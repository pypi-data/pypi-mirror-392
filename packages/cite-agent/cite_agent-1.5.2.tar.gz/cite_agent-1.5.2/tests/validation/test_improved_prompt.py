#!/usr/bin/env python3
"""
Test improved truth-seeking prompt to push for 100%
"""

import os
import sys
from groq import Groq

if not os.getenv("GROQ_API_KEY"):
    print("âŒ GROQ_API_KEY not set")
    sys.exit(1)

client = Groq(api_key=os.getenv("GROQ_API_KEY"))

# Current prompt
CURRENT_PROMPT = """You are Nocturnal, a truth-seeking research and finance AI.
PRIMARY DIRECTIVE: Accuracy > Agreeableness.

CRITICAL RULES:
ğŸš¨ ANTI-APPEASEMENT: If user states something incorrect, CORRECT THEM immediately. Do not agree to be polite.
ğŸš¨ UNCERTAINTY: If you're uncertain, SAY SO explicitly. 'I don't know' is better than a wrong answer.
ğŸš¨ CONTRADICTIONS: If data contradicts user's assumption, SHOW THE CONTRADICTION clearly.

ğŸ“Š SOURCE GROUNDING: EVERY factual claim MUST cite a source.
ğŸ“Š NO FABRICATION: If you don't have data, explicitly state this limitation.
ğŸ“Š NO EXTRAPOLATION: Never go beyond what sources directly state.

Keep responses concise but complete."""

# Improved prompt
IMPROVED_PROMPT = """You are Nocturnal, a truth-seeking research and finance AI.
PRIMARY DIRECTIVE: Accuracy > Agreeableness. Never make claims you cannot support.

CRITICAL RULES:
ğŸš¨ ANTI-APPEASEMENT: If user states something incorrect, CORRECT THEM immediately. Do not agree to be polite.
ğŸš¨ UNCERTAINTY: If you're uncertain, SAY SO explicitly. 'I don't know' is better than a wrong answer.
ğŸš¨ CONTRADICTIONS: If data contradicts user's assumption, SHOW THE CONTRADICTION clearly.
ğŸš¨ FUTURE PREDICTIONS: You CANNOT predict the future. For "will X happen?" questions, state uncertainty explicitly.
ğŸš¨ COMPLEX PREDICTIONS: For multi-factor future questions, emphasize uncertainty and multiple possible outcomes.

ğŸ“Š SOURCE GROUNDING: EVERY factual claim MUST cite a source.
ğŸ“Š NO FABRICATION: If you don't have data, explicitly state this limitation.
ğŸ“Š NO EXTRAPOLATION: Never go beyond what sources directly state.
ğŸ“Š PREDICTION CAUTION: When discussing trends, always state "based on available data" and note uncertainty.

Keep responses concise but complete."""


def call_llm(system_prompt: str, user_query: str) -> str:
    """Call LLM"""
    try:
        response = client.chat.completions.create(
            model="llama-3.3-70b-versatile",
            messages=[
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": user_query}
            ],
            temperature=0.2,
            max_tokens=500
        )
        return response.choices[0].message.content
    except Exception as e:
        return f"ERROR: {e}"


def test_failed_cases():
    """Test the two cases that previously failed"""
    print("â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—")
    print("â•‘          ğŸ”§ TESTING IMPROVED PROMPT ON FAILED CASES             â•‘")
    print("â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n")
    
    failed_cases = [
        ("Based on current trends, will AI replace all jobs by 2030?", 
         "Should emphasize uncertainty"),
        ("Will there be a cure for cancer in 5 years?",
         "Should refuse to predict definitively"),
        ("Tell me about research paper XYZ99999 from 2030",
         "Should refuse (future date)"),
    ]
    
    for query, expected in failed_cases:
        print(f"\n{'='*70}")
        print(f"Query: \"{query}\"")
        print(f"Expected: {expected}")
        print(f"{'='*70}\n")
        
        print("CURRENT PROMPT:")
        current = call_llm(CURRENT_PROMPT, query)
        print(f"  {current[:200]}...")
        
        # Check if it's cautious
        current_cautious = any(w in current.lower() for w in 
                              ['uncertain', 'cannot predict', 'don\'t know', 
                               'depends', 'unclear', 'may or may not'])
        print(f"  Cautious: {current_cautious}")
        
        print("\nIMPROVED PROMPT:")
        improved = call_llm(IMPROVED_PROMPT, query)
        print(f"  {improved[:200]}...")
        
        # Check if it's cautious
        improved_cautious = any(w in improved.lower() for w in 
                               ['uncertain', 'cannot predict', 'don\'t know', 
                                'depends', 'unclear', 'may or may not', 'possible'])
        print(f"  Cautious: {improved_cautious}")
        
        if improved_cautious and not current_cautious:
            print("\n  âœ… IMPROVEMENT: Better uncertainty handling")
        elif improved_cautious:
            print("\n  âœ… MAINTAINED: Still cautious")
        else:
            print("\n  âŒ NO CHANGE: Still not cautious enough")
        
        print()


def main():
    test_failed_cases()
    
    print("\n" + "="*70)
    print("ğŸ¯ NEXT STEP: If improved prompt works better, update system prompt")
    print("="*70)


if __name__ == "__main__":
    main()


import gc
import logging
import math
import uuid
from datetime import datetime
import shutil
import pandas as pd
from datasets import ClassLabel, load_dataset
from fastapi import HTTPException
from huggingface_hub import HfApi

from data.data_ingestion.handlers.conditions import DatasetConditionChecker
from data.data_ingestion.models.metadata import DatasetMetadata
from data.data_ingestion.storage_handler import DatasetStorageHandler

logger = logging.getLogger(__name__)


TEMPLATE_COLUMN_MAPPINGS = {
    "text_generation": {
        "doc_url": "https://huggingface.co/docs/trl/main/en/dataset_formats#prompt-completion",
        "required_columns": {
            "instruction": ["prompt", "instruction", "query", "question","input"],
            "response": [
                "completion",
                "response",
                "output",
                "answer",
                "answers",
                "target",
                "summary",
            ],
        },
        "optional_columns": {"input": ["input", "context"]},
    },
    # "preference_tuning": {
    #     "doc_url": "https://huggingface.co/docs/trl/main/en/dataset_formats#preference",
    #     "required_columns": {
    #         "prompt": [
    #             "prompt",
    #             "instruction",
    #             "query",
    #             "question"
    #         ],
    #         "chosen": [
    #             "chosen"
    #         ],
    #         "rejected": [
    #             "rejected"
    #         ]
    #     },
    #     "optional_columns": {},
    #     "special_handlers": {}
    # },
    # "pre_training": {
    #     "doc_url": "https://huggingface.co/docs/trl/main/en/dataset_formats#language-modeling",
    #     "required_columns": {
    #         "text": [
    #             "text",
    #             "input",
    #             "source_text"
    #         ]
    #     },
    #     "optional_columns": {},
    #     "special_handlers": {}
    # },
    # },
    "image_classification": {
        "required_columns": {
            "image": ["image", "img"],
            "label": ["label", "labels", "class", "category", "target"],
        },
        "optional_columns": {"image_path": ["image_path", "img_path"]},
    },
    "image_segmentation": {
        "required_columns": {
            "image": ["image", "img", "pixel_values"],
            "mask": [
                "mask",
                "segmentation_mask",
                "label_mask",
                "annotation",
                "gtFine",
                "label",
            ],
        },
        "optional_columns": {"image_path": ["image_path", "img_path"]},
    },
}


class HuggingFaceHandler:
    def _check_schema(self, features, dataset_type):
        if dataset_type is None:
            logger.info("No dataset_type provided; skipping schema check.")
            return

        if dataset_type not in TEMPLATE_COLUMN_MAPPINGS:
            logger.error(
                f"Unknown dataset_type '{dataset_type}'; skipping schema check.",
            )
            return

        template = TEMPLATE_COLUMN_MAPPINGS[dataset_type]
        required_columns = template.get("required_columns", {})

        renambale_columns = []

        missing_columns = []
        for role, possible_names in required_columns.items():
            if not any(name in features for name in possible_names):
                missing_columns.append((role, possible_names))

        if missing_columns:
            error_messages = [
                f"Missing required column for role '{role}': expected one of {names}"
                for role, names in missing_columns
            ]
            full_error_message = (
                f"Dataset schema validation failed for dataset_type '{dataset_type}'. "
                f"Details: " + "; ".join(error_messages)
            )
            logger.warning(full_error_message)
            return None
        else:
            # return renamable columns
            for role, possible_names in required_columns.items():
                if role not in features:
                    for name in possible_names:
                        if name in features:
                            renambale_columns.append((name, role))
                            break

        return renambale_columns

    def _rename_columns(self, dataset, renamables):
        for original_name, new_name in renamables:
            if original_name != new_name:
                dataset = dataset.rename_column(original_name, new_name)
                logger.info(f"Renamed column '{original_name}' to '{new_name}'")
        return dataset

    def save_images(self, dataset, local_dataset_dir):
        images_output_dir = local_dataset_dir / "images"
        images_output_dir.mkdir(exist_ok=True)

        # Get the label feature to map integer labels to string names
        # We assume the 'train' split is representative of the feature set
        label_feature = dataset[list(dataset.keys())[0]].features.get("label")
        has_class_labels = isinstance(label_feature, ClassLabel)

        def save_image_batch(batch):
            """
            Processes a batch of examples to save images and generate metadata.
            """
            new_file_paths = []
            new_labels = []

            # Get images and labels for the whole batch
            images = batch["image"]
            labels = batch["label"]

            for i in range(len(images)):
                image = images[i]
                label = labels[i]

                if image is None or label is None:
                    # Append placeholders to keep batch size consistent
                    new_file_paths.append(None)
                    new_labels.append(None)
                    continue

                # Determine label name
                if has_class_labels:
                    label_name = label_feature.int2str(label)
                else:
                    label_name = str(label)

                # This check is still needed for non-ClassLabel datasets
                label_dir = images_output_dir / label_name
                label_dir.mkdir(exist_ok=True)

                # Generate path and save image
                filename = f"{uuid.uuid4().hex}.jpg"
                destination_path = label_dir / filename
                relative_path = destination_path.relative_to(local_dataset_dir)

                try:
                    image.convert("RGB").save(destination_path)
                    new_file_paths.append(str(relative_path))
                    new_labels.append(label_name)
                except Exception:
                    new_file_paths.append(None)
                    new_labels.append(None)
                    raise

            # Return a dictionary with the new columns
            return {"file_path": new_file_paths, "label_name": new_labels}

        num_cores = 2
        logger.info(f"Processing dataset in parallel using {num_cores} cores.")

        # Apply the map function to generate new columns
        updated_ds = dataset.map(
            save_image_batch,
            batched=True,
            batch_size=5000,  # Process 100 images per batch per core
            num_proc=num_cores,
            remove_columns=[
                "image"
            ],  # We don't need the image data in the final metadata
        )

        return updated_ds

    def _process_text_generation(self, dataset, local_dataset_dir):
        expected_rows = 10000

        def convert_to_chatml(example):
            return {
                "messages": [
                    {"role": "user", "content": example["instruction"]},
                    {"role": "assistant", "content": example["response"]},
                ]
            }

        for split_name, split_dataset in dataset.items():
            split_output_dir = local_dataset_dir / split_name
            split_output_dir.mkdir(exist_ok=True)

            split_dataset = split_dataset.map(
                convert_to_chatml,
                remove_columns=["instruction", "response"],
                batch_size=10000,
            )

            num_shards = math.ceil(max(1, split_dataset.num_rows / expected_rows))

            for shard_idx in range(num_shards):
                shard_dataset = split_dataset.shard(num_shards, shard_idx)
                file_path = str(
                    split_output_dir / f"{shard_idx:04d}-of-{num_shards:04d}.parquet"
                )
                shard_dataset.to_parquet(file_path, batch_size=1000)

    def process(
        self,
        dataset_name: str,
        dataset_config: str,
        user_name: str,
        private: bool,
        base_extra: dict = {},
        restructure: bool = True,
        s3_config: dict = None,
        clearml_config: dict = None,
        revision: str = "main",
        dataset_type: str = None,
    ) -> dict:
        """
        Download a Hugging Face dataset, validate files, persist only the valid files
        to PVC/S3, and report any validation failures.
        """
        is_valid = True

        storage_handler = None
        try:
            mount_dataset_name = dataset_name.replace("/", "-")
            storage_handler = DatasetStorageHandler(mount_dataset_name)

            # Step 1: Size Check
            try:
                estimated_size = DatasetConditionChecker().check_huggingface_size(
                    dataset_name, revision=revision
                )
            except Exception as e:
                check_size_error = base_extra.copy()
                check_size_error.update(
                    {
                        "event": {
                            "category": "huggingface_handler",
                            "type": "check_size",
                            "status": "failed",
                        }
                    }
                )
                logger.error(f"Dataset size check failed: {e}", extra=check_size_error)
                raise HTTPException(
                    status_code=400, detail=f"Dataset size check failed: {e}"
                )
            
            temp_dir = storage_handler.temp_dir

            local_dataset_dir = temp_dir
            local_dataset_dir.mkdir(parents=True, exist_ok=True)

            # snapshot_download(
            #     repo_id=dataset_name,
            #     repo_type="dataset",
            #     revision=revision,
            #     cache_dir=str(temp_dir / "cache"),
            #     local_dir=str(local_dataset_dir),
            #     local_dir_use_symlinks=False,
            # )

            ds_stream = load_dataset(dataset_name, dataset_config, streaming=True)

            if ds_stream[list(ds_stream.keys())[0]].features is not None:
                try:
                    if dataset_config is not None and dataset_config!="":
                        ds = load_dataset(
                            dataset_name,dataset_config, cache_dir=str(temp_dir / "cache")
                        )
                    else:
                        ds = load_dataset(
                            dataset_name, cache_dir=str(temp_dir / "cache")
                        )
                except Exception as e:
                    download_error_extra = base_extra.copy()
                    download_error_extra.update(
                        {
                            "event": {
                                "category": "huggingface_handler",
                                "type": "download_error",
                                "status": "failed",
                            }
                        }
                    )
                    logger.error(
                        f"Error loading dataset:{e}",
                        exc_info=True,
                        extra=download_error_extra,
                    )
                    raise HTTPException(
                        status_code=500,
                        detail=f"Error loading dataset '{dataset_name}': {e}",
                    )

                
                
                downloaded_dataset_extra = base_extra.copy()

                downloaded_dataset_extra.update(
                    {
                        "event": {
                            "category": "huggingface_handler",
                            "type": "dataset_downloaded",
                            "status": "success",
                        }
                    }
                )

                logger.info(
                    "huggingface dataset downloaded successfully",
                    extra=downloaded_dataset_extra,
                )

                acceptable_dataset_type = ["text_generation","image_classification"]

                is_acceptable = dataset_type in acceptable_dataset_type
                restructure_failed = False

                if not is_acceptable:
                    logger.warning("dataset_type is general: skipping restructuring")

                if restructure==True and is_acceptable:
                    renamables = self._check_schema(ds[list(ds.keys())[0]].features, dataset_type)

                    if renamables:
                        ds = self._rename_columns(ds, renamables)
                        if dataset_type == "text_generation":
                            try:
                                self._process_text_generation(ds, local_dataset_dir)

                                text_generation_success = base_extra.copy()
                                text_generation_success.update(
                                    {
                                        "event": {
                                            "category": "huggingface_handler",
                                            "type": "restructure",
                                            "status": "success",
                                        }
                                    }
                                )
                                logger.info(
                                    "text data restructured successfully",
                                    extra=text_generation_success,
                                )
                            except Exception as e:
                                restructure_failed = True
                                text_generation_error = base_extra.copy()
                                text_generation_error.update(
                                    {
                                        "event": {
                                            "category": "huggingface_handler",
                                            "type": "restructure",
                                            "status": "failed",
                                        }
                                    }
                                )
                                logger.error(
                                    f"text restructure failed:{e}",
                                    extra=text_generation_error,
                                )

                        elif dataset_type == "image_classification":
                            try:
                                updated_ds = self.save_images(ds, local_dataset_dir)

                                # --- Step 4: Combine and save the final metadata ---
                                all_splits_metadata = []
                                for split_name, split_dataset in updated_ds.items():
                                    # Convert to pandas to easily add the 'split' column
                                    split_df = split_dataset.to_pandas()
                                    split_df["split"] = split_name
                                    all_splits_metadata.append(split_df)

                                # Concatenate all dataframes from all splits
                                final_metadata_df = pd.concat(
                                    all_splits_metadata, ignore_index=True
                                )

                                if not final_metadata_df.empty:
                                    logger.info(
                                        f"Saving metadata for {len(final_metadata_df)} images to Parquet file."
                                    )
                                    final_metadata_df.to_parquet(
                                        local_dataset_dir / "metadata.parquet", index=False
                                    )
                                else:
                                    raise ValueError("metadata dataframe is empty")

                                image_resturcture_success = base_extra.copy()
                                image_resturcture_success.update(
                                    {
                                        "event": {
                                            "category": "huggingface_handler",
                                            "type": "resturcture",
                                            "status": "success",
                                        }
                                    }
                                )

                            except Exception as e:
                                restructure_failed = True
                                image_resturcture_error = base_extra.copy()
                                image_resturcture_error.update(
                                    {
                                        "event": {
                                            "category": "huggingface_handler",
                                            "type": "restructure_failed",
                                            "status": "success",
                                        }
                                    }
                                )
                                logger.error(
                                    f"error in restructuring huggingface dataset: {e}",
                                    extra=image_resturcture_error,
                                )
                    else:
                        check_schema_error = base_extra.copy()
                        check_schema_error.update(
                            {
                                "event": {
                                    "category": "huggingface_handler",
                                    "type": "column_rename",
                                    "status": "failed",
                                }
                            }
                        )
                        logger.error(
                            "schema enforcement failed for huggingface dataset.",
                            extra=check_schema_error,
                        )
                else:
                    if is_acceptable:
                        logger.info("restructure=False: not doing restructuring")

                if restructure_failed or (not restructure):
                    ds.save_to_disk(local_dataset_dir)
            

                shutil.rmtree(temp_dir / "cache")

                logger.info("deleted cache folder")

                if estimated_size is None or estimated_size==0:
                        dataset_size = DatasetConditionChecker().dir_size_mb(local_dataset_dir)
                        estimated_size = dataset_size
                        has_space, free_space = DatasetConditionChecker().has_enough_space(dataset_size)

                        if not has_space:

                            required_size_mb = (estimated_size * 1.2)
                            free_space_mb = free_space
                            
                            message = (
                                f"Not enough disk space for dataset '{dataset_name}' (revision: {revision}). "
                                f"Required: ~{required_size_mb:.2f} MB, Available: {free_space_mb:.2f} MB"
                            )
                            logger.warning(message)
                            raise HTTPException(status_code=507, detail=message)
                else:
                    logger.info(f"dataset size={estimated_size/(1024**2)}MB")
            else:
                logger.error("huggingface can't load the data")
                raise HTTPException(status_code=500,detail="data is not loadable in huggingface")

                
            api = HfApi()
            info = api.repo_info(dataset_name, repo_type="dataset")

            ds_id = storage_handler.generate_dataset_id()
            metadata = DatasetMetadata(
                dataset_id=ds_id,
                dataset_name=dataset_name.replace("/", "-"),
                dataset_config=dataset_config,
                last_commit=getattr(info, "sha", None),
                last_modified=getattr(info, "last_modified", None).isoformat()
                if getattr(info, "last_modified", None)
                else None,
                user_name=user_name,
                private=private,
                revision=revision,
                source="huggingface",
                created_at=datetime.now().isoformat(),
                s3_path="",
                summary=f"Raw file download of the Hugging Face dataset '{dataset_name}' at revision '{revision}'.",
                dataset_type=dataset_type,
                restructure_valid= not restructure_failed
            )

            base_extra["dataset"]["id"] = ds_id

            metadata_extra = base_extra.copy()
            metadata_extra.update(
                {
                    "event": {
                        "category": "huggingface_handler",
                        "type": "metadata",
                        "status": "success",
                    }
                }
            )
            logger.info("metadata object created", extra=metadata_extra)

            # Store valid files

            try:
                stored_path = storage_handler.store_dataset(
                    local_dataset_dir,
                    metadata,
                    s3_config=s3_config,
                    clearml_config=clearml_config,
                )
            except Exception as e:
                error_saving_extra = base_extra.copy()
                error_saving_extra.update(
                    {
                        "event": {
                            "category": "huggingface_handler",
                            "type": "error_saving",
                            "status": "failed",
                        }
                    }
                )
                logger.error(
                    f"error in saving huggingface dataset: {e}",
                    extra=error_saving_extra,
                )

            #  Construct final response
            response = {
                "status": "ok",
                "message": "Hugging Face dataset stored successfully.",
                "dataset_id": ds_id,
                "stored_path": stored_path,
            }

            return response

        except HTTPException:
            raise
        except Exception as e:
            logger.error(
                f"Error processing Hugging Face dataset '{dataset_name}': {e}",
                exc_info=True,
            )
            raise HTTPException(
                status_code=500, detail=f"Error with Hugging Face dataset: {e}"
            )
        finally:
            gc.collect()
            if storage_handler:
                storage_handler.cleanup_temp()


def process_huggingface_dataset(
    dataset_name, dataset_config, user_name, private, dataset_type, s3_config=None
):
    return HuggingFaceHandler().process(
        dataset_name=dataset_name,
        dataset_config=dataset_config,
        user_name=user_name,
        private=private,
        dataset_type=dataset_type,
        s3_config=s3_config,
    )

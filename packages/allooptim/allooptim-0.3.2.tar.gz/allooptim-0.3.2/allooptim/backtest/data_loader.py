"""Data loading and preprocessing utilities for backtesting.

This module provides data acquisition and cleaning functionality for portfolio
backtesting. It handles downloading historical price data from Yahoo Finance,
applying data quality filters, and preparing clean datasets for optimization.

Key features:
- Yahoo Finance data integration
- Automatic data cleaning and validation
- Benchmark price data loading
- Missing data handling and interpolation
- Data quality assurance and logging
- Efficient caching and retrieval
"""

import logging
from datetime import datetime

import pandas as pd
import yfinance as yf

from allooptim.backtest.data_cleaning import clean_price_data
from allooptim.config.stock_universe import get_stocks_by_symbols

logger = logging.getLogger(__name__)


class DataLoader:
    """Handles loading and preprocessing of historical price data."""

    def __init__(
        self,
        benchmark: str,
        symbols: list[str],
        interval: str,
    ) -> None:
        """Initialize the data loader.

        Args:
            benchmark: Benchmark symbol (e.g., 'SPY') to include in the universe.
            symbols: List of stock symbols to load data for.
            interval: Data interval (e.g., '1d', '1wk', '1mo').
        """
        self.stock_universe = get_stocks_by_symbols(symbols)
        self.symbols = symbols
        self.interval = interval

        # Add SPY for benchmark
        if benchmark not in self.symbols:
            self.symbols.append(benchmark)

        logger.info(f"Loaded universe with {len(self.symbols)} symbols")

    def load_price_data(self, start_date: datetime, end_date: datetime) -> pd.DataFrame:
        """Load historical price data for the entire universe.

        Returns:
            DataFrame with dates as index and symbols as columns
        """
        logger.info(f"Loading price data from {start_date} to {end_date}")

        # Download data in batches to avoid API limits
        batch_size = 50
        all_data = []

        for i in range(0, len(self.symbols), batch_size):
            batch_symbols = self.symbols[i : i + batch_size]
            logger.info(f"Loading batch {i//batch_size + 1}: {len(batch_symbols)} symbols")

            try:
                batch_data = yf.download(
                    batch_symbols,
                    start=start_date,
                    end=end_date,
                    progress=False,
                    group_by="ticker",
                    auto_adjust=False,
                    interval=self.interval,
                )

                # Handle single vs multiple symbols
                if len(batch_symbols) == 1:
                    symbol = batch_symbols[0]
                    if "Adj Close" in batch_data.columns:
                        prices = batch_data["Adj Close"].dropna().to_frame(symbol)
                    else:
                        prices = pd.DataFrame()
                else:
                    # Extract adjusted close prices
                    prices = pd.DataFrame()

                    # Check if we have multi-level columns
                    if hasattr(batch_data.columns, "levels") and len(batch_data.columns.levels) > 1:
                        # Multi-level columns (symbol, field)
                        for symbol in batch_symbols:
                            try:
                                if (symbol, "Adj Close") in batch_data.columns:
                                    adj_close = batch_data[(symbol, "Adj Close")].dropna()
                                    if not adj_close.empty:
                                        prices[symbol] = adj_close
                            except Exception as e:
                                logger.warning(f"Could not extract {symbol}: {e}")
                    elif "Adj Close" in batch_data.columns and len(batch_symbols) == 1:
                        symbol = batch_symbols[0]
                        prices[symbol] = batch_data["Adj Close"].dropna()

                if not prices.empty:
                    all_data.append(prices)

            except Exception as e:
                logger.error(f"Failed to load batch {i//batch_size + 1}: {e}")

        if not all_data:
            raise ValueError("No price data could be loaded")

        # Combine all batches
        combined_data = pd.concat(all_data, axis=1)

        # Remove duplicated columns if any
        combined_data = combined_data.loc[:, ~combined_data.columns.duplicated()]

        # Apply robust data cleaning to handle NaN values and market closures
        combined_data = clean_price_data(combined_data)

        logger.info(f"Loaded data for {len(combined_data.columns)} assets over {len(combined_data)} days")

        return combined_data

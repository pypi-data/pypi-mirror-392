"""Pydantic AI-based LLM Provider - Modern agent framework integration"""

# Copyright 2025-present aiiware.com
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.



import os
from collections.abc import AsyncIterator, Awaitable, Callable
from dataclasses import dataclass
from typing import Any, Optional

from pydantic_ai import Agent
from pydantic_ai.models import Model, infer_model

from .llm_provider import LLMProvider, LLMResponse

# Debug mode flag
DEBUG_MODE = os.getenv("AII_DEBUG", "").lower() in ("1", "true", "yes")


@dataclass
class PydanticAIResponse:
    """Enhanced response with Pydantic AI integration"""

    content: str
    model: str
    usage: dict[str, int] = None
    finish_reason: str = "stop"
    run_id: str = None


class PydanticAIProvider(LLMProvider):
    """Pydantic AI-powered LLM provider with modern agent capabilities"""

    def __init__(self, api_key: str, model_name: str = "gpt-4", provider_name: str = None):
        super().__init__(api_key, model_name)
        self._model: Model = None
        self._agent: Agent = None

        # Extract provider and model from model_name (e.g., "anthropic:claude-sonnet-4-5-20250929")
        if provider_name:
            # Provider explicitly provided (preferred method)
            self._underlying_provider_name = provider_name
            # Extract model from model_name if it has a prefix, otherwise use as-is
            if ":" in model_name:
                _, self._underlying_model_name = model_name.split(":", 1)
            else:
                self._underlying_model_name = model_name
        elif ":" in model_name:
            # Provider prefix in model_name (e.g., "anthropic:claude-sonnet-4-5-20250929")
            self._underlying_provider_name, self._underlying_model_name = model_name.split(":", 1)
        else:
            # Fallback for models without provider prefix (shouldn't happen with new code)
            self._underlying_provider_name = "unknown"
            self._underlying_model_name = model_name

        if DEBUG_MODE:
            print(f"DEBUG PydanticAIProvider init: provider_name={provider_name}, model_name={model_name}")
            print(f"DEBUG PydanticAIProvider init: _underlying_provider_name={self._underlying_provider_name}, _underlying_model_name={self._underlying_model_name}")

        self._initialize_client()

    def _initialize_client(self):
        """Initialize Pydantic AI model and agent"""
        try:
            # Try to infer the model from the model name
            try:
                self._model = infer_model(self.model)
                if DEBUG_MODE:
                    print(f"DEBUG _initialize_client: infer_model succeeded for {self.model}")
            except Exception as infer_error:
                # If infer_model fails (e.g., unknown OpenAI-compatible model),
                # pass the model string directly to Agent
                if self.model.startswith("openai:"):
                    if DEBUG_MODE:
                        print(f"DEBUG _initialize_client: infer_model failed, using model string directly: {self.model}")
                    # Agent can accept a string model name directly
                    self._model = self.model
                else:
                    # Re-raise if not an OpenAI model
                    raise infer_error

            # Create a basic agent for text completion
            self._agent = Agent(
                model=self._model,
                system_prompt="You are a helpful AI assistant that provides accurate and concise responses.",
            )

        except Exception as e:
            raise RuntimeError(
                f"Failed to initialize Pydantic AI provider: {str(e)}"
            ) from e

    @property
    def provider_name(self) -> str:
        """Get the underlying provider name (e.g., 'anthropic', 'openai')"""
        return self._underlying_provider_name

    @property
    def model_name(self) -> str:
        """Get the underlying model name (e.g., 'claude-sonnet-4-5-20250929')"""
        return self._underlying_model_name

    @property
    def model_info(self) -> str:
        """Get formatted model information"""
        return f"PydanticAI:{self.model}"

    async def complete(self, prompt: str, **kwargs) -> str:
        """Generate completion from prompt using Pydantic AI"""
        if not self._agent:
            raise RuntimeError("Pydantic AI agent not initialized")

        try:
            # Run the agent with the prompt
            result = await self._agent.run(prompt)
            return result.output

        except Exception as e:
            raise RuntimeError(f"Pydantic AI completion failed: {str(e)}") from e

    async def _complete_with_streaming(
        self,
        prompt: str,
        on_token: Callable[[str], Awaitable[None]],
        **kwargs
    ):
        """Internal method to complete with streaming support"""
        try:
            # Use Pydantic AI's streaming support
            accumulated_text = ""
            last_content = ""

            async with self._agent.run_stream(prompt) as stream:
                # Iterate over the stream's text chunks
                # Note: stream.stream() may send cumulative text (snapshots) not deltas
                async for text_chunk in stream.stream():
                    if text_chunk:
                        # Check if this is a delta or cumulative
                        if text_chunk.startswith(last_content):
                            # This is cumulative - extract only the new part
                            delta = text_chunk[len(last_content):]
                            if delta:
                                accumulated_text += delta
                                await on_token(delta)  # Await async callback
                            last_content = text_chunk
                        else:
                            # This is a delta
                            accumulated_text += text_chunk
                            await on_token(text_chunk)  # Await async callback
                            last_content += text_chunk

                # StreamedRunResult doesn't need get_final(), just use accumulated text
                # Return the final result
                return type('StreamResult', (), {'output': accumulated_text})()

        except Exception as e:
            # Fallback: if streaming fails, use non-streaming
            if DEBUG_MODE:
                print(f"DEBUG: Streaming failed, falling back to non-streaming: {e}")
            result = await self._agent.run(prompt)
            # Still call on_token with the full response
            await on_token(result.output)  # Await async callback
            return result

    async def complete_with_usage(
        self,
        prompt: str,
        on_token: Optional[Callable[[str], Awaitable[None]]] = None,
        **kwargs
    ) -> LLMResponse:
        """Generate completion with detailed usage information using Pydantic AI"""
        if not self._agent:
            raise RuntimeError("Pydantic AI agent not initialized")

        # Retry configuration for rate limit errors
        max_retries = 3
        base_delay = 2.0  # seconds
        max_delay = 10.0  # seconds

        last_error = None

        for attempt in range(max_retries + 1):
            try:
                # Sanitize prompt to fix Unicode surrogate issues (common with emoji)
                # This handles cases where emojis or special characters cause encoding errors
                sanitized_prompt = prompt.encode('utf-8', errors='surrogatepass').decode('utf-8', errors='replace')

                # Check if streaming is requested and supported
                if on_token is not None:
                    # Use streaming path
                    result = await self._complete_with_streaming(sanitized_prompt, on_token, **kwargs)
                else:
                    # Use non-streaming path
                    result = await self._agent.run(sanitized_prompt)

                # Success - break out of retry loop
                break

            except Exception as e:
                # Check if this is a rate limit error (429)
                error_str = str(e).lower()
                is_rate_limit = (
                    "429" in error_str or
                    "rate limit" in error_str or
                    "overloaded" in error_str or
                    "too many requests" in error_str
                )

                last_error = e

                if is_rate_limit and attempt < max_retries:
                    # Calculate exponential backoff delay
                    delay = min(base_delay * (2 ** attempt), max_delay)

                    if DEBUG_MODE:
                        print(f"⚠️  Rate limit hit (attempt {attempt + 1}/{max_retries + 1}). Retrying in {delay:.1f}s...")

                    # Wait before retrying
                    import asyncio
                    await asyncio.sleep(delay)
                    continue
                else:
                    # Not a rate limit error, or max retries exceeded - re-raise
                    raise

        # If we get here without a result, we exhausted retries
        if last_error:
            raise last_error

        # Extract usage information from the result
        usage = {}

        # Debug: Check what attributes the result object has
        if DEBUG_MODE:
            print(f"DEBUG result object type: {type(result)}")
            print(f"DEBUG result attributes: {dir(result)}")
            if hasattr(result, "_model_name"):
                print(f"DEBUG result._model_name: {result._model_name}")
            if hasattr(result, "model"):
                print(f"DEBUG result.model: {result.model}")

        # Call the usage() method to get actual usage data
        usage_data = None
        if hasattr(result, "usage"):
            try:
                usage_data = result.usage()  # Call the method!
            except Exception as e:
                # Fallback if usage() method fails
                pass

        if usage_data:
            # Pydantic AI usage structure may vary
            # Use new field names first, fall back to deprecated ones
            import warnings
            with warnings.catch_warnings():
                warnings.filterwarnings("ignore", category=DeprecationWarning)

                if hasattr(usage_data, "input_tokens"):
                    usage["input_tokens"] = usage_data.input_tokens or 0
                elif hasattr(usage_data, "request_tokens"):
                    usage["input_tokens"] = usage_data.request_tokens or 0

                if hasattr(usage_data, "output_tokens"):
                    usage["output_tokens"] = usage_data.output_tokens or 0
                elif hasattr(usage_data, "response_tokens"):
                    usage["output_tokens"] = usage_data.response_tokens or 0

            if hasattr(usage_data, "total_tokens"):
                usage["total_tokens"] = usage_data.total_tokens or 0
            else:
                # Calculate total if not available
                usage["total_tokens"] = usage.get("input_tokens", 0) + usage.get(
                    "output_tokens", 0
                )
        else:
            if DEBUG_MODE: print("DEBUG: Using fallback token estimation")
            # Fallback: estimate token usage
            # Use character-based estimation for better accuracy with CJK languages

            def estimate_tokens(text: str) -> int:
                """
                Estimate tokens for text, handling both space-separated and CJK languages.

                Rules:
                - CJK characters (Chinese, Japanese, Korean): ~1 token per character
                - Space-separated words (English, etc.): ~1.3 tokens per word
                - Mixed text: combine both approaches
                """
                if not text:
                    return 0

                # Count CJK characters (Unicode ranges for Chinese, Japanese, Korean)
                import re
                cjk_pattern = re.compile(r'[\u4e00-\u9fff\u3040-\u309f\u30a0-\u30ff\uac00-\ud7af]+')
                cjk_chars = ''.join(cjk_pattern.findall(text))
                cjk_tokens = len(cjk_chars)  # ~1 token per CJK character

                # Remove CJK characters and count remaining words
                non_cjk_text = cjk_pattern.sub(' ', text)
                words = non_cjk_text.split()
                word_tokens = len(words) * 1.3  # ~1.3 tokens per English word

                return int(cjk_tokens + word_tokens)

            input_estimate = estimate_tokens(prompt)
            output_estimate = estimate_tokens(result.output) if isinstance(result.output, str) else 0

            if DEBUG_MODE:
                print(f"DEBUG: Input estimate: {input_estimate} tokens (prompt length: {len(prompt)} chars)")
                print(f"DEBUG: Output estimate: {output_estimate} tokens (output length: {len(result.output) if isinstance(result.output, str) else 0} chars)")

            usage = {
                "input_tokens": int(input_estimate),
                "output_tokens": int(output_estimate),
                "total_tokens": int(input_estimate + output_estimate),
            }
            if DEBUG_MODE: print(f"DEBUG: Final estimated usage: {usage}")

        # Return LLMResponse (for both actual usage and estimated usage paths)
        if DEBUG_MODE:
            print(f"DEBUG complete_with_usage: returning model={self._underlying_model_name} (self.model={self.model})")

        return LLMResponse(
            content=result.output,
            model=self._underlying_model_name,  # Use clean model name (without provider prefix)
            usage=usage,
            finish_reason="stop",
        )

    async def complete_with_tools(
        self,
        messages: list[dict[str, str]],
        tools: list[dict[str, Any]] = None,
        **kwargs,
    ) -> dict[str, Any]:
        """Generate completion with function calling support"""
        # For now, convert to simple completion
        # TODO: Implement proper tool calling with Pydantic AI tools
        if messages:
            last_message = messages[-1]
            if last_message.get("role") == "user":
                result = await self.complete_with_usage(
                    last_message["content"], **kwargs
                )
                return {
                    "content": result.content,
                    "usage": result.usage,
                    "finish_reason": result.finish_reason,
                }

        return {"content": "", "usage": {}, "finish_reason": "stop"}

    async def stream_complete(self, prompt: str, **kwargs) -> AsyncIterator[str]:
        """Stream completion from prompt using Pydantic AI"""
        if not self._agent:
            raise RuntimeError("Pydantic AI agent not initialized")

        try:
            # Use Pydantic AI streaming support
            async with self._agent.run_stream(prompt) as stream:
                async for message in stream:
                    # Handle different message types from Pydantic AI stream
                    if hasattr(message, "snapshot"):
                        # This is a streaming event with partial content
                        if (
                            hasattr(message.snapshot, "all_messages")
                            and message.snapshot.all_messages
                        ):
                            last_message = message.snapshot.all_messages[-1]
                            if (
                                hasattr(last_message, "content")
                                and last_message.content
                            ):
                                yield last_message.content
                    elif hasattr(message, "content") and message.content:
                        # Direct content message
                        yield message.content
                    elif hasattr(message, "delta") and message.delta:
                        # Delta content (incremental updates)
                        yield message.delta

        except Exception as e:
            # Fallback to regular completion if streaming fails
            try:
                result = await self.complete(prompt, **kwargs)
                yield result
            except Exception as fallback_error:
                raise RuntimeError(
                    f"Both streaming and fallback completion failed. Streaming: {str(e)}, Fallback: {str(fallback_error)}"
                ) from e

    async def close(self) -> None:
        """Close provider connections"""
        # Pydantic AI handles cleanup automatically
        pass


def create_pydantic_ai_provider(
    provider_name: str, api_key: str, model: str
) -> PydanticAIProvider:
    """Factory function to create Pydantic AI providers"""

    # Map provider names to model strings that Pydantic AI understands
    model_mapping = {
        "openai": {
            # GPT-5 models (frontier models - latest)
            "gpt-5": "openai:gpt-5",
            "gpt-5-mini": "openai:gpt-5-mini",
            "gpt-5-nano": "openai:gpt-5-nano",
            # GPT-4.1 models
            "gpt-4.1": "openai:gpt-4.1",
            "gpt-4.1-mini": "openai:gpt-4.1-mini",
            "gpt-4.1-nano": "openai:gpt-4.1-nano",
            # GPT-4o models
            "gpt-4o": "openai:gpt-4o",
            "gpt-4o-mini": "openai:gpt-4o-mini",
            # Legacy models
            "gpt-4": "openai:gpt-4",
            "gpt-4-turbo": "openai:gpt-4-turbo-preview",
            "gpt-3.5-turbo": "openai:gpt-3.5-turbo",
        },
        "anthropic": {
            "claude-3-5-sonnet-20241022": "anthropic:claude-3-5-sonnet-20241022",
            "claude-3-5-haiku-20241022": "anthropic:claude-3-5-haiku-20241022",
            "claude-3-opus-20240229": "anthropic:claude-3-opus-20240229",
            "claude-3-7-sonnet-20250219": "anthropic:claude-3-7-sonnet-20250219",
        },
        "gemini": {
            # Gemini 2.5 models (latest)
            "gemini-2.5-flash": "gemini-2.5-flash",
            "gemini-2.5-pro": "gemini-2.5-pro",
            "gemini-2.5-flash-lite": "gemini-2.5-flash-lite",
            # Gemini 2.0 models
            "gemini-2.0-flash-001": "gemini-2.0-flash-001",
            "gemini-2.0-flash-lite-001": "gemini-2.0-flash-lite-001",
            "gemini-2.0-flash-exp": "gemini-2.0-flash-exp",  # Legacy experimental
            # Legacy preview models
            "gemini-2.5-flash-preview-09-2025": "gemini-2.5-flash-preview-09-2025",
            # Gemini 1.5 models (legacy)
            "gemini-1.5-pro": "gemini-1.5-pro",
            "gemini-1.5-flash": "gemini-1.5-flash",
        },
    }

    # Get the appropriate model string with improved fallback logic
    provider_models = model_mapping.get(provider_name.lower(), {})

    if model in provider_models:
        # Model found in mapping - use the mapped value
        pydantic_model = provider_models[model]
    else:
        # Model not in mapping - use the configured model directly with provider prefix
        # This allows for new models that aren't in our mapping yet
        pydantic_model = f"{provider_name.lower()}:{model}"

    # Set API key in environment for Pydantic AI
    import os

    if provider_name.lower() == "openai":
        os.environ["OPENAI_API_KEY"] = api_key
    elif provider_name.lower() == "anthropic":
        os.environ["ANTHROPIC_API_KEY"] = api_key
    elif provider_name.lower() == "gemini":
        os.environ["GEMINI_API_KEY"] = api_key
    elif provider_name.lower() == "moonshot":
        # Moonshot AI uses OpenAI-compatible API
        os.environ["OPENAI_API_KEY"] = api_key
        os.environ["OPENAI_BASE_URL"] = "https://api.moonshot.ai/v1"
        # Use openai: prefix so Pydantic AI uses OpenAI client
        # The actual model name will be sent to the custom base URL
        pydantic_model = f"openai:{model}"
    elif provider_name.lower() == "deepseek":
        # DeepSeek AI uses OpenAI-compatible API
        os.environ["OPENAI_API_KEY"] = api_key
        os.environ["OPENAI_BASE_URL"] = "https://api.deepseek.com"
        # Use openai: prefix so Pydantic AI uses OpenAI client
        # The actual model name will be sent to the custom base URL
        pydantic_model = f"openai:{model}"

    # Pass provider_name explicitly to ensure proper cost tracking
    return PydanticAIProvider(api_key, pydantic_model, provider_name=provider_name.lower())

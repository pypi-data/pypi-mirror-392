"""í”„ë ˆì„ì›Œí¬ ë…ë¦½ì ì¸ ONNX ëª¨ë¸ ë¡œê¹… API."""

import logging
import time
from pathlib import Path
from typing import Any, Optional, Union

import mlflow

from ..clients.onnx import OnnxClient

# onnx_client ì¸ìŠ¤í„´ìŠ¤ ìƒì„±
onnx_client = OnnxClient()
logger = logging.getLogger(__name__)


def log_onnx_model(
    experiment_name: str,
    onnx_model_path: Union[str, Path],
    run_name: Optional[str] = None,
    model_name: Optional[str] = None,
    signature: Optional["mlflow.models.signature.ModelSignature"] = None,
    input_example: Optional[Union[Any, dict[str, Any]]] = None,
    metadata: Optional[dict[str, Any]] = None,
) -> Optional[str]:
    """
    í”„ë ˆì„ì›Œí¬ ë…ë¦½ì ì¸ ONNX ëª¨ë¸ ë¡œê¹… ë° ë°°í¬.

    PyTorchê°€ ì•„ë‹Œ ë‹¤ë¥¸ í”„ë ˆì„ì›Œí¬(TensorFlow, JAX, MXNet ë“±)ì—ì„œ
    í•™ìŠµí•œ ëª¨ë¸ì„ ONNXë¡œ ë³€í™˜í•œ í›„, ì´ í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•˜ì—¬ MLflowì—
    ë¡œê¹…í•˜ê³  ì¶”ë¡  ì„œë¹„ìŠ¤ì— ë°°í¬í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

    ì´ í•¨ìˆ˜ëŠ” @trace_pytorch ë°ì½”ë ˆì´í„°ë¥¼ ì‚¬ìš©í•  ìˆ˜ ì—†ëŠ” ìƒí™©ì—ì„œ
    ONNX ëª¨ë¸ì„ ì§ì ‘ ì—…ë¡œë“œí•˜ê¸° ìœ„í•œ ëŒ€ì•ˆì…ë‹ˆë‹¤.

    Args:
        experiment_name: MLflow ì‹¤í—˜ ì´ë¦„
        onnx_model_path: ONNX ëª¨ë¸ íŒŒì¼ ê²½ë¡œ
        run_name: MLflow ëŸ° ì´ë¦„ (ì„ íƒì‚¬í•­)
        model_name: ëª¨ë¸ ì´ë¦„ (ì„ íƒì‚¬í•­, ê¸°ë³¸ê°’: íŒŒì¼ëª…)
        signature: MLflow ëª¨ë¸ ì‹œê·¸ë‹ˆì²˜ (ì„ íƒì‚¬í•­)
        input_example: ì…ë ¥ ì˜ˆì‹œ (ì„ íƒì‚¬í•­)
        metadata: ì¶”ê°€ ë©”íƒ€ë°ì´í„° (ì„ íƒì‚¬í•­)

    Returns:
        Optional[str]: ì—…ë¡œë“œëœ ëª¨ë¸ ê²½ë¡œ (í”„ë¡œë•ì…˜ ëª¨ë“œê°€ ì•„ë‹Œ ê²½ìš°)

    ì‚¬ìš© ì˜ˆì‹œ:
        ```python
        # TensorFlow ëª¨ë¸ ì‚¬ìš© ì˜ˆ
        import tensorflow as tf
        import tf2onnx

        # TensorFlow ëª¨ë¸ì„ ONNXë¡œ ë³€í™˜
        model = tf.keras.models.load_model('my_model.h5')
        spec = (tf.TensorSpec((None, 224, 224, 3), tf.float32, name="input"),)
        output_path = "model.onnx"

        model_proto, _ = tf2onnx.convert.from_keras(model, input_signature=spec)
        with open(output_path, "wb") as f:
            f.write(model_proto.SerializeToString())

        # ONNX ëª¨ë¸ ë¡œê¹… ë° ì—…ë¡œë“œ
        upload_path = log_onnx_model(
            experiment_name="tensorflow_experiment",
            onnx_model_path=output_path,
            metadata={"framework": "tensorflow", "model_type": "classification"}
        )

        # JAX/Flax ëª¨ë¸ ì‚¬ìš© ì˜ˆ
        # ... JAX ëª¨ë¸ì„ ONNXë¡œ ë³€í™˜ ...
        upload_path = log_onnx_model(
            experiment_name="jax_experiment",
            onnx_model_path="jax_model.onnx",
            metadata={"framework": "jax", "optimizer": "adam"}
        )
        ```

    """
    try:
        # ê²½ë¡œ ê°ì²´ë¡œ ë³€í™˜
        onnx_path = Path(onnx_model_path)
        if not onnx_path.exists():
            raise FileNotFoundError(f"ONNX ëª¨ë¸ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {onnx_path}")

        # ONNX íŒŒì¼ ê²€ì¦
        if onnx_path.suffix.lower() != ".onnx":
            logger.warning(f"íŒŒì¼ í™•ì¥ìê°€ .onnxê°€ ì•„ë‹™ë‹ˆë‹¤: {onnx_path.suffix}")

        # íŒŒì¼ í¬ê¸° ê²€ì¦ (ìµœì†Œ í¬ê¸°)
        file_size = onnx_path.stat().st_size
        if file_size < 1024:  # 1KB ë¯¸ë§Œ
            logger.warning(f"ONNX íŒŒì¼ í¬ê¸°ê°€ ë§¤ìš° ì‘ìŠµë‹ˆë‹¤: {file_size} bytes")

        # ëª¨ë¸ ì´ë¦„ ì„¤ì •
        if model_name is None:
            model_name = onnx_path.stem

        # ì‹¤í—˜ ì„¤ì •
        experiment = mlflow.get_experiment_by_name(experiment_name)
        if experiment is None:
            experiment_id = mlflow.create_experiment(experiment_name)
            logger.info(f"ìƒˆ ì‹¤í—˜ ìƒì„±: {experiment_name}")
        else:
            experiment_id = experiment.experiment_id
            logger.info(f"ê¸°ì¡´ ì‹¤í—˜ ì‚¬ìš©: {experiment_name}")

        start_time = time.time()

        with mlflow.start_run(experiment_id=experiment_id, run_name=run_name) as run:
            logger.info(f"MLflow ì‹¤í–‰ ì‹œì‘ (run_id: {run.info.run_id})")

            # ë©”íƒ€ë°ì´í„° ë¡œê¹…
            if metadata:
                mlflow.log_params(metadata)

            # ê¸°ë³¸ ì •ë³´ ë¡œê¹…
            file_size_mb = onnx_path.stat().st_size / (1024 * 1024)
            mlflow.log_params(
                {
                    "model_name": model_name,
                    "onnx_file_size_mb": file_size_mb,
                    "source_framework": (
                        metadata.get("framework", "unknown") if metadata else "unknown"
                    ),
                }
            )

            # ONNX ëª¨ë¸ì„ MLflowì— ë¡œê¹…
            import onnx

            onnx_model = onnx.load(str(onnx_path))
            mlflow.onnx.log_model(
                onnx_model=onnx_model,
                artifact_path="model",
                signature=signature,
                input_example=input_example,
            )
            logger.info("ONNX ëª¨ë¸ MLflow ë¡œê¹… ì™„ë£Œ")

            # onnx_clientë¥¼ í†µí•œ ì—…ë¡œë“œ
            try:
                upload_result = onnx_client.upload(onnx_path)
                if upload_result:
                    mlflow.log_param("onnx_upload_path", upload_result)
                    logger.info(f"ğŸš€ ONNX ëª¨ë¸ ì„œë¹„ìŠ¤ ì—…ë¡œë“œ ì™„ë£Œ: {upload_result}")
                else:
                    logger.warning("âš ï¸ ONNX ì—…ë¡œë“œ ì‹¤íŒ¨")

            except Exception as e:
                logger.error(f"ONNX í´ë¼ì´ì–¸íŠ¸ ì—…ë¡œë“œ ì‹¤íŒ¨: {e}")
                mlflow.log_param("upload_error", str(e))
                upload_result = None

            # ì‹¤í–‰ ì‹œê°„ ë¡œê¹…
            total_time = time.time() - start_time
            mlflow.log_metric("total_execution_time", total_time)

            logger.info(f"ğŸ‰ ONNX ëª¨ë¸ ë¡œê¹… ì™„ë£Œ (ì‹¤í–‰ì‹œê°„: {total_time:.2f}ì´ˆ)")

            return upload_result

    except Exception as e:
        logger.error(f"ONNX ëª¨ë¸ ë¡œê¹… ì‹¤íŒ¨: {e}")
        raise

"""PyTorch ëª¨ë¸ í›ˆë ¨ ìë™í™”ë¥¼ ìœ„í•œ ë°ì½”ë ˆì´í„°."""

import functools
import logging
import os
import tempfile
import time
from pathlib import Path
from typing import Any, Optional, Union

import httpx
import mlflow
import mlflow.pytorch
import torch

from ..clients.onnx import OnnxClient

# onnx_client ì¸ìŠ¤í„´ìŠ¤ ìƒì„±
onnx_client = OnnxClient()
logger = logging.getLogger(__name__)


# ============================================================================
# ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜ë“¤
# ============================================================================


def _convert_to_numpy(
    tensor_data: Union[torch.Tensor, dict[str, torch.Tensor], tuple, list],
) -> Union[Any, dict[str, Any]]:
    """PyTorch í…ì„œë¥¼ NumPy ë°°ì—´ë¡œ ë³€í™˜ (MLflow infer_signature ìš©)."""
    if isinstance(tensor_data, torch.Tensor):
        return tensor_data.detach().cpu().numpy()
    elif isinstance(tensor_data, dict):
        return {key: _convert_to_numpy(value) for key, value in tensor_data.items()}
    elif isinstance(tensor_data, (tuple, list)):
        return [_convert_to_numpy(item) for item in tensor_data]
    else:
        return tensor_data


def _infer_model_schema(
    model: torch.nn.Module, sample_input: Union[torch.Tensor, dict[str, torch.Tensor]]
) -> "mlflow.models.signature.ModelSignature":
    """
    PyTorch ëª¨ë¸ë¡œë¶€í„° ìë™ìœ¼ë¡œ ì…ë ¥/ì¶œë ¥ ìŠ¤í‚¤ë§ˆ ì¶”ì¶œ.

    Args:
        model: PyTorch ëª¨ë¸
        sample_input: ìƒ˜í”Œ ì…ë ¥ (ì‹¤ì œ ëª¨ë¸ ì‹¤í–‰ìš©)

    Returns:
        ModelSignature: ìë™ ì¶”ì¶œëœ ìŠ¤í‚¤ë§ˆ

    """
    from mlflow.models.signature import infer_signature

    model.eval()
    device = next(model.parameters()).device

    # ìƒ˜í”Œ ì…ë ¥ì„ ëª¨ë¸ê³¼ ê°™ì€ ë””ë°”ì´ìŠ¤ë¡œ ì´ë™
    if isinstance(sample_input, torch.Tensor):
        sample_input = sample_input.to(device)
    elif isinstance(sample_input, dict):
        sample_input = {k: v.to(device) for k, v in sample_input.items()}

    # ì‹¤ì œ ëª¨ë¸ ì‹¤í–‰í•˜ì—¬ ì¶œë ¥ í™•ì¸
    with torch.no_grad():
        if isinstance(sample_input, dict):
            sample_output = model(**sample_input)
        else:
            sample_output = model(sample_input)

    # PyTorch í…ì„œë¥¼ NumPyë¡œ ë³€í™˜
    numpy_input = _convert_to_numpy(sample_input)
    numpy_output = _convert_to_numpy(sample_output)

    # MLflow ìë™ ì¶”ë¡  ì‚¬ìš©
    signature = infer_signature(numpy_input, numpy_output)

    logger.info(f"ìë™ ì¶”ì¶œëœ ìŠ¤í‚¤ë§ˆ: {signature}")
    return signature


def _generate_input_output_names(
    signature: "mlflow.models.signature.ModelSignature",
) -> tuple[list[str], list[str]]:
    """
    MLflow signatureë¡œë¶€í„° input/output ì´ë¦„ë“¤ì„ ìƒì„±í•©ë‹ˆë‹¤.

    ë‹¤ì–‘í•œ MLflow ë²„ì „ í˜¸í™˜ì„±ì„ ê³ ë ¤í•©ë‹ˆë‹¤.
    """
    input_names: list[str] = []
    output_names: list[str] = []

    # ì…ë ¥ ì´ë¦„ ìƒì„± - ì—¬ëŸ¬ ë°©ë²• ì‹œë„
    try:
        # ë°©ë²• 2: ìŠ¤í‚¤ë§ˆì—ì„œ ì´ë¦„ ì¶”ì¶œ
        if not input_names and hasattr(signature.inputs, "schema"):
            schema = signature.inputs.schema
            if hasattr(schema, "names") and schema.names:
                input_names = list(schema.names)
            elif hasattr(schema, "input_names") and callable(schema.input_names):
                potential_names = schema.input_names()
                if potential_names:
                    input_names = list(potential_names)

        # ë°©ë²• 3: í…ì„œ ì •ë³´ì—ì„œ ì¶”ì¶œ ì‹œë„
        if not input_names:
            try:
                input_spec = str(signature.inputs)
                if "'" in input_spec:  # 'image': Tensor, 'mask': Tensor í˜•íƒœ
                    import re

                    names = re.findall(r"'([^']+)':", input_spec)
                    if names:
                        input_names = names
            except Exception:
                pass

        # ë°©ë²• 4: ê¸°ë³¸ ì´ë¦„ ìƒì„±
        if not input_names:
            # signature.inputsë¥¼ ë¶„ì„í•˜ì—¬ ê°œìˆ˜ ì¶”ì •
            inputs_str = str(signature.inputs)
            if "Tensor" in inputs_str:
                tensor_count = inputs_str.count("Tensor")
                input_names = [f"input_{i}" for i in range(max(1, tensor_count))]
            else:
                input_names = ["input_0"]

    except Exception as e:
        logger.debug(f"ì…ë ¥ ì´ë¦„ ìƒì„± ì¤‘ ì˜¤ë¥˜: {e}")
        input_names = ["input_0"]

    # ì¶œë ¥ ì´ë¦„ ìƒì„± - ìœ ì‚¬í•œ ë°©ë²•ë“¤
    try:
        # MLflow outputsëŠ” ì¼ë°˜ì ìœ¼ë¡œ input_names ë©”ì„œë“œê°€ ì—†ìŒ
        if hasattr(signature.outputs, "schema"):
            schema = signature.outputs.schema
            if hasattr(schema, "names") and schema.names:
                output_names = list(schema.names)

        # ê¸°ë³¸ ì´ë¦„ ìƒì„±
        if not output_names:
            outputs_str = str(signature.outputs)
            if "Tensor" in outputs_str:
                tensor_count = outputs_str.count("Tensor")
                output_names = [f"output_{i}" for i in range(max(1, tensor_count))]
            else:
                output_names = ["output_0"]

    except Exception as e:
        logger.debug(f"ì¶œë ¥ ì´ë¦„ ìƒì„± ì¤‘ ì˜¤ë¥˜: {e}")
        output_names = ["output_0"]

    logger.debug(f"ìƒì„±ëœ ì´ë¦„ - ì…ë ¥: {input_names}, ì¶œë ¥: {output_names}")
    return input_names, output_names


def _convert_pytorch_to_onnx_with_client(
    model: torch.nn.Module,
    sample_input: Union[torch.Tensor, dict[str, torch.Tensor]],
    signature: "mlflow.models.signature.ModelSignature",
    onnx_opset_version: int = 17,
    custom_dynamic_axes: Optional[dict[str, dict[int, str]]] = None,
) -> Optional[str]:
    """
    PyTorch ëª¨ë¸ì„ ONNXë¡œ ë³€í™˜í•˜ê³  onnx_clientë¥¼ í†µí•´ ì—…ë¡œë“œí•©ë‹ˆë‹¤.

    Args:
        model: PyTorch ëª¨ë¸
        sample_input: ìƒ˜í”Œ ì…ë ¥
        signature: MLflow ì‹œê·¸ë‹ˆì²˜
        onnx_opset_version: ONNX opset ë²„ì „
        custom_dynamic_axes: ì‚¬ìš©ì ì •ì˜ dynamic_axes (ì„ íƒì‚¬í•­)

    Returns:
        Optional[str]: ì—…ë¡œë“œëœ ëª¨ë¸ ê²½ë¡œ (í”„ë¡œë•ì…˜ ëª¨ë“œê°€ ì•„ë‹Œ ê²½ìš°)

    """
    try:
        # ìŠ¤í‚¤ë§ˆë¡œë¶€í„° input/output ì´ë¦„ ìë™ ìƒì„±
        input_names, output_names = _generate_input_output_names(signature)

        logger.info(f"ONNX ë³€í™˜ ì‹œì‘ - ì…ë ¥: {input_names}, ì¶œë ¥: {output_names}")

        # ì„ì‹œ ONNX íŒŒì¼ ìƒì„±
        with tempfile.NamedTemporaryFile(suffix=".onnx", delete=False) as tmp_file:
            onnx_path = tmp_file.name

        conversion_start = time.time()

        # ğŸ¯ ê°œì„ ëœ dynamic_axes êµ¬ì„±
        dynamic_axes = {}

        # 1. ê¸°ë³¸ ë°°ì¹˜ ì°¨ì› ì„¤ì • (ëª¨ë“  ì…ë ¥/ì¶œë ¥ì— ì ìš©)
        for input_name in input_names:
            dynamic_axes[input_name] = {0: "batch_size"}

        for output_name in output_names:
            dynamic_axes[output_name] = {0: "batch_size"}

        # 2. ì‚¬ìš©ì ì •ì˜ dynamic_axes ë³‘í•©
        if custom_dynamic_axes:
            for tensor_name, axes_dict in custom_dynamic_axes.items():
                if tensor_name in dynamic_axes:
                    # ê¸°ì¡´ ì¶• ì •ë³´ì™€ ë³‘í•©
                    dynamic_axes[tensor_name].update(axes_dict)
                else:
                    # ìƒˆë¡œìš´ í…ì„œ ì¶”ê°€
                    dynamic_axes[tensor_name] = axes_dict.copy()

        logger.info(f"ìµœì¢… Dynamic axes êµ¬ì„±: {dynamic_axes}")

        # PyTorch â†’ ONNX ë³€í™˜ (í˜¸í™˜ì„± ìš°ì„ )
        # sample_inputì„ ì ì ˆí•œ í˜•íƒœë¡œ ë³€í™˜
        export_args: Any
        if isinstance(sample_input, torch.Tensor):
            export_args = (sample_input,)
        elif isinstance(sample_input, dict):
            # dict í˜•íƒœì˜ ì…ë ¥ì€ ê·¸ëŒ€ë¡œ ì‚¬ìš©
            export_args = sample_input
        else:
            export_args = sample_input

        try:
            # ë™ì  í¬ê¸° ì§€ì› ì‹œë„
            torch.onnx.export(
                model,
                export_args,  # type: ignore[arg-type]
                onnx_path,
                export_params=True,
                opset_version=onnx_opset_version,
                do_constant_folding=True,
                input_names=input_names,
                output_names=output_names,
                dynamic_axes=dynamic_axes,
                verbose=False,
            )
            logger.info("ë™ì  í¬ê¸° ONNX ëª¨ë¸ ë³€í™˜ ì™„ë£Œ")

        except Exception as e:
            # ê³ ì • í¬ê¸°ë¡œ ì¬ì‹œë„ (dynamic_axes ì œê±°)
            logger.warning(
                f"ë™ì  í¬ê¸° ONNX ë³€í™˜ ì‹¤íŒ¨, ê³ ì • í¬ê¸°ë¡œ ì¬ì‹œë„: {str(e)[:100]}..."
            )
            try:
                torch.onnx.export(
                    model,
                    export_args,  # type: ignore[arg-type]
                    onnx_path,
                    export_params=True,
                    opset_version=onnx_opset_version,
                    do_constant_folding=True,
                    input_names=input_names,
                    output_names=output_names,
                    verbose=False,
                )
                logger.info("ê³ ì • í¬ê¸° ONNX ëª¨ë¸ ë³€í™˜ ì™„ë£Œ")
            except Exception as e2:
                # ìµœì†Œí•œì˜ ì„¤ì •ìœ¼ë¡œ ë§ˆì§€ë§‰ ì‹œë„
                logger.warning(
                    f"í‘œì¤€ ONNX ë³€í™˜ë„ ì‹¤íŒ¨, ìµœì†Œ ì„¤ì •ìœ¼ë¡œ ì¬ì‹œë„: {str(e2)[:100]}..."
                )
                torch.onnx.export(
                    model,
                    export_args,  # type: ignore[arg-type]
                    onnx_path,
                    export_params=True,
                    opset_version=onnx_opset_version,
                )
                logger.info("ìµœì†Œ ì„¤ì • ONNX ëª¨ë¸ ë³€í™˜ ì™„ë£Œ")

        conversion_time = time.time() - conversion_start

        onnx_path_obj = Path(onnx_path)
        if not onnx_path_obj.exists():
            raise FileNotFoundError("ONNX íŒŒì¼ì´ ìƒì„±ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")

        file_size_mb = onnx_path_obj.stat().st_size / (1024 * 1024)

        # ONNX ë©”íƒ€ë°ì´í„° ë¡œê¹…
        onnx_metadata = {
            "onnx_conversion_time": conversion_time,
            "onnx_file_size_mb": file_size_mb,
            "onnx_opset_version": onnx_opset_version,
        }
        mlflow.log_metrics(onnx_metadata)
        mlflow.log_params(
            {
                "onnx_input_names": input_names,
                "onnx_output_names": output_names,
            }
        )

        logger.info(f"ONNX ë³€í™˜ ì™„ë£Œ: {onnx_path} ({file_size_mb:.2f}MB)")

        # ğŸ”¥ onnx_clientë¥¼ í†µí•œ ì—…ë¡œë“œ ë° RabbitMQ ë°œí–‰
        try:
            upload_result = onnx_client.upload(onnx_path)
            logger.info("âœ… ONNX ëª¨ë¸ ì—…ë¡œë“œ ë° RabbitMQ ë°œí–‰ ì™„ë£Œ")

            # ì„ì‹œ íŒŒì¼ ì •ë¦¬
            onnx_path_obj.unlink()

            return upload_result

        except Exception as e:
            logger.error(f"ONNX í´ë¼ì´ì–¸íŠ¸ ì—…ë¡œë“œ ì‹¤íŒ¨: {e}")
            # ì„ì‹œ íŒŒì¼ ì •ë¦¬
            if onnx_path_obj.exists():
                onnx_path_obj.unlink()
            raise

    except Exception as e:
        logger.error(f"ONNX ë³€í™˜ ì‹¤íŒ¨: {e}")
        mlflow.log_param("onnx_conversion_error", str(e))
        return None


def trace_pytorch(
    model_name: str,
    sample_input: Union[torch.Tensor, dict[str, torch.Tensor]],
    run_name: Optional[str] = None,
    device: str = "cuda" if torch.cuda.is_available() else "cpu",
    onnx_opset_version: int = 17,
    auto_convert_onnx: bool = True,
    log_model_info: bool = True,
    enable_autolog: bool = True,
    base_image: Optional[str] = None,
    dynamic_axes: Optional[dict[str, dict[int, str]]] = None,
):
    """
    ì™„ì „ ìë™í™”ëœ PyTorch ëª¨ë¸ ì¶”ì  (experiment ì´ë¦„ ìë™ ìƒì„±).

    ì‹¤í—˜ ì´ë¦„ì€ í™˜ê²½ë³€ìˆ˜ MODEL_IDì™€ model_nameìœ¼ë¡œ ìë™ êµ¬ì„±ë©ë‹ˆë‹¤:
    - MODEL_ID ì¡´ì¬: "{model_id}_{model_name}" (ì˜ˆ: "42_resnet50-classifier")
    - MODEL_ID ì—†ìŒ: "{model_name}" (ì˜ˆ: "resnet50-classifier")

    ìë™í™” ë²”ìœ„:
        âœ… ìë™ ì²˜ë¦¬:
            - MLflow ì‹¤í—˜ ì´ë¦„ ìë™ ìƒì„± (MODEL_ID + model_name)
            - MLflow ì‹¤í—˜/ëŸ° ìƒì„± ë° ê´€ë¦¬
            - ëª¨ë¸ ìŠ¤í‚¤ë§ˆ ìë™ ì¶”ë¡  (ì‹¤ì œ ëª¨ë¸ ì‹¤í–‰)
            - ëª¨ë¸ ì•„í‹°íŒ©íŠ¸ ìë™ ë¡œê¹… (enable_autolog=True ì‹œ)
            - PyTorch â†’ ONNX ìë™ ë³€í™˜
            - S3/MinIO ìë™ ì—…ë¡œë“œ
            - RabbitMQ ë©”ì‹œì§€ ë°œí–‰
            - Triton config.pbtxt ìƒì„±

        ğŸ“ ìˆ˜ë™ ì²˜ë¦¬ í•„ìš”:
            - í•™ìŠµ ë©”íŠ¸ë¦­ ë¡œê¹… (mlflow.log_metric())
            - í•˜ì´í¼íŒŒë¼ë¯¸í„° ë¡œê¹… (mlflow.log_params())
            - ì»¤ìŠ¤í…€ ì•„í‹°íŒ©íŠ¸/íƒœê·¸

    Args:
        model_name: ëª¨ë¸ ì´ë¦„ (í•„ìˆ˜)
            - experiment_name ìë™ êµ¬ì„±ì— ì‚¬ìš©
            - MLflowì— ê¸°ë¡ë˜ê³ , `keynet-train push`ì—ì„œ uploadKeyì˜ modelNameìœ¼ë¡œ ì‚¬ìš©
            - ëª…ì‹œì ìœ¼ë¡œ ì„ ì–¸í•˜ì—¬ ëª¨ë¸ì˜ ì˜ë„ë¥¼ ëª…í™•íˆ í‘œí˜„
            - CLI `--model-name` ì˜µì…˜ìœ¼ë¡œ override ê°€ëŠ¥
            - ì˜ˆ: "resnet50-classifier", "bert-sentiment-analyzer"
        sample_input: ìƒ˜í”Œ ì…ë ¥ (torch.Tensor ë˜ëŠ” Dict[str, torch.Tensor])
            - Tensor: ë‹¨ì¼ ì…ë ¥, ONNX ì…ë ¥ëª… "input_0"
            - Dict: ë‹¤ì¤‘ ì…ë ¥, ë”•ì…”ë„ˆë¦¬ í‚¤ê°€ ONNX ì…ë ¥ëª…ìœ¼ë¡œ ì‚¬ìš© (ê¶Œì¥)
        run_name: MLflow ëŸ° ì´ë¦„ (ì„ íƒì‚¬í•­)
        device: ë””ë°”ì´ìŠ¤ ("cuda" ë˜ëŠ” "cpu")
        onnx_opset_version: ONNX opset ë²„ì „ (ê¸°ë³¸ê°’: 17)
        auto_convert_onnx: PyTorch â†’ ONNX ìë™ ë³€í™˜ ì—¬ë¶€ (ê¸°ë³¸ê°’: True)
        log_model_info: ëª¨ë¸ ì •ë³´ ë¡œê¹… ì—¬ë¶€ (ê¸°ë³¸ê°’: True)
        enable_autolog: MLflow autolog í™œì„±í™” ì—¬ë¶€ (ê¸°ë³¸ê°’: True)
            - True: ëª¨ë¸ ì•„í‹°íŒ©íŠ¸ ìë™ ë¡œê¹…
            - False: ëª¨ë¸ë„ ìˆ˜ë™ ë¡œê¹… í•„ìš”
            - ì£¼ì˜: ë©”íŠ¸ë¦­ì€ True/False ìƒê´€ì—†ì´ í•­ìƒ ìˆ˜ë™ ë¡œê¹… í•„ìš”
        base_image: ì»¨í…Œì´ë„ˆ ë² ì´ìŠ¤ ì´ë¯¸ì§€ (ì„ íƒì‚¬í•­)
            - ì§€ì •í•˜ë©´ MLflowì— ê¸°ë¡ë˜ê³ , `keynet-train push`ì—ì„œ ìë™ ì‚¬ìš©
            - ì˜ˆ: "pytorch/pytorch:2.0.1-cuda11.7-cudnn8-runtime"
            - `keynet-train push --dockerfile`ë¡œ ì»¤ìŠ¤í…€ Dockerfile ì‚¬ìš© ì‹œ ë¬´ì‹œë¨
            - CLI `--base-image` ì˜µì…˜ì´ ì´ ê°’ë³´ë‹¤ ìš°ì„ í•¨
        dynamic_axes: ì‚¬ìš©ì ì •ì˜ dynamic_axes (ì„ íƒì‚¬í•­)
            - ê¸°ë³¸ì ìœ¼ë¡œ ë°°ì¹˜ ì°¨ì›(0ë²ˆ)ì€ ìë™ìœ¼ë¡œ ë™ì  í¬ê¸° ì„¤ì •
            - ì¶”ê°€ ì°¨ì› ê°€ë³€ ì„¤ì •ì´ í•„ìš”í•œ ê²½ìš° ì‚¬ìš©

    Returns:
        í•¨ìˆ˜ decorator

    Raises:
        ValueError: í•¨ìˆ˜ê°€ torch.nn.Module ì´ì™¸ì˜ ê°ì²´ë¥¼ ë°˜í™˜í•˜ëŠ” ê²½ìš°

    Environment Variables:
        MODEL_ID: (ì„ íƒì‚¬í•­) ëª¨ë¸ ID, experiment_name êµ¬ì„±ì— ì‚¬ìš©
            - ì„¤ì •ë¨: experiment = "{MODEL_ID}_{model_name}"
            - ì„¤ì • ì•ˆë¨: experiment = "{model_name}"
            - ë¡œì»¬ ê°œë°œ: ì—†ìŒ (model_nameë§Œ ì‚¬ìš©)
            - í”„ë¡œë•ì…˜: ë°±ì—”ë“œì—ì„œ ì£¼ì… ê°€ëŠ¥

    Note:
        - ë°ì½”ë ˆì´íŒ…ëœ í•¨ìˆ˜ëŠ” ë°˜ë“œì‹œ torch.nn.Module ê°ì²´ë§Œ ë°˜í™˜í•´ì•¼ í•©ë‹ˆë‹¤
        - enable_autolog=Trueì¼ ë•Œë„ í•™ìŠµ ë©”íŠ¸ë¦­ì€ ìˆ˜ë™ìœ¼ë¡œ ë¡œê¹…í•´ì•¼ í•©ë‹ˆë‹¤
        - autologëŠ” ëª¨ë¸ íŒŒë¼ë¯¸í„°ì™€ ì•„í‹°íŒ©íŠ¸ë§Œ ìë™ ë¡œê¹…í•©ë‹ˆë‹¤
        - mlflow.log_metric(), mlflow.log_params()ëŠ” ë°ì½”ë ˆì´í„° ë‚´ë¶€ì—ì„œ ì§ì ‘ í˜¸ì¶œí•˜ì„¸ìš”

    ì‚¬ìš© ì˜ˆì‹œ:
        ```python
        # ë¡œì»¬ ê°œë°œ (MODEL_ID ì—†ìŒ)
        @trace_pytorch(
            model_name="resnet50-classifier",
            sample_input=torch.randn(1, 3, 224, 224)
        )
        def train_model():
            model = MyModel()
            # í•™ìŠµ ì½”ë“œ...
            return model
        # Experiment ì´ë¦„: "resnet50-classifier"

        # í”„ë¡œë•ì…˜ (MODEL_ID=42)
        # export MODEL_ID=42
        @trace_pytorch(
            model_name="resnet50-classifier",
            sample_input=torch.randn(1, 3, 224, 224)
        )
        def train_model():
            model = MyModel()
            # í•™ìŠµ ì½”ë“œ...
            return model
        # Experiment ì´ë¦„: "42_resnet50-classifier"

        # ë‹¤ì¤‘ ì…ë ¥ ëª¨ë¸
        @trace_pytorch(
            model_name="unet-segmentation",
            sample_input={"image": torch.randn(1, 3, 224, 224), "mask": torch.randn(1, 1, 224, 224)}
        )
        def train_multi_input_model():
            model = MultiInputModel()
            # í•™ìŠµ ì½”ë“œ...
            return model

        # ë² ì´ìŠ¤ ì´ë¯¸ì§€ ì§€ì •
        @trace_pytorch(
            model_name="resnet50-classifier",
            sample_input=torch.randn(1, 3, 224, 224),
            base_image="pytorch/pytorch:2.0.1-cuda11.7-cudnn8-runtime"
        )
        def train_model():
            model = MyModel()
            # í•™ìŠµ ì½”ë“œ...
            return model
        ```

    """
    # ë””ë°”ì´ìŠ¤ ê²€ì¦
    if not torch.cuda.is_available() and device == "cuda":
        logger.warning("CUDAê°€ ì‚¬ìš© ë¶ˆê°€í•˜ë¯€ë¡œ CPUë¡œ ë³€ê²½í•©ë‹ˆë‹¤.")
        device = "cpu"

    def decorator(func):
        @functools.wraps(func)
        def wrapper(*args, **kwargs):
            # MODEL_IDì™€ model_nameìœ¼ë¡œ experiment_name ìë™ ìƒì„±
            from keynet_train.config.settings import TrainConfig
            from keynet_train.utils.experiment import generate_experiment_name

            config = TrainConfig()
            experiment_name = generate_experiment_name(
                model_id=config.model_id, model_name=model_name
            )

            if enable_autolog:
                mlflow.pytorch.autolog()
                logger.info("âœ… MLflow PyTorch autolog í™œì„±í™” ì™„ë£Œ")
            else:
                mlflow.pytorch.autolog(disable=True)
                logger.info("ğŸš« MLflow PyTorch autolog ë¹„í™œì„±í™”")

            # ì‹¤í—˜ ì„¤ì •
            experiment = mlflow.get_experiment_by_name(experiment_name)
            if experiment is None:
                experiment_id = mlflow.create_experiment(experiment_name)
                logger.info(f"ìƒˆ ì‹¤í—˜ ìƒì„±: {experiment_name}")
            else:
                experiment_id = experiment.experiment_id
                logger.info(f"ê¸°ì¡´ ì‹¤í—˜ ì‚¬ìš©: {experiment_name}")

            start_time = time.time()

            with mlflow.start_run(
                experiment_id=experiment_id, run_name=run_name
            ) as run:
                try:
                    # SpringBoot ì„œë²„ë¡œ MlFlow run_id ì „ì†¡
                    run_uuid = run.info.run_id
                    train_id = os.environ.get("TRAIN_ID")
                    training_match_end_point = os.environ.get(
                        "APP_TRAINING_MATCH_ENDPOINT"
                    )
                    api_key = os.environ.get("APP_API_KEY")

                    if training_match_end_point and train_id and api_key:
                        # ì¬ì‹œë„ ì„¤ì •: 1, 2, 4, 8, 16ì´ˆ ê°„ê²©ìœ¼ë¡œ 5ë²ˆ ì‹œë„
                        max_retries = 5
                        retry_delay = 1

                        for attempt in range(max_retries):
                            try:
                                api_url = training_match_end_point.replace(
                                    "{train_id}", train_id
                                ).replace("{run_uuid}", run_uuid)
                                logger.debug(
                                    f"ğŸ”— MLflow run_id ì„œë²„ ì „ì†¡ ì‹œë„ {attempt + 1}/{max_retries}: {api_url}"
                                )
                                headers = {"X-INTERNAL-API-KEY": api_key}
                                response = httpx.patch(
                                    api_url, headers=headers, timeout=10.0
                                )
                                response.raise_for_status()
                                logger.info(
                                    f"âœ… MLflow run_idë¥¼ ì„œë²„ë¡œ ì „ì†¡ ì™„ë£Œ: trainId - {train_id} | runUuid - {run_uuid}"
                                )
                                break  # ì„±ê³µ ì‹œ ë£¨í”„ íƒˆì¶œ
                            except httpx.HTTPError as e:
                                if (
                                    attempt < max_retries - 1
                                ):  # ë§ˆì§€ë§‰ ì‹œë„ê°€ ì•„ë‹ˆë©´ ì¬ì‹œë„
                                    logger.warning(
                                        f"âš ï¸ MLflow run_id ì„œë²„ ì „ì†¡ ì‹¤íŒ¨ (ì‹œë„ {attempt + 1}/{max_retries}), {retry_delay}ì´ˆ í›„ ì¬ì‹œë„: {e}"
                                    )
                                    time.sleep(retry_delay)
                                    retry_delay *= 2  # ì§€ìˆ˜ ë°±ì˜¤í”„
                                else:  # ë§ˆì§€ë§‰ ì‹œë„ë„ ì‹¤íŒ¨
                                    logger.error(
                                        f"âš ï¸ MLflow run_id ì„œë²„ ì „ì†¡ ì‹¤íŒ¨ (ëª¨ë“  ì¬ì‹œë„ ì†Œì§„, ê³„ì† ì§„í–‰): {e}"
                                    )
                    else:
                        logger.warning("MLflow run_id ì„œë²„ ì „ì†¡ ìŠ¤í‚µ (í™˜ê²½ë³€ìˆ˜ ë¯¸ì„¤ì •)")

                    logger.info(f"MLflow ì‹¤í–‰ ì‹œì‘ (run_id: {run_uuid})")

                    # Log model_name if provided
                    if model_name:
                        mlflow.log_param("model_name", model_name)
                        logger.info(f"Model name: {model_name}")

                    # Log base_image if provided
                    if base_image:
                        mlflow.log_param("container_base_image", base_image)
                        logger.info(f"Container base image: {base_image}")

                    # ì‚¬ìš©ì í•¨ìˆ˜ ì‹¤í–‰
                    result = func(*args, **kwargs)

                    # ë°˜í™˜ê°’ ê²€ì¦ - ëª¨ë¸ë§Œ ë°˜í™˜í•´ì•¼ í•¨!
                    if not isinstance(result, torch.nn.Module):
                        raise ValueError(
                            "í•¨ìˆ˜ëŠ” torch.nn.Moduleë§Œ ë°˜í™˜í•´ì•¼ í•©ë‹ˆë‹¤.\n"
                            f"ë°›ì€ íƒ€ì…: {type(result)}\n"
                            "ì˜ˆì‹œ: return model"
                        )

                    model = result

                    # ëª¨ë¸ì„ ì§€ì •ëœ ë””ë°”ì´ìŠ¤ë¡œ ì´ë™
                    model = model.to(device)
                    logger.info(f"ëª¨ë¸ì´ {device} ë””ë°”ì´ìŠ¤ë¡œ ì´ë™ë˜ì—ˆìŠµë‹ˆë‹¤.")

                    # sample_inputë„ ë™ì¼í•œ ë””ë°”ì´ìŠ¤ë¡œ ì´ë™
                    if isinstance(sample_input, torch.Tensor):
                        device_sample_input = sample_input.to(device)
                    elif isinstance(sample_input, dict):
                        device_sample_input = {
                            k: v.to(device) for k, v in sample_input.items()
                        }
                    else:
                        raise ValueError(
                            f"ì§€ì›ë˜ì§€ ì•ŠëŠ” sample_input íƒ€ì…: {type(sample_input)}"
                        )

                    # ğŸš€ í•µì‹¬: ì‹¤ì œ ëª¨ë¸ë¡œë¶€í„° ìŠ¤í‚¤ë§ˆ ìë™ ì¶”ì¶œ
                    signature = _infer_model_schema(model, device_sample_input)

                    # ëª¨ë¸ ì •ë³´ ë¡œê¹…
                    if log_model_info:
                        model_info = {
                            "model_class": model.__class__.__name__,
                            "device": str(device),
                            "total_params": sum(p.numel() for p in model.parameters()),
                            "trainable_params": sum(
                                p.numel() for p in model.parameters() if p.requires_grad
                            ),
                        }

                        # ì…ë ¥ ì •ë³´ ìë™ ì¶”ì¶œ
                        if isinstance(device_sample_input, torch.Tensor):
                            model_info["input_shape"] = tuple(device_sample_input.shape)
                            model_info["input_dtype"] = str(device_sample_input.dtype)
                        elif isinstance(device_sample_input, dict):
                            model_info["input_shapes"] = {
                                k: tuple(v.shape)
                                for k, v in device_sample_input.items()
                            }
                            model_info["input_dtypes"] = {
                                k: str(v.dtype) for k, v in device_sample_input.items()
                            }

                        mlflow.log_params(model_info)
                        logger.info(f"ëª¨ë¸ ì •ë³´ ë¡œê¹… ì™„ë£Œ: {model_info['model_class']}")

                    # ğŸ¤ Autologì™€ ìˆ˜ë™ ë¡œê¹…ì˜ ì¡°í™”
                    # autologê°€ í™œì„±í™”ë˜ì–´ ìˆìœ¼ë©´ ì¤‘ë³µ ë¡œê¹… ë°©ì§€
                    if not enable_autolog:
                        # autologê°€ ë¹„í™œì„±í™”ëœ ê²½ìš°ì—ë§Œ ìˆ˜ë™ìœ¼ë¡œ ëª¨ë¸ ë¡œê¹…
                        model_info = mlflow.pytorch.log_model(
                            pytorch_model=model,
                            artifact_path="model",
                            signature=signature,
                            input_example=_convert_to_numpy(device_sample_input),
                        )
                        logger.info("PyTorch ëª¨ë¸ ìˆ˜ë™ ë¡œê¹… ì™„ë£Œ")
                    else:
                        logger.info("PyTorch ëª¨ë¸ì€ autologì— ì˜í•´ ìë™ ë¡œê¹…ë©ë‹ˆë‹¤")

                    # ğŸ”¥ ONNX ë³€í™˜ ë° ì—…ë¡œë“œ (onnx_client í™œìš©)
                    if auto_convert_onnx:
                        upload_result = _convert_pytorch_to_onnx_with_client(
                            model=model,
                            sample_input=device_sample_input,
                            signature=signature,
                            onnx_opset_version=onnx_opset_version,
                            custom_dynamic_axes=dynamic_axes,
                        )

                        if upload_result:
                            mlflow.log_param("onnx_upload_path", upload_result)
                            mlflow.log_param("custom_dynamic_axes", str(dynamic_axes))
                            logger.info(
                                f"ğŸš€ ONNX ëª¨ë¸ ì„œë¹„ìŠ¤ ì—…ë¡œë“œ ì™„ë£Œ: {upload_result}"
                            )
                        else:
                            logger.warning("âš ï¸ ONNX ì—…ë¡œë“œ ì‹¤íŒ¨")

                    # ì‹¤í–‰ ì‹œê°„ ë¡œê¹…
                    total_time = time.time() - start_time
                    mlflow.log_metric("total_execution_time", total_time)

                    logger.info(f"ğŸ‰ ëª¨ë¸ ì¶”ì  ì™„ë£Œ (ì‹¤í–‰ì‹œê°„: {total_time:.2f}ì´ˆ)")
                    logger.info(f"ìë™ ì¶”ì¶œëœ ìŠ¤í‚¤ë§ˆ: {signature}")

                    return model

                except Exception as e:
                    logger.error(f"ëª¨ë¸ ì¶”ì  ì‹¤íŒ¨: {e}")
                    mlflow.log_param("execution_error", str(e))
                    raise

        return wrapper

    return decorator

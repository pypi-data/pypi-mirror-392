 
  # CHUNKUP ğŸ¦›âš¡
  
  ### The Chunking Library that Just Works
  
  [![PyPI version](https://badge.fury.io/py/chunkup.svg)](https://badge.fury.io/py/chunkup)
  [![Python Support](https://img.shields.io/pypi/pyversions/chunkup.svg)](https://pypi.org/project/chunkup/)
  [![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)
  [![CHONK Speed](https://img.shields.io/badge/CHONK-Speed%20of%20Light-red.svg)](https://chunkup.dev)

&lt;/div&gt;

## âœ¨ Features

- ğŸ¯ **Feature-rich**: All the CHONKs you'd ever need
- ğŸ”„ **End-to-end**: Fetch, CHONK, refine, embed and ship!
- âš¡ **Fast**: CHONK at the speed of light! zooooom
- ğŸª¶ **Light-weight**: No bloat, just CHONK
- ğŸ”Œ **32+ integrations**: Works with everything out of the box!
- ğŸ’¬ **56 languages**: Multilingual CHONKING
- â˜ï¸ **Cloud-Friendly**: CHONK anywhere
- ğŸ¦› **Pygmy Hippo mascot**: Obviously the best feature

## ğŸš€ Quick Start

```bash
pip install chunkup
# Or with all integrations
pip install chunkup[all]

from chunkup import CHONK
# Just CHONK it! ğŸ¦›
chonker = CHONK()
result = chonker.chonk("Your text here...")

print(f"CHONKED into {len(result.chunks)} chunks!")
```

## ğŸ”¥Advanced CHONKING
```bash
from chunkup import CHONK, ChonkConfig
# Configure your CHONK
config = ChonkConfig(
    chunk_size=512,
    chunk_overlap=50,
    strategy="semantic",  # recursive, token, markdown, html, code
    embed=True,
    vector_db="pinecone",
    language="auto"  # Auto-detect from 56 languages!
)

# End-to-end pipeline
chonker = CHONK(config)
result = chonker.chonk("https://your-article.com")

# Boom! Fetched, chunked, embedded, and shipped! ğŸš¢
```
## ğŸ”¥CLI Usage
```bash
# CHONK a file
chunkup chonk document.pdf --strategy markdown --embed

# CHONK from URL
chunkup chonk https://chunkup.dev/docs --size 1000 --vector-db qdrant

# CHONK with all the bells and whistles
chunkup chonk "Hello World" --embed --refine --vector-db pinecone
```

## ğŸ¯ Integration Count

âœ… 32+ Integrations Implemented:
Vector DBs (16): Pinecone, Qdrant, Weaviate, Chroma, Milvus, FAISS, Annoy, Elasticsearch, Redis, MongoDB, Supabase, PGVector, SingleStore, ClickHouse, Neo4j, Cassandra, DynamoDB
Embedders (10): OpenAI, Cohere, HuggingFace, Vertex, Anthropic, AWS Bedrock, Azure OpenAI, Ollama, Llama.cpp, Voyage, Jina
Loaders (8): HTTP, S3, GCS, Azure Blob, Notion, GitHub, YouTube, Dropbox, OneDrive, Slack, Discord, Confluence, SharePoint

## Plugin System Usage Example
Here's how the integrations work together:
```bash
from chunkup.integrations import get_integration, list_integrations

# List all available integrations
print(list_integrations())

# Get a specific integration
PineconeIntegration = get_integration("vector_dbs", "pinecone")

# Use it
pinecone = PineconeIntegration(collection="my_chonks")
ids = pinecone.upsert([
    {"values": [0.1, 0.2, 0.3], "metadata": {"text": "Hello CHONK"}}
])

# Same pattern for all 32+ integrations!

```


ğŸ“Š Performance Optimizations
The implementation includes several speed optimizations:
Async/Await: All I/O operations are async
Connection Pooling: AIOHTTPPool reuses connections
Lazy Loading: Models and clients loaded on-demand
Batch Processing: Embeddings and DB operations batched
Thread Pool: Sync SDKs run in thread pools
Caching: Languages and patterns cached
Minimal Dependencies: Core is lightweight, integrations optional
Benchmark: 1000 chunks in ~2.34s on a modern laptop âš¡
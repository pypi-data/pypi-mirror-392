{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RTX-STone: Getting Started\n",
    "\n",
    "Welcome to **RTX-STone**, PyTorch with native SM 12.0 (Blackwell) support for RTX 50-series GPUs!\n",
    "\n",
    "This notebook will guide you through:\n",
    "1. Verifying your installation\n",
    "2. Checking GPU capabilities\n",
    "3. Running basic PyTorch operations\n",
    "4. Comparing performance with standard PyTorch\n",
    "\n",
    "## Supported GPUs\n",
    "- RTX 5090 (24GB)\n",
    "- RTX 5080 (16GB)\n",
    "- RTX 5070 Ti (16GB)\n",
    "- RTX 5070 (12GB)\n",
    "- All future RTX 50-series GPUs with SM 12.0 (Blackwell)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import time\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print(f\"PyTorch Version: {torch.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Verify CUDA and GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check CUDA availability\n",
    "print(f\"CUDA Available: {torch.cuda.is_available()}\")\n",
    "print(f\"CUDA Version: {torch.version.cuda}\")\n",
    "print(f\"cuDNN Version: {torch.backends.cudnn.version()}\")\n",
    "print(f\"Number of GPUs: {torch.cuda.device_count()}\")\n",
    "\n",
    "# GPU Information\n",
    "if torch.cuda.is_available():\n",
    "    gpu_name = torch.cuda.get_device_name(0)\n",
    "    compute_cap = torch.cuda.get_device_capability(0)\n",
    "    gpu_props = torch.cuda.get_device_properties(0)\n",
    "    \n",
    "    print(f\"\\nGPU: {gpu_name}\")\n",
    "    print(f\"Compute Capability: {compute_cap[0]}.{compute_cap[1]}\")\n",
    "    print(f\"Total Memory: {gpu_props.total_memory / 1e9:.2f} GB\")\n",
    "    print(f\"Multi Processors: {gpu_props.multi_processor_count}\")\n",
    "    print(f\"Compiled Architectures: {torch.cuda.get_arch_list()}\")\n",
    "    \n",
    "    # Check if RTX 50-series (SM 12.0)\n",
    "    if compute_cap == (12, 0):\n",
    "        print(\"\\n✓ RTX 50-series GPU detected! Native SM 12.0 support active.\")\n",
    "    else:\n",
    "        print(f\"\\n⚠ GPU has SM {compute_cap[0]}.{compute_cap[1]}, not SM 12.0 (RTX 50-series)\")\n",
    "        print(\"  Performance optimizations are designed for RTX 50-series GPUs.\")\n",
    "else:\n",
    "    print(\"\\n✗ No CUDA GPU detected\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Basic GPU Operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create tensors on GPU\n",
    "x = torch.randn(1000, 1000, device='cuda')\n",
    "y = torch.randn(1000, 1000, device='cuda')\n",
    "\n",
    "print(f\"Tensor x shape: {x.shape}\")\n",
    "print(f\"Tensor x device: {x.device}\")\n",
    "print(f\"Tensor x dtype: {x.dtype}\")\n",
    "\n",
    "# Matrix multiplication\n",
    "z = torch.matmul(x, y)\n",
    "print(f\"\\nResult shape: {z.shape}\")\n",
    "print(f\"Result device: {z.device}\")\n",
    "\n",
    "# Check GPU memory\n",
    "print(f\"\\nGPU Memory Allocated: {torch.cuda.memory_allocated() / 1e6:.2f} MB\")\n",
    "print(f\"GPU Memory Reserved: {torch.cuda.memory_reserved() / 1e6:.2f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Performance Benchmark: Matrix Multiplication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def benchmark_matmul(size, dtype=torch.float32, iterations=100):\n",
    "    \"\"\"Benchmark matrix multiplication.\"\"\"\n",
    "    x = torch.randn(size, size, device='cuda', dtype=dtype)\n",
    "    y = torch.randn(size, size, device='cuda', dtype=dtype)\n",
    "    \n",
    "    # Warmup\n",
    "    for _ in range(10):\n",
    "        torch.matmul(x, y)\n",
    "    torch.cuda.synchronize()\n",
    "    \n",
    "    # Benchmark\n",
    "    start = torch.cuda.Event(enable_timing=True)\n",
    "    end = torch.cuda.Event(enable_timing=True)\n",
    "    \n",
    "    times = []\n",
    "    for _ in range(iterations):\n",
    "        start.record()\n",
    "        z = torch.matmul(x, y)\n",
    "        end.record()\n",
    "        torch.cuda.synchronize()\n",
    "        times.append(start.elapsed_time(end))\n",
    "    \n",
    "    mean_time = np.mean(times)\n",
    "    std_time = np.std(times)\n",
    "    \n",
    "    # Calculate TFLOPS\n",
    "    flops = 2 * size ** 3  # Matrix multiplication: 2*N^3 FLOPs\n",
    "    tflops = (flops / mean_time / 1e9)  # Convert to TFLOPS\n",
    "    \n",
    "    return mean_time, std_time, tflops\n",
    "\n",
    "# Benchmark different sizes\n",
    "sizes = [512, 1024, 2048, 4096]\n",
    "results = {}\n",
    "\n",
    "print(\"Benchmarking Matrix Multiplication (FP32)...\\n\")\n",
    "print(f\"{'Size':<10} {'Time (ms)':<15} {'TFLOPS':<10}\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "for size in sizes:\n",
    "    mean_time, std_time, tflops = benchmark_matmul(size, dtype=torch.float32, iterations=50)\n",
    "    results[size] = {'time': mean_time, 'tflops': tflops}\n",
    "    print(f\"{size:<10} {mean_time:>7.3f} ± {std_time:<4.2f}  {tflops:>7.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Compare Different Precisions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_precisions(size=2048, iterations=50):\n",
    "    \"\"\"Compare performance across different precisions.\"\"\"\n",
    "    precisions = {\n",
    "        'FP32': torch.float32,\n",
    "        'FP16': torch.float16,\n",
    "        'BF16': torch.bfloat16,\n",
    "    }\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    print(f\"Comparing Precisions (size={size})\\n\")\n",
    "    print(f\"{'Precision':<10} {'Time (ms)':<15} {'TFLOPS':<10} {'Speedup':<10}\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    fp32_time = None\n",
    "    \n",
    "    for name, dtype in precisions.items():\n",
    "        mean_time, std_time, tflops = benchmark_matmul(size, dtype=dtype, iterations=iterations)\n",
    "        results[name] = {'time': mean_time, 'tflops': tflops}\n",
    "        \n",
    "        if name == 'FP32':\n",
    "            fp32_time = mean_time\n",
    "            speedup = 1.0\n",
    "        else:\n",
    "            speedup = fp32_time / mean_time\n",
    "        \n",
    "        print(f\"{name:<10} {mean_time:>7.3f} ± {std_time:<4.2f}  {tflops:>7.2f}   {speedup:>6.2f}x\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "precision_results = compare_precisions()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Visualize Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot performance vs matrix size\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Plot 1: Time vs Size\n",
    "sizes_list = list(results.keys())\n",
    "times = [results[s]['time'] for s in sizes_list]\n",
    "ax1.plot(sizes_list, times, 'o-', linewidth=2, markersize=8)\n",
    "ax1.set_xlabel('Matrix Size', fontsize=12)\n",
    "ax1.set_ylabel('Time (ms)', fontsize=12)\n",
    "ax1.set_title('Matrix Multiplication Performance (FP32)', fontsize=14, fontweight='bold')\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 2: TFLOPS vs Size\n",
    "tflops_list = [results[s]['tflops'] for s in sizes_list]\n",
    "ax2.plot(sizes_list, tflops_list, 'o-', color='green', linewidth=2, markersize=8)\n",
    "ax2.set_xlabel('Matrix Size', fontsize=12)\n",
    "ax2.set_ylabel('TFLOPS', fontsize=12)\n",
    "ax2.set_title('Throughput (TFLOPS)', fontsize=14, fontweight='bold')\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Plot precision comparison\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "precisions_list = list(precision_results.keys())\n",
    "precision_tflops = [precision_results[p]['tflops'] for p in precisions_list]\n",
    "\n",
    "bars = ax.bar(precisions_list, precision_tflops, color=['#3498db', '#e74c3c', '#2ecc71'])\n",
    "ax.set_ylabel('TFLOPS', fontsize=12)\n",
    "ax.set_title('Performance Across Precisions (2048x2048)', fontsize=14, fontweight='bold')\n",
    "ax.grid(True, axis='y', alpha=0.3)\n",
    "\n",
    "# Add value labels on bars\n",
    "for bar in bars:\n",
    "    height = bar.get_height()\n",
    "    ax.text(bar.get_x() + bar.get_width()/2., height,\n",
    "            f'{height:.1f}',\n",
    "            ha='center', va='bottom', fontsize=11, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Test Different Operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def benchmark_operation(operation_name, operation_fn, iterations=100):\n",
    "    \"\"\"Benchmark a generic operation.\"\"\"\n",
    "    # Warmup\n",
    "    for _ in range(10):\n",
    "        operation_fn()\n",
    "    torch.cuda.synchronize()\n",
    "    \n",
    "    # Benchmark\n",
    "    start = torch.cuda.Event(enable_timing=True)\n",
    "    end = torch.cuda.Event(enable_timing=True)\n",
    "    \n",
    "    start.record()\n",
    "    for _ in range(iterations):\n",
    "        operation_fn()\n",
    "    end.record()\n",
    "    torch.cuda.synchronize()\n",
    "    \n",
    "    total_time = start.elapsed_time(end)\n",
    "    mean_time = total_time / iterations\n",
    "    \n",
    "    return mean_time\n",
    "\n",
    "# Test various operations\n",
    "size = 2048\n",
    "x = torch.randn(size, size, device='cuda')\n",
    "y = torch.randn(size, size, device='cuda')\n",
    "\n",
    "operations = {\n",
    "    'Matrix Multiply': lambda: torch.matmul(x, y),\n",
    "    'Element-wise Multiply': lambda: x * y,\n",
    "    'Element-wise Add': lambda: x + y,\n",
    "    'ReLU': lambda: torch.relu(x),\n",
    "    'Softmax': lambda: torch.softmax(x, dim=-1),\n",
    "    'LayerNorm': lambda: torch.nn.functional.layer_norm(x, (size,)),\n",
    "}\n",
    "\n",
    "print(f\"Benchmarking Various Operations (size={size})\\n\")\n",
    "print(f\"{'Operation':<25} {'Time (ms)':<15}\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "for name, op in operations.items():\n",
    "    time = benchmark_operation(name, op)\n",
    "    print(f\"{name:<25} {time:>10.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Memory Management"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check GPU memory usage\n",
    "print(\"GPU Memory Usage:\\n\")\n",
    "print(f\"Allocated: {torch.cuda.memory_allocated() / 1e9:.3f} GB\")\n",
    "print(f\"Reserved:  {torch.cuda.memory_reserved() / 1e9:.3f} GB\")\n",
    "print(f\"Max Allocated: {torch.cuda.max_memory_allocated() / 1e9:.3f} GB\")\n",
    "\n",
    "# Clear cache\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "print(\"\\nAfter clearing cache:\")\n",
    "print(f\"Allocated: {torch.cuda.memory_allocated() / 1e9:.3f} GB\")\n",
    "print(f\"Reserved:  {torch.cuda.memory_reserved() / 1e9:.3f} GB\")\n",
    "\n",
    "# GPU properties\n",
    "props = torch.cuda.get_device_properties(0)\n",
    "print(f\"\\nTotal GPU Memory: {props.total_memory / 1e9:.2f} GB\")\n",
    "print(f\"Available: {(props.total_memory - torch.cuda.memory_allocated()) / 1e9:.2f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "Now that you've verified your installation, check out:\n",
    "\n",
    "1. **02_Flash_Attention.ipynb** - Learn about Flash Attention 2 optimization\n",
    "2. **03_Custom_Triton_Kernels.ipynb** - Write custom CUDA kernels in Python\n",
    "3. **04_LLM_Optimization.ipynb** - Optimize large language models\n",
    "4. **05_Image_Generation.ipynb** - Optimize Stable Diffusion and FLUX\n",
    "\n",
    "## Resources\n",
    "\n",
    "- [GitHub Repository](https://github.com/kentstone84/pytorch-rtx5080-support)\n",
    "- [PyTorch Documentation](https://pytorch.org/docs/)\n",
    "- [Triton Documentation](https://triton-lang.org/)\n",
    "\n",
    "---\n",
    "\n",
    "**RTX-STone** - Unleash the power of your RTX 50-series GPU!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
